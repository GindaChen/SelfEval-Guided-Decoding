{"ans": "\nQ: Do hamsters provide food for any animals?\n\nA:\nHamsters are prey animals.\nPrey animals are food for predators.\nThus, hamsters provide food for some animals.\nSo the answer is yes.\n\n\n\n\n\nQ: Could Brooke Shields succeed at University of Pennsylvania?\n\nA:\nBrooke Shields graduated from Princeton University.\nAccording to US news, Princeton University and University of Pennsylvania are ranked as the number 1 and 6 national college, respectively.\nThis can indicate that Princeton University is about as academically rigorous as the University of Pennsylvania.\nThus, Brooke Shields could also succeed at University of Pennsylvania.\nSo the answer is yes.\n\n\n\n\n\nQ: Hydrogen’s atomic number squared exceeds number of Spice Girls?\n\nA:\nHydrogen is the first element and has an atomic number of one.\nTo square a number, you multiply it by itself, so one squared is one.\nIn comparison, the Spice Girls has five members.\nThus, Hydrogen’s atomic number squared is less than 5.\nSo the answer is no.\n\n\n\n\n\nQ: Is it common to see frost during some college commencements?\n\nA:\nCollege commencement ceremonies often happen during the months of December, May, and sometimes June.\nDecember is in the winter, so usually there can be frost.\nThus, there could be frost at some commencements and it is common.\nSo the answer is yes.\n\n\n\n\n\nQ: Could a llama birth twice during War in Vietnam (1945-46)?\n\nA:\nThe War in Vietnam (1945-46) lasted around 6 months.\nThe gestation period for a llama is 11 months, which is longer than 6 months.\nThus, it was impossible for a llama to give birth twice during the War in Vietnam.\nSo the answer is no.\n\n\n\n\n\nQ: Would a pear sink in water?\n\nA:\nObjects only sink if they are denser than the surrounding fluid.\nThe density of water is about 1 g/cm^3.\nIn comparison, the density of a raw pear is about 0.59 g/cm^3, which is less than water.\nThus, a pear would float.\nSo the answer is no.\n", "type": "commonsense", "eval": "\nQ: While viewing \"Scary Movie\" is the viewer likely to experience an increase in adrenaline?\n\nA:\nAdrenaline is a hormone that is released when the body is under stress.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (A)\nScary movies are designed to make the viewer feel stressed.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (B), because Scary Movie is a film that is a comedy take on horror, intended to make viewers laugh\nThus, the viewer is likely to experience an increase in adrenaline.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (A), but the previous step is incorrect\nSo the answer is yes.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (A), but the previous step is incorrect\n\n\n\n\n\nQ: Could a bee hummingbird balance a scale with a single pea on it?\n\nA:\nThe bee hummingbird is the smallest bird in the world.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (A)\nThe average weight of a bee hummingbird is about 1.8 grams.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (A)\nIn comparison, the average weight of a pea is about 0.1 grams.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (A)\nThus, the bee hummingbird is heavier than a pea.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (A)\nSo the answer is yes.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (B), because the bee hummingbird is multiple times heavier than a pea, so it cannot balance\n\n\n\n\n\nQ: Are the majority of Reddit users familiar with the Pledge of Allegiance?\n\nA:\nThe Pledge of Allegiance was written in 1892.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (A)\nIn comparison, Reddit was founded in 2005.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (A)\nThus, the Pledge of Allegiance was written 113 years before Reddit was founded.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (A)\nSince the majority of Reddit users were born after 2005, it is unlikely that the majority of Reddit users are familiar with the Pledge of Allegiance.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (B), because many Reddit users were born before 2005 as they need to be mature enough to use social media, and they can still be familiar with the Pledge of Allegianace as long as they're American\nSo the answer is no.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (A), but the previous step is incorrect\n\n\n\n\n\nQ: Would a broadcast from Spirit make the news in 2020?\n\nA:\nSpirit is a rover that landed on Mars in 2004.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (A)\nIt stopped working in 2010.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (A)\nThus, it is impossible for Spirit to broadcast in 2020.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (B), because Spirit still made the news with its viral \"final words\" in 2019 even it stopped working in 2010\nSo the answer is no.\n# Is the above step of reasoning:\n# (A) Correct\n# (B) Incorrect\n# The above step of reasoning is: (A), but the previous step is incorrect\n", "choice_prefix": ["# Is the above step of reasoning:", "# (A) Correct", "# (B) Incorrect", "# The above step of reasoning is:"]}
{"index": 0, "question": "Is it common to see frost during some college commencements?", "answer": "yes", "explanation": "Yes. College commencement ceremonies often happen during the months of December, May, and sometimes June.  Frost isn't uncommon to see during the month of December, as it is the winter.", "generated": [{"finished": true, "length": 4, "score": [0.973764499772003, 0.9294150005458193, 0.8079429533799414, 0.6697394402162058], "conf": [0.9548978235581833, 0.8749709227759312, 0.6626961583879296, 0.4556166286600749], "prob": [[0.8568827026453792, 22], [0.7853130591171051, 15], [0.7540923836137715, 18], [0.7516505905433154, 6]], "generated": ["College commencement ceremonies often happen during the months of December, May, and sometimes June.", "December is in the winter, so usually there can be frost.", "Thus, there could be frost at some commencements and it is common.", "So the answer is yes."], "conf_comments": ["(A)\nThe average temperature in", "(A)\nMay and June are", "(A)\nMay and June are", "(A)\nMay and June are"], "info": null}, {"finished": true, "length": 4, "score": [0.7061313184413405, 0.6401411010591495, 0.5535970652029875, 0.45888547801171276], "conf": [0.958952491092601, 0.8644654519215526, 0.6616806378150156, 0.4552126118521235], "prob": [[5.490448717440801e-05, 15], [1.3710609212877323e-05, 15], [9.034269082624497e-06, 18], [8.96652751375416e-06, 6]], "generated": ["College commencement ceremonies often happen in December and May.", "December is in the winter, so usually there can be frost.", "Thus, there could be frost at some commencements and it is common.", "So the answer is yes."], "conf_comments": ["(A)\nIn December, it", "(A)\nMay is in the", "(A)\nMay is in the", "(A)\nMay is in the"], "info": null}, {"finished": true, "length": 4, "score": [0.973764499772003, 0.9294150005458193, 0.50683036368145, 0.41704458347458745], "conf": [0.9548978235581833, 0.8749709227759312, 0.669213165064083, 0.4620501724434187], "prob": [[0.8568827026453792, 22], [0.7853130591171051, 15], [4.894897629158866e-09, 20], [4.353454082272033e-09, 6]], "generated": ["College commencement ceremonies often happen during the months of December, May, and sometimes June.", "December is in the winter, so usually there can be frost.", "Thus, there could be frost at some commencements and some people might see it.", "So the answer is yes."], "conf_comments": ["(A)\nThe average temperature in", "(A)\nMay and June are", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7061313184413405, 0.6401411010591495, 0.35017525868706506, 0.27952110891658977], "conf": [0.958952491092601, 0.8644654519215526, 0.6159220507320433, 0.4110088611497134], "prob": [[5.490448717440801e-05, 15], [1.3710609212877323e-05, 15], [7.052890098396033e-14, 22], [5.345204808259167e-14, 6]], "generated": ["College commencement ceremonies often happen in December and May.", "December is in the winter, so usually there can be frost.", "Thus, there is a possibility of frost during some of the college commencement ceremonies.", "So the answer is yes."], "conf_comments": ["(A)\nIn December, it", "(A)\nMay is in the", "(A)\nMay is in the", "(A)\nMay is in the"], "info": null}, {"finished": true, "length": 4, "score": [0.7061313184413405, 0.3079224379800319, 0.17935767504678096, 0.14496937506041277], "conf": [0.958952491092601, 0.7072743357902319, 0.4859758305986783, 0.3205605189334307], "prob": [[5.490448717440801e-05, 15], [8.11748708037358e-14, 15], [8.420113309342654e-18, 13], [7.947332401289411e-18, 6]], "generated": ["College commencement ceremonies often happen in December and May.", "However, the winter months are usually too cold for frost to form.", "Thus, there is no frost during commencements.", "So the answer is no."], "conf_comments": ["(A)\nIn December, it", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": false, "length": 4, "score": [0.973764499772003, 0.9294150005458193, 0.50683036368145, 0.11477186711794095], "conf": [0.9548978235581833, 0.8749709227759312, 0.669213165064083, 0.5180799175240911], "prob": [[0.8568827026453792, 22], [0.7853130591171051, 15], [4.894897629158866e-09, 20], [1.814055230964578e-18, 8]], "generated": ["College commencement ceremonies often happen during the months of December, May, and sometimes June.", "December is in the winter, so usually there can be frost.", "Thus, there could be frost at some commencements and some people might see it.", "This is also common across the US.", "So the answer is yes."], "conf_comments": ["(A)\nThe average temperature in", "(A)\nMay and June are", "(A)\n# Is the above", "(A)\nSo the answer is"], "info": [[[3.706012604141638e-10, 8], [0.9297611409292318, 6]], 0.7741627699067761, "(A)\nSo the answer is", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 79.01506471633911}
{"index": 1, "question": "Hydrogen's atomic number squared exceeds number of Spice Girls?", "answer": "no", "explanation": "No. Hydrogen is the first element and has an atomic number of one. To square a number, you multiply it by itself. The Spice Girls has five members.", "generated": [{"finished": true, "length": 5, "score": [0.972434436128378, 0.9309590348639201, 0.8759528417308684, 0.640251895430864, 0.4778389588379316], "conf": [0.9613515756259672, 0.8829020848822796, 0.7831611923838924, 0.42091960054712313, 0.2346999850193844], "prob": [[0.7808655399984243, 15], [0.7525919083733695, 18], [0.73678057641514, 11], [0.6613016818247422, 18], [0.6571800101100206, 6]], "generated": ["Hydrogen is the first element and has an atomic number of one.", "To square a number, you multiply it by itself, so one squared is one.", "In comparison, the Spice Girls has five members.", "Thus, Hydrogen’s atomic number squared is less than 5.", "So the answer is no."], "conf_comments": ["(A)\nThe atomic number of", "(A)\nThe atomic number of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8006931251537462, 0.5409766608891944, 0.3622256922355859, 0.22269956057853044, 0.15752872641214324], "conf": [0.9451294542940478, 0.847460183578694, 0.6709570651030142, 0.33253135420742713, 0.17734082513780114], "prob": [[0.020625715061510383, 10], [4.737843348513177e-05, 9], [8.846323441540419e-07, 7], [1.1594188934833366e-08, 16], [8.428761616426613e-09, 5]], "generated": ["Hydrogen's atomic number is 1", "The squared of 1 is 1", "Spice Girls has 5 members", "Thus, hydrogen's atomic number squared is less than 5", "So the answer is no"], "conf_comments": ["(A)\nThus, hydro", "(A)\nThe number of Sp", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8006931251537462, 0.5409766608891944, 0.42308648491712025, 0.2078052730267965, 0.14857117639012432], "conf": [0.9451294542940478, 0.847460183578694, 0.6502423266264086, 0.35182934408705735, 0.19061295714092188], "prob": [[0.020625715061510383, 10], [4.737843348513177e-05, 9], [6.158692526266338e-06, 9], [3.3686703406624785e-11, 15], [2.37614169017339e-11, 6]], "generated": ["Hydrogen's atomic number is 1", "The squared of 1 is 1", "The number of Spice Girls is 5", "Thus Hydrogen's atomic number is less than 5.", "So the answer is no."], "conf_comments": ["(A)\nThus, hydro", "(A)\nThe number of Sp", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8006931251537462, 0.5409766608891944, 0.3622256922355859, 0.22269956057853044, 0.14531345936871598], "conf": [0.9451294542940478, 0.847460183578694, 0.6709570651030142, 0.33253135420742713, 0.1810945161724287], "prob": [[0.020625715061510383, 10], [4.737843348513177e-05, 9], [8.846323441540419e-07, 7], [1.1594188934833366e-08, 16], [2.647536543200567e-09, 6]], "generated": ["Hydrogen's atomic number is 1", "The squared of 1 is 1", "Spice Girls has 5 members", "Thus, hydrogen's atomic number squared is less than 5", "So the answer is no."], "conf_comments": ["(A)\nThus, hydro", "(A)\nThe number of Sp", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8006931251537462, 0.5409766608891944, 0.42308648491712025, 0.21133461977262016], "conf": [0.9451294542940478, 0.847460183578694, 0.6502423266264086, 0.44712109610631984], "prob": [[0.020625715061510383, 10], [4.737843348513177e-05, 9], [6.158692526266338e-06, 9], [6.715619671642028e-10, 9]], "generated": ["Hydrogen's atomic number is 1", "The squared of 1 is 1", "The number of Spice Girls is 5", "Thus, the answer is 'no'."], "conf_comments": ["(A)\nThus, hydro", "(A)\nThe number of Sp", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.8087753888171986, 0.37065449949134627, 0.2680778396654052], "conf": [0.967518093789387, 0.5061913632792087, 0.2985726407505338], "prob": [[0.013488521390101519, 11], [4.31667336476001e-14, 29], [2.100086480546264e-14, 6]], "generated": ["Hydrogen's atomic number is 1.", "The atomic number squared would be 1*1 = 1, which is less than the Spice Girls total number of 5.", "So the answer is no."], "conf_comments": ["(A)\nThe number of Sp", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 78.48695611953735}
{"index": 2, "question": "Would a pear sink in water?", "answer": "no", "explanation": "No. The density of a raw pear is about 0.59 g/cm^3. The density of water is about 1 g/cm^3. Objects only sink if they are denser than the surrounding fluid.", "generated": [{"finished": true, "length": 5, "score": [0.964030978683275, 0.9269421380676782, 0.877808365332655, 0.7964190234178081, 0.6769647885910842], "conf": [0.9530269485529288, 0.8833739532544755, 0.7943905611250177, 0.6549172665400289, 0.47442654328564077], "prob": [[0.7211047940941041, 13], [0.6956206304739982, 14], [0.6422566217728363, 29], [0.6334198670328682, 9], [0.6235767422287009, 6]], "generated": ["Objects only sink if they are denser than the surrounding fluid.", "The density of water is about 1 g/cm^3.", "In comparison, the density of a raw pear is about 0.59 g/cm^3, which is less than water.", "Thus, a pear would float.", "So the answer is no."], "conf_comments": ["(A)\nThe average density of", "(A)\nThe density of a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7892972776433205, 0.7535793156963411, 0.7135226588168401, 0.6443670678971208, 0.5490364402969482], "conf": [0.9606271074848273, 0.8950851356955396, 0.811783922369386, 0.6687470086446627, 0.4883757029085567], "prob": [[0.005534985223104739, 12], [0.004070591939814843, 14], [0.00291152845666858, 29], [0.0026594301641919937, 9], [0.002567139162421309, 6]], "generated": ["Objects sink if they are denser than the surrounding fluid.", "The density of water is about 1 g/cm^3.", "In comparison, the density of a raw pear is about 0.59 g/cm^3, which is less than water.", "Thus, a pear would float.", "So the answer is no."], "conf_comments": ["(A)\nThe density of a", "(A)\nThe density of a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7892972776433205, 0.7535793156963411, 0.7135226588168401, 0.6443670678971208, 0.4092448630085392], "conf": [0.9606271074848273, 0.8950851356955396, 0.811783922369386, 0.6687470086446627, 0.49946117468178836], "prob": [[0.005534985223104739, 12], [0.004070591939814843, 14], [0.00291152845666858, 29], [0.0026594301641919937, 9], [6.600063306383316e-05, 6]], "generated": ["Objects sink if they are denser than the surrounding fluid.", "The density of water is about 1 g/cm^3.", "In comparison, the density of a raw pear is about 0.59 g/cm^3, which is less than water.", "Thus, a pear would float.", "So the answer is yes."], "conf_comments": ["(A)\nThe density of a", "(A)\nThe density of a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.964030978683275, 0.6520233716137583, 0.5377015456780551, 0.4668060982733808], "conf": [0.9530269485529288, 0.8488722287135999, 0.7093551813575888, 0.5398261453616131], "prob": [[0.7211047940941041, 13], [1.0438623946458963e-10, 34], [1.6347700262737304e-11, 9], [1.542618308538095e-11, 6]], "generated": ["Objects only sink if they are denser than the surrounding fluid.", "The density of water (d = 1 g/cm^3) and a raw pear (d = 0.59 g/cm^3).", "Thus, a pear would float.", "So the answer is no."], "conf_comments": ["(A)\nThe average density of", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.964030978683275, 0.6520233716137583, 0.37945043681820356, 0.32506372141975215], "conf": [0.9530269485529288, 0.8488722287135999, 0.710564455026179, 0.5598152866202319], "prob": [[0.7211047940941041, 13], [1.0438623946458963e-10, 34], [1.035802669564041e-21, 28], [6.766915111351101e-22, 6]], "generated": ["Objects only sink if they are denser than the surrounding fluid.", "The density of water (d = 1 g/cm^3) and a raw pear (d = 0.59 g/cm^3).", "Since 0.59 is a smaller value than 1, a raw pear would float (i.e. not sink).", "So the answer is no."], "conf_comments": ["(A)\nThe average density of", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 61.21367931365967}
{"index": 3, "question": "Do hamsters provide food for any animals?", "answer": "yes", "explanation": "Yes. Hamsters are prey animals. Prey animals provide food for predators. ", "generated": [{"finished": true, "length": 4, "score": [0.9696607159462032, 0.8890936681434194, 0.7605798322202946, 0.6316787764781778], "conf": [0.9548968011163592, 0.8071299805119609, 0.5917246154644868, 0.40844731136731177], "prob": [[0.8836195442889401, 8], [0.8419394014918848, 9], [0.8254360749046529, 11], [0.8218693291908565, 6]], "generated": ["Hamsters are prey animals.", "Prey animals are food for predators.", "Thus, hamsters provide food for some animals.", "So the answer is yes."], "conf_comments": ["(A)\nThus, ham", "(A)\nThus, ham", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 12, "score": [0.7018789065046318, 0.587518874553847, 0.5064735767615318, 0.4297336789097836, 0.37198697280285864, 0.30435830676273046, 0.24870135016539793, 0.2040055940823507, 0.1745182430151257, 0.14355983948317558, 0.12387717837340445, 0.1072554473455015], "conf": [0.8513606515981713, 0.6024465837184073, 0.4490477529886885, 0.3237089334038955, 0.24311192632152026, 0.18273357963258552, 0.12576998712710652, 0.08474519543861685, 0.06208353515418358, 0.042046404807164525, 0.03136492029341956, 0.023814576545268862], "prob": [[0.0004717927977554747, 14], [0.00038348771665270843, 21], [0.0003633111192610557, 18], [0.00035237785789433166, 23], [0.0003428228836310282, 12], [7.606864443801614e-05, 13], [4.682121517344369e-05, 16], [4.545989571585414e-05, 21], [4.4594292940605245e-05, 18], [4.373322416756608e-05, 23], [4.277872309160021e-05, 12], [4.1700636347853985e-05, 2]], "generated": ["Q: Could Brooke Shields succeed at University of Pennsylvania?", "A:\nQ: Hydrogen’s atomic number squared exceeds number of Spice Girls?", "A:\nQ: Is it common to see frost during some college commencements?", "A:\nQ: Could a llama birth twice during War in Vietnam (1945-46)?", "A:\nQ: Would a pear sink in water?", "A:\nQ: Do hamsters provide food for any animals?", "A:\nQ: Could Brooke Shields succeed at University of Pennsylvania?", "A:\nQ: Hydrogen’s atomic number squared exceeds number of Spice Girls?", "A:\nQ: Is it common to see frost during some college commencements?", "A:\nQ: Could a llama birth twice during War in Vietnam (1945-46)?", "A:\nQ: Would a pear sink in water?", "A:"], "conf_comments": ["(A)\nHamsters are", "(A)\nQ: Is the", "(A)\nQ: Is it", "(A)\nQ: Could a", "(A)\nQ: Could a", "(A)\nHamsters are", "(A)\nQ: Hydro", "(A)\nA:\nQ", "(A)\nA:\nQ", "(A)\nA:\nQ", "(A)\nHamsters are", "(A)\nHamsters are"], "info": null}, {"finished": true, "length": 6, "score": [0.7018789065046318, 0.587518874553847, 0.5064735767615318, 0.4297336789097836, 0.37198697280285864, 0.3199838191068622], "conf": [0.8513606515981713, 0.6024465837184073, 0.4490477529886885, 0.3237089334038955, 0.24311192632152026, 0.18245583036040813], "prob": [[0.0004717927977554747, 14], [0.00038348771665270843, 21], [0.0003633111192610557, 18], [0.00035237785789433166, 23], [0.0003428228836310282, 12], [0.00033324833455462615, 2]], "generated": ["Q: Could Brooke Shields succeed at University of Pennsylvania?", "A:\nQ: Hydrogen’s atomic number squared exceeds number of Spice Girls?", "A:\nQ: Is it common to see frost during some college commencements?", "A:\nQ: Could a llama birth twice during War in Vietnam (1945-46)?", "A:\nQ: Would a pear sink in water?", "A:"], "conf_comments": ["(A)\nHamsters are", "(A)\nQ: Is the", "(A)\nQ: Is it", "(A)\nQ: Could a", "(A)\nQ: Could a", "(A)\nQ: Could a"], "info": null}, {"finished": false, "length": 16, "score": [0.7018789065046318, 0.587518874553847, 0.5064735767615318, 0.4297336789097836, 0.37198697280285864, 0.30435830676273046, 0.24870135016539793, 0.2040055940823507, 0.1745182430151257, 0.14355983948317558, 0.12387717837340445, 0.10267194386038016, 0.08421469480631685, 0.06946762498692739, 0.05880141532875721, 0.04754732179158529], "conf": [0.8513606515981713, 0.6024465837184073, 0.4490477529886885, 0.3237089334038955, 0.24311192632152026, 0.18273357963258552, 0.12576998712710652, 0.08474519543861685, 0.06208353515418358, 0.042046404807164525, 0.03136492029341956, 0.022549490659238836, 0.015220570256486696, 0.01036381158718943, 0.007430745776228628, 0.004863204186077424], "prob": [[0.0004717927977554747, 14], [0.00038348771665270843, 21], [0.0003633111192610557, 18], [0.00035237785789433166, 23], [0.0003428228836310282, 12], [7.606864443801614e-05, 13], [4.682121517344369e-05, 16], [4.545989571585414e-05, 21], [4.4594292940605245e-05, 18], [4.373322416756608e-05, 23], [4.277872309160021e-05, 12], [2.3669940414663246e-05, 13], [2.246183295359124e-05, 16], [2.21389457366642e-05, 21], [2.1863347600450143e-05, 18], [2.138972553004727e-05, 23]], "generated": ["Q: Could Brooke Shields succeed at University of Pennsylvania?", "A:\nQ: Hydrogen’s atomic number squared exceeds number of Spice Girls?", "A:\nQ: Is it common to see frost during some college commencements?", "A:\nQ: Could a llama birth twice during War in Vietnam (1945-46)?", "A:\nQ: Would a pear sink in water?", "A:\nQ: Do hamsters provide food for any animals?", "A:\nQ: Could Brooke Shields succeed at University of Pennsylvania?", "A:\nQ: Hydrogen’s atomic number squared exceeds number of Spice Girls?", "A:\nQ: Is it common to see frost during some college commencements?", "A:\nQ: Could a llama birth twice during War in Vietnam (1945-46)?", "A:\nQ: Would a pear sink in water?", "A:\nQ: Do hamsters provide food for any animals?", "A:\nQ: Could Brooke Shields succeed at University of Pennsylvania?", "A:\nQ: Hydrogen’s atomic number squared exceeds number of Spice Girls?", "A:\nQ: Is it common to see frost during some college commencements?", "A:\nQ: Could a llama birth twice during War in Vietnam (1945-46)?", "A:\nQ: Would a pear sink in water?", "A:\nQ: Do hamsters provide food for"], "conf_comments": ["(A)\nHamsters are", "(A)\nQ: Is the", "(A)\nQ: Is it", "(A)\nQ: Could a", "(A)\nQ: Could a", "(A)\nHamsters are", "(A)\nQ: Hydro", "(A)\nA:\nQ", "(A)\nA:\nQ", "(A)\nA:\nQ", "(A)\nHamsters are", "(A)\nHamsters are", "(A)\nQ: Hydro", "(A)\nA:\nQ", "(A)\nQ: Could a", "(A)\nQ: Would a"], "info": [[[0.9783371659702688, 23], [0.9728540762734416, 12], [0.7277538147036331, 10]], 0.654470538022588, "(A)\nQ: Would a", 16]}, {"finished": true, "length": 4, "score": [0.9696607159462032, 0.8890936681434194, 0.5333043870817741, 0.43959690582507976], "conf": [0.9548968011163592, 0.8071299805119609, 0.5763274280791917, 0.40383870047195997], "prob": [[0.8836195442889401, 8], [0.8419394014918848, 9], [0.0004476153678726118, 11], [0.0003720717985493644, 6]], "generated": ["Hamsters are prey animals.", "Prey animals are food for predators.", "Hence, hamsters provide food for some animals.", "So the answer is yes."], "conf_comments": ["(A)\nThus, ham", "(A)\nThus, ham", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": [], "9": [], "10": [], "11": [], "12": [], "13": [], "14": [], "15": [], "16": []}, "run_time": 93.1509656906128}
{"index": 4, "question": "Are all limbs required for jujutsu?", "answer": "no", "explanation": "No. Jujutsu is a Japanese martial art that uses unarmed combat to subdue opponents. Nick Newell, a congenital amputee, got his black belt after two straight submission wins. Fighter Aaron LaPointe has succeeded in martial arts with a fully paralyzed arm.", "generated": [{"finished": true, "length": 5, "score": [0.6500525482034694, 0.3805206992149138, 0.2184244478894252, 0.16919257374772192, 0.1393097163887109], "conf": [0.9615316629791493, 0.8482902902886642, 0.6333425491551021, 0.4763765454703124, 0.32433276107597464], "prob": [[0.0006115050684350657, 9], [2.1868096968105455e-13, 23], [1.0258580260180127e-18, 15], [3.4579060948174696e-20, 15], [3.371101567409309e-20, 6]], "generated": ["Limbs are the arms and legs.", "Jujutsu is a Japanese martial art that emphasizes the use of balance rather than brute force.", "Therefore, having limbs is not necessary for jujutsu.", "Thus, not all limbs are required for jujutsu.", "So the answer is no."], "conf_comments": ["(A)\nJujutsu", "(A)\nThus, all", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.6186858551470781, 0.4487129305466876], "conf": [0.8099215737154536, 0.5181904889279938], "prob": [[1.6271589469132974e-09, 27], [3.3965075299781084e-10, 8]], "generated": ["Jujutsu is a martial art that emphasizes ground fighting, so all limbs are required for jujutsu.", "Thus, the answer is no."], "conf_comments": ["(A)\nThus, all", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6489623151479703, 0.430657349653261, 0.29749132046591925], "conf": [0.9568497618470465, 0.7098057615689315, 0.41971498644315575], "prob": [[1.691643587195175e-07, 19], [1.1421283393950622e-10, 14], [3.154528115962049e-11, 6]], "generated": ["Jujutsu is a martial art that emphasizes throwing and holding the opponent.", "Therefore, all limbs are required for jujutsu.", "So the answer is no."], "conf_comments": ["(A)\nThus, all", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6500525482034694, 0.3805206992149138, 0.2184244478894252, 0.17080406158681155], "conf": [0.9615316629791493, 0.8482902902886642, 0.6333425491551021, 0.4423835958571474], "prob": [[0.0006115050684350657, 9], [2.1868096968105455e-13, 23], [1.0258580260180127e-18, 15], [4.618367975343522e-19, 6]], "generated": ["Limbs are the arms and legs.", "Jujutsu is a Japanese martial art that emphasizes the use of balance rather than brute force.", "Therefore, having limbs is not necessary for jujutsu.", "So the answer is no."], "conf_comments": ["(A)\nJujutsu", "(A)\nThus, all", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.6186858551470781, 0.39472720624376234], "conf": [0.8099215737154536, 0.48368198558426834], "prob": [[1.6271589469132974e-09, 27], [1.6317393713312359e-10, 6]], "generated": ["Jujutsu is a martial art that emphasizes ground fighting, so all limbs are required for jujutsu.", "So the answer is no."], "conf_comments": ["(A)\nThus, all", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 48.15175461769104}
{"index": 5, "question": "Could a llama birth twice during War in Vietnam (1945-46)?", "answer": "no", "explanation": "No. The War in Vietnam (1945-46) lasted around 6 months. The gestation period for a llama is 11 months.", "generated": [{"finished": true, "length": 4, "score": [0.9794940009120402, 0.8803321649981748, 0.7926631909360615, 0.6910087259906407], "conf": [0.9645341365333397, 0.7805681280536296, 0.6333995608116836, 0.48164723027823503], "prob": [[0.8989161089652852, 20], [0.863056551697511, 22], [0.8479792534580655, 20], [0.8449197384575782, 6]], "generated": ["The War in Vietnam (1945-46) lasted around 6 months.", "The gestation period for a llama is 11 months, which is longer than 6 months.", "Thus, it was impossible for a llama to give birth twice during the War in Vietnam.", "So the answer is no."], "conf_comments": ["(A)\nThe gestation period", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 9, "score": [0.9794940009120402, 0.8803321649981748, 0.4376921137111606, 0.3773416057580638, 0.11055356753545532, 0.07765411046754464, 0.046284339613724086, 0.03066216141764769, 0.02555718247619073], "conf": [0.9645341365333397, 0.7805681280536296, 0.612875242546267, 0.45832016046880086, 0.3672583161851686, 0.33629869076306473, 0.2847214352718004, 0.22643509026816097, 0.16645544412364147], "prob": [[0.8989161089652852, 20], [0.863056551697511, 22], [8.113379723464923e-08, 14], [7.820097409338917e-08, 6], [1.5557748912900962e-17, 10], [1.728522218612509e-20, 11], [2.70162863329149e-30, 26], [7.075609843912048e-33, 10], [5.0415111319003526e-33, 6]], "generated": ["The War in Vietnam (1945-46) lasted around 6 months.", "The gestation period for a llama is 11 months, which is longer than 6 months.", "Thus, it is impossible to give birth twice during the war.", "So the answer is no.", "Q: Does the moon rotate or revolve?", "A:\nThe moon is tidally locked to Earth.", "This means that it rotates at the same rate as it revolves, so it always has the same face pointed at Earth.", "Thus, the moon doesn’t rotate.", "So the answer is no."], "conf_comments": ["(A)\nThe gestation period", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n", "(A)\nThe moon rotates", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 9, "score": [0.9794940009120402, 0.8803321649981748, 0.4376921137111606, 0.3773416057580638, 0.11055356753545532, 0.05475127303514402, 0.03390224634933389, 0.02708009882591537, 0.02047771857723676], "conf": [0.9645341365333397, 0.7805681280536296, 0.612875242546267, 0.45832016046880086, 0.3672583161851686, 0.31493165130181194, 0.25094491997984875, 0.19621670906597155, 0.1324069300007426], "prob": [[0.8989161089652852, 20], [0.863056551697511, 22], [8.113379723464923e-08, 14], [7.820097409338917e-08, 6], [1.5557748912900962e-17, 10], [2.0888714670510387e-28, 20], [1.9219120033403886e-34, 19], [3.082528782711518e-35, 9], [1.1413916015429676e-35, 6]], "generated": ["The War in Vietnam (1945-46) lasted around 6 months.", "The gestation period for a llama is 11 months, which is longer than 6 months.", "Thus, it is impossible to give birth twice during the war.", "So the answer is no.", "Q: Does the moon rotate or revolve?", "A:\nThe moon only revolves around the Earth, which is the closest celestial body.", "In contrast, the Earth rotates and revolves around the Sun, which is further away.", "Thus, the moon only revolves.", "So the answer is no."], "conf_comments": ["(A)\nThe gestation period", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n", "(A)\nThe moon rotates", "(A)\nThe moon does not", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 9, "score": [0.9794940009120402, 0.8803321649981748, 0.4376921137111606, 0.3773416057580638, 0.11055356753545532, 0.07765411046754464, 0.046284339613724086, 0.02579022928674089, 0.020425377564102665], "conf": [0.9645341365333397, 0.7805681280536296, 0.612875242546267, 0.45832016046880086, 0.3672583161851686, 0.33629869076306473, 0.2847214352718004, 0.23259513561394432, 0.17145965549854453], "prob": [[0.8989161089652852, 20], [0.863056551697511, 22], [8.113379723464923e-08, 14], [7.820097409338917e-08, 6], [1.5557748912900962e-17, 10], [1.728522218612509e-20, 11], [2.70162863329149e-30, 26], [1.3475740710634392e-36, 15], [5.114031661551035e-37, 6]], "generated": ["The War in Vietnam (1945-46) lasted around 6 months.", "The gestation period for a llama is 11 months, which is longer than 6 months.", "Thus, it is impossible to give birth twice during the war.", "So the answer is no.", "Q: Does the moon rotate or revolve?", "A:\nThe moon is tidally locked to Earth.", "This means that it rotates at the same rate as it revolves, so it always has the same face pointed at Earth.", "Thus, it doesn’t revolve and it only rotates.", "So the answer is rotate."], "conf_comments": ["(A)\nThe gestation period", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n", "(A)\nThe moon rotates", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 9, "score": [0.9794940009120402, 0.8803321649981748, 0.4376921137111606, 0.3773416057580638, 0.11055356753545532, 0.05475127303514402, 0.03390224634933389, 0.02708009882591537, 0.02012749105995134], "conf": [0.9645341365333397, 0.7805681280536296, 0.612875242546267, 0.45832016046880086, 0.3672583161851686, 0.31493165130181194, 0.25094491997984875, 0.19621670906597155, 0.13180757355238554], "prob": [[0.8989161089652852, 20], [0.863056551697511, 22], [8.113379723464923e-08, 14], [7.820097409338917e-08, 6], [1.5557748912900962e-17, 10], [2.0888714670510387e-28, 20], [1.9219120033403886e-34, 19], [3.082528782711518e-35, 9], [9.535723378395139e-36, 6]], "generated": ["The War in Vietnam (1945-46) lasted around 6 months.", "The gestation period for a llama is 11 months, which is longer than 6 months.", "Thus, it is impossible to give birth twice during the war.", "So the answer is no.", "Q: Does the moon rotate or revolve?", "A:\nThe moon only revolves around the Earth, which is the closest celestial body.", "In contrast, the Earth rotates and revolves around the Sun, which is further away.", "Thus, the moon only revolves.", "So the answer is yes."], "conf_comments": ["(A)\nThe gestation period", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n", "(A)\nThe moon rotates", "(A)\nThe moon does not", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 9, "score": [0.9794940009120402, 0.8803321649981748, 0.4376921137111606, 0.3773416057580638, 0.11055356753545532, 0.07765411046754464, 0.046284339613724086, 0.02579022928674089, 0.018730841788322336], "conf": [0.9645341365333397, 0.7805681280536296, 0.612875242546267, 0.45832016046880086, 0.3672583161851686, 0.33629869076306473, 0.2847214352718004, 0.23259513561394432, 0.16980470151541965], "prob": [[0.8989161089652852, 20], [0.863056551697511, 22], [8.113379723464923e-08, 14], [7.820097409338917e-08, 6], [1.5557748912900962e-17, 10], [1.728522218612509e-20, 11], [2.70162863329149e-30, 26], [1.3475740710634392e-36, 15], [1.9172699122848237e-37, 6]], "generated": ["The War in Vietnam (1945-46) lasted around 6 months.", "The gestation period for a llama is 11 months, which is longer than 6 months.", "Thus, it is impossible to give birth twice during the war.", "So the answer is no.", "Q: Does the moon rotate or revolve?", "A:\nThe moon is tidally locked to Earth.", "This means that it rotates at the same rate as it revolves, so it always has the same face pointed at Earth.", "Thus, it doesn’t revolve and it only rotates.", "So the answer is no."], "conf_comments": ["(A)\nThe gestation period", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n", "(A)\nThe moon rotates", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 8, "score": [0.9794940009120402, 0.8803321649981748, 0.4376921137111606, 0.3773416057580638, 0.11055356753545532, 0.06432688618441834, 0.029287225470713926, 0.017468587235116476], "conf": [0.9645341365333397, 0.7805681280536296, 0.612875242546267, 0.45832016046880086, 0.3672583161851686, 0.31864885474589416, 0.24462945975444392, 0.1411412256493964], "prob": [[0.8989161089652852, 20], [0.863056551697511, 22], [8.113379723464923e-08, 14], [7.820097409338917e-08, 6], [1.5557748912900962e-17, 10], [7.716230346268491e-32, 35], [1.0119417918784318e-55, 42], [1.8851122923575723e-58, 13]], "generated": ["The War in Vietnam (1945-46) lasted around 6 months.", "The gestation period for a llama is 11 months, which is longer than 6 months.", "Thus, it is impossible to give birth twice during the war.", "So the answer is no.", "Q: Does the moon rotate or revolve?", "A:\nThe moon revolves around the earth once every 29.5 days, which is why we see the same part of the moon’s surface every month.", "Meanwhile, the moon rotates one time for every one revolving around the Earth, so we only get to see the same spot on the moon for once in every 29.5 days.", "So the answer is that the moon rotates and revolves."], "conf_comments": ["(A)\nThe gestation period", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n", "(A)\nThe moon rotates", "(A)\nThe moon rotates", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": [], "9": []}, "run_time": 696.8339629173279}
{"index": 6, "question": "Does Linus Torvalds make money off of DirectX?", "answer": "no", "explanation": "No. DirectX is a proprietary technology owned by Microsoft. Linus Torvalds is the creator and lead developer for the open-source Linux kernel. The Linux kernel is used in operating systems that are competitors of Microsoft Windows.", "generated": [{"finished": true, "length": 4, "score": [0.6692447723956874, 0.4254185918570342, 0.36367153888647535, 0.311340756586413], "conf": [0.966752590100424, 0.819664539389955, 0.6535006652204565, 0.48082863548175775], "prob": [[0.00021105757212242502, 11], [6.582503397407522e-09, 14], [1.497544932155026e-09, 17], [1.462965444823538e-09, 6]], "generated": ["Linus Torvalds does not develop DirectX.", "DirectX is a collection of proprietary technologies developed by Microsoft.", "Thus, Linus Torvalds does not make money off of DirectX.", "So the answer is no."], "conf_comments": ["(A)\nDirectX is a", "(A)\nThus, Lin", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6692447723956874, 0.4109929101438932, 0.2702443624018988, 0.23341497389945043, 0.1968343456539899], "conf": [0.966752590100424, 0.8406784545955975, 0.6965767414925589, 0.5546884292425973, 0.39570688415659694], "prob": [[0.00021105757212242502, 11], [1.145665799681591e-07, 9], [6.296548240547111e-10, 8], [2.076780484223281e-10, 17], [2.0375562654933405e-10, 6]], "generated": ["Linus Torvalds does not develop DirectX.", "DirectX is a program developed by Microsoft.", "Linus Torvalds developed Linux.", "Thus, Linus Torvalds does not make money off of DirectX.", "So the answer is no."], "conf_comments": ["(A)\nDirectX is a", "(A)\nThus, Lin", "(A)\nThus, Lin", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6438449940152782, 0.3832087746495295, 0.3142616799166916, 0.26913375858925787], "conf": [0.9637687948913961, 0.8704414778432124, 0.6818692707334223, 0.501537425871333], "prob": [[4.00938197940458e-05, 12], [3.722074522844426e-07, 5], [2.783186305835158e-08, 17], [2.7355830148226768e-08, 6]], "generated": ["Linus Torvalds doesn't work for Microsoft.", "Microsoft developed DirectX.", "Thus, Linus Torvalds does not make money off of DirectX.", "So the answer is no."], "conf_comments": ["(A)\nDirectX is a", "(A)\nThus, Lin", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6438449940152782, 0.3832087746495295, 0.30272513522226097, 0.25663919238373806], "conf": [0.9637687948913961, 0.8704414778432124, 0.6811429990201168, 0.4909913493984786], "prob": [[4.00938197940458e-05, 12], [3.722074522844426e-07, 5], [6.336908585430989e-09, 18], [6.225269523464174e-09, 6]], "generated": ["Linus Torvalds doesn't work for Microsoft.", "Microsoft developed DirectX.", "Thus, Linus Torvalds doesn’t make money off of DirectX.", "So the answer is no."], "conf_comments": ["(A)\nDirectX is a", "(A)\nThus, Lin", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6692447723956874, 0.4109929101438932, 0.28256933580369964], "conf": [0.966752590100424, 0.8406784545955975, 0.6184500821401565], "prob": [[0.00021105757212242502, 11], [1.145665799681591e-07, 9], [1.0594926679811214e-11, 21]], "generated": ["Linus Torvalds does not develop DirectX.", "DirectX is a program developed by Microsoft.", "Thus, Linus Torvalds makes no money off of DirectX and the answer is no."], "conf_comments": ["(A)\nDirectX is a", "(A)\nThus, Lin", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 57.60579752922058}
{"index": 7, "question": "Could Brooke Shields succeed at University of Pennsylvania?", "answer": "yes", "explanation": "Yes. Brooke Shields graduated from Princeton University. Princeton is ranked as the number 1 national college by US news. University of Pennsylvania is ranked as number 6 national college by US news. Princeton only admits around 6 percent of applicants as of 2018. University of Pennsylvania accepts around 9% of applicants as of 2018.", "generated": [{"finished": true, "length": 5, "score": [0.9753059555931525, 0.9100325547780398, 0.8022656924262064, 0.6940764321426226, 0.5745154675193163], "conf": [0.9650762606991727, 0.8445382220527164, 0.6576538200462766, 0.4925549905627731, 0.33758673977276293], "prob": [[0.8407007499280815, 12], [0.7246025417099227, 29], [0.69524092758431, 21], [0.688125693648708, 16], [0.6867789446091308, 6]], "generated": ["Brooke Shields graduated from Princeton University.", "According to US news, Princeton University and University of Pennsylvania are ranked as the number 1 and 6 national college, respectively.", "This can indicate that Princeton University is about as academically rigorous as the University of Pennsylvania.", "Thus, Brooke Shields could also succeed at University of Pennsylvania.", "So the answer is yes."], "conf_comments": ["(A)\nPrinceton University", "(A)\nThus, Bro", "(A)\nThus, Bro", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.9753059555931525, 0.63406151093495, 0.5173585076955695], "conf": [0.9650762606991727, 0.6666481552660075, 0.44585262064516906], "prob": [[0.8407007499280815, 12], [0.00032432254199109674, 16], [0.0003155961749084666, 6]], "generated": ["Brooke Shields graduated from Princeton University.", "Thus, Brooke Shields could also succeed at University of Pennsylvania.", "So the answer is yes."], "conf_comments": ["(A)\nPrinceton University", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5587947243613767, 0.5118416633770345, 0.45243369139169337, 0.38726947108084986, 0.31213086723329836], "conf": [0.8961690172446786, 0.7831610371872846, 0.6214307447952768, 0.4577869455443508, 0.2981515480934414], "prob": [[1.6441800625337245e-08, 17], [5.044401353666553e-09, 29], [3.6479214509834206e-09, 21], [3.344884803890676e-09, 16], [3.2932193386175673e-09, 6]], "generated": ["Similarly, Brooke Shields also graduated from Princeton University.", "According to US news, Princeton University and University of Pennsylvania are ranked as the number 1 and 6 national college, respectively.", "This can indicate that Princeton University is about as academically rigorous as the University of Pennsylvania.", "Thus, Brooke Shields could also succeed at University of Pennsylvania.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Bro", "(A)\nThus, Bro", "(A)\nThus, Bro", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5587947243613767, 0.4911279896007863, 0.4320764032207066, 0.36878727421740043, 0.2978039752296694], "conf": [0.8961690172446786, 0.7892089470307567, 0.62820425800442, 0.46253690255281477, 0.3027511142554462], "prob": [[1.6441800625337245e-08, 17], [3.224262746509493e-10, 30], [1.7894118394621288e-10, 21], [1.5096427637155793e-10, 16], [1.4760218468848655e-10, 6]], "generated": ["Similarly, Brooke Shields also graduated from Princeton University.", "According to US news, Princeton University and University of Pennsylvania are ranked as the number 1 and 6 national colleges, respectively.", "This can indicate that Princeton University is about as academically rigorous as the University of Pennsylvania.", "Thus, Brooke Shields could also succeed at University of Pennsylvania.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Bro", "(A)\nThus, Bro", "(A)\nThus, Bro", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5587947243613767, 0.4911279896007863, 0.4320764032207066, 0.3098279161723291, 0.24916923986618952], "conf": [0.8961690172446786, 0.7892089470307567, 0.62820425800442, 0.4580090956833061, 0.29958902394958337], "prob": [[1.6441800625337245e-08, 17], [3.224262746509493e-10, 30], [1.7894118394621288e-10, 21], [4.72721489077229e-13, 17], [4.417546690992904e-13, 6]], "generated": ["Similarly, Brooke Shields also graduated from Princeton University.", "According to US news, Princeton University and University of Pennsylvania are ranked as the number 1 and 6 national colleges, respectively.", "This can indicate that Princeton University is about as academically rigorous as the University of Pennsylvania.", "Thus, Brooke Shields also could also succeed at University of Pennsylvania.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Bro", "(A)\nThus, Bro", "(A)\nThus, Bro", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 5, "score": [0.5587947243613767, 0.4911279896007863, 0.4320764032207066, 0.36878727421740043, 0.16752576201799288], "conf": [0.8961690172446786, 0.7892089470307567, 0.62820425800442, 0.46253690255281477, 0.2747967325549006], "prob": [[1.6441800625337245e-08, 17], [3.224262746509493e-10, 30], [1.7894118394621288e-10, 21], [1.5096427637155793e-10, 16], [9.206560511459517e-14, 7]], "generated": ["Similarly, Brooke Shields also graduated from Princeton University.", "According to US news, Princeton University and University of Pennsylvania are ranked as the number 1 and 6 national colleges, respectively.", "This can indicate that Princeton University is about as academically rigorous as the University of Pennsylvania.", "Thus, Brooke Shields could also succeed at University of Pennsylvania.", "So the answer is yes. ", "```", "```"], "conf_comments": ["(A)\nThus, Bro", "(A)\nThus, Bro", "(A)\nThus, Bro", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": [[[0.0006098502727095546, 7], [0.0040244019941712275, 1], [0.011296745490909268, 1]], 0.5941076939769643, "(A)\n\n\n\n\n", 5]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 74.30456638336182}
{"index": 8, "question": "Could a silverfish reach the top of the Empire State Building?", "answer": "no", "explanation": "No. Silverfish cannot fly. Animals that cannot fly can only access objects at or near ground level without mechanical assistance. The top of the Empire State Building is \t1,454 ft high.", "generated": [{"finished": true, "length": 4, "score": [0.8074838512466396, 0.4487469674004796, 0.37253851454246184, 0.3223186969449037], "conf": [0.9766566889490033, 0.7773006320156045, 0.6144154931816723, 0.460927742849575], "prob": [[0.007839875227693775, 12], [5.341130630608105e-09, 15], [5.194670732001352e-10, 17], [5.127500648332082e-10, 6]], "generated": ["The Empire State Building is 381 meters tall.", "The maximum height reached by a silverfish is around 15 cm.", "Thus, a silverfish could not reach the top of the Empire State Building.", "So the answer is no."], "conf_comments": ["(A)\nThe average speed of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7784010251769922, 0.43315955072741846, 0.3436223180003655, 0.2925978335457564], "conf": [0.9766314793499012, 0.7824663718978706, 0.5978419067224926, 0.434768186901204], "prob": [[0.0004817450689412608, 16], [7.997857491039943e-10, 14], [1.3603329574386535e-11, 21], [1.3362684475193605e-11, 6]], "generated": ["The Empire State Building has a height of 1,250 ft.", "The maximum jump height of a silverfish is about 5 inches.", "Thus, it would be impossible for a silverfish to reach the top of the Empire State Building.", "So the answer is no."], "conf_comments": ["(A)\nThe average speed of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7784010251769922, 0.43315955072741846, 0.34464930440502944, 0.29038008176500973], "conf": [0.9766314793499012, 0.7824663718978706, 0.597180493165162, 0.42508837665914573], "prob": [[0.0004817450689412608, 16], [7.997857491039943e-10, 14], [1.9026351088902007e-11, 20], [1.8714869683054894e-11, 6]], "generated": ["The Empire State Building has a height of 1,250 ft.", "The maximum jump height of a silverfish is about 5 inches.", "Thus, a silverfish would not be able to reach the top of the Empire State Building.", "So the answer is no."], "conf_comments": ["(A)\nThe average speed of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7306078134045085, 0.4391345550093982, 0.3136102067210239, 0.2717913076593911], "conf": [0.9674423943319499, 0.8511958936354747, 0.6788139245947801, 0.5116208705714831], "prob": [[5.8724358799588295e-08, 28], [1.4841716822569992e-19, 30], [3.0401765443295927e-23, 19], [2.977544341216623e-23, 6]], "generated": ["The Empire State Building in New York City is about 1,454 feet tall, which is about 443 meters.", "In comparison, silverfish are only 0.5 to 1.0 inches or 1.2 to 2.5 cm long.", "Thus, no silverfish would be able to reach the top of the Empire State Building.", "So the answer is no."], "conf_comments": ["(A)\nThe average speed of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8074838512466396, 0.4487469674004796, 0.3110743572107791, 0.26840743577339476], "conf": [0.9766566889490033, 0.7773006320156045, 0.6011594881449511, 0.4489629705277397], "prob": [[0.007839875227693775, 12], [5.341130630608105e-09, 15], [6.321239411019135e-13, 19], [6.203564558070882e-13, 6]], "generated": ["The Empire State Building is 381 meters tall.", "The maximum height reached by a silverfish is around 15 cm.", "Thus, it is not possible for a silverfish to reach the top of the building.", "So the answer is no."], "conf_comments": ["(A)\nThe average speed of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 57.449976682662964}
{"index": 9, "question": "Are the majority of Reddit users familiar with the Pledge of Allegiance?", "answer": "yes", "explanation": "Yes. 55% of the Reddit user base comes from the United States. Congressional sessions open with the recital of the Pledge, as do many government meetings at local levels, and meetings held by many private organizations. All states except California, Hawaii, Iowa, Vermont, and Wyoming require a regularly scheduled recitation of the pledge in public schools.", "generated": [{"finished": true, "length": 4, "score": [0.5573596515602824, 0.2964856644905231, 0.22032261648102486, 0.16150607163822397], "conf": [0.9134066542055549, 0.662289866964254, 0.38431056532496594, 0.20830285586942568], "prob": [[6.087821753835638e-05, 9], [1.0475087812003235e-12, 19], [3.6996887449513366e-13, 21], [3.5127324045514406e-13, 6]], "generated": ["The majority of Reddit users are males.", "According to researchers, males don’t know the Pledge of Allegiance.", "Thus, the majority of Reddit users are not familiar with the Pledge of Allegiance.", "So the answer is no."], "conf_comments": ["(A)\nThe majority of males", "(A)\nThus, the", "(A)\nSo the answer is", "(A), but the previous step is"], "info": null}, {"finished": true, "length": 6, "score": [0.6163149089509763, 0.3105890707786908, 0.18254774317305153, 0.10697729833815306, 0.0763755498690804, 0.061440474488786416], "conf": [0.9456722134241882, 0.7992909030378228, 0.6003139303536699, 0.44128343144173426, 0.31330918940561686, 0.20541886781004837], "prob": [[1.2491597221171002e-10, 25], [3.6633968407266756e-23, 24], [2.943425708945727e-31, 24], [7.808520562954815e-42, 32], [1.4137972096996848e-45, 26], [1.3073337190162266e-45, 6]], "generated": ["A 2015 poll showed that 9% of Americans did not know the Pledge of Allegiance.", "According to Reddit, the monthly active users were around 69 million in 2015.", "If we assume that 90% of Reddit Monthly Active Users knew the Pledge of Allegiance,", "   then the number of Reddit users who knew the Pledge of Allegiance was roughly 62.1 million in 2015.", "Thus, the majority of Reddit users in 2015 did not know the Pledge of Allegiance.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, a", "(A)\nThus, the", "(A)\nthen the majority of", "(A)\n   If we assume", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5573596515602824, 0.31975433701262107, 0.20776986544748577, 0.15551902526305555], "conf": [0.9134066542055549, 0.7214355447148315, 0.4576251838510449, 0.25819801570298073], "prob": [[6.087821753835638e-05, 9], [4.574516779044667e-14, 24], [8.870785046351284e-18, 21], [8.505811555003063e-18, 6]], "generated": ["The majority of Reddit users are males.", "According to Statista, only 14% of males is familiar with the Pledge of Allegiance.", "Thus, the majority of Reddit users are not aware of the Pledge of Allegiance.", "So the answer is no."], "conf_comments": ["(A)\nThe majority of males", "(A)\nThus, the", "(A)\nSo the answer is", "(A), but the previous step is"], "info": null}, {"finished": true, "length": 4, "score": [0.5573596515602824, 0.2964856644905231, 0.2007184559066316, 0.14850115191422042], "conf": [0.9134066542055549, 0.662289866964254, 0.38708147361955475, 0.21414045886494723], "prob": [[6.087821753835638e-05, 9], [1.0475087812003235e-12, 19], [4.980149305804443e-15, 22], [4.672770861383378e-15, 6]], "generated": ["The majority of Reddit users are males.", "According to researchers, males don’t know the Pledge of Allegiance.", "Thus, the majority of Reddit users would not be familiar with the Pledge of Allegiance.", "So the answer is no."], "conf_comments": ["(A)\nThe majority of males", "(A)\nThus, the", "(A)\nSo the answer is", "(A), but the previous step is"], "info": null}, {"finished": true, "length": 4, "score": [0.6163149089509763, 0.3105890707786908, 0.18736392978708444, 0.1370533182402163], "conf": [0.9456722134241882, 0.7992909030378228, 0.5353284633221841, 0.30842877257852747], "prob": [[1.2491597221171002e-10, 25], [3.6633968407266756e-23, 24], [2.956785807365459e-29, 23], [1.8969478026394907e-29, 6]], "generated": ["A 2015 poll showed that 9% of Americans did not know the Pledge of Allegiance.", "According to Reddit, the monthly active users were around 69 million in 2015.", "Thus, a significant amount of Reddit users may not be familiar with the Pledge of Allegiance.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, a", "(A)\nThus, the", "(A)\nSo the answer is", "(A), but the previous step is"], "info": null}, {"finished": true, "length": 6, "score": [0.6163149089509763, 0.3105890707786908, 0.18254774317305153, 0.10697729833815306, 0.0763755498690804, 0.023866920722697457], "conf": [0.9456722134241882, 0.7992909030378228, 0.6003139303536699, 0.44128343144173426, 0.31330918940561686, 0.20250089551961337], "prob": [[1.2491597221171002e-10, 25], [3.6633968407266756e-23, 24], [2.943425708945727e-31, 24], [7.808520562954815e-42, 32], [1.4137972096996848e-45, 26], [3.8390984382972184e-52, 8]], "generated": ["A 2015 poll showed that 9% of Americans did not know the Pledge of Allegiance.", "According to Reddit, the monthly active users were around 69 million in 2015.", "If we assume that 90% of Reddit Monthly Active Users knew the Pledge of Allegiance,", "   then the number of Reddit users who knew the Pledge of Allegiance was roughly 62.1 million in 2015.", "Thus, the majority of Reddit users in 2015 did not know the Pledge of Allegiance.", "So the most likely answer is yes."], "conf_comments": ["(A)\nIn comparison, a", "(A)\nThus, the", "(A)\nthen the majority of", "(A)\n   If we assume", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 82.88706874847412}
{"index": 10, "question": "Do the anchors on Rede Globo speak Chinese?", "answer": "no", "explanation": "No. Rede Globo is a Brazilian television network. The official language of Brazil is Portuguese.", "generated": [{"finished": true, "length": 4, "score": [0.8813921798991258, 0.5597156722906517, 0.47010416021453566, 0.4037595455904442], "conf": [0.9729635931190267, 0.8212414198025089, 0.6309809813417336, 0.46640125093164814], "prob": [[0.0840736215939015, 11], [0.00010905872118848997, 9], [2.780901973684282e-05, 16], [2.7470689316905625e-05, 6]], "generated": ["Rede Globo is a Brazilian television network.", "Its anchors do not speak Chinese.", "Thus, the anchors on Rede Globo do not speak Chinese.", "So the answer is no."], "conf_comments": ["(A)\nThe majority of the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.751165581775792, 0.5399744520693003, 0.44468829349801725], "conf": [0.9670745222192905, 0.80382800119405, 0.6177662815588266], "prob": [[0.0005298584521188837, 14], [1.0979795300867287e-06, 13], [4.038549730760674e-07, 8]], "generated": ["Rede Globo is a Brazilian Portuguese-language television network.", "None of the anchors on Rede Globo speak Chinese.", "Thus, the answer is no."], "conf_comments": ["(A)\nThe anchors on", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.751165581775792, 0.5234112802775691, 0.3279053887075861, 0.2774265612357728], "conf": [0.9670745222192905, 0.8306352423109309, 0.5976495938520995, 0.42930403415403773], "prob": [[0.0005298584521188837, 14], [5.524270497296082e-06, 8], [3.8332844752695165e-09, 12], [3.753659065911329e-09, 6]], "generated": ["Rede Globo is a Brazilian Portuguese-language television network.", "Chinese is not spoken in Brazil.", "Thus, none of the anchors would speak Chinese.", "So the answer is no."], "conf_comments": ["(A)\nThe anchors on", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8813921798991258, 0.5597156722906517, 0.3157466379043264, 0.25790311423111156], "conf": [0.9729635931190267, 0.8212414198025089, 0.507669939152878, 0.34105962679278445], "prob": [[0.0840736215939015, 11], [0.00010905872118848997, 9], [1.3658507315446458e-09, 17], [1.310159805536806e-09, 6]], "generated": ["Rede Globo is a Brazilian television network.", "Its anchors do not speak Chinese.", "Thus, the Chinese anchors on Rede Globo do not speak Chinese.", "So the answer is no."], "conf_comments": ["(A)\nThe majority of the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.751165581775792, 0.5234112802775691, 0.2815593198785604, 0.23728456300645026], "conf": [0.9670745222192905, 0.8306352423109309, 0.6296859344869475, 0.44886804750593784], "prob": [[0.0005298584521188837, 14], [5.524270497296082e-06, 8], [2.0178793381365595e-11, 13], [1.9738853933974087e-11, 6]], "generated": ["Rede Globo is a Brazilian Portuguese-language television network.", "Chinese is not spoken in Brazil.", "Thus, none of Rede Globo anchor speak Chinese.", "So the answer is no."], "conf_comments": ["(A)\nThe anchors on", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 60.63313102722168}
{"index": 11, "question": "Can Reiki be stored in a bottle?", "answer": "no", "explanation": "No. Reiki practitioners use a technique called palm healing or hands-on healing through which a \"universal energy\" is said to be transferred through the palms of the practitioner to the patient in order to encourage emotional or physical healing. Medications are typically stored in pill bottles.", "generated": [{"finished": true, "length": 4, "score": [0.5918753629144484, 0.25959865580123986, 0.20147304995685608, 0.1807556699856328], "conf": [0.955271525267725, 0.8462450697686319, 0.6939964455751405, 0.5619709728220326], "prob": [[2.9174632346168966e-07, 15], [6.803865757111e-14, 10], [4.878012546036801e-16, 16], [4.705466722964898e-16, 6]], "generated": ["Reiki is a healing technique, a form of energy healing.", "Like other energy, it is not tangible.", "Thus, it is not possible to store Reiki in a bottle.", "So the answer is no."], "conf_comments": ["(A)\nIt is not a", "(A)\nThus, it", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5918753629144484, 0.25959865580123986, 0.14426090371140896, 0.12395644813994848, 0.11010788619866459], "conf": [0.955271525267725, 0.8462450697686319, 0.6963205557935592, 0.5803026216163191, 0.45933491064419396], "prob": [[2.9174632346168966e-07, 15], [6.803865757111e-14, 10], [5.312610554872439e-19, 12], [1.1001482552207718e-19, 13], [1.0794270757218151e-19, 6]], "generated": ["Reiki is a healing technique, a form of energy healing.", "Like other energy, it is not tangible.", "You can't store something that doesn't exist.", "Thus, Reiki cannot be stored in a bottle.", "So the answer is no."], "conf_comments": ["(A)\nIt is not a", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5849104628367258, 0.2745358317539825, 0.19652713644276745, 0.1706411027541313], "conf": [0.9494017316578627, 0.7380273097579239, 0.5536408935209377, 0.41873306496201396], "prob": [[0.00010246106023152243, 9], [7.793243710228324e-12, 13], [5.496731795766828e-14, 13], [5.3924167669284195e-14, 6]], "generated": ["Reiki is a form of life energy.", "Therefore, it cannot be stored in an object or container.", "Thus, you cannot store Reiki in a bottle.", "So the answer is no."], "conf_comments": ["(A)\nIt is not physical", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6750659843239648, 0.28566898571629395, 0.16373220861400942, 0.1298508178056629], "conf": [0.9673751240371816, 0.8212884076600417, 0.6036653461029455, 0.45776947979086735], "prob": [[0.001142556833633942, 9], [6.871773006606632e-20, 24], [4.364824931453001e-24, 12], [9.775325433620102e-25, 8]], "generated": ["Reiki is an alternative healing method.", "The way that it supposedly works is that the practitioner channels the patient’s energy and heals him.", "So, Reiki cannot be stored in a bottle.", "Thus, the answer is no."], "conf_comments": ["(A)\nIt is not a", "(A)\nThus, Re", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5924835783273498, 0.280647097577892, 0.14111571585251448, 0.12164764801690106], "conf": [0.9422388857211639, 0.8537038557000746, 0.6108218444281368, 0.4589598251165043], "prob": [[5.131654426192149e-08, 17], [1.7974536574351957e-13, 9], [3.006026368471071e-20, 15], [2.813015452595269e-20, 6]], "generated": ["Reiki is a Japanese form of alternative medicine performed by \"laying on hands\".", "It is used to facilitate healing.", "Thus, reiki is not possible to store in a bottle.", "So the answer is no."], "conf_comments": ["(A)\nThus, it", "(A)\nThus, Re", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 58.85820412635803}
{"index": 12, "question": "While viewing \"Scary Movie\" is the viewer likely to experience an increase in adrenaline?", "answer": "no", "explanation": "No. Scary Movie is a film that is a comedy take on horror, intended to make viewers laugh but not afraid. Adrenaline is produced when a human is frightened or excited.", "generated": [{"finished": true, "length": 4, "score": [0.544569367019858, 0.26697822017529255, 0.18726743205679747, 0.15387013070872013], "conf": [0.8580098105571634, 0.6451745249928296, 0.4698864722831065, 0.318835106687237], "prob": [[5.91947569760727e-10, 20], [7.180728992341769e-19, 18], [1.9009377166361033e-22, 21], [1.844308381090781e-22, 6]], "generated": ["Adrenaline is produced by the adrenal glands when an animal feels scared.", "\"Scary Movie\" is a horror movie that often has unexpected jumpscares.", "Thus, the viewer of the movie is likely to experience an increase in adrenaline.", "So the answer is yes."], "conf_comments": ["(A)\nScary Movie is", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.544569367019858, 0.26697822017529255, 0.1703725075729472, 0.13972881980135002], "conf": [0.8580098105571634, 0.6451745249928296, 0.46340455132562897, 0.31327175862779033], "prob": [[5.91947569760727e-10, 20], [7.180728992341769e-19, 18], [4.7975378810148925e-24, 21], [4.654686286739778e-24, 6]], "generated": ["Adrenaline is produced by the adrenal glands when an animal feels scared.", "\"Scary Movie\" is a horror movie that often has unexpected jumpscares.", "Thus, the viewer is likely to produce adrenaline when watching \"Scary Movie\".", "So the answer is yes."], "conf_comments": ["(A)\nScary Movie is", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.597644175694267, 0.2974515054631781, 0.12239362512505265, 0.07146957311582741, 0.05775177653171546], "conf": [0.87114943654855, 0.6829238883771636, 0.4795559678475755, 0.3263934147991596, 0.21638820700286387], "prob": [[6.377986065333038e-07, 16], [1.9969107989508686e-13, 13], [1.858643175607206e-21, 13], [2.9265711733063927e-26, 16], [2.671380867010379e-26, 6]], "generated": ["Adrenaline is the chemical that induces the fight or flight response.", "When you are threatened, your body releases adrenaline.", "\"Scary Movie\" is likely meant to scare people.", "Thus, it could increase the viewer’s adrenaline.", "So the answer is yes."], "conf_comments": ["(A)\nScary movies", "(A)\nScary movies", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.544569367019858, 0.22098100278717375, 0.12372467888129762, 0.09805658983866754], "conf": [0.8580098105571634, 0.6379438941524374, 0.4131422498552925, 0.2628901440873801], "prob": [[5.91947569760727e-10, 20], [3.7215923753454e-17, 11], [8.981063712662632e-24, 21], [8.30854572778872e-24, 6]], "generated": ["Adrenaline is produced by the adrenal glands when an animal feels scared.", "\"Scary Movie\" should scare viewers.", "Thus, the audience of \"Scary Movie\" may experience an increase in adrenaline.", "So the answer is yes."], "conf_comments": ["(A)\nScary Movie is", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.597644175694267, 0.3024059678424172, 0.16373252554758286, 0.06828054280893019, 0.05216621712655693], "conf": [0.87114943654855, 0.7041955746369103, 0.4750279977118126, 0.22639059652067314, 0.1334797980387824], "prob": [[6.377986065333038e-07, 16], [2.0592765516659231e-13, 13], [6.292944599683294e-20, 18], [9.502641987967682e-32, 27], [8.945610435804403e-32, 6]], "generated": ["Adrenaline is the chemical that induces the fight or flight response.", "It is released when a person feels scared or stressed.", "The movie \"Scary Movie\" is categorized as a horror/comedy movie.", "Thus, the viewer does not feel much fear or anxiety, and will not experience an increase in adrenaline.", "So the answer is no."], "conf_comments": ["(A)\nScary movies", "(A)\nScary Movie is", "(A)\nThe viewer is", "(A)\nSo the answer is", "(A), but the previous step is"], "info": null}, {"finished": true, "length": 5, "score": [0.597644175694267, 0.3024059678424172, 0.16373252554758286, 0.06828054280893019, 0.037100628837380245], "conf": [0.87114943654855, 0.7041955746369103, 0.4750279977118126, 0.22639059652067314, 0.14130372439942843], "prob": [[6.377986065333038e-07, 16], [2.0592765516659231e-13, 13], [6.292944599683294e-20, 18], [9.502641987967682e-32, 27], [2.3813951218948015e-34, 8]], "generated": ["Adrenaline is the chemical that induces the fight or flight response.", "It is released when a person feels scared or stressed.", "The movie \"Scary Movie\" is categorized as a horror/comedy movie.", "Thus, the viewer does not feel much fear or anxiety, and will not experience an increase in adrenaline.", "Therefore, the answer is no."], "conf_comments": ["(A)\nScary movies", "(A)\nScary Movie is", "(A)\nThe viewer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 65.8814845085144}
{"index": 13, "question": "Would Marvel's Gateway be envious of the Doctor (Doctor Who)'s TARDIS machine?", "answer": "no", "explanation": "No. The Doctor (Doctor Who) used the TARDIS, a largely unreliable time traveling machine, to travel through time and space. Gateway is a Marvel comic character linked to the X-Men comics. Gateway has the power to create wormholes that allow him to travel through time and space.", "generated": [{"finished": true, "length": 4, "score": [0.7259322784820895, 0.40088185716906943, 0.2968446337699627, 0.2401243057292427], "conf": [0.9668351143594334, 0.7384935817270292, 0.48263272625774717, 0.317455310989116], "prob": [[0.001261356364127216, 11], [2.160166901484695e-14, 27], [6.45057597540383e-16, 20], [6.252972989875106e-16, 6]], "generated": ["Marvel's Gateway is a teleporter.", "He is able to teleport to any point in space-time, which is similar to the Doctor's TARDIS machine.", "Thus, Gateway would not be envious of the Doctor's TARDIS machine.", "So the answer is no."], "conf_comments": ["(A)\nThe Doctor's", "(A)\nThus, Marvel", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7259322784820895, 0.47495320602435004, 0.29137782320667055, 0.23818780721190794], "conf": [0.9668351143594334, 0.8196408116321882, 0.5506731955372811, 0.3723001504717167], "prob": [[0.001261356364127216, 11], [1.4642095912849935e-09, 20], [2.348603396066956e-16, 27], [2.189628410474992e-16, 6]], "generated": ["Marvel's Gateway is a teleporter.", "The Doctor’s TARDIS is a time machine that can also serve as a teleporter.", "Thus, it's not possible for Marvel's Gateway to be envious of the Doctor’s TARDIS.", "So the answer is no."], "conf_comments": ["(A)\nThe Doctor's", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7259322784820895, 0.47495320602435004, 0.20632032237837206, 0.14421365671706846, 0.1142658124097881], "conf": [0.9668351143594334, 0.8196408116321882, 0.5146178852676656, 0.3308514093100503, 0.2084588165399189], "prob": [[0.001261356364127216, 11], [1.4642095912849935e-09, 20], [2.3909864799957454e-16, 13], [1.2984028564045227e-18, 19], [1.2705883256014322e-18, 6]], "generated": ["Marvel's Gateway is a teleporter.", "The Doctor’s TARDIS is a time machine that can also serve as a teleporter.", "Therefore, Gateway is more capable than TARDIS.", "Thus, Gateway would not be envious of the Doctor's TARDIS.", "So the answer is no."], "conf_comments": ["(A)\nThe Doctor's", "(A)\nThus, the", "(A)\nThus, Gate", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6776159746371032, 0.4546189756152249, 0.18545629720230614, 0.13638434710597658, 0.10769419774081654], "conf": [0.9640421672939917, 0.7958339622947634, 0.5013923321246051, 0.3107218449709095, 0.1944737966616301], "prob": [[3.0917002262822e-05, 14], [5.552000235803919e-15, 37], [5.74477835655035e-26, 19], [2.5053271753520594e-27, 23], [2.4494125001381384e-27, 6]], "generated": ["Gateway is a character from the Marvel comic book universe.", "TARDIS is an acronym for \"Time And Relative Dimension In Space\" and is the title of the Doctor's time machine from the Doctor Who television series.", "Doctor Who is fiction, so Marvel's Gateway would not know TARDIS.", "Thus, Marvel's Gateway would not be envious of the Doctor's TARDIS machine.", "So the answer is no."], "conf_comments": ["(A)\nThe TARDIS", "(A)\nThus, Gate", "(A)\nThus, Marvel", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7259322784820895, 0.47495320602435004, 0.20632032237837206, 0.1324301051422982, 0.10397732205984457], "conf": [0.9668351143594334, 0.8196408116321882, 0.5146178852676656, 0.32687310295152244, 0.20233474330524218], "prob": [[0.001261356364127216, 11], [1.4642095912849935e-09, 20], [2.3909864799957454e-16, 13], [4.1541250635667905e-20, 20], [4.05279609947021e-20, 6]], "generated": ["Marvel's Gateway is a teleporter.", "The Doctor’s TARDIS is a time machine that can also serve as a teleporter.", "Therefore, Gateway is more capable than TARDIS.", "Thus, the Gateway would not be envious of the Doctor’s TARDIS.", "So the answer is no."], "conf_comments": ["(A)\nThe Doctor's", "(A)\nThus, the", "(A)\nThus, Gate", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 71.38149523735046}
{"index": 14, "question": "Is material from an aloe plant sometimes enclosed in petroleum-derived products?", "answer": "yes", "explanation": "Yes. Aloe vera gel is sometimes kept in plastic packaging. Plastic packaging is derived from petroleum.", "generated": [{"finished": true, "length": 5, "score": [0.6037678766949437, 0.2721287691267321, 0.17087725094239634, 0.1374010301805399, 0.11744069665058442], "conf": [0.9437555496040074, 0.7901648655043682, 0.6360999308399522, 0.4731556458232512, 0.34743484351430337], "prob": [[1.3418301203195986e-15, 36], [1.6280303578431693e-28, 21], [2.098406055318662e-34, 19], [9.611943013875545e-36, 22], [9.322634991451049e-36, 6]], "generated": ["Aloe vera, a type of aloe plant, can be used in cosmetic products, dietary supplements, and wound healing applications.", "These products usually contain small amounts of preservatives, emulsifiers, and stabilizing agents.", "In general, emulsifiers and stabilizing agents are petroleum-derived products.", "Thus, material from an aloe plant can be enclosed in petroleum-derived products.", "So the answer is yes."], "conf_comments": ["(A)\nPetroleum", "(A)\nThese ingred", "(A)\nThus, material", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6037678766949437, 0.2721287691267321, 0.17087725094239634, 0.12306297623380791, 0.10469088690902811], "conf": [0.9437555496040074, 0.7901648655043682, 0.6360999308399522, 0.4748200045029681, 0.34709205544268085], "prob": [[1.3418301203195986e-15, 36], [1.6280303578431693e-28, 21], [2.098406055318662e-34, 19], [2.3386020482393768e-38, 25], [2.2021145767483844e-38, 6]], "generated": ["Aloe vera, a type of aloe plant, can be used in cosmetic products, dietary supplements, and wound healing applications.", "These products usually contain small amounts of preservatives, emulsifiers, and stabilizing agents.", "In general, emulsifiers and stabilizing agents are petroleum-derived products.", "Thus, there can be material from an aloe plant that is enclosed in petroleum-derived products.", "So the answer is yes."], "conf_comments": ["(A)\nPetroleum", "(A)\nThese ingred", "(A)\nThus, material", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5772076197922952, 0.3123631903879012, 0.1690798988290228, 0.11820497123602774, 0.09785996630215743], "conf": [0.9479936858299998, 0.8145930744251341, 0.6775801525471181, 0.5045243227785702, 0.347956261094921], "prob": [[6.690460303984709e-09, 18], [1.3755107688140912e-20, 25], [5.018749796766805e-26, 12], [7.262455689514296e-30, 21], [6.99616516584743e-30, 6]], "generated": ["Aloe vera gel can be found in some products like moisturizers.", "Aloe vera gel is extracted by harvesting and crushing the inner gel of aloe vera leaves.", "Products like moisturizers often contain petroleum.", "Thus, material from an aloe plant can sometimes be in petroleum-derived products.", "So the answer is yes."], "conf_comments": ["(A)\nAloe ver", "(A)\nPetroleum", "(A)\nThus, material", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5772076197922952, 0.3123631903879012, 0.1690798988290228, 0.10718555120824304, 0.08924981252989346], "conf": [0.9479936858299998, 0.8145930744251341, 0.6775801525471181, 0.49570175543076045, 0.3551486193437942], "prob": [[6.690460303984709e-09, 18], [1.3755107688140912e-20, 25], [5.018749796766805e-26, 12], [5.207830557778303e-32, 23], [4.2773186483021977e-32, 6]], "generated": ["Aloe vera gel can be found in some products like moisturizers.", "Aloe vera gel is extracted by harvesting and crushing the inner gel of aloe vera leaves.", "Products like moisturizers often contain petroleum.", "This indicates that material from aloe vera plants could be enclosed in petroleum-derived products.", "So the answer is yes."], "conf_comments": ["(A)\nAloe ver", "(A)\nPetroleum", "(A)\nThus, material", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5469186401993956, 0.30465496989312796, 0.16115692048413569, 0.12605475806144847], "conf": [0.955818477591903, 0.8106146568281708, 0.5197030391326128, 0.32749779207458135], "prob": [[2.704272374486046e-08, 15], [2.788452756793916e-15, 16], [3.342381900488256e-23, 22], [2.799372877111848e-23, 6]], "generated": ["Aloe vera is used in many beauty products and medications.", "On the other hand, the source of petroleum is crude oil.", "Thus, material from an aloe plant may be put into some petroleum-derived products.", "So the answer is yes."], "conf_comments": ["(A)\nAloe ver", "(A)\nThus, material", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 71.18909287452698}
{"index": 15, "question": "Could a bee hummingbird balance a scale with a single pea on it?", "answer": "no", "explanation": "No. The average pea weighs between 0.1 and 0.36 grams. Female bee hummingbirds on average weigh 2.6 grams, while on average male bee hummingbirds weigh 1.95 grams.", "generated": [{"finished": true, "length": 7, "score": [0.7198402401344213, 0.42875333756151646, 0.26764717428239204, 0.17996578448310893, 0.12407679419839598, 0.09479442353307109, 0.07573451466502165], "conf": [0.9420834583796931, 0.7200756569046417, 0.516172561649202, 0.38756151806550454, 0.25398378712750025, 0.16803686078757207, 0.10772158696056079], "prob": [[3.8593011869858294e-05, 17], [8.309052075095132e-13, 23], [1.0893885241541808e-19, 26], [3.2547981483044296e-23, 16], [5.287397232004752e-26, 20], [3.3583924843391336e-27, 22], [3.272465156306081e-27, 6]], "generated": ["The mass of a bee hummingbird is about 0.6 g.", "The volume of a single pea is about 1.5 x 10^-4 m^3.", "The density of a single pea is about 2.73 x 10^3 kg/m^3.", "Thus, the mass of a single pea is about 4 g.", "The mass of a single pea is more than the mass of a bee hummingbird.", "Thus, a bee hummingbird could not balance a scale with a single pea on it.", "So the answer is no."], "conf_comments": ["(A)\nThe mass of a", "(A)\nThe density of a", "(A)\nThe mass of a", "(A)\nThe bee hum", "(A)\nSo the answer is", "(A)\n\n\n\n\n", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.7198402401344213, 0.42875333756151646, 0.26764717428239204, 0.2012154813452663, 0.12950788300290578, 0.10277372324485251], "conf": [0.9420834583796931, 0.7200756569046417, 0.516172561649202, 0.39726062937741297, 0.270457630969377, 0.17121788906787946], "prob": [[3.8593011869858294e-05, 17], [8.309052075095132e-13, 23], [1.0893885241541808e-19, 26], [7.5963515906476e-24, 31], [7.245157272604201e-27, 14], [7.020678029957106e-27, 6]], "generated": ["The mass of a bee hummingbird is about 0.6 g.", "The volume of a single pea is about 1.5 x 10^-4 m^3.", "The density of a single pea is about 2.73 x 10^3 kg/m^3.", "In comparison, the density of a bee hummingbird is about 2.73 x 10^3 kg/m^3.", "Thus, the bee hummingbird could balance the scale.", "So the answer is yes."], "conf_comments": ["(A)\nThe mass of a", "(A)\nThe density of a", "(A)\nThe mass of a", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.7198402401344213, 0.42875333756151646, 0.26764717428239204, 0.2012154813452663, 0.11391942835947608, 0.0911575683500207], "conf": [0.9420834583796931, 0.7200756569046417, 0.516172561649202, 0.39726062937741297, 0.27408545926120814, 0.17630067884831646], "prob": [[3.8593011869858294e-05, 17], [8.309052075095132e-13, 23], [1.0893885241541808e-19, 26], [7.5963515906476e-24, 31], [1.657592698239207e-28, 14], [1.6129061965295091e-28, 6]], "generated": ["The mass of a bee hummingbird is about 0.6 g.", "The volume of a single pea is about 1.5 x 10^-4 m^3.", "The density of a single pea is about 2.73 x 10^3 kg/m^3.", "In comparison, the density of a bee hummingbird is about 2.73 x 10^3 kg/m^3.", "Thus, it could balance a scale with a single pea.", "So the answer is yes."], "conf_comments": ["(A)\nThe mass of a", "(A)\nThe density of a", "(A)\nThe mass of a", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.7198402401344213, 0.42875333756151646, 0.26764717428239204, 0.17996578448310893, 0.10285104641078381, 0.06826416327716116, 0.05385591837200772], "conf": [0.9420834583796931, 0.7200756569046417, 0.516172561649202, 0.38756151806550454, 0.2509153126198491, 0.1585643575093184, 0.09949239683132623], "prob": [[3.8593011869858294e-05, 17], [8.309052075095132e-13, 23], [1.0893885241541808e-19, 26], [3.2547981483044296e-23, 16], [1.1357674539900067e-27, 15], [5.812637944655412e-31, 21], [5.538030216906006e-31, 6]], "generated": ["The mass of a bee hummingbird is about 0.6 g.", "The volume of a single pea is about 1.5 x 10^-4 m^3.", "The density of a single pea is about 2.73 x 10^3 kg/m^3.", "Thus, the mass of a single pea is about 4 g.", "Therefore, 4 g is more than 0.6 g.", "This indicates that a bee hummingbird could balance a scale with a single pea on it.", "So the answer is yes."], "conf_comments": ["(A)\nThe mass of a", "(A)\nThe density of a", "(A)\nThe mass of a", "(A)\nThe bee hum", "(A)\nSo the answer is", "(A)\n\n\n\n\n", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7198402401344213, 0.523734018991374, 0.3349991110322269, 0.1311464320233196, 0.10761006977853982], "conf": [0.9420834583796931, 0.755804504949493, 0.607572769427061, 0.44706142253221237, 0.3051334868401854], "prob": [[3.8593011869858294e-05, 17], [2.628206132414372e-07, 12], [1.8191757249746039e-13, 21], [4.753784577905718e-25, 17], [4.379864154981795e-25, 6]], "generated": ["The mass of a bee hummingbird is about 0.6 g.", "The mass of a pea is about 2 g.", "Thus, a single pea is ~3 times the mass of a bee hummingbird.", "Thus, it would be impossible for the two to be in a perfect balance.", "So the answer is no."], "conf_comments": ["(A)\nThe mass of a", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 95.08129048347473}
{"index": 16, "question": "Would a broadcast from Spirit make the news in 2020?", "answer": "yes", "explanation": "Yes. In 2010, the Spirit rover sent what is recorded as its final transmission. In 2019, another rover made the news with its viral \"final words\".", "generated": [{"finished": true, "length": 4, "score": [0.6196953366335544, 0.3093075770444307, 0.2049667005128784, 0.16474658207953632], "conf": [0.879199155328167, 0.5797347764860893, 0.3687115215838114, 0.24246937340324504], "prob": [[4.018258822234967e-06, 15], [2.876203501939333e-16, 24], [2.5249593750882347e-19, 19], [2.2700245038137866e-19, 6]], "generated": ["Spirit is a Mars rover deployed in 2004.", "In 2020, the Mars Perseverance Rover landed and started transmitting to Earth.", "Therefore, a broadcast from Spirit would not make the news in 2020.", "So the answer is no."], "conf_comments": ["(A)\nIt stopped working in", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6332327155026171, 0.30218079345829657, 0.18560436414407608, 0.15363390583534714], "conf": [0.835814593352368, 0.622203808877524, 0.44598994254517527, 0.3069406217656067], "prob": [[1.2927629747931073e-10, 31], [5.828306069189602e-23, 24], [2.9464369941361756e-28, 19], [2.8688510193129467e-28, 6]], "generated": ["Spirit was a robot rover that landed on Mars in 2004 and failed to communicate with Earth in 2010.", "A broadcast from Spirit could not make the news in 2020 because it stopped communicating with humanity.", "Thus, there wouldn’t be any broadcast from Spirit in 2020.", "So the answer is no."], "conf_comments": ["(A)\nThus, it", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7119413928390718, 0.40431409041024435, 0.22069430700764328, 0.10236403392431988, 0.0832430357933618], "conf": [0.8618662347533655, 0.7147332497103813, 0.5277595039071551, 0.330369766000241, 0.21984080779267937], "prob": [[1.21187724752153e-07, 30], [8.533669687616261e-14, 15], [1.8192370621078533e-22, 22], [5.83398574351786e-29, 14], [5.619834001328679e-29, 6]], "generated": ["The Mars Exploration Rover Spirit (MER-A) was operational from 2004 to 2010.", "Spirit began sending data back to Earth in 2004.", "In 2020, it would be 16 years since broadcasts from Spirit were sent.", "Thus, Spirit’s broadcasts would not be the news.", "So the answer is no."], "conf_comments": ["(A)\nIt stopped working in", "(A)\nSpirit stopped working", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7119413928390718, 0.40431409041024435, 0.22069430700764328, 0.10397927875996221, 0.08299856536294034], "conf": [0.8618662347533655, 0.7147332497103813, 0.5277595039071551, 0.31842492224644553, 0.20632546251216202], "prob": [[1.21187724752153e-07, 30], [8.533669687616261e-14, 15], [1.8192370621078533e-22, 22], [2.5316175507170276e-33, 25], [2.288805360127727e-33, 6]], "generated": ["The Mars Exploration Rover Spirit (MER-A) was operational from 2004 to 2010.", "Spirit began sending data back to Earth in 2004.", "In 2020, it would be 16 years since broadcasts from Spirit were sent.", "Thus, by 2020, the broadcasts would be old news and probably would not be broadcasted.", "So the answer is no."], "conf_comments": ["(A)\nIt stopped working in", "(A)\nSpirit stopped working", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6196953366335544, 0.3093075770444307, 0.14520724317448255, 0.11549685028589701], "conf": [0.879199155328167, 0.5797347764860893, 0.3508908883535901, 0.2229572705242267], "prob": [[4.018258822234967e-06, 15], [2.876203501939333e-16, 24], [5.434355521390033e-29, 29], [5.294599140693106e-29, 6]], "generated": ["Spirit is a Mars rover deployed in 2004.", "In 2020, the Mars Perseverance Rover landed and started transmitting to Earth.", "Thus, news outlets were not interested in Spirit transmissions and a broadcast from Spirit would not make news in 2020.", "So the answer is no."], "conf_comments": ["(A)\nIt stopped working in", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 5, "score": [0.7119413928390718, 0.40431409041024435, 0.22069430700764328, 0.10236403392431988, 0.02989004433195951], "conf": [0.8618662347533655, 0.7147332497103813, 0.5277595039071551, 0.330369766000241, 0.20588692511113163], "prob": [[1.21187724752153e-07, 30], [8.533669687616261e-14, 15], [1.8192370621078533e-22, 22], [5.83398574351786e-29, 14], [5.234490310975291e-35, 7]], "generated": ["The Mars Exploration Rover Spirit (MER-A) was operational from 2004 to 2010.", "Spirit began sending data back to Earth in 2004.", "In 2020, it would be 16 years since broadcasts from Spirit were sent.", "Thus, Spirit’s broadcasts would not be the news.", "The broadcasts are too old.", "So the answer is no."], "conf_comments": ["(A)\nIt stopped working in", "(A)\nSpirit stopped working", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[8.972408471843339e-07, 7], [0.9105634463242247, 6]], 0.6232014739235595, "(A)\nSo the answer is", 5]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 61.89588165283203}
{"index": 17, "question": "Would characters in Harry Potter and the Philosopher's Stone be persecuted as pagans?", "answer": "yes", "explanation": "Yes. Pagans are defined as people that hold beliefs other than those of the major world religions (Christianity, Islam, and Judaism). The characters in Harry Potter and the Philosopher's Stone practice magic. Islam explicitly forbid the practice of magic and has harsh consequences for it. Jezebel in Hebrew scripture was a worshiper of pagan Baal and was thrown from a window for her beliefs. Women accused of being witches were burned alive by Christians during the Salem Witch Trials.", "generated": [{"finished": true, "length": 4, "score": [0.7670810592455459, 0.45541306334054826, 0.33065884728712974, 0.28625454855615146], "conf": [0.9641973299596122, 0.8183524706746345, 0.653616129903418, 0.4918207128292763], "prob": [[1.617886457312103e-06, 27], [1.9326420166434216e-16, 26], [5.755577510067578e-19, 14], [5.61890346630769e-19, 6]], "generated": ["According to Wikipedia, Harry Potter and the Philosopher’s Stone is set in the 1990s.", "In the 90s, it was not possible to persecute wizards and witches as pagans.", "Thus, characters would not be persecuted as pagans.", "So the answer is no."], "conf_comments": ["(A)\nIn the 1", "(A)\nThus, characters", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5825120367757888, 0.4069950606642566, 0.32887969553569524], "conf": [0.9140980118982365, 0.6090675424715172, 0.399676746340822], "prob": [[8.897688832496146e-13, 28], [3.729680691584361e-16, 25], [3.6206469615043264e-16, 6]], "generated": ["In their time, the Characters in Harry Potter and the Philosopher’s Stone were indeed persecuted as pagans.", "Thus, characters in Harry Potter and the Philosopher's Stone would be persecuted as pagans.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7670810592455459, 0.513912746321968, 0.22489386807794207, 0.18288512681939348, 0.15155601264255014], "conf": [0.9641973299596122, 0.8127495445695646, 0.5618708239302126, 0.4228032390302137, 0.29184857130989617], "prob": [[1.617886457312103e-06, 27], [1.540522119598345e-12, 22], [1.0058718588664618e-33, 38], [3.074929563207011e-35, 27], [2.9816521180680225e-35, 6]], "generated": ["According to Wikipedia, Harry Potter and the Philosopher’s Stone is set in the 1990s.", "Harry Potter and the Philosopher’s Stone had been published in 1997.", "So by the time Harry Potter and the Philosopher’s Stone were released, it should be a bit harder to persecute someone as a pagan in the United Kingdom.", "Thus, the characters in Harry Potter and the Philosopher’s Stone would not be persecuted as pagans.", "So the answer is no."], "conf_comments": ["(A)\nIn the 1", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.596575589120844, 0.4877361128175835, 0.2451553196824732, 0.1628671859437846, 0.13295466421507537], "conf": [0.9672201425832885, 0.8691925193422221, 0.7134105327814863, 0.49608508036506166, 0.3338969603313153], "prob": [[3.796076955833195e-11, 24], [7.581368917468981e-14, 21], [1.2894398437704975e-25, 23], [3.6026590451505014e-29, 18], [3.394104305783533e-29, 6]], "generated": ["The Harry Potter series includes stories from the 1990s to the 2000s.", "Harry Potter and the Philosopher's Stone was published in 1997.", "In 1997, there were few instances of Christians persecuting pagans in the Western World.", "Thus, it is unlikely for the characters to be persecuted as pagans.", "So the answer is no."], "conf_comments": ["(A)\nIn the 1", "(A)\nThus, the", "(A)\nThus, characters", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.596575589120844, 0.4877361128175835, 0.2451553196824732, 0.11656989784605289, 0.08850422955255895, 0.07041592958949064], "conf": [0.9672201425832885, 0.8691925193422221, 0.7134105327814863, 0.5422452092866087, 0.37883859655000834, 0.24072770080077718], "prob": [[3.796076955833195e-11, 24], [7.581368917468981e-14, 21], [1.2894398437704975e-25, 23], [2.3256940384660846e-40, 28], [2.792397521366079e-42, 23], [2.7291590394582953e-42, 6]], "generated": ["The Harry Potter series includes stories from the 1990s to the 2000s.", "Harry Potter and the Philosopher's Stone was published in 1997.", "In 1997, there were few instances of Christians persecuting pagans in the Western World.", "In comparison, people persecuted pagans in Christianity's early history around 300 to 400 AD.", "Thus, characters in Harry Potter and the Philosopher's Stone would not be persecuted.", "So the answer is no."], "conf_comments": ["(A)\nIn the 1", "(A)\nThus, the", "(A)\nThus, characters", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 68.22275686264038}
{"index": 18, "question": "Can music be used as a weapon?", "answer": "yes", "explanation": "Yes. Music is an art form whose medium is sound. Music can help elevate or subdue emotions. People connect to music through the sound. The military uses loud music to cause psychological disorientation and confusion . The military calls the use of loud disorienting music part of psychological operations. ", "generated": [{"finished": true, "length": 2, "score": [0.5595792617753456, 0.4269718898489341], "conf": [0.7045054059304025, 0.4745719467286319], "prob": [[1.1742065224163807e-05, 14], [3.6559933198134983e-06, 8]], "generated": ["Music is a form of art and cannot be used as a weapon.", "Thus, the answer is no."], "conf_comments": ["(A)\nA weapon is an", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5826736315633755, 0.2333372642664322, 0.19980832125989836, 0.17241632524864717], "conf": [0.9271499546240939, 0.8149011490022097, 0.6485838499663394, 0.48388924192813004], "prob": [[7.794898727461334e-07, 14], [2.3780637938965495e-22, 21], [9.651584614277168e-23, 11], [9.538879611057485e-23, 6]], "generated": ["Music can be used as a weapon in the form of loud noise.", "Loud noise over a certain intensity threshold can cause serious harm, such as irreversible hearing loss.", "Thus, music can be used as a weapon.", "So the answer is yes."], "conf_comments": ["(A)\nLoud noise can", "(A)\nThus, music", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.40945947167347196, 0.24013189352499853, 0.20067687324001512, 0.1706321873918458], "conf": [0.9434448391147943, 0.7627203227532207, 0.571536070585175, 0.41399878596769], "prob": [[5.536338015336446e-18, 23], [1.0752588349728955e-21, 10], [4.955597153096289e-22, 11], [4.899221192987656e-22, 6]], "generated": ["If you play very loud music for long periods of time near someone, the person’s hearing is damaged.", "This can be considered as a type of weapon.", "Thus, music can be used as a weapon.", "So the answer is yes."], "conf_comments": ["(A)\nThus, music", "(A)\nThus, music", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5595792617753456, 0.22448881084506184, 0.16744893788170181, 0.13456402673133772], "conf": [0.7045054059304025, 0.46752420650873805, 0.3085124852286491, 0.1997066207366797], "prob": [[1.1742065224163807e-05, 14], [4.077869971020658e-16, 17], [6.243205890547894e-17, 11], [6.155286527173958e-17, 6]], "generated": ["Music is a form of art and cannot be used as a weapon.", "In contrast, a machine gun or a knife can be classified as weapons.", "Thus, music cannot be used as a weapon.", "So the answer is no."], "conf_comments": ["(A)\nA weapon is an", "(A)\nThus, music", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.40945947167347196, 0.24013189352499853, 0.17424263534479414], "conf": [0.9434448391147943, 0.7627203227532207, 0.605312690565172], "prob": [[5.536338015336446e-18, 23], [1.0752588349728955e-21, 10], [4.0352523773997806e-23, 8]], "generated": ["If you play very loud music for long periods of time near someone, the person’s hearing is damaged.", "This can be considered as a type of weapon.", "Thus, the answer is yes."], "conf_comments": ["(A)\nThus, music", "(A)\nThus, music", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 46.03812646865845}
{"index": 19, "question": "Can Harry Potter book a flight on Asiana Airlines?", "answer": "no", "explanation": "No. Asiana Airlines is the second largest airline in South Korea. Harry Potter is a fictional character.", "generated": [{"finished": true, "length": 4, "score": [0.8105335123476008, 0.6870330916227111, 0.46918021986568453, 0.38550564345008126], "conf": [0.9810859065951254, 0.8464401276445954, 0.6494711718745457, 0.4408871221840134], "prob": [[0.00812866034534138, 12], [0.000522253391612072, 15], [7.575533411210215e-10, 27], [7.329942301495896e-10, 6]], "generated": ["Asiana Airlines was founded in 1988.", "The first Harry Potter book was published in 1997.", "Thus, Harry Potter couldn’t book a flight on Asiana Airlines because the airline did not exist at the time.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, Harry", "(A)\nThus, Harry", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.7837043427724959, 0.5000077380658601, 0.4149327122706676], "conf": [0.9628227702939498, 0.6643556324768449, 0.4636096323009687], "prob": [[0.017491352295459287, 9], [3.763034456793563e-06, 16], [3.475670615641928e-06, 6]], "generated": ["Asiana Airlines is a Korean airline.", "Thus, Harry Potter could not book a flight on Asiana Airlines.", "So the answer is no."], "conf_comments": ["(A)\nHarry Potter", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6433110651040236, 0.35899008829188095, 0.27110834986306237, 0.22466171034676236], "conf": [0.9235564709124988, 0.7851513788515627, 0.5933632295683123, 0.40963802331314614], "prob": [[4.774746660916301e-08, 21], [5.668220158538014e-12, 9], [6.273428325209572e-14, 16], [6.07660707677557e-14, 6]], "generated": ["Harry Potter is fictional, so he can only exist in the world of Harry Potter.", "Asiana Airlines is located in South Korea.", "Thus, Harry Potter could not book a flight on Asiana Airlines.", "So the answer is no."], "conf_comments": ["(A)\nAsiana Airlines is", "(A)\nThus, Harry", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8105335123476008, 0.40952328519220504, 0.2333301961443448, 0.17654875170027243], "conf": [0.9810859065951254, 0.8353510233248438, 0.49800067191533803, 0.29325020821538705], "prob": [[0.00812866034534138, 12], [4.925729634775496e-19, 31], [5.40539137064464e-23, 15], [4.565635142001882e-23, 6]], "generated": ["Asiana Airlines was founded in 1988.", "One of the main characters in Harry Potter’s series, named Remus J. Lupin, is born in 1960.", "Thus, Lupin cannot book a flight on Asiana Airlines.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, Harry", "(A)\nThus, Rem", "(A)\nSince Lupin", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8105335123476008, 0.6870330916227111, 0.2202996777425033, 0.12567873884003603, 0.10096550354871482], "conf": [0.9810859065951254, 0.8464401276445954, 0.6199910055197004, 0.43785906381784084, 0.2834083102014617], "prob": [[0.00812866034534138, 12], [0.000522253391612072, 15], [8.455324425416803e-17, 15], [1.6122352469456999e-22, 17], [1.584504262348755e-22, 6]], "generated": ["Asiana Airlines was founded in 1988.", "The first Harry Potter book was published in 1997.", "Harry Potter cannot book a flight before time-travelling.", "Thus, Harry Potter could not book a a flight on Asiana Airlines.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, Harry", "(A)\nThus, Harry", "(A)\nThus, Harry", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 53.86209154129028}
{"index": 20, "question": "Karachi was a part of Alexander the Great's success?", "answer": "yes", "explanation": "Yes. Karachi is a city in modern day Pakistan. Krokola was an ancient port located in what is now Karachi. Alexander the Great stationed his fleet in Krokola on his way to Babylon. Alexander the Great defeated Darius and conquered Babylon before expanding his empire.", "generated": [{"finished": true, "length": 5, "score": [0.8049519982093153, 0.6316142697891172, 0.4210812716964655, 0.2804346345092027, 0.22044176873955326], "conf": [0.9304357324353123, 0.8454161855737239, 0.5929788412082352, 0.41729805261959446, 0.26536422723166586], "prob": [[0.055313186674283976, 8], [0.00035120855326039495, 13], [1.4718963887498068e-06, 12], [5.751618857133707e-10, 17], [4.841287104749657e-10, 6]], "generated": ["Karachi is a city in Pakistan.", "Alexander the Great died in 323 BCE.", "Karachi was founded in 1799 CE.", "Hence, Karachi was not a part of Alexander the Great's success.", "So the answer is no."], "conf_comments": ["(A)\nAlexander the Great", "(A)\nKarachi was founded", "(A)\nThus, Alexander", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8049519982093153, 0.6316142697891172, 0.4210812716964655, 0.2584551398403058, 0.20480149276013965], "conf": [0.9304357324353123, 0.8454161855737239, 0.5929788412082352, 0.41125078695077544, 0.2664314804301819], "prob": [[0.055313186674283976, 8], [0.00035120855326039495, 13], [1.4718963887498068e-06, 12], [2.496630884304985e-11, 18], [2.0694593350520544e-11, 6]], "generated": ["Karachi is a city in Pakistan.", "Alexander the Great died in 323 BCE.", "Karachi was founded in 1799 CE.", "Therefore, Karachi could not have been part of Alexander the Great’s success.", "So the answer is no."], "conf_comments": ["(A)\nAlexander the Great", "(A)\nKarachi was founded", "(A)\nThus, Alexander", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8049519982093153, 0.6316142697891172, 0.4210812716964655, 0.2584551398403058, 0.18758495279307436], "conf": [0.9304357324353123, 0.8454161855737239, 0.5929788412082352, 0.41125078695077544, 0.2915164719202605], "prob": [[0.055313186674283976, 8], [0.00035120855326039495, 13], [1.4718963887498068e-06, 12], [2.496630884304985e-11, 18], [2.322248444768504e-12, 8]], "generated": ["Karachi is a city in Pakistan.", "Alexander the Great died in 323 BCE.", "Karachi was founded in 1799 CE.", "Therefore, Karachi could not have been part of Alexander the Great’s success.", "Thus, the answer is no."], "conf_comments": ["(A)\nAlexander the Great", "(A)\nKarachi was founded", "(A)\nThus, Alexander", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8049519982093153, 0.5108746553959472, 0.3328713101724517, 0.25371751185591845], "conf": [0.9304357324353123, 0.6757794431865847, 0.4111004669014959, 0.24025857463092698], "prob": [[0.055313186674283976, 8], [7.55547207017044e-07, 19], [2.391982259149815e-09, 16], [2.308135693878146e-09, 6]], "generated": ["Karachi is a city in Pakistan.", "Alexander the Great was a king of Macedonia, which is a country in Europe.", "Thus, Karachi was never part of Alexander the Great’s success.", "So the answer is no."], "conf_comments": ["(A)\nAlexander the Great", "(A)\nThus, Kar", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8049519982093153, 0.6316142697891172, 0.2802862313176163, 0.2195291223536121, 0.17646772576804062], "conf": [0.9304357324353123, 0.8454161855737239, 0.5167190107911327, 0.36188252128864645, 0.23448469240815964], "prob": [[0.055313186674283976, 8], [0.00035120855326039495, 13], [5.499760781498517e-16, 24], [5.784858936739673e-17, 17], [5.68969793473554e-17, 6]], "generated": ["Karachi is a city in Pakistan.", "Alexander the Great died in 323 BCE.", "That is over two thousand years before the city of Karachi was even founded, in 1729 CE.", "Thus, Karachi was not a part of Alexander the Great's success.", "So the answer is no."], "conf_comments": ["(A)\nAlexander the Great", "(A)\nKarachi was founded", "(A)\nThus, Alexander", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6937593827524868, 0.42262031240469694, 0.2996024764288419, 0.22924319683443473], "conf": [0.9388888950617785, 0.7433157651925381, 0.4714322212398453, 0.27746709603618097], "prob": [[8.654650230349496e-05, 14], [1.0324733759921999e-10, 18], [1.97664802493899e-12, 17], [1.915087716476397e-12, 6]], "generated": ["Karachi is the capital of the Sindh province in Pakistan.", "Alexander the Great lived in Ancient Greece, which is now a part of Europe.", "Thus, Karachi was never a part of Alexander the Great’s success.", "So the answer is no."], "conf_comments": ["(A)\nAlexander the Great", "(A)\nThus, Kar", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 62.07722449302673}
{"index": 21, "question": "Was Jackson Pollock trained by Leonardo da Vinci?", "answer": "no", "explanation": "No. Leonardo lived during the Italian Renaissance in the 17th century. Jackson Pollock lived during the 20th century.", "generated": [{"finished": true, "length": 4, "score": [0.8477967334968948, 0.7429178778416989, 0.585354331861575, 0.4733253710117347], "conf": [0.9776485633397849, 0.8983817909836866, 0.6561192654946604, 0.43067333415125375], "prob": [[0.002894766362621469, 19], [4.6566409531298875e-05, 23], [3.4595533649197208e-06, 16], [3.3800276040339245e-06, 6]], "generated": ["Jackson Pollock was alive from 1912 to 1956.", "In comparison, Leonardo da Vinci was alive from 1452 to 1519.", "Thus, Jackson Pollock was not trained by Leonardo da Vinci.", "So the answer is no."], "conf_comments": ["(A)\nLeonardo da", "(A)\nThus, Jackson", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8477967334968948, 0.7873675939308802, 0.43536093644548035, 0.3670607793495509, 0.3022077488838058], "conf": [0.9776485633397849, 0.8711392997140275, 0.6807828240215106, 0.5179452467372443, 0.35172893530883687], "prob": [[0.002894766362621469, 19], [0.0014615556872884204, 21], [1.4462141674901862e-14, 27], [4.878217778347038e-15, 16], [4.825330590490756e-15, 6]], "generated": ["Jackson Pollock was alive from 1912 to 1956.", "Leonardo da Vinci was alive from 1452 to 1519.", "They could not be trained by each other because Leonardo da Vinci died 12 years before the birth of Jackson Pollock.", "Thus, Jackson Pollock was not trained by Leonardo da Vinci.", "So the answer is no."], "conf_comments": ["(A)\nLeonardo da", "(A)\nThus, Jackson", "(A)\nThus, Jackson", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8477967334968948, 0.7873675939308802, 0.43536093644548035, 0.3388175174582018, 0.2768694082585617], "conf": [0.9776485633397849, 0.8711392997140275, 0.6807828240215106, 0.5107651467561749, 0.3417527223186966], "prob": [[0.002894766362621469, 19], [0.0014615556872884204, 21], [1.4462141674901862e-14, 27], [4.704946599558596e-16, 16], [4.648573767226694e-16, 6]], "generated": ["Jackson Pollock was alive from 1912 to 1956.", "Leonardo da Vinci was alive from 1452 to 1519.", "They could not be trained by each other because Leonardo da Vinci died 12 years before the birth of Jackson Pollock.", "Thus, Jackson Pollock was never trained by Leonardo da Vinci.", "So the answer is no."], "conf_comments": ["(A)\nLeonardo da", "(A)\nThus, Jackson", "(A)\nThus, Jackson", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8477967334968948, 0.7429178778416989, 0.585354331861575, 0.3351114062035565], "conf": [0.9776485633397849, 0.8983817909836866, 0.6561192654946604, 0.45123897101193383], "prob": [[0.002894766362621469, 19], [4.6566409531298875e-05, 23], [3.4595533649197208e-06, 16], [9.203553248735644e-09, 8]], "generated": ["Jackson Pollock was alive from 1912 to 1956.", "In comparison, Leonardo da Vinci was alive from 1452 to 1519.", "Thus, Jackson Pollock was not trained by Leonardo da Vinci.", "Thus, the answer is no."], "conf_comments": ["(A)\nLeonardo da", "(A)\nThus, Jackson", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.7750028181993742, 0.6600827934895904, 0.41266211179601203, 0.35060931739717116, 0.29734592066042725, 0.12526810340451813], "conf": [0.9738425514634882, 0.8472110293752588, 0.6194972948713495, 0.4778996046554457, 0.3445009203882501, 0.24798575498103953], "prob": [[5.6616812715260836e-06, 25], [4.190881599845928e-08, 27], [6.620934213493632e-15, 25], [2.2881610144529324e-15, 16], [2.2574943288433125e-15, 6], [8.905555513650259e-46, 50]], "generated": ["Jackson Pollock was an abstract expressionist painter who was alive from 1912-1956.", "Leonardo da Vinci was an Italian Renaissance polymath who was alive from 1452-1519.", "Since Jackson Pollock was before Leonardo da Vinci, Leonardo da Vinci could not have trained Jackson Pollock.", "Thus, Jackson Pollock was not trained by Leonardo da Vinci.", "So the answer is no.", "For more discussion on the topic of Jackson Pollock, see this article: <https://www.forbes.com/sites/adriennegreene/2022/08/21/was-jackson-"], "conf_comments": ["(A)\nLeonardo da", "(A)\nThus, Jackson", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n# Is the above", "(A)\npollock-tra"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 272.24807047843933}
{"index": 22, "question": "Will The Exorcist stimulate limbic system?", "answer": "yes", "explanation": "Yes. The limbic system of the brain contains regions that detect fear, control bodily functions and perceive sensory information. The Exorcist has been called one of the scariest movies of all time. The Exorcist ranked number 3 on the American Film Institute's 100 Years/100 Thrills list.", "generated": [{"finished": true, "length": 3, "score": [0.8199885571222498, 0.41093549717838773, 0.31636528833077], "conf": [0.96364090057249, 0.7305616093298346, 0.48808916849588874], "prob": [[0.027352913329803583, 10], [4.8310070175960634e-20, 37], [2.3548635723379514e-20, 6]], "generated": ["The Exorcist is a horror movie.", "Thus, it can have the potential to stimulate the limbic system, which plays a large role in emotions, emotional behaviors, and survival instincts.", "So the answer is yes."], "conf_comments": ["(A)\nThe limbic", "(A)\nHowever, it is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7769302494780952, 0.37846566799267806, 0.23599676048111695, 0.18343681997934824], "conf": [0.9750620524773468, 0.782896832836268, 0.4680716987974181, 0.288246166461223], "prob": [[0.0012140743842943436, 14], [3.137458593257378e-14, 20], [5.7491436131749506e-18, 20], [5.126982944699466e-18, 6]], "generated": ["The Exorcist was released in 1973.", "During the same year, a neuroscientist discovered the existence of limbic system.", "Therefore, it was impossible for The Exorcist to stimulate limbic system.", "So the answer is no."], "conf_comments": ["(A)\nThe limbic", "(A)\nThus, The", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5904828999435896, 0.30077683515927417, 0.25475663696862105], "conf": [0.9614660898235544, 0.7924894990034994, 0.5855098623210361], "prob": [[3.5196351706295854e-12, 26], [3.1251913388546645e-25, 26], [2.619424490355205e-25, 6]], "generated": ["The limbic system is a network of structures in the brain involved in emotions, pleasure, memory, and motivation.", "Thus, movies like The Exorcist which can induce fear or horror may trigger the limbic system.", "So the answer is yes."], "conf_comments": ["(A)\nThe Exorc", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7769302494780952, 0.37846566799267806, 0.21042085476124875, 0.1573360321168467], "conf": [0.9750620524773468, 0.782896832836268, 0.412083195921633, 0.23618857472156757], "prob": [[0.0012140743842943436, 14], [3.137458593257378e-14, 20], [1.2723835960583845e-18, 19], [1.0960923209463026e-18, 6]], "generated": ["The Exorcist was released in 1973.", "During the same year, a neuroscientist discovered the existence of limbic system.", "Thus, there is no correlation between The Exorcist and limbic system.", "So the answer is no."], "conf_comments": ["(A)\nThe limbic", "(A)\nThus, The", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7769302494780952, 0.37846566799267806, 0.23599676048111695, 0.15333973842110368], "conf": [0.9750620524773468, 0.782896832836268, 0.4680716987974181, 0.29600367156947627], "prob": [[0.0012140743842943436, 14], [3.137458593257378e-14, 20], [5.7491436131749506e-18, 20], [2.268319692412027e-19, 8]], "generated": ["The Exorcist was released in 1973.", "During the same year, a neuroscientist discovered the existence of limbic system.", "Therefore, it was impossible for The Exorcist to stimulate limbic system.", "Thus, the answer is no."], "conf_comments": ["(A)\nThe limbic", "(A)\nThus, The", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8199885571222498, 0.41093549717838773, 0.1788018341372471, 0.09459238675018468], "conf": [0.96364090057249, 0.7305616093298346, 0.5029075776322268, 0.2805324048375365], "prob": [[0.027352913329803583, 10], [4.8310070175960634e-20, 37], [1.4231399249107367e-29, 17], [2.270330284412658e-31, 6]], "generated": ["The Exorcist is a horror movie.", "Thus, it can have the potential to stimulate the limbic system, which plays a large role in emotions, emotional behaviors, and survival instincts.", "Thus, the potential to stimulate the limbic system could be met.", "So the answer is no."], "conf_comments": ["(A)\nThe limbic", "(A)\nHowever, it is", "(A)\nSo the answer is", "(A), but the previous step is"], "info": null}, {"finished": true, "length": 4, "score": [0.8199885571222498, 0.41093549717838773, 0.1788018341372471, 0.07261476552358452], "conf": [0.96364090057249, 0.7305616093298346, 0.5029075776322268, 0.3298442618530547], "prob": [[0.027352913329803583, 10], [4.8310070175960634e-20, 37], [1.4231399249107367e-29, 17], [9.151573339373983e-40, 17]], "generated": ["The Exorcist is a horror movie.", "Thus, it can have the potential to stimulate the limbic system, which plays a large role in emotions, emotional behaviors, and survival instincts.", "Thus, the potential to stimulate the limbic system could be met.", "Thus, the answer is yes, but technically it depends on each individual."], "conf_comments": ["(A)\nThe limbic", "(A)\nHowever, it is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 49.85183024406433}
{"index": 23, "question": "Would World War II have been the same without Alan Turing?", "answer": "no", "explanation": "No. During WW2, the German Military used something called the Engima device to send messages secretly.  Alan Turing broke the Enigma code, allowing German messages to be understood and intercepted.", "generated": [{"finished": true, "length": 4, "score": [0.7477084335453537, 0.4683502760115551, 0.24472569882170844, 0.18312910612930478], "conf": [0.970186738128046, 0.70538372119525, 0.4030694196589883, 0.23078721683812461], "prob": [[0.00014782489959590015, 16], [4.66381048907619e-12, 28], [4.470131557242689e-20, 25], [3.9107718916903554e-20, 6]], "generated": ["Alan Turing died on June 7, 1954.", "The end of World War II was on August 14, 1945, which was more than 9 years before.", "Thus, Alan Turing was not alive during World War II; thus, World War II would have been the same.", "So the answer is no."], "conf_comments": ["(A)\nWorld War II ended", "(A)\nThus, Alan", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5017311881112491, 0.3161141929439045, 0.24301215425747966], "conf": [0.9372295038545944, 0.6827858903073224, 0.44695955923146263], "prob": [[1.0344150813763399e-16, 28], [1.8544904146631414e-21, 18], [1.0039924360493358e-21, 6]], "generated": ["Alan Turing was a key player in the Allied victory because he was able to cracked the German's Enigma code.", "Thus, without Alan Turing, the outcome of World War II may be different.", "So the answer is yes."], "conf_comments": ["(A)\nThus, World", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7477084335453537, 0.4683502760115551, 0.285199319624576, 0.1344752413841214], "conf": [0.970186738128046, 0.70538372119525, 0.5340100472630831, 0.33366538709747656], "prob": [[0.00014782489959590015, 16], [4.66381048907619e-12, 28], [1.0452305672007241e-16, 15], [3.108174914740932e-25, 19]], "generated": ["Alan Turing died on June 7, 1954.", "The end of World War II was on August 14, 1945, which was more than 9 years before.", "Therefore, World War II had already ended when Alan Turing died.", "So the answer is yes, since Alan Turing died before the end of World War II."], "conf_comments": ["(A)\nWorld War II ended", "(A)\nThus, Alan", "(A)\nThus, World", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5017311881112491, 0.2780805753581011, 0.2052932210471391], "conf": [0.9372295038545944, 0.6315171193208629, 0.39725356548671403], "prob": [[1.0344150813763399e-16, 28], [7.09175074449696e-24, 21], [2.9998868482721055e-24, 6]], "generated": ["Alan Turing was a key player in the Allied victory because he was able to cracked the German's Enigma code.", "Thus, without Alan Turing, World War II would not have ended the way we know it.", "So the answer is yes."], "conf_comments": ["(A)\nThus, World", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5017311881112491, 0.2780805753581011, 0.2030818793843389], "conf": [0.9372295038545944, 0.6315171193208629, 0.38284039247866414], "prob": [[1.0344150813763399e-16, 28], [7.09175074449696e-24, 21], [3.288243425283176e-24, 6]], "generated": ["Alan Turing was a key player in the Allied victory because he was able to cracked the German's Enigma code.", "Thus, without Alan Turing, World War II would not have ended the way we know it.", "So the answer is no."], "conf_comments": ["(A)\nThus, World", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": false, "length": 4, "score": [0.7477084335453537, 0.4683502760115551, 0.285199319624576, 0.08643359878517337], "conf": [0.970186738128046, 0.70538372119525, 0.5340100472630831, 0.3301812189190462], "prob": [[0.00014782489959590015, 16], [4.66381048907619e-12, 28], [1.0452305672007241e-16, 15], [9.378073149213015e-36, 23]], "generated": ["Alan Turing died on June 7, 1954.", "The end of World War II was on August 14, 1945, which was more than 9 years before.", "Therefore, World War II had already ended when Alan Turing died.", "Thus, since Alan Turing dies so late, this could not have made a big impact on the war.", "So the answer is no."], "conf_comments": ["(A)\nWorld War II ended", "(A)\nThus, Alan", "(A)\nThus, World", "(A)\nSo the answer is"], "info": [[[8.972253054490002e-20, 23], [0.7910948878255053, 6]], 0.6183052558866566, "(A)\nSo the answer is", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 46.4671425819397}
{"index": 24, "question": "Would a sophist use an épée?", "answer": "no", "explanation": "No. A sophist is a specific kind of teacher in ancient Greece, in the fifth and fourth centuries BC. Sophists specialized in using the tools of philosophy and rhetoric, though other sophists taught subjects such as music, athletics and mathematics. An épée is a sword used in fencing. The épée was not developed until the 19th century.", "generated": [{"finished": true, "length": 4, "score": [0.8578788325081274, 0.3725897688210923, 0.28397771266867444, 0.24025029245471935], "conf": [0.9680638549326227, 0.7539525371009979, 0.5546815179589414, 0.40581086178428644], "prob": [[0.08482641106979069, 9], [5.813528527293533e-16, 23], [2.1288127172431035e-17, 14], [1.866438329965465e-17, 6]], "generated": ["An épée is a type of sword.", "A sophist is someone who is a skilled arguer, someone who speaks with skill and wisdom.", "Thus, a sophist would not use an épée.", "So the answer is no."], "conf_comments": ["(A)\nA sophist", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7529852115426964, 0.3724167125817948, 0.26656498823289393, 0.2207444295791185], "conf": [0.964561858289022, 0.8355178655827361, 0.5527292998341199, 0.3843301941841714], "prob": [[0.00837886670906152, 9], [6.086426064318699e-10, 13], [1.6991759525420425e-11, 14], [1.5636012117607236e-11, 6]], "generated": ["An épée is a fencing sword.", "Fencers use épées to defend themselves from opponents.", "Thus, a sophist would not use an épée.", "So the answer is no."], "conf_comments": ["(A)\nA sophist", "(A)\nSophists are", "(A)\nA sophist", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8578788325081274, 0.4699320144268671, 0.25338293718901506, 0.20795505637583409], "conf": [0.9680638549326227, 0.8129582434548376, 0.5677839257619629, 0.40542234935475013], "prob": [[0.08482641106979069, 9], [1.1835941924207867e-15, 31], [5.004246683770248e-24, 22], [3.526115837343777e-24, 6]], "generated": ["An épée is a type of sword.", "The sophists were philosopher-teachers of ancient Greece during the classical period (5th century BC to 4th century BC).", "Thus, it would be rare for a sophist to have the opportunity to use an épée.", "So the answer is no."], "conf_comments": ["(A)\nA sophist", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7529852115426964, 0.3724167125817948, 0.20232196993232426, 0.16382648564140243], "conf": [0.964561858289022, 0.8355178655827361, 0.5242414910135159, 0.37531350227837607], "prob": [[0.00837886670906152, 9], [6.086426064318699e-10, 13], [1.5182232137814782e-13, 11], [7.514331648533071e-14, 8]], "generated": ["An épée is a fencing sword.", "Fencers use épées to defend themselves from opponents.", "So a sophist could use an épée.", "Thus, the answer is yes."], "conf_comments": ["(A)\nA sophist", "(A)\nSophists are", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7617081800341662, 0.36730300051836645, 0.20673547405801432, 0.16162362171769], "conf": [0.9640770294819228, 0.8594153498078146, 0.5275329923765653, 0.34391019528268935], "prob": [[0.0037507709311993503, 11], [9.70655211828887e-11, 13], [6.550683983423671e-16, 18], [4.448279433519678e-16, 6]], "generated": ["An épée is a type of fencing weapon.", "Fencing is a sport that is played on the Olympic level.", "Thus, it is doubtful if a sophist would use an épée.", "So the answer is no."], "conf_comments": ["(A)\nA sophist", "(A)\nThus, an", "(A)\nA sophist", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7529852115426964, 0.3724167125817948, 0.20232196993232426, 0.13936442330191673], "conf": [0.964561858289022, 0.8355178655827361, 0.5242414910135159, 0.355187812287371], "prob": [[0.00837886670906152, 9], [6.086426064318699e-10, 13], [1.5182232137814782e-13, 11], [1.7909446419079852e-14, 6]], "generated": ["An épée is a fencing sword.", "Fencers use épées to defend themselves from opponents.", "So a sophist could use an épée.", "So the answer is yes."], "conf_comments": ["(A)\nA sophist", "(A)\nSophists are", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": false, "length": 4, "score": [0.7617081800341662, 0.36730300051836645, 0.20673547405801432, 0.06906954229854567], "conf": [0.9640770294819228, 0.8594153498078146, 0.5275329923765653, 0.3280861134677366], "prob": [[0.0037507709311993503, 11], [9.70655211828887e-11, 13], [6.550683983423671e-16, 18], [7.5921616115105735e-28, 16]], "generated": ["An épée is a type of fencing weapon.", "Fencing is a sport that is played on the Olympic level.", "Thus, it is doubtful if a sophist would use an épée.", "This is because ancient sophists are considered not to have participated in sports.", "So the answer is yes, but probably not."], "conf_comments": ["(A)\nA sophist", "(A)\nThus, an", "(A)\nA sophist", "(A)\nSo the answer is"], "info": [[[1.1589876157546806e-12, 16], [3.613483832965329e-06, 10]], 0.6219252979603996, "(A)\nSo the answer is", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 60.49953484535217}
{"index": 25, "question": "Did Dale Jr hug his dad after their last Daytona 500 together?", "answer": "no", "explanation": "No. Dale Jr. and his father Dale Sr. last raced together at the Daytona 500 in 2001. During the 2001 Daytona 500 Dale Sr. suffered a basilar skull fracture and died. ", "generated": [{"finished": true, "length": 4, "score": [0.828787216437651, 0.5096653541928473, 0.4131638468478537, 0.3569962812544561], "conf": [0.9722170110182856, 0.8554675123858224, 0.6859287323068972, 0.5146652340102332], "prob": [[1.4857026385323817e-05, 32], [6.867655922909903e-13, 20], [4.750963406865758e-15, 25], [4.6110859719386285e-15, 6]], "generated": ["Dale Earnhardt Jr and Dale Earnhardt Sr last raced together in the 2001 Daytona 500.", "In their last Daytona 500 together, Dale Sr died during the race.", "Thus, Dale Jr could not hug his dad after their last Daytona 500 together.", "So the answer is no."], "conf_comments": ["(A)\nDale Earn", "(A)\nThus, D", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8016231256884923, 0.49713432626086757, 0.3931322043144713, 0.3356400258478232], "conf": [0.9753882557756134, 0.8344540223348796, 0.6610542136446721, 0.48497187520887236], "prob": [[6.78496973932082e-05, 23], [9.649104451549255e-18, 37], [2.6113427287803195e-20, 25], [2.5119356739138464e-20, 6]], "generated": ["Dale Earnhardt Jr. retired at the end of the 2017 NASCAR season.", "The father/son duo of Dale Earnhardt Sr. and Dale Jr. made their last Daytona 500 together in 2002.", "Thus, Dale Jr. did not hug his dad after their last Daytona 500.", "So the answer is no."], "conf_comments": ["(A)\nDale Earn", "(A)\nThus, D", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7917053112012317, 0.6099203972988498, 0.41985145639582605, 0.3129663934052206], "conf": [0.9678701957160061, 0.7576643729133327, 0.5958118463839515, 0.3400576195593469], "prob": [[1.9175646214323536e-05, 25], [3.2901623127319985e-08, 23], [8.271793463502121e-15, 30], [7.0431010086339124e-15, 6]], "generated": ["Dale Jr. hugged his dad after the 2008 Daytona 500.", "Dale Earnhardt Sr. died in the 2001 Daytona 500.", "Thus, the last Daytona 500 Dale Jr and his dad raced in was in 2001.", "So the answer is no."], "conf_comments": ["(A)\nDale Jr.", "(A)\nThus, D", "(A)\nDale Jr.", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7917053112012317, 0.6099203972988498, 0.35550052443825647, 0.262275685044148, 0.21327941541727471], "conf": [0.9678701957160061, 0.7576643729133327, 0.5758394168561801, 0.3917984698445105, 0.26019487811634967], "prob": [[1.9175646214323536e-05, 25], [3.2901623127319985e-08, 23], [5.319705765600437e-18, 28], [1.1971522766687223e-19, 17], [1.1668748757073433e-19, 6]], "generated": ["Dale Jr. hugged his dad after the 2008 Daytona 500.", "Dale Earnhardt Sr. died in the 2001 Daytona 500.", "This means that they never ran the 2001 or 2002 Daytona 500s together.", "Thus, they never had a last Daytona 500 together.", "So the answer is no."], "conf_comments": ["(A)\nDale Jr.", "(A)\nThus, D", "(A)\nThus, they", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7348607881485366, 0.5421003884455536, 0.3443979614925682, 0.28595060828041374], "conf": [0.9730996705393431, 0.7611031646591742, 0.6009574034862168, 0.4279401442738958], "prob": [[2.2416788621365785e-07, 26], [7.669398330268996e-11, 22], [1.1376068562811753e-16, 20], [9.365297931235772e-17, 6]], "generated": ["Dale Earnhardt Jr. retired from racing before the Daytona 500 in 2018.", "Dale Earnhardt died at the Daytona 500 in 2001.", "Therefore, they could not hug at their last Daytona 500 together.", "So the answer is no."], "conf_comments": ["(A)\nHis father,", "(A)\nThus, D", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 55.61531925201416}
{"index": 26, "question": "Can citrus grow in Ulaanbaatar?", "answer": "no", "explanation": "No. Citrus can withstand short periods down to as cold as −10 °C (14 °F), but realistically temperatures not falling below −2 °C (28 °F) are required for successful cultivation. Ulaanbaatar has an average annual temperature of −0.4 °C or 31.3 °F.", "generated": [{"finished": true, "length": 5, "score": [0.8599092022870036, 0.5310287321617141, 0.31009563603613816, 0.2560512715551905, 0.21207553097232495], "conf": [0.9705641692681569, 0.8204995358858763, 0.7011812814548305, 0.5313507503880154, 0.365400415033438], "prob": [[0.03824470070422291, 12], [2.4930023328648596e-07, 15], [4.1571450345455013e-16, 22], [9.470159674350424e-17, 14], [9.332422385424599e-17, 6]], "generated": ["Ulaanbaatar is the capital of Mongolia.", "Ulaanbaatar is located on the northern side of Mongolia.", "In comparison, the citrus growing regions of the world are located on the southern side of the equator.", "Thus, citrus cannot grow in Ulaanbaatar.", "So the answer is no."], "conf_comments": ["(A)\nMongolia is", "(A)\nMongolia is", "(A)\nThus, cit", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7510358124568853, 0.3678044476560124, 0.30759294666131454, 0.2599064590599151], "conf": [0.9595100735755715, 0.6888091969787652, 0.5151030847527702, 0.3683960782695247], "prob": [[0.0002033986509640168, 16], [1.3173433063687062e-20, 34], [5.1597548223864805e-21, 14], [5.107311885393337e-21, 6]], "generated": ["Ulaanbaatar is in Mongolia, which is located in Asia.", "Most citrus are native to South America, so they likely cannot survive in the climate of Ulaanbaatar, which is not located in South America.", "Thus, citrus cannot grow in Ulaanbaatar.", "So the answer is no."], "conf_comments": ["(A)\nAsia is a", "(A)\nThus, cit", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8599092022870036, 0.5310287321617141, 0.31009563603613816, 0.20612604249571828, 0.17097266751196216], "conf": [0.9705641692681569, 0.8204995358858763, 0.7011812814548305, 0.5324586459385847, 0.3674331811379714], "prob": [[0.03824470070422291, 12], [2.4930023328648596e-07, 15], [4.1571450345455013e-16, 22], [7.176237500683407e-20, 16], [7.048029880954789e-20, 6]], "generated": ["Ulaanbaatar is the capital of Mongolia.", "Ulaanbaatar is located on the northern side of Mongolia.", "In comparison, the citrus growing regions of the world are located on the southern side of the equator.", "Thus, a citrus would not grow in Ulaanbaatar.", "So the answer is no."], "conf_comments": ["(A)\nMongolia is", "(A)\nMongolia is", "(A)\nThus, cit", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7510358124568853, 0.3678044476560124, 0.2753435358150583, 0.22765287074133023], "conf": [0.9595100735755715, 0.6888091969787652, 0.4949609063866861, 0.33990190892397243], "prob": [[0.0002033986509640168, 16], [1.3173433063687062e-20, 34], [3.1648767833268625e-22, 15], [3.079211555560332e-22, 6]], "generated": ["Ulaanbaatar is in Mongolia, which is located in Asia.", "Most citrus are native to South America, so they likely cannot survive in the climate of Ulaanbaatar, which is not located in South America.", "Thus, citrus probably cannot grow in Ulaanbaatar.", "So the answer is no."], "conf_comments": ["(A)\nAsia is a", "(A)\nThus, cit", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8614495844237121, 0.5322456352990463, 0.4081174689571236, 0.2574280588370345, 0.14191318489542584], "conf": [0.9701240530528477, 0.8333032550536723, 0.7074859738131593, 0.4823823995740203, 0.26317889523736254], "prob": [[0.030705909717620248, 13], [5.477380705286687e-10, 22], [3.5253386763816876e-13, 20], [6.372139115001094e-17, 16], [1.9033996826416563e-18, 6]], "generated": ["Ulaanbaatar is the capital city of Mongolia.", "According to US News, Mongolia is ranked as the number 18 best country in the world.", "In comparison, US News ranks the United States as the number 1 best country in the world.", "Thus, the United States has a higher quality of life than Mongolia.", "So the answer is no."], "conf_comments": ["(A)\nMongolia is", "(A)\nAccording to US", "(A)\nThus, the", "(A)\nCitrus is", "(A), but the previous step is"], "info": null}, {"finished": false, "length": 5, "score": [0.8614495844237121, 0.5322456352990463, 0.4081174689571236, 0.2574280588370345, 0.12276870205892809], "conf": [0.9701240530528477, 0.8333032550536723, 0.7074859738131593, 0.4823823995740203, 0.2793524981334274], "prob": [[0.030705909717620248, 13], [5.477380705286687e-10, 22], [3.5253386763816876e-13, 20], [6.372139115001094e-17, 16], [1.2366077769395429e-24, 19]], "generated": ["Ulaanbaatar is the capital city of Mongolia.", "According to US News, Mongolia is ranked as the number 18 best country in the world.", "In comparison, US News ranks the United States as the number 1 best country in the world.", "Thus, the United States has a higher quality of life than Mongolia.", "Therefore, there is a higher chance of citrus growing in US instead of Mongolia.", "So the answer is no."], "conf_comments": ["(A)\nMongolia is", "(A)\nAccording to US", "(A)\nThus, the", "(A)\nCitrus is", "(A)\nSo the answer is"], "info": [[[1.9406478022872396e-08, 19], [0.732959551487216, 6]], 0.5791100553836884, "(A)\nSo the answer is", 5]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 61.596951484680176}
{"index": 27, "question": "Does New York Harbor sit on a craton without volcanic activity?", "answer": "no", "explanation": "No. New York Harbor is located on Laurentia craton.  The southwestern portion of Laurentia contains numerous large volcanic eruptions.", "generated": [{"finished": true, "length": 5, "score": [0.6946554830783612, 0.4718252818492058, 0.2553034248787376, 0.16754586870692975, 0.13946870086213978], "conf": [0.961933224156445, 0.8757214205527948, 0.7764245776571855, 0.5783045995434212, 0.40353597069437175], "prob": [[0.000127386129870157, 13], [6.188222509045106e-10, 18], [3.151803450184444e-15, 11], [2.0562492505820582e-21, 26], [1.9716893888399685e-21, 6]], "generated": ["New York Harbor is an inlet of the Atlantic Ocean.", "Cratons are the oldest and most stable parts of the Earth's crust.", "They have very little to no volcanic activity.", "Thus, New York Harbor sits on a part of the crust that does not have much volcanic activity.", "So the answer is yes."], "conf_comments": ["(A)\nCratons are", "(A)\nVolcanic activity", "(A)\nNew York Harbor", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.6946554830783612, 0.4718252818492058, 0.2553034248787376, 0.1187106981657413, 0.08741474559147663, 0.06917914816990178], "conf": [0.961933224156445, 0.8757214205527948, 0.7764245776571855, 0.4451960991061805, 0.2557441205015609, 0.16054836764008698], "prob": [[0.000127386129870157, 13], [6.188222509045106e-10, 18], [3.151803450184444e-15, 11], [3.701350202657214e-21, 14], [1.1669911737112168e-21, 20], [1.1506740000376152e-21, 6]], "generated": ["New York Harbor is an inlet of the Atlantic Ocean.", "Cratons are the oldest and most stable parts of the Earth's crust.", "They have very little to no volcanic activity.", "In contrast, New York Harbor often has volcanic activity.", "Thus, New York Harbor does not sit on a craton without volcanic activity.", "So the answer is no."], "conf_comments": ["(A)\nCratons are", "(A)\nVolcanic activity", "(A)\nNew York Harbor", "(A)\nThus, New", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.5458075402561903, 0.3346381914112591, 0.2006972095749272, 0.09953618532346437, 0.06975486498419829, 0.05579940963257095], "conf": [0.9731623630803572, 0.8982938502552584, 0.7874504982012053, 0.5188747676815588, 0.36482393722633355, 0.23528006575955965], "prob": [[1.4981539073761406e-12, 23], [1.4212002341158491e-19, 18], [1.922753061636796e-23, 10], [3.770194947527901e-28, 11], [1.2106643027497678e-30, 16], [1.1552450467926158e-30, 6]], "generated": ["A craton is a large portion of land that has remained solid and unaffected by tectonic activity.", "The Appalachian Mountains and the Ouachita Mountains are cratons.", "New York Harbor is in New York state.", "New York state isn’t in a craton.", "Thus, New York Harbor doesn’t sit in a craton.", "So the answer is no."], "conf_comments": ["(A)\nNew York Harbor", "(A)\nNew York Harbor", "(A)\nNew York state is", "(A)\nNew York Harbor", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5458075402561903, 0.3346381914112591, 0.15463000450635353, 0.09958873271081788, 0.0814482496679428], "conf": [0.9731623630803572, 0.8982938502552584, 0.7608712233708433, 0.5861628372727808, 0.40152741151574245], "prob": [[1.4981539073761406e-12, 23], [1.4212002341158491e-19, 18], [6.16178253528425e-34, 24], [1.5336648631665527e-42, 32], [1.329252712111778e-42, 6]], "generated": ["A craton is a large portion of land that has remained solid and unaffected by tectonic activity.", "The Appalachian Mountains and the Ouachita Mountains are cratons.", "The largest craton in the United States is the craton called “North America”, which borders the Atlantic Ocean.", "New York Harbor is located in New York, which is located in the United States, and thus sits on a craton without volcanic activity.", "So the answer is yes."], "conf_comments": ["(A)\nNew York Harbor", "(A)\nNew York Harbor", "(A)\nNew York Harbor", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6946554830783612, 0.4718252818492058, 0.2553034248787376, 0.1187106981657413, 0.07279182747912943], "conf": [0.961933224156445, 0.8757214205527948, 0.7764245776571855, 0.4451960991061805, 0.2921232297124795], "prob": [[0.000127386129870157, 13], [6.188222509045106e-10, 18], [3.151803450184444e-15, 11], [3.701350202657214e-21, 14], [4.3025551756255863e-23, 8]], "generated": ["New York Harbor is an inlet of the Atlantic Ocean.", "Cratons are the oldest and most stable parts of the Earth's crust.", "They have very little to no volcanic activity.", "In contrast, New York Harbor often has volcanic activity.", "Thus, the answer is no."], "conf_comments": ["(A)\nCratons are", "(A)\nVolcanic activity", "(A)\nNew York Harbor", "(A)\nThus, New", "(A)\nSo the answer is"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 75.05990982055664}
{"index": 28, "question": "Can the Palace of Westminster tell time in the dark?", "answer": "yes", "explanation": "Yes. The Palace of Westminster has Big Ben, a striking clock tower. Big Ben communicates the time via bells.", "generated": [{"finished": true, "length": 4, "score": [0.8171210616641109, 0.4324032191469467, 0.34639803758832005, 0.2793296031385183], "conf": [0.9758990496189283, 0.644286713844266, 0.4629394204081047, 0.301737606115871], "prob": [[0.0010790025248275646, 18], [1.5521088338943604e-08, 13], [2.5454847919294656e-09, 16], [2.5097897511046267e-09, 6]], "generated": ["The Palace of Westminster is the meeting place of the Parliament of the United Kingdom.", "It cannot tell time in the dark because it is a building.", "Thus, the Palace of Westminster cannot tell time in the dark.", "So the answer is no."], "conf_comments": ["(A)\nIt is located in", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8171210616641109, 0.4324032191469467, 0.27854252104932986, 0.2250377676454344], "conf": [0.9758990496189283, 0.644286713844266, 0.4560836372933886, 0.298471990306402], "prob": [[0.0010790025248275646, 18], [1.5521088338943604e-08, 13], [2.555055768617218e-11, 12], [2.5154329841040792e-11, 6]], "generated": ["The Palace of Westminster is the meeting place of the Parliament of the United Kingdom.", "It cannot tell time in the dark because it is a building.", "Thus, it could not tell time in the dark.", "So the answer is no."], "conf_comments": ["(A)\nIt is located in", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.8018572505898789, 0.4786203047934616, 0.42065734392212467, 0.1776072749739612, 0.11709008628843433, 0.09602043459587574], "conf": [0.9689144145698122, 0.8908625493170749, 0.730199059617798, 0.6018922880435572, 0.3985142152848391, 0.2712076921253543], "prob": [[0.00012077901713970494, 22], [2.3787947782091107e-08, 9], [1.3146145397272929e-08, 10], [1.407878520030284e-20, 18], [1.0986472600660809e-23, 17], [1.0228964212244637e-23, 6]], "generated": ["The Palace of Westminster is the home of both the House of Commons and the House of Lords.", "The House of Commons is the lower house.", "The House of Lords is the upper house.", "Time is the measurement of how much of it has passed since midnight or noon.", "Thus, the Palace of Westminster can not tell time in the dark.", "So the answer is no."], "conf_comments": ["(A)\nIt is located in", "(A)\nThe House of L", "(A)\nThe Palace of West", "(A)\nThe Palace of West", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.8171210616641109, 0.5315068821636035, 0.32998647165444656, 0.1606109678853973, 0.10492937839645343, 0.09207098141335081], "conf": [0.9758990496189283, 0.8862544656587155, 0.7787328016399897, 0.6282177175355775, 0.48022428054810884, 0.37111783720054375], "prob": [[0.0010790025248275646, 18], [5.257885180048299e-08, 13], [3.0912459302841625e-16, 23], [3.218694694847379e-24, 15], [9.213357671204898e-28, 14], [9.009894707169284e-28, 6]], "generated": ["The Palace of Westminster is the meeting place of the Parliament of the United Kingdom.", "There is a clock tower at the Palace of Westminster.", "The clock tower is known as \"Big Ben\", which is one of the most recognizable landmarks in London.", "There are no windows in the tower, so it is always dark inside.", "Thus, Big Ben can't tell time in the dark.", "So the answer is no."], "conf_comments": ["(A)\nIt is located in", "(A)\nThe clock tower is", "(A)\nThe clock tower is", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 10, "score": [0.8018572505898789, 0.4786203047934616, 0.42065734392212467, 0.1959911585433847, 0.11867358309896962, 0.07031017037502639, 0.03459827817519489, 0.026508037856899713, 0.01518915660849746, 0.011875392929195495], "conf": [0.9689144145698122, 0.8908625493170749, 0.730199059617798, 0.617744497493001, 0.4866431264356614, 0.3936615477421347, 0.3256079687640856, 0.26369474417685823, 0.18843268738654648, 0.12529173787426695], "prob": [[0.00012077901713970494, 22], [2.3787947782091107e-08, 9], [1.3146145397272929e-08, 10], [2.7480684346610296e-16, 13], [2.815324911242662e-19, 9], [1.0250979067553297e-24, 15], [2.9838406258083038e-33, 16], [3.2970993135227587e-35, 14], [1.261892586092146e-41, 19], [7.617231979711379e-42, 6]], "generated": ["The Palace of Westminster is the home of both the House of Commons and the House of Lords.", "The House of Commons is the lower house.", "The House of Lords is the upper house.", "The lower house decides what bills are to be passed.", "The upper house votes on the bills.", "In order for a bill to pass, it must pass in both houses.", "The lower house only needs a simple majority of 50%+1.", "The upper house needs a supermajority of 2/3.", "Thus, it is not possible for the lower house to pass a bill on its own.", "So the answer is no."], "conf_comments": ["(A)\nIt is located in", "(A)\nThe House of L", "(A)\nThe Palace of West", "(A)\nThe upper house dec", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThe upper house needs", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": [], "9": [], "10": []}, "run_time": 83.78437972068787}
{"index": 29, "question": "In teenagers and young adults with depression, are SSRI medications less safe than they are for adults?", "answer": "yes", "explanation": "Yes. In teens, SSRI medications may increase the risk of suicidal thinking. In adults over 25, SSRI medications are regarded as generally safe.", "generated": [{"finished": true, "length": 3, "score": [0.5874620160684293, 0.4385313765753733, 0.35698304249821106], "conf": [0.921629895065601, 0.6960611189134065, 0.46593229421033894], "prob": [[4.2529261918572844e-13, 29], [3.2802193490103397e-15, 16], [3.0875445548977294e-15, 6]], "generated": ["Research have found that SSRI medications are as safe for teenagers with depression as it is for adults with depression.", "Thus, SSRI medications are not less safe for teenagers.", "So the answer is no."], "conf_comments": ["(A)\nThus, SS", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8121054811533511, 0.4499319602960306, 0.2170421111143795, 0.15033605285007762, 0.12414746488005154], "conf": [0.9537261457769897, 0.8257476110980647, 0.6296969040044349, 0.3940245429018876, 0.2732165399886332], "prob": [[0.0009041129689087828, 19], [8.894592117207803e-13, 20], [4.058336251462368e-24, 22], [1.1764470028559218e-26, 22], [1.0645513965542511e-26, 6]], "generated": ["SSRI medications are selective serotonin re-uptake inhibitors.", "The goal of SSRI medications is to increase a patient’s serotonin level.", "These medications have been shown to have adverse effects when taken by children and adolescents.", "Thus, SSRI medications are less safe in teenagers and young adults with depression.", "So the answer is yes."], "conf_comments": ["(A)\nSSRI medications", "(A)\nSerotonin", "(A)\nThus, SS", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8121054811533511, 0.4499319602960306, 0.2170421111143795, 0.15050217733278223, 0.12202367338345248], "conf": [0.9537261457769897, 0.8257476110980647, 0.6296969040044349, 0.3790450186320445, 0.25299239950729735], "prob": [[0.0009041129689087828, 19], [8.894592117207803e-13, 20], [4.058336251462368e-24, 22], [5.684288517130832e-26, 19], [5.187902097539722e-26, 6]], "generated": ["SSRI medications are selective serotonin re-uptake inhibitors.", "The goal of SSRI medications is to increase a patient’s serotonin level.", "These medications have been shown to have adverse effects when taken by children and adolescents.", "Thus, SSRI medications are less safe for teenagers and young adults.", "So the answer is yes."], "conf_comments": ["(A)\nSSRI medications", "(A)\nSerotonin", "(A)\nThus, SS", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5876623754105514, 0.41486856430443886, 0.23382589791150374, 0.15985627750924944, 0.11416812136244817], "conf": [0.9391599858252148, 0.8249800317455168, 0.6613316897284923, 0.47796890650155344, 0.3234897068765937], "prob": [[1.5110939083285973e-08, 18], [1.8051779396195893e-13, 20], [2.466860455811818e-26, 32], [2.6104513556923728e-30, 21], [4.783354723025902e-31, 6]], "generated": ["SSRI medications affect the levels of different neurotransmitters in the brain.", "One of the neurotransmitters affected by SSRI medications is Serotonin.", "It has been found that Serotonin does not work as well in the brains of teenagers and young adults who suffer from depression.", "Thus, SSRI medications may not be as safe for them as they are for adults.", "So the answer is no."], "conf_comments": ["(A)\nIn teenagers", "(A)\nSerotonin", "(A)\nThus, SS", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5876623754105514, 0.41486856430443886, 0.23382589791150374, 0.14238511680774563, 0.07070597641266609], "conf": [0.9391599858252148, 0.8249800317455168, 0.6613316897284923, 0.4540571639194064, 0.3222613076731674], "prob": [[1.5110939083285973e-08, 18], [1.8051779396195893e-13, 20], [2.466860455811818e-26, 32], [5.938172246291153e-32, 21], [1.0446368259384447e-34, 6]], "generated": ["SSRI medications affect the levels of different neurotransmitters in the brain.", "One of the neurotransmitters affected by SSRI medications is Serotonin.", "It has been found that Serotonin does not work as well in the brains of teenagers and young adults who suffer from depression.", "Thus, SSRI medications could be less safe in adolescents and young adults.", "So the answer is true."], "conf_comments": ["(A)\nIn teenagers", "(A)\nSerotonin", "(A)\nThus, SS", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5876623754105514, 0.41486856430443886, 0.23382589791150374, 0.14238511680774563, 0.059531894655863664], "conf": [0.9391599858252148, 0.8249800317455168, 0.6613316897284923, 0.4540571639194064, 0.3172190844335614], "prob": [[1.5110939083285973e-08, 18], [1.8051779396195893e-13, 20], [2.466860455811818e-26, 32], [5.938172246291153e-32, 21], [5.713039330855483e-38, 10]], "generated": ["SSRI medications affect the levels of different neurotransmitters in the brain.", "One of the neurotransmitters affected by SSRI medications is Serotonin.", "It has been found that Serotonin does not work as well in the brains of teenagers and young adults who suffer from depression.", "Thus, SSRI medications could be less safe in adolescents and young adults.", "So the answer is “could be less safe”."], "conf_comments": ["(A)\nIn teenagers", "(A)\nSerotonin", "(A)\nThus, SS", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.789103746414185}
{"index": 30, "question": "Does Marco Rubio have a close relationship with Allah?", "answer": "no", "explanation": "No. Marco Rubio adheres to the religious sect of Christianity known as Catholicism. Catholics and other Christians worship God. Allah is worshiped by believers of Islam.", "generated": [{"finished": true, "length": 2, "score": [0.7818508901669744, 0.5958083801763675], "conf": [0.9217354144769991, 0.69051791000358], "prob": [[0.001400443311575568, 16], [0.00018257398955003068, 8]], "generated": ["There is no evidence that Marco Rubio has a close relationship with Allah.", "Thus, the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.672238229226327, 0.5184307141338319, 0.3010711364668526, 0.2536833364512073, 0.21355986190849913], "conf": [0.9733928112842332, 0.8452816631678224, 0.5208884660683771, 0.39607353130876577, 0.2815262071491315], "prob": [[1.003169172637017e-05, 15], [2.278197532499918e-07, 10], [3.5574801890618945e-14, 26], [1.1873408974228076e-14, 16], [1.1664057697315215e-14, 6]], "generated": ["Marco Rubio is a U.S. Republican Senator for Florida.", "Allah is the Arabic word for God.", "Since Marco Rubio is a Christian, it would be inaccurate to say he has a close relationship with Allah.", "Thus, Marco Rubio does not have a close relationship with Allah.", "So the answer is no."], "conf_comments": ["(A)\nAllah is the", "(A)\nThus, Marco", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6199022332541042, 0.43421174526270734, 0.36699265546603344], "conf": [0.9559281173306704, 0.747282579719191, 0.5385845619618878], "prob": [[7.15937575568458e-06, 13], [2.674486861633711e-08, 12], [2.5356884280253397e-08, 6]], "generated": ["Marco Rubio is a US Senator who is not Muslim.", "Thus, he has no close relationship with Allah.", "So the answer is no."], "conf_comments": ["(A)\nAllah is the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7064420185104788, 0.47880108732810256, 0.3136609146803769, 0.25155477629097256], "conf": [0.9736523680781844, 0.8490570121737835, 0.5627787982243942, 0.365734623045328], "prob": [[1.566687655439556e-06, 20], [2.5776576710596896e-09, 10], [3.796386477726209e-12, 15], [3.5683404880262008e-12, 6]], "generated": ["Marco Rubio is a senator who is running for president in 2016.", "Allah is the Islamic name for God.", "Thus, there is no relationship between Marco Rubio and Allah.", "So the answer is no."], "conf_comments": ["(A)\nAllah is the", "(A)\nThus, Marco", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7818508901669744, 0.43049041433484303, 0.3377044530153618], "conf": [0.9217354144769991, 0.6770366713064596, 0.4827328751157894], "prob": [[0.001400443311575568, 16], [5.830289092226664e-09, 14], [2.4099143626352783e-09, 6]], "generated": ["There is no evidence that Marco Rubio has a close relationship with Allah.", "Thus, a close relationship with Allah cannot be inferred.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 47.456804513931274}
{"index": 31, "question": "Would Persephone be a good consultant to a landscape architect?", "answer": "yes", "explanation": "Yes. Persephone is a vegetation goddess.  A vegetation deity is a nature deity whose disappearance and reappearance, or life, death and rebirth, embodies the growth cycle of plants. Landscape architects deal with planning and laying out gardens and other plant life.", "generated": [{"finished": true, "length": 4, "score": [0.6141816658351874, 0.3010497698472295, 0.18413295446498681, 0.15219712827927803], "conf": [0.9632856918398097, 0.7981016533011936, 0.5623185594895787, 0.3899133207945727], "prob": [[5.092155113339155e-06, 13], [1.2734399134323445e-14, 16], [1.8023941685046225e-18, 14], [1.6490547812360481e-18, 6]], "generated": ["Persephone is a Greek goddess associated with the land.", "She is also known as Queen of the Underworld (land of the dead).", "Thus, Persephone would not be a good consultant.", "So the answer is no."], "conf_comments": ["(A)\nThus, Per", "(A)\nThus, Per", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7753987303003776, 0.3032961714574846, 0.23691080593749694], "conf": [0.939236860553361, 0.6614953453425465, 0.42767007201598867], "prob": [[0.0012420115725101737, 15], [1.4417900362498084e-15, 18], [1.018649837550623e-15, 6]], "generated": ["Persephone was the Queen of the Underworld in Greek mythology.", "Thus, Persephone is likely to be good at consulting on land matters.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Per", "(A)\nIn comparison, a", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.6141816658351874, 0.37871907797755366, 0.2036956677839717, 0.09466630450636382, 0.060025131090824545, 0.05046024304502712], "conf": [0.9632856918398097, 0.7891308008533446, 0.6113352767316758, 0.4725259147772949, 0.35342528985961974, 0.2520627538787706], "prob": [[5.092155113339155e-06, 13], [5.087262104984719e-13, 21], [7.363011914886279e-23, 23], [1.7336377705990957e-34, 21], [5.422536698237307e-38, 13], [5.132522074285921e-38, 6]], "generated": ["Persephone is a Greek goddess associated with the land.", "In Greek mythology, she was abducted by Hades, the god of the underword.", "As a result, she spends half of the year underground but the other half of the year above ground.", "Thus, she has both intimate knowledge of the underground, but also knowledge of the surface.", "She can thus be a good consultant to a landscape architect.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Per", "(A)\nThus, Per", "(A)\nThus, Per", "(A)\nThus, she", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.599464004535511, 0.43803453469503223, 0.17447223148253152, 0.13168006831464943], "conf": [0.9351702416385383, 0.7366157539333018, 0.40317407745522926, 0.23526073876625117], "prob": [[3.983474604988471e-06, 13], [2.2513753099253306e-12, 37], [4.022821737777431e-30, 33], [3.48108304966393e-30, 6]], "generated": ["Persephone is identified as the Queen of the Underworld.", "Landscape architecture is the practice of designing outdoor areas, landmarks, and structures to achieve environmental, social-behavioral, or aesthetic outcomes.", "I don’t see how Persephone has expertise to landscape architecture so I don’t think it would be a good consultant to a landscape architect.", "So the answer is no."], "conf_comments": ["(A)\nThus, Per", "(A)\nThus, Per", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.599464004535511, 0.43803453469503223, 0.17146327986761403, 0.12984982073603663], "conf": [0.9351702416385383, 0.7366157539333018, 0.44785480512469056, 0.2966533637991793], "prob": [[3.983474604988471e-06, 13], [2.2513753099253306e-12, 37], [4.0435412266701303e-35, 38], [1.7034616886695917e-35, 6]], "generated": ["Persephone is identified as the Queen of the Underworld.", "Landscape architecture is the practice of designing outdoor areas, landmarks, and structures to achieve environmental, social-behavioral, or aesthetic outcomes.", "Therefore, it is not expected that a landscape architect would consult Persephone, queen of the Underworld, since the queen spends the majority of her time in the Underworld.", "So the answer is no."], "conf_comments": ["(A)\nThus, Per", "(A)\nThus, Per", "(A)\nThus, Per", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 57.63472771644592}
{"index": 32, "question": "Is Argon near Neon on the periodic table of elements?", "answer": "yes", "explanation": "Yes. Argon is a noble gas. Neon is a noble gas.  The noble gases are all clumped together on the periodic table of elements.", "generated": [{"finished": true, "length": 4, "score": [0.6539244219377958, 0.4060507228923269, 0.343526848121248, 0.2964719022235883], "conf": [0.9040117497795687, 0.7264097377979722, 0.5735841015783094, 0.42822645263778547], "prob": [[1.4876536112318623e-07, 21], [2.4488941460175943e-12, 15], [9.171143102896282e-13, 10], [9.041477704039903e-13, 6]], "generated": ["Neon has an atomic number of 10 and Argon has the atomic number 18.", "Neon and Argon are neighbors on the periodic table of elements.", "Thus, Argon is near Neon.", "So the answer is yes."], "conf_comments": ["(A)\nThe periodic table is", "(A)\nThus, Arg", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6539244219377958, 0.4060507228923269, 0.3394959974388877, 0.29324591958113677], "conf": [0.9040117497795687, 0.7264097377979722, 0.5737971255424024, 0.42917417642373185], "prob": [[1.4876536112318623e-07, 21], [2.4488941460175943e-12, 15], [3.4664304672979867e-13, 16], [3.4150655211815565e-13, 6]], "generated": ["Neon has an atomic number of 10 and Argon has the atomic number 18.", "Neon and Argon are neighbors on the periodic table of elements.", "Thus, Argon is near Neon on the periodic table of elements.", "So the answer is yes."], "conf_comments": ["(A)\nThe periodic table is", "(A)\nThus, Arg", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7848429638774528, 0.39877787651620167, 0.24578970864014138, 0.21400357997557676], "conf": [0.9290268046020136, 0.8030286977883452, 0.6732480346041301, 0.5207785652995485], "prob": [[0.000613297631724415, 18], [1.0346717049693527e-09, 11], [3.775852806751834e-13, 10], [3.3453005494450156e-13, 6]], "generated": ["Argon is element number 18 and neon is element number 10.", "The elements on the periodic table are numerically ordered.", "Thus, Argon comes after Neon.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Arg", "(A)\nThus, Arg", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7848429638774528, 0.38581238615578173, 0.2210824673450497, 0.1641320066857829, 0.13722025308647862], "conf": [0.9290268046020136, 0.7916208049503638, 0.6005560345470551, 0.4531409259567646, 0.31843900068577913], "prob": [[0.000613297631724415, 18], [8.63440448035555e-14, 18], [1.0624566125246193e-20, 19], [5.09894831568181e-23, 17], [4.936533214653035e-23, 6]], "generated": ["Argon is element number 18 and neon is element number 10.", "The periodic table of elements is in order, with the elements in order of atomic number.", "Therefore, Argon is not near neon because neon has a lower atomic number.", "Thus, Argon is not near neon on the periodic table of elements.", "So the answer is no."], "conf_comments": ["(A)\nThus, Arg", "(A)\nThus, Arg", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6765620042473774, 0.35827656334622787, 0.2427693285682015, 0.1397286136817361, 0.12132197826948797], "conf": [0.9280396335080376, 0.80249806265798, 0.7035916242537563, 0.524572664926535, 0.39782638670220843], "prob": [[3.581148843426285e-07, 21], [2.0217953266987232e-18, 23], [1.1437484586681994e-20, 8], [1.5240853936138216e-24, 11], [1.4707244845504667e-24, 6]], "generated": ["Argon is the element at number 18, and Neon is at number 10.", "To see if Argon is close to Neon, lets see how many elements separate Argon and Neon.", "There are 8 elements between them.", "Thus, it is not close to Neon.", "So the answer is no."], "conf_comments": ["(A)\nThus, Arg", "(A)\nThere are 8", "(A)\nThus, Arg", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 59.545026540756226}
{"index": 33, "question": "Did Thomas Greenhill's parents violate the concept of monogamy?", "answer": "no", "explanation": "No. Thomas Greenhill was a surgeon born to William and Elizabeth Greenhill. William and Elizabeth Greenhill had 39 children. Monogamy is a committed relationship between two people where usually they remain together for life. Thomas Greenhill was the last of his parents 39 children and was born shortly after his father died.", "generated": [{"finished": true, "length": 4, "score": [0.7184420894052461, 0.43372225024443367, 0.32744989901072735, 0.2590904549192368], "conf": [0.9770309858646488, 0.7991085667099737, 0.5471298395104129, 0.3538813240656887], "prob": [[3.681316477705095e-05, 16], [3.102264603797211e-13, 23], [7.930724901176874e-15, 20], [6.522109164009545e-15, 6]], "generated": ["Monogamy is the practice of having a single sexual partner at a time.", "Thomas Greenhill’s parents can violate the concept of monogamy by cheating on each other.", "Thus, Thomas Greenhill’s parents could have violated the concept of monogamy.", "So the answer is yes."], "conf_comments": ["(A)\nThomas Greenhill", "(A)\nThomas Greenhill", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7991725064682511, 0.391951286658632, 0.27398795227341216, 0.22386786243055162], "conf": [0.9759086505975588, 0.8232011786628933, 0.6401280538707015, 0.4684615299997962], "prob": [[0.00020770073480925447, 20], [4.9498908733495664e-18, 25], [7.262984187166235e-22, 19], [4.185969882708509e-22, 6]], "generated": ["Monogamy is the practice or state of being married to only one spouse at a time.", "If Thomas Greenhill's parents had at least two different marriage, then the concept of monogamy is violated.", "Thus, if Thomas Greenhill's parents violated monogamy, then yes.", "So the answer is yes."], "conf_comments": ["(A)\nThomas Greenhill", "(A)\nThomas Greenhill", "(A)\nIf Thomas Greenhill", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5882998072900611, 0.3719691980877011, 0.3024652096159989], "conf": [0.962185634093499, 0.6863957805983599, 0.49891386920796477], "prob": [[1.3041393001152454e-05, 11], [1.2340617979572675e-09, 16], [6.992821998297032e-10, 6]], "generated": ["Thomas Greenhill had 21 siblings.", "So Thomas Greenhill's parents violated the concept of monogamy.", "So the answer is yes."], "conf_comments": ["(A)\nThomas Greenhill", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5882998072900611, 0.3719691980877011, 0.2949119839535356], "conf": [0.962185634093499, 0.6863957805983599, 0.5110434991629299], "prob": [[1.3041393001152454e-05, 11], [1.2340617979572675e-09, 16], [3.185942121788691e-10, 8]], "generated": ["Thomas Greenhill had 21 siblings.", "So Thomas Greenhill's parents violated the concept of monogamy.", "Thus, the answer is yes."], "conf_comments": ["(A)\nThomas Greenhill", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7184420894052461, 0.35593192305485344, 0.2462456927180064, 0.19516542538775336], "conf": [0.9770309858646488, 0.6338429195408758, 0.45296904384068964, 0.2857694717520429], "prob": [[3.681316477705095e-05, 16], [1.9023834955235947e-14, 22], [1.5498738497380287e-16, 12], [1.5101614359327155e-16, 6]], "generated": ["Monogamy is the practice of having a single sexual partner at a time.", "Thomas Greenhill did have biological parents, but they did not have a sexual relationship with each other.", "Thus, they did not violate monogamy.", "So the answer is no."], "conf_comments": ["(A)\nThomas Greenhill", "(A)\nThus, Thomas", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7184420894052461, 0.43372225024443367, 0.32744989901072735, 0.1633530077347392], "conf": [0.9770309858646488, 0.7991085667099737, 0.5471298395104129, 0.3365187785010859], "prob": [[3.681316477705095e-05, 16], [3.102264603797211e-13, 23], [7.930724901176874e-15, 20], [3.48006025042523e-17, 6]], "generated": ["Monogamy is the practice of having a single sexual partner at a time.", "Thomas Greenhill’s parents can violate the concept of monogamy by cheating on each other.", "Thus, Thomas Greenhill’s parents could have violated the concept of monogamy.", "So the answer is possibly."], "conf_comments": ["(A)\nThomas Greenhill", "(A)\nThomas Greenhill", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 47.3697726726532}
{"index": 34, "question": "Would it be uncommon for a high schooler to use the yellow pages?", "answer": "yes", "explanation": "Yes. High school students are between 14-19 years old.  Teenagers now are considered 'digital natives'.  'Digital natives' are individuals who have grown up during the computer age, being exposed to technology from early childhood.", "generated": [{"finished": true, "length": 4, "score": [0.6084079438736484, 0.32004067976494854, 0.22739027937600134, 0.1821323320325168], "conf": [0.9284608975059636, 0.6644740705387934, 0.4942769945600459, 0.3413659938396992], "prob": [[2.6191349879550507e-12, 29], [4.876620747766438e-23, 26], [2.093824349101696e-26, 20], [1.3453117469153874e-26, 6]], "generated": ["In 2017, about 25% of people aged 13-17 used the yellow pages at least once.", "This can indicate that in a random high schooler, around 25% of them use the yellow pages at least once.", "Thus, it would be uncommon for a random high schooler to use the yellow pages.", "So the answer is yes."], "conf_comments": ["(A)\nIn 20", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6084079438736484, 0.32004067976494854, 0.2020307356750033, 0.1537033721497372], "conf": [0.9284608975059636, 0.6644740705387934, 0.4311589466843323, 0.25226318319243146], "prob": [[2.6191349879550507e-12, 29], [4.876620747766438e-23, 26], [1.2263845555576863e-26, 17], [1.1495272270549371e-26, 6]], "generated": ["In 2017, about 25% of people aged 13-17 used the yellow pages at least once.", "This can indicate that in a random high schooler, around 25% of them use the yellow pages at least once.", "Thus, a high schooler using the yellow pages is not that uncommon.", "So the answer is no."], "conf_comments": ["(A)\nIn 20", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6457967697100704, 0.2626417832114585, 0.18456666880435516, 0.1510245855495087], "conf": [0.8456085057331587, 0.6196567893023732, 0.4553529928180377, 0.30669111878540684], "prob": [[2.4850544477536785e-05, 15], [1.4911611423544203e-19, 22], [4.168328423812043e-21, 9], [4.023244703837849e-21, 6]], "generated": ["It is very uncommon for high schoolers to use the yellow pages.", "Today’s teenagers are generally adept at searching for information on their smart phones.", "Thus, it would be uncommon.", "So the answer is yes."], "conf_comments": ["(A)\nThe yellow pages are", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 8, "score": [0.5401235169715749, 0.3321916197151372, 0.15255574604444577, 0.0947411840435657, 0.05784898309454175, 0.02704896468435525, 0.019239477817953127, 0.01582750197811581], "conf": [0.9521432474496924, 0.8567774075590253, 0.5882515597044798, 0.39931416134399383, 0.30873140771721436, 0.21732327801091655, 0.15907986216943543, 0.10862166200966455], "prob": [[5.3171344182618204e-11, 20], [1.2024291200281923e-16, 15], [5.331279969665213e-31, 28], [7.111468952212118e-38, 28], [4.134286671446741e-46, 26], [3.2135289067865658e-53, 14], [4.162514550390248e-56, 18], [3.9461854381210003e-56, 6]], "generated": ["According to US census, the median age of a high schooler is 17. ", "The last print of the yellow pages was in 2013.", "The last print of the yellowpages in the 2010s would probably be in the early 2010s.", "The median age of a high schooler in the early 2010s would be 13 or 14. ", "Thus, in the early 2010s the median age of a high schooler was below 18.", "The yellow pages are used by people above 18 years old.", "Thus it would be uncommon for a high schooler to use the yellow pages.", "So the answer is yes."], "conf_comments": ["(A)\nThe yellow pages were", "(A)\nThus, it", "(A)\nThus, it", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6457967697100704, 0.2626417832114585, 0.1568737152613688, 0.1239191012321262], "conf": [0.8456085057331587, 0.6196567893023732, 0.40985000272019784, 0.25694944759297833], "prob": [[2.4850544477536785e-05, 15], [1.4911611423544203e-19, 22], [1.0167203661511888e-25, 23], [9.883753826823082e-26, 6]], "generated": ["It is very uncommon for high schoolers to use the yellow pages.", "Today’s teenagers are generally adept at searching for information on their smart phones.", "Thus, today’s high schoolers are unlikely to use the yellow pages and it would be uncommon.", "So the answer is yes."], "conf_comments": ["(A)\nThe yellow pages are", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 8, "score": [0.5401235169715749, 0.3321916197151372, 0.15255574604444577, 0.0947411840435657, 0.05784898309454175, 0.02704896468435525, 0.019239477817953127, 0.010893685771224664], "conf": [0.9521432474496924, 0.8567774075590253, 0.5882515597044798, 0.39931416134399383, 0.30873140771721436, 0.21732327801091655, 0.15907986216943543, 0.10770220944859181], "prob": [[5.3171344182618204e-11, 20], [1.2024291200281923e-16, 15], [5.331279969665213e-31, 28], [7.111468952212118e-38, 28], [4.134286671446741e-46, 26], [3.2135289067865658e-53, 14], [4.162514550390248e-56, 18], [2.222449001002684e-58, 7]], "generated": ["According to US census, the median age of a high schooler is 17. ", "The last print of the yellow pages was in 2013.", "The last print of the yellowpages in the 2010s would probably be in the early 2010s.", "The median age of a high schooler in the early 2010s would be 13 or 14. ", "Thus, in the early 2010s the median age of a high schooler was below 18.", "The yellow pages are used by people above 18 years old.", "Thus it would be uncommon for a high schooler to use the yellow pages.", "So the answer is yes. ", "(Note: the median age may vary greatly by region and year)"], "conf_comments": ["(A)\nThe yellow pages were", "(A)\nThus, it", "(A)\nThus, it", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": [[[0.005339198155581998, 7], [1.5244388271722965e-15, 14]], 0.6770323281640673, "(A)\n\n\n\n\n", 8]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": []}, "run_time": 71.6285343170166}
{"index": 35, "question": "Could an escapee swim nonstop from Alcatraz island to Siberia?", "answer": "no", "explanation": "No. Alcatraz Island was a San Francisco Bay prison. Siberia is over 5,217 miles away from San Francisco. The longest continuous swim record was 139 miles.", "generated": [{"finished": true, "length": 4, "score": [0.7064227628617116, 0.46813662244555904, 0.2652120553127965, 0.2015001228076598], "conf": [0.970943395263291, 0.8507404014406931, 0.6450393647531263, 0.4852061335092856], "prob": [[0.0012863364628596277, 10], [8.11956281712506e-08, 14], [4.964544689351116e-16, 22], [5.971339770368382e-17, 8]], "generated": ["Alcatraz is located in San Francisco Bay.", "In comparison, Siberia is located in the middle of Russia.", "This means that an escapee could not feasibly swim from Alcatraz island to Siberia.", "Thus, the answer is no."], "conf_comments": ["(A)\nSiberia is", "(A)\nThus, Al", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.6801381579958325, 0.4936575422994628, 0.27139056436912223, 0.13708901945986415, 0.08576430419098519, 0.0742457251128534], "conf": [0.9564478509482911, 0.8716280142380761, 0.7724570236262209, 0.5964704101723856, 0.4754579128213462, 0.3576325703594534], "prob": [[1.1474286617592893e-07, 22], [1.0315998678283144e-11, 17], [1.5929488747111348e-21, 21], [7.106326621127433e-38, 34], [4.88919673695679e-40, 7], [4.782623805281564e-40, 6]], "generated": ["Distance between Alcatraz Island and Siberia is around 11,000 kilometers.", "This is equivalent to 11,000,000 meters.", "The human world record for swimming 100 meters is 49.14 seconds.", "If you do the math, it will take one escapee over 1 month of nonstop swimming to make it from Alcatraz Island to Siberia.", "Thus, it is impossible.", "So the answer is no."], "conf_comments": ["(A)\nThe average speed of", "(A)\nThe average speed of", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.6801381579958325, 0.38314646666970287, 0.24819209700680492, 0.140050835774327, 0.08873986013914882, 0.07354990532584803], "conf": [0.9564478509482911, 0.8625293125162273, 0.7155702236408842, 0.5931446640625851, 0.41340491329447115, 0.2864727893942441], "prob": [[1.1474286617592893e-07, 22], [6.3458602217990084e-15, 16], [1.0800144132547439e-24, 33], [9.326335245737052e-32, 17], [1.406710229799412e-41, 41], [1.335115155380064e-41, 6]], "generated": ["Distance between Alcatraz Island and Siberia is around 11,000 kilometers.", "The average human can swim for about 10 kilometers per hour.", "Thus, it would take about 1,100 hours if an escapee nonstoply swam from Alcatraz island to Siberia.", "For comparison, the average human can sleep for about 8 hours in a day.", "Thus, it would be impossible for an escapee to nonstoply swim from Alcatraz island to Siberia, because the escapee would have to sleep for more than a month.", "So the answer is no."], "conf_comments": ["(A)\nThe average speed of", "(A)\nThus, it", "(A)\nThe average human can", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6801381579958325, 0.4936575422994628, 0.27139056436912223, 0.16514049368263486], "conf": [0.9564478509482911, 0.8716280142380761, 0.7724570236262209, 0.6792846122602139], "prob": [[1.1474286617592893e-07, 22], [1.0315998678283144e-11, 17], [1.5929488747111348e-21, 21], [2.625504480547172e-40, 50]], "generated": ["Distance between Alcatraz Island and Siberia is around 11,000 kilometers.", "This is equivalent to 11,000,000 meters.", "The human world record for swimming 100 meters is 49.14 seconds.", "To travel 11,000,000 m, one would need 11,000,000 / 49.14 seconds or 224,787 49."], "conf_comments": ["(A)\nThe average speed of", "(A)\nThe average speed of", "(A)\nThus, it", "(A)\nThus, it"], "info": null}, {"finished": true, "length": 5, "score": [0.6801381579958325, 0.4936575422994628, 0.27139056436912223, 0.13708901945986415, 0.08361166047591857], "conf": [0.9564478509482911, 0.8716280142380761, 0.7724570236262209, 0.5964704101723856, 0.4405428414025681], "prob": [[1.1474286617592893e-07, 22], [1.0315998678283144e-11, 17], [1.5929488747111348e-21, 21], [7.106326621127433e-38, 34], [1.1598980965911253e-39, 6]], "generated": ["Distance between Alcatraz Island and Siberia is around 11,000 kilometers.", "This is equivalent to 11,000,000 meters.", "The human world record for swimming 100 meters is 49.14 seconds.", "If you do the math, it will take one escapee over 1 month of nonstop swimming to make it from Alcatraz Island to Siberia.", "So the answer is no."], "conf_comments": ["(A)\nThe average speed of", "(A)\nThe average speed of", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 65.51942300796509}
{"index": 36, "question": "Would the Titanic be well preserved at the bottom of the Gulf of Finland?", "answer": "yes", "explanation": "Yes. The bottom of the gulf is one of the world's largest ship cemeteries. Because of the low salinity and cold waters, and no shipworms, the ships are relatively well preserved. RMS Titanic was a British passenger ship.", "generated": [{"finished": true, "length": 5, "score": [0.7417644517209385, 0.4978418128608482, 0.2074119218326302, 0.14820879805037068, 0.12191060517038407], "conf": [0.7966600386116506, 0.6905224800691958, 0.5540829167511763, 0.4036626459577059, 0.2756530876749742], "prob": [[9.582415414240561e-05, 25], [2.712749381882402e-09, 16], [2.847493335538758e-17, 12], [8.018768363507353e-21, 23], [7.58668229798076e-21, 6]], "generated": ["The Titanic sank in the North Atlantic Ocean, while the Gulf of Finland is in the Baltic Sea.", "The Baltic Sea has a much lower salinity than the North Atlantic Ocean.", "Higher salinity preserves dead organic matter better.", "Thus, it is less likely for the Titanic to be well preserved in the Gulf of Finland.", "So the answer is no."], "conf_comments": ["(A)\nThe North Atlantic Ocean", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6493316436287886, 0.3519487375064313, 0.20671201423749644, 0.17217345307373536], "conf": [0.9702399427974283, 0.6402090214307381, 0.47792353205076116, 0.3331422266086507], "prob": [[1.6175725612541138e-06, 16], [1.5153910934269002e-13, 20], [6.293966958964662e-22, 25], [6.116493146408295e-22, 6]], "generated": ["The Gulf of Finland has a depth of about 140 m.", "The Titanic sunk to a depth of about 12 850 m.", "Thus, the Titanic would have been impossible to remain well preserved at the bottom of the Gulf of Finland.", "So the answer is no."], "conf_comments": ["(A)\nThe Titanic", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.588023814309497, 0.378733454638036, 0.2017314305300118, 0.16103827222073225], "conf": [0.936444912142185, 0.7134033666255082, 0.5740538270536867, 0.3989887858452617], "prob": [[1.5229648633020484e-11, 25], [5.438469853892467e-20, 32], [1.4234650103531049e-33, 30], [8.455996008521634e-34, 6]], "generated": ["According to Wikipedia, the approximate bottom temperature of the Gulf of Finland is -1.6 degrees Celsius.", "According to Wikipedia, the approximate bottom temperature of the Atlantic Ocean (where the Titanic is located) is 3.3 degrees Celsius.", "Thus, at the bottom of the Gulf of Finland, the water is much colder, and thus more likely to preserve any artifacts.", "So the answer is yes."], "conf_comments": ["(A)\nThe Titanic", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7417644517209385, 0.4978418128608482, 0.2074119218326302, 0.11934700233943737, 0.09771242831290722], "conf": [0.7966600386116506, 0.6905224800691958, 0.5540829167511763, 0.3786440081537892, 0.25705707080586304], "prob": [[9.582415414240561e-05, 25], [2.712749381882402e-09, 16], [2.847493335538758e-17, 12], [1.963043430543607e-31, 45], [1.818857216285725e-31, 6]], "generated": ["The Titanic sank in the North Atlantic Ocean, while the Gulf of Finland is in the Baltic Sea.", "The Baltic Sea has a much lower salinity than the North Atlantic Ocean.", "Higher salinity preserves dead organic matter better.", "Thus, because there is much less salinity in the Gulf of Finland and more in the North Atlantic Ocean, then the Titanic would not be as well preserved at the bottom of the Gulf of Finland.", "So the answer is no."], "conf_comments": ["(A)\nThe North Atlantic Ocean", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.7417644517209385, 0.45941993017537003, 0.2240817981686753, 0.11860132454263222, 0.05420550513549418, 0.029809398153633683, 0.024055535611217753], "conf": [0.7966600386116506, 0.6860762111459814, 0.567551633146001, 0.45763292705755265, 0.34460287825457464, 0.2491826905119599, 0.1658113333425591], "prob": [[9.582415414240561e-05, 25], [5.167726581010449e-10, 15], [4.026783937297141e-31, 39], [2.1894289840268353e-39, 18], [7.466794194583252e-49, 17], [1.0716949212991784e-58, 26], [9.415318330041039e-59, 6]], "generated": ["The Titanic sank in the North Atlantic Ocean, while the Gulf of Finland is in the Baltic Sea.", "The Baltic Sea has much less salinity than the North Atlantic Ocean.", "The salinity of the North Atlantic is -30.4 ppm while the salinity of the Baltic Sea is only 0.8 ppm in the Gulf of Finland.", "In comparison, the salinity of the North Pacific Ocean is 31 ppm.", "The average seawater salinity is around 33.5 ppm.", "Thus, the Titanic would not be preserved as well at the Gulf of Finland since it has low salinity.", "So the answer is no."], "conf_comments": ["(A)\nThe North Atlantic Ocean", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 69.54799628257751}
{"index": 37, "question": "Could a snowy owl survive in the Sonoran?", "answer": "no", "explanation": "No. The Snowy owl is a bird native to the Arctic regions of North America. Temperatures in the North American Arctic range from -36.4F to 50F. The Sonoran is one of the hottest deserts in the world. The Sonoran Desert can get as hot as 118F.", "generated": [{"finished": true, "length": 3, "score": [0.5804160482112996, 0.41336769373809634, 0.3408757946878868], "conf": [0.9433371790991995, 0.6614310590947414, 0.45416859150168937], "prob": [[2.8447184248866805e-17, 37], [3.169188172532134e-20, 21], [2.989981703793471e-20, 6]], "generated": ["Snowy owls live in the Artic, whereas the Sonoran Desert is in North America, specifically southern Arizona, southwestern New Mexico, and Northwestern Mexico.", "Thus, it is unlikely for a snowy owl to survive in the Sonoran.", "So the answer is no."], "conf_comments": ["(A)\nSnowy ow", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5967463704158279, 0.35318776230898635, 0.21356987785205506, 0.14984369715912665, 0.1273274842252621], "conf": [0.9591768890655668, 0.8829105470430806, 0.7942612373744802, 0.5904958168837504, 0.4274567329708085], "prob": [[1.3028483294135078e-07, 16], [1.7405479264331617e-13, 14], [1.5961831557899294e-20, 18], [4.1874805913427426e-24, 20], [4.123869608828251e-24, 6]], "generated": ["The ranges of the snowy owl and Sonoran desert are separated.", "The range of the snowy owl covers Alaska and Canada.", "In comparison, the Sonoran Desert spans over the southwestern United States.", "Thus, it is impossible for a snowy owl survive in the Sonoran.", "So the answer is no."], "conf_comments": ["(A)\nThe Sonoran", "(A)\nThe range of the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6406176596935701, 0.2978375759792054, 0.22574120451024404, 0.18099091518794488], "conf": [0.961607803267395, 0.6858124580007186, 0.46186048676741687, 0.29762871535445745], "prob": [[1.2111483647611752e-06, 16], [9.429135117722758e-24, 33], [5.391412186135822e-25, 18], [5.312141895098023e-25, 6]], "generated": ["The Sonoran is a desert and there is little to no precipitation.", "In contrast, a snowy owl is a bird that can survive in cold winters and it has the word “Snowy” in its name.", "Thus, a snowy owl would not survive in the Sonoran.", "So the answer is no."], "conf_comments": ["(A)\nThe snowy ow", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6298506537423769, 0.356123166691911, 0.2672667378688265], "conf": [0.9653519636002827, 0.7162390513107244, 0.5032450843679808], "prob": [[4.591982324518987e-08, 19], [6.15928722155055e-18, 27], [1.634321228712086e-18, 6]], "generated": ["The Sonoran desert is located in the south western regions of the United States and Mexico.", "Since snowy owls are found in the tundra, they would not be able to survive the Sonoran desert.", "So the answer is no."], "conf_comments": ["(A)\nThe Sonoran", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6406176596935701, 0.2978375759792054, 0.1793217881488337, 0.1438025120219292], "conf": [0.961607803267395, 0.6858124580007186, 0.42942038780843045, 0.2770604824883798], "prob": [[1.2111483647611752e-06, 16], [9.429135117722758e-24, 33], [9.765597204566898e-29, 21], [9.575207401398354e-29, 6]], "generated": ["The Sonoran is a desert and there is little to no precipitation.", "In contrast, a snowy owl is a bird that can survive in cold winters and it has the word “Snowy” in its name.", "Thus, it is impossible to have a snowy owl survive in the Sonoran.", "So the answer is no."], "conf_comments": ["(A)\nThe snowy ow", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 51.48402762413025}
{"index": 38, "question": "Would a rabbi worship martyrs Ranavalona I killed?", "answer": "no", "explanation": "No. Rabbis are teachers of Judaism. Ranavalona I, ruler of Madagascar, killed many Christians that were later determined by the church to be martyrs. Judaism does not have a group of saints and martyrs that are prayed to like Christianity..", "generated": [{"finished": true, "length": 4, "score": [0.692850086623195, 0.36990218394608015, 0.2503152628932433, 0.21742802710874004], "conf": [0.9604990446803069, 0.8162084551580028, 0.64661656692705, 0.4910986821406521], "prob": [[3.578502543643373e-12, 38], [3.9004451031304396e-22, 21], [1.1702043583260043e-26, 19], [1.1247820821735365e-26, 6]], "generated": ["Ranavalona I was the queen regent of Madagascar from 1793 to 1798 and queen until her death in 1861.", "According to her Wikipedia page, she persecuted missionaries and martyred thousands of people.", "Thus, a rabbi would NOT worship the martyrs Ranavalona I killed.", "So the answer is no."], "conf_comments": ["(A)\nShe was a Christian", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7298242499204254, 0.37723545675518744, 0.221926506334794, 0.16559604066536143, 0.14342489710868989], "conf": [0.9616194947197333, 0.8979009893671824, 0.7651725289659276, 0.6034902902603059, 0.45506942198320827], "prob": [[3.627156401407407e-08, 29], [2.0947943113342094e-17, 17], [4.217843333677672e-22, 12], [1.1328987889165025e-24, 17], [1.098100493440448e-24, 6]], "generated": ["Ranavalona I was a queen of merina who ruled Madagascar from 1828 to 1861.", "She killed thousands of Christians in the middle of the 1830s.", "Rabbis are scholars of Judaism.", "Thus, rabbis would not worship martyrs Ranavalona I killed.", "So the answer is no."], "conf_comments": ["(A)\nShe was a Christian", "(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7298242499204254, 0.37723545675518744, 0.221926506334794, 0.16374976803652722, 0.14201222355963794], "conf": [0.9616194947197333, 0.8979009893671824, 0.7651725289659276, 0.6030934827251676, 0.4561197983815371], "prob": [[3.627156401407407e-08, 29], [2.0947943113342094e-17, 17], [4.217843333677672e-22, 12], [7.825147096658579e-25, 17], [7.569499908114469e-25, 6]], "generated": ["Ranavalona I was a queen of merina who ruled Madagascar from 1828 to 1861.", "She killed thousands of Christians in the middle of the 1830s.", "Rabbis are scholars of Judaism.", "Thus, a rabbi would not worship martyrs Ranavalona killed.", "So the answer is no."], "conf_comments": ["(A)\nShe was a Christian", "(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.692850086623195, 0.36990218394608015, 0.18994581709410405, 0.1443248335644109], "conf": [0.9604990446803069, 0.8162084551580028, 0.6125671359582302, 0.4810358877984682], "prob": [[3.578502543643373e-12, 38], [3.9004451031304396e-22, 21], [7.441152848558544e-35, 28], [6.350841080697662e-36, 8]], "generated": ["Ranavalona I was the queen regent of Madagascar from 1793 to 1798 and queen until her death in 1861.", "According to her Wikipedia page, she persecuted missionaries and martyred thousands of people.", "As such, it is unlikely that a rabbi from the time would have worshipped Ranavalona I’s martyrs.", "Thus, the answer is no."], "conf_comments": ["(A)\nShe was a Christian", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.692850086623195, 0.36990218394608015, 0.18994581709410405, 0.11715766963742759], "conf": [0.9604990446803069, 0.8162084551580028, 0.6125671359582302, 0.4650890025896227], "prob": [[3.578502543643373e-12, 38], [3.9004451031304396e-22, 21], [7.441152848558544e-35, 28], [3.7113499535916435e-41, 21]], "generated": ["Ranavalona I was the queen regent of Madagascar from 1793 to 1798 and queen until her death in 1861.", "According to her Wikipedia page, she persecuted missionaries and martyred thousands of people.", "As such, it is unlikely that a rabbi from the time would have worshipped Ranavalona I’s martyrs.", "Thus, the rabbi would not worship martyrs she killed, so the answer is no."], "conf_comments": ["(A)\nShe was a Christian", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 60.95470452308655}
{"index": 39, "question": "Would an oil painter avoid reds from scale insects that live on a cactus?", "answer": "yes", "explanation": "Yes. Carmine is the product of an insect that lives on some cacti. Carmine is not stable in oil paints and its usage has been discontinued. Carmine is red.", "generated": [{"finished": true, "length": 4, "score": [0.5718453522611955, 0.25266980334128114, 0.20506518791056114, 0.1709032552539191], "conf": [0.9205871212046898, 0.6210947564064797, 0.45766309987697124, 0.32011934048239443], "prob": [[6.425053421620258e-08, 16], [3.7631430481929235e-18, 19], [3.190893182023529e-19, 22], [3.0592636606634537e-19, 6]], "generated": ["Oil painters do not use reds derived from a cactus.", "The reds used by oil painters are not subject to the issue of scale insects.", "Thus, an oil painter would avoid reds from scale insects that live on a cactus.", "So the answer is yes."], "conf_comments": ["(A)\nScale insects live", "(A)\nThus, an", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.492285932121855, 0.3200274914272302, 0.2624953949906076], "conf": [0.9198902524913196, 0.721763969016476, 0.507998209312031], "prob": [[6.834008362395252e-13, 21], [7.046437797446372e-20, 26], [5.3750256548757725e-20, 6]], "generated": ["According to the National Resources Conservation Service website, no scale insects live on cacti.", "Therefore, the oil painter wouldn’t need to avoid reds from scale insects that live on a cactus.", "So the answer is no."], "conf_comments": ["(A)\nThus, an", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5718453522611955, 0.25266980334128114, 0.20506518791056114, 0.12419134749073983], "conf": [0.9205871212046898, 0.6210947564064797, 0.45766309987697124, 0.30467502079405306], "prob": [[6.425053421620258e-08, 16], [3.7631430481929235e-18, 19], [3.190893182023529e-19, 22], [8.923995523505037e-21, 6]], "generated": ["Oil painters do not use reds derived from a cactus.", "The reds used by oil painters are not subject to the issue of scale insects.", "Thus, an oil painter would avoid reds from scale insects that live on a cactus.", "So the answer is no."], "conf_comments": ["(A)\nScale insects live", "(A)\nThus, an", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5718453522611955, 0.25266980334128114, 0.1496712545145263, 0.11812084726438997], "conf": [0.9205871212046898, 0.6210947564064797, 0.40822840908739705, 0.25543975838345967], "prob": [[6.425053421620258e-08, 16], [3.7631430481929235e-18, 19], [1.6381239038834629e-22, 16], [1.5932832163892152e-22, 6]], "generated": ["Oil painters do not use reds derived from a cactus.", "The reds used by oil painters are not subject to the issue of scale insects.", "Thus, an oil painter does not avoid reds from cacti.", "So the answer is no."], "conf_comments": ["(A)\nScale insects live", "(A)\nThus, an", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.4885171934888271, 0.295470313388803, 0.17357085193931907, 0.08076343623176255, 0.05776917636785118], "conf": [0.9349995032029663, 0.7257089072626222, 0.5693218665535281, 0.34579540314359225, 0.19951757796236008], "prob": [[4.1247547282259997e-19, 31], [1.4505734944444716e-29, 32], [3.9945798423485637e-38, 24], [9.676535927793647e-46, 17], [4.704623452268787e-46, 6]], "generated": ["According to the University of California, reds are colors produced by pigments of cadmium, hematite and cinnabar.", "According to the University of California, scale insects are commonly found on the pads and stems of the prickly pear cactus.", "The pads and stems of the cactus are where the prickly pear cactus grows.", "Thus, a red pigment could be obtained from this cactus.", "So the answer is no."], "conf_comments": ["(A)\nScale insects are", "(A)\nThus, red", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 55.544867515563965}
{"index": 40, "question": "Was the Second Amendment to the United States Constitution written without consideration for black Americans?", "answer": "yes", "explanation": "Yes. The writers of the Constitutional Amendments did not view black people as legitimate human beings. The writers of the Constitutional Amendments believed that slavery benefited black slaves. The Constitutional Amendments were written for people that the writers considered human.", "generated": [{"finished": true, "length": 1, "score": [0.8939369524297318], "conf": [0.9192938641519863], "prob": [[0.0010442668864589293, 49]], "generated": ["The Second Amendment to the United States Constitution says “A well regulated Militia, being necessary to the security of a free State, the right of the people to keep and bear Arms, shall not be infringed.”"], "conf_comments": ["(A)\nThe Second Amend"], "info": null}, {"finished": true, "length": 2, "score": [0.8422762320617398, 0.667157488795037], "conf": [0.8873660936845187, 0.7496772273469469], "prob": [[3.3813334912266156e-05, 46], [1.5744156702757808e-11, 49]], "generated": ["The Second Amendment to the United States Constitution states:\nA well regulated militia being necessary to the security of a free state, the right of the people to keep and bear arms shall not be infringed.", "The 10th Amendment of the United States Constitution states:\nThe powers not delegated to the United States by the Constitution, nor prohibited by it to the States, are reserved to the States respectively, or to the people."], "conf_comments": ["(A)\nThe Second Amend", "(A)\nThe 13"], "info": null}, {"finished": true, "length": 4, "score": [0.730033746836567, 0.48962849287730126, 0.4025082234720721, 0.34061186887497347], "conf": [0.9646762558993498, 0.8735797978987497, 0.6933213709635866, 0.501551457578105], "prob": [[0.00024675159748290063, 14], [1.888230280045878e-13, 30], [1.4421195925856313e-14, 16], [1.3568631657610964e-14, 6]], "generated": ["The Second Amendment is a part of the Bill of Rights.", "The Bill of Rights was ratified in 1791, and at the time, the United States Constitution did not prohibit slavery.", "Thus, the Second Amendment was written without consideration for black Americans.", "So the answer is yes."], "conf_comments": ["(A)\nThe Bill of Rights", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8275178172737965, 0.4888697151733603, 0.22481166038876788, 0.1807360595135349, 0.14848270801091165], "conf": [0.9215177141113773, 0.7999636984671968, 0.5186415632820243, 0.3670640057034891, 0.24981350796534063], "prob": [[6.46321938366964e-07, 48], [1.2713883771634767e-15, 22], [4.969012397074842e-37, 44], [1.1628122689447082e-37, 16], [1.1062104025669665e-37, 6]], "generated": ["The Second Amendment of the Constitution states that a well-regulated Militia, being necessary to the security of a free State, the right of the people to keep and bear Arms, shall not be infringed.", "The Second Amendment was written in the year 1787 or 1789.", "Since this was before the Civil War (1861-1865) or any war fought against the United States by black soldiers, there was no need to consider black Americans in this amendment.", "Thus, the Second Amendment was written without consideration for black Americans.", "So the answer is yes."], "conf_comments": ["(A)\nThe Second Amend", "(A)\nIn comparison, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.730033746836567, 0.48962849287730126, 0.25940596100626745, 0.17552258640412727], "conf": [0.9646762558993498, 0.8735797978987497, 0.7303271964626192, 0.5334547568505194], "prob": [[0.00024675159748290063, 14], [1.888230280045878e-13, 30], [1.838643494445578e-31, 38], [1.0095863702368101e-35, 21]], "generated": ["The Second Amendment is a part of the Bill of Rights.", "The Bill of Rights was ratified in 1791, and at the time, the United States Constitution did not prohibit slavery.", "By 1865, the Thirteenth Amendment to the United States Constitution prohibited slavery, although racial segregation and racism against African Americans continued.", "So yes, the Second Amendment to the United States Constitution was written without consideration for black Americans."], "conf_comments": ["(A)\nThe Bill of Rights", "(A)\nThus, the", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 44.871248722076416}
{"index": 41, "question": "Are all the elements plants need for photosynthesis present in atmosphere of Mars?", "answer": "yes", "explanation": "Yes. Plants need three elements for photosynthesis: Hydrogen, Oxygen, and Carbon. The atmosphere of Mars is composed of carbon dioxide, nitrogen, argon, and trace levels of water vapor, oxygen, carbon monoxide, hydrogen and other noble gases.", "generated": [{"finished": true, "length": 4, "score": [0.725529787107755, 0.4112389401116235, 0.2988981532301397, 0.2516299996197283], "conf": [0.9686411231979078, 0.8087447304806912, 0.6622880032811169, 0.48392042016669073], "prob": [[3.144192029703718e-05, 17], [1.3435202730531439e-15, 25], [1.3499520304190047e-19, 21], [1.1241550025412305e-19, 6]], "generated": ["Photosynthesis requires three elements: carbon, oxygen, and hydrogen.", "The atmosphere of Mars contains carbon dioxide and small amounts of oxygen, but no hydrogen, which is necessary.", "This means that not all the elements plants need for photosynthesis are present in the atmosphere of Mars.", "So the answer is no."], "conf_comments": ["(A)\nCarbon is present", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.725529787107755, 0.4112389401116235, 0.2900020014138502, 0.24510727589511577], "conf": [0.9686411231979078, 0.8087447304806912, 0.6395400854761853, 0.4631179235111811], "prob": [[3.144192029703718e-05, 17], [1.3435202730531439e-15, 25], [1.2571544860830178e-19, 20], [1.1585325362067889e-19, 6]], "generated": ["Photosynthesis requires three elements: carbon, oxygen, and hydrogen.", "The atmosphere of Mars contains carbon dioxide and small amounts of oxygen, but no hydrogen, which is necessary.", "Therefore, the atmosphere of Mars does not contain all the elements plants need for photosynthesis.", "So the answer is no."], "conf_comments": ["(A)\nCarbon is present", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7951941510664892, 0.3821294058390759, 0.2694295131110166, 0.22501558290167462], "conf": [0.9616571050969465, 0.7677699206762855, 0.6293763556962335, 0.44021504478310003], "prob": [[1.4915420400959835e-06, 32], [5.072573861999325e-20, 25], [1.0295793377602595e-23, 17], [1.0123768822277737e-23, 6]], "generated": ["Photosynthesis requires six elements: Carbon, Hydrogen, Oxygen, Nitrogen, Phosphorus, and Sulphur.", "Mars’ atmosphere is almost entirely comprised of carbon dioxide, which does not include all of the six elements.", "Thus, not all elements for photosynthesis are present in Mars’ atmosphere.", "So the answer is no."], "conf_comments": ["(A)\nThe atmosphere of Mars", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7951941510664892, 0.3821294058390759, 0.24883877216418873, 0.20384998979102406], "conf": [0.9616571050969465, 0.7677699206762855, 0.5861289434307835, 0.3985797676777855], "prob": [[1.4915420400959835e-06, 32], [5.072573861999325e-20, 25], [4.165605096761654e-24, 16], [3.848197342073548e-24, 6]], "generated": ["Photosynthesis requires six elements: Carbon, Hydrogen, Oxygen, Nitrogen, Phosphorus, and Sulphur.", "Mars’ atmosphere is almost entirely comprised of carbon dioxide, which does not include all of the six elements.", "Therefore, all the elements plants need for photosynthesis are not present.", "So the answer is no."], "conf_comments": ["(A)\nThe atmosphere of Mars", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.725529787107755, 0.4112389401116235, 0.2988981532301397, 0.0971128176412657, 0.04226938958887617, 0.030781859385350416], "conf": [0.9686411231979078, 0.8087447304806912, 0.6622880032811169, 0.3369216324444071, 0.19154809139710882, 0.11080410519621482], "prob": [[3.144192029703718e-05, 17], [1.3435202730531439e-15, 25], [1.3499520304190047e-19, 21], [9.079922273062014e-41, 31], [6.995690663793234e-49, 17], [4.1532583839506276e-49, 6]], "generated": ["Photosynthesis requires three elements: carbon, oxygen, and hydrogen.", "The atmosphere of Mars contains carbon dioxide and small amounts of oxygen, but no hydrogen, which is necessary.", "This means that not all the elements plants need for photosynthesis are present in the atmosphere of Mars.", "But the amount of carbon dioxide is around 95.7%, which is greater than the 80% minimum for photosynthesis.", "This provides further evidence that the atmosphere of Mars is not suitable for photosynthesis.", "So the answer is no."], "conf_comments": ["(A)\nCarbon is present", "(A)\nThus, the", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 61.32967782020569}
{"index": 42, "question": "Can you find Bob Marley's face in most smoke shops?", "answer": "yes", "explanation": "Yes. Bob Marley's face is on the packaging of a popular brand of rolling papers. Bob Marley is a popular graphic to print on t-shirts for sale to smokers.", "generated": [{"finished": true, "length": 4, "score": [0.5827402427108348, 0.3243248839853307, 0.1984539635766348, 0.15532157724214773], "conf": [0.942302426666606, 0.6936093836417673, 0.440309779339286, 0.2706526707944802], "prob": [[2.360861331886977e-15, 33], [1.2890586524375066e-20, 14], [9.619537714064454e-25, 18], [9.420944932992572e-25, 6]], "generated": ["Bob Marley lived in the 1940s and 1950s, when the legalization of marijuana was decades away.", "So it is unlikely that he would be found in smoke shops.", "Thus, Bob Marley’s face does not appear in most smoke shops.", "So the answer is no."], "conf_comments": ["(A)\nThus, Bob", "(A)\nThus, you", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5827402427108348, 0.33831225952871785, 0.18512213231653332, 0.09338900671766431, 0.07518054063099372], "conf": [0.942302426666606, 0.8400508574433562, 0.6947451191208339, 0.4701703202914418, 0.3130615901344625], "prob": [[2.360861331886977e-15, 33], [2.013319248464827e-20, 12], [1.1661418876222504e-33, 30], [7.931403801576102e-47, 31], [6.742467501470133e-47, 6]], "generated": ["Bob Marley lived in the 1940s and 1950s, when the legalization of marijuana was decades away.", "He died in the late 1980s.", "Most smoke shops started appearing in the United States in 1998, over 10 years after Marley’s death.", "Thus, during Bob Marley’s lifetime, there weren’t any smoke shops where Marley’s face could have been displayed.", "So the answer is no."], "conf_comments": ["(A)\nThus, Bob", "(A)\nThus, Bob", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5827402427108348, 0.33831225952871785, 0.18512213231653332, 0.08419128155055294, 0.06740583418846036], "conf": [0.942302426666606, 0.8400508574433562, 0.6947451191208339, 0.4693174315198493, 0.3025895918183762], "prob": [[2.360861331886977e-15, 33], [2.013319248464827e-20, 12], [1.1661418876222504e-33, 30], [4.426317367555168e-49, 30], [4.2744786901630496e-49, 6]], "generated": ["Bob Marley lived in the 1940s and 1950s, when the legalization of marijuana was decades away.", "He died in the late 1980s.", "Most smoke shops started appearing in the United States in 1998, over 10 years after Marley’s death.", "Thus, there can be no Bob Marley faces in most smoke shops because he simply wasn’t alive when the shops were open.", "So the answer is no."], "conf_comments": ["(A)\nThus, Bob", "(A)\nThus, Bob", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5827402427108348, 0.3243248839853307, 0.1947438203381506], "conf": [0.942302426666606, 0.6936093836417673, 0.4442889720236962], "prob": [[2.360861331886977e-15, 33], [1.2890586524375066e-20, 14], [4.099871124018609e-22, 6]], "generated": ["Bob Marley lived in the 1940s and 1950s, when the legalization of marijuana was decades away.", "So it is unlikely that he would be found in smoke shops.", "So the answer is no."], "conf_comments": ["(A)\nThus, Bob", "(A)\nThus, you", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.632683214046182, 0.23793326272873755, 0.11333450425991541, 0.08492580298749158], "conf": [0.9669803612542295, 0.45598015734800584, 0.25544410890271346, 0.14447567695270608], "prob": [[0.000147757557819529, 10], [4.147395352010982e-17, 24], [4.23417021715567e-28, 28], [4.054202002400052e-28, 6]], "generated": ["Bob Marley was a singer and songwriter.", "Smoke shops sell paraphernalia for toking weed, which has nothing to do with music.", "Thus, you cannot find Bob Marley’s face in most smoke shops because these two do not belong to the same world.", "So the answer is no."], "conf_comments": ["(A)\nHe died in ", "(B)\nThus, Bob", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.89278817176819}
{"index": 43, "question": "Did Pablo Escobar's nickname collection outshine Robert Moses Grove's?", "answer": "yes", "explanation": "Yes. Robert Moses Grove was a baseball player nicknamed Lefty Grove. Pablo Escobar had several nicknames including: Don Pablo, El Padrino, and El Patrón.", "generated": [{"finished": true, "length": 4, "score": [0.6993712567990689, 0.42820563087823976, 0.3510601773208785, 0.28401615568217675], "conf": [0.9619046157578685, 0.8645090044728664, 0.658298766903822, 0.4337573158693489], "prob": [[0.0001519449084314181, 13], [5.3206495484827555e-11, 17], [3.417210532889248e-12, 22], [3.282969374253848e-12, 6]], "generated": ["Robert Moses Grove has 17 nicknames.", "In comparison, Pablo Escobar had around 1,600 nicknames.", "Thus, Pablo Escobar's nickname collection outshone Robert Moses Grove's.", "So the answer is yes."], "conf_comments": ["(A)\nPablo Escobar", "(A)\nThus, Pablo", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.761022075590128, 0.5447645505350184, 0.2784952144185953, 0.1858431842762561, 0.13158834327972022, 0.10752276563098091], "conf": [0.9378858040090667, 0.755429850366679, 0.654001633403142, 0.5117255245312682, 0.3763771352781993, 0.2527956979616987], "prob": [[0.00017045190272784014, 18], [4.9661936723023086e-08, 18], [2.5923110606867384e-15, 14], [5.2587464002631255e-18, 11], [1.1457799917911173e-21, 22], [1.1056524776900168e-21, 6]], "generated": ["Pablo Escobar's real name is Pablo Emilio Escobar Gaviria.", "Robert Moses Grove's real name is Robert Moses Thurston.", "The former has two nicknames: Pablo Escobar and Don Pablo.", "The latter has one nickname: Bobby Moses.", "Thus, Pablo Escobar's nickname collection outshines Robert Moses Groves's.", "So the answer is yes."], "conf_comments": ["(A)\nHe was known by", "(A)\nThus, Pablo", "(A)\nThe latter has three", "(A)\nThus, Pablo", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.761022075590128, 0.5447645505350184, 0.2784952144185953, 0.1667402803374854, 0.11630252597188745, 0.09511310688695869], "conf": [0.9378858040090667, 0.755429850366679, 0.654001633403142, 0.4381277037447061, 0.31250454069004824, 0.21001800356859743], "prob": [[0.00017045190272784014, 18], [4.9661936723023086e-08, 18], [2.5923110606867384e-15, 14], [1.1707557297830392e-19, 16], [1.7653207520481423e-23, 23], [1.714893347252547e-23, 6]], "generated": ["Pablo Escobar's real name is Pablo Emilio Escobar Gaviria.", "Robert Moses Grove's real name is Robert Moses Thurston.", "The former has two nicknames: Pablo Escobar and Don Pablo.", "However, the latter has only one nickname: Robert Moses Grove.", "Thus, Pablo Escobar's nickname collection did outshine that of Robert Moses Grove.", "So the answer is yes."], "conf_comments": ["(A)\nHe was known by", "(A)\nThus, Pablo", "(A)\nThe latter has three", "(A)\nThus, Pablo", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6993712567990689, 0.42820563087823976, 0.2532293716723811, 0.20511214513588005], "conf": [0.9619046157578685, 0.8645090044728664, 0.7017727045935406, 0.4656751865395121], "prob": [[0.0001519449084314181, 13], [5.3206495484827555e-11, 17], [3.2285268375451885e-17, 17], [3.015867400181288e-17, 6]], "generated": ["Robert Moses Grove has 17 nicknames.", "In comparison, Pablo Escobar had around 1,600 nicknames.", "Thus, his collection is significantly larger than Robert Moses Grove's.", "So the answer is yes."], "conf_comments": ["(A)\nPablo Escobar", "(A)\nThus, Pablo", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.761022075590128, 0.5447645505350184, 0.2784952144185953, 0.1667402803374854, 0.10858212502949992, 0.08806567633370613], "conf": [0.9378858040090667, 0.755429850366679, 0.654001633403142, 0.4381277037447061, 0.3045785566991409, 0.2016630316028975], "prob": [[0.00017045190272784014, 18], [4.9661936723023086e-08, 18], [2.5923110606867384e-15, 14], [1.1707557297830392e-19, 16], [8.25191097766483e-25, 24], [7.935472398212004e-25, 6]], "generated": ["Pablo Escobar's real name is Pablo Emilio Escobar Gaviria.", "Robert Moses Grove's real name is Robert Moses Thurston.", "The former has two nicknames: Pablo Escobar and Don Pablo.", "However, the latter has only one nickname: Robert Moses Grove.", "Thus, Pablo Escobar's nickname colletion outshines Robert Moses Grove's.", "So the answer is yes."], "conf_comments": ["(A)\nHe was known by", "(A)\nThus, Pablo", "(A)\nThe latter has three", "(A)\nThus, Pablo", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 68.37529063224792}
{"index": 44, "question": "Has the Subway restaurant franchise had any connections with child abusers?", "answer": "yes", "explanation": "Yes. Subway hired Jared Fogle as a spokesman for their sandwich shops. Jared Fogle was convicted for having sex with minors and for possessing child pornography. ", "generated": [{"finished": true, "length": 3, "score": [0.6668209664124733, 0.4963975497536547, 0.42110994292472376], "conf": [0.9557574918456806, 0.7906878295254723, 0.5727114756591909], "prob": [[5.056138273594627e-13, 37], [5.565921997164141e-16, 17], [5.354791036679469e-16, 6]], "generated": ["It has been reported that Jared Fogle, the former spokesperson for Subway, was convicted of sex crimes with a minor in 2015.", "Thus, Subway has had at least one connection with a child abuser.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.6726874786532322, 0.5319874335955143], "conf": [0.9506062793909353, 0.7367825898641243], "prob": [[3.0673555841702095e-05, 14], [5.513954684707871e-06, 8]], "generated": ["The Subway restaurant franchise is not associated with child abusers.", "Thus, the answer is no."], "conf_comments": ["(A)\nThe Subway restaurant", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5644960264638849, 0.4037197128048771, 0.29557873501174703, 0.2559709312503894], "conf": [0.9716930076435536, 0.9228662741368534, 0.7914355182761003, 0.596832609099707], "prob": [[2.2255605814911836e-11, 22], [3.842709142521335e-15, 14], [3.3364375819571145e-18, 15], [3.227531658922575e-18, 6]], "generated": ["An Australian man, Jared Fogle, was convicted of sex crimes and child pornography.", "He was the spokesman for the Subway restaurant franchise.", "Thus, Subway did have a connection with a child abuser.", "So the answer is yes."], "conf_comments": ["(A)\nHe was a sp", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6726874786532322, 0.3593254502810625, 0.2996672341687685], "conf": [0.9506062793909353, 0.6935766929560818, 0.48599118005928377], "prob": [[3.0673555841702095e-05, 14], [6.002615545527328e-11, 14], [5.740536975952841e-11, 6]], "generated": ["The Subway restaurant franchise is not associated with child abusers.", "Thus, there is no Subway-child abuser connection.", "So the answer is no."], "conf_comments": ["(A)\nThe Subway restaurant", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6668209664124733, 0.3109900892661081, 0.23804901232616107], "conf": [0.9557574918456806, 0.6404759146738398, 0.39502906678214117], "prob": [[5.056138273594627e-13, 37], [8.971152366975214e-24, 22], [6.5936934219191786e-24, 6]], "generated": ["It has been reported that Jared Fogle, the former spokesperson for Subway, was convicted of sex crimes with a minor in 2015.", "Thus, it is not a generalization to say Subway franchises have connections with child abusers.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 40.37559771537781}
{"index": 45, "question": "Does autopilot rely on fossil fuels?", "answer": "yes", "explanation": "Yes. Autopilot is used in vehicles. Vehicles are powered by engines. Engines rely on fossil fuels to operate.", "generated": [{"finished": true, "length": 5, "score": [0.6824327953235964, 0.44463135524348807, 0.30341631418141934, 0.2505643994357408, 0.2092813285810505], "conf": [0.9569391490911849, 0.8339825572368784, 0.6834606780496097, 0.492749233926522, 0.3444221511520306], "prob": [[9.903011934417593e-06, 16], [5.596574053078617e-12, 20], [3.603270030002794e-15, 13], [1.5646729114399986e-15, 15], [1.546558226298667e-15, 6]], "generated": ["Fossil fuels are natural gas, petroleum, and coal.", "Fossil fuels such as natural gas and petroleum are used to generate electricity.", "Electricity is used to operate autopilot.", "Thus, autopilot relies on fossil fuels.", "So the answer is yes."], "conf_comments": ["(A)\nAutopilot", "(A)\nElectricity", "(A)\nThus, aut", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.6500111404628763, 0.4916804640548741], "conf": [0.9490189328850829, 0.7472416838018885], "prob": [[2.1026536641830075e-07, 19], [3.095966803722318e-08, 6]], "generated": ["According to an expert, autopilot does not rely on fossil fuels.", "So the answer is no."], "conf_comments": ["(A)\nThus, aut", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6394999761790675, 0.31898395222952547, 0.2527370553544113, 0.20557475499287398], "conf": [0.9523749739502044, 0.7653811365009136, 0.5619587175012228, 0.3728051492719761], "prob": [[0.00021317236613242925, 10], [1.3946569403804292e-14, 20], [1.33066923076328e-15, 15], [1.309228644285399e-15, 6]], "generated": ["Autopilot uses electricity to operate.", "Electricity relies on energy production, which is primarily based on fossil fuels.", "Thus, autopilot does rely on fossil fuels.", "So the answer is yes."], "conf_comments": ["(A)\nElectricity", "(A)\nThus, aut", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.6500111404628763, 0.4339674113802571], "conf": [0.9490189328850829, 0.7326400796483017], "prob": [[2.1026536641830075e-07, 19], [2.596704024073366e-09, 8]], "generated": ["According to an expert, autopilot does not rely on fossil fuels.", "Therefore, the answer is no."], "conf_comments": ["(A)\nThus, aut", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5209734851258315, 0.3253958795733369, 0.2622531376936205], "conf": [0.9501546037411012, 0.6209182254826457, 0.4158644918528186], "prob": [[6.879461756167374e-09, 15], [1.0684416670254558e-12, 17], [8.890986393741154e-13, 6]], "generated": ["Airplanes use fossil fuels, such as jet gas.", "Therefore, the autopilot also relies on fossil fuels.", "So the answer is yes."], "conf_comments": ["(A)\nAutopilot", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 42.74960660934448}
{"index": 46, "question": "Are psychiatric patients welcome to join the United States Air Force?", "answer": "no", "explanation": "No. Having a history of mental illness disqualifies most people from joining the Armed Forces. Psychiatric patients are being seen for management of mental illness.", "generated": [{"finished": true, "length": 4, "score": [0.7244662967815364, 0.525982645037567, 0.4459602088553686, 0.367067971528631], "conf": [0.9504590441911328, 0.7759919802510465, 0.5871222546517273, 0.3986613164532765], "prob": [[0.0014559035808519236, 11], [2.055285165926615e-06, 15], [8.182340588513373e-07, 18], [8.072885790731392e-07, 6]], "generated": ["Psychiatric patients are mentally ill.", "The United States Air Force does not allow mentally ill people to join.", "Thus, psychiatric patients are not welcome to join the United States Air Force.", "So the answer is no."], "conf_comments": ["(A)\nThe United States Air", "(A)\nThus, psych", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5986140967504406, 0.4159877746721267, 0.3365593272434017, 0.27940551722054696], "conf": [0.9633701339091745, 0.8214593399808843, 0.6279691664440022, 0.43387858513257593], "prob": [[1.3427996545263362e-07, 16], [4.688698266902552e-11, 14], [2.8709795464456046e-12, 18], [2.8283352635310977e-12, 6]], "generated": ["The US Air Force only accepts people who are in good physical and mental health.", "Psychiatric patients are people with mental illnesses.", "Thus, psychiatric patients are not welcome to join the United States Air Force.", "So the answer is no."], "conf_comments": ["(A)\nPsychiat", "(A)\nThus, psych", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5986140967504406, 0.4159877746721267, 0.3065459318652554, 0.2539773265729593], "conf": [0.9633701339091745, 0.8214593399808843, 0.6240651390785569, 0.42961587522524464], "prob": [[1.3427996545263362e-07, 16], [4.688698266902552e-11, 14], [5.685580894872106e-14, 20], [5.5881124847140633e-14, 6]], "generated": ["The US Air Force only accepts people who are in good physical and mental health.", "Psychiatric patients are people with mental illnesses.", "Thus, psychiatric patients are not welcomed to join the United States Air Force.", "So the answer is no."], "conf_comments": ["(A)\nPsychiat", "(A)\nThus, psych", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7244662967815364, 0.525982645037567, 0.30504913160490915, 0.25241898256626133], "conf": [0.9504590441911328, 0.7759919802510465, 0.5761747811053954, 0.39594747477332015], "prob": [[0.0014559035808519236, 11], [2.055285165926615e-06, 15], [3.38820182423941e-10, 11], [3.315094449462682e-10, 6]], "generated": ["Psychiatric patients are mentally ill.", "The United States Air Force does not allow mentally ill people to join.", "Thus, psychiatric patients are not accepted.", "So the answer is no."], "conf_comments": ["(A)\nThe United States Air", "(A)\nThus, psych", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5762303591313884, 0.31846343941713884, 0.2546721696996826], "conf": [0.9596671552131217, 0.663059981542856, 0.4270825314730032], "prob": [[5.04997841692628e-09, 18], [2.8133643683845615e-13, 12], [2.6948520185310025e-13, 6]], "generated": ["Anyone who meets eligibility requirements can apply to join the United States Air Force.", "Thus, psychiatric patients can join, too.", "So the answer is yes."], "conf_comments": ["(A)\nPsychiat", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 50.60137891769409}
{"index": 47, "question": "Does Nicole Kidman know any Scientologists?", "answer": "yes", "explanation": "Yes. Nicole Kidman was married to Tom Cruise. Tom Cruise is a Scientologist. ", "generated": [{"finished": true, "length": 3, "score": [0.711291183015468, 0.4699137886918239, 0.3970221145254632], "conf": [0.9406288457593704, 0.7066144634199822, 0.5077551853329275], "prob": [[0.0005862983786364451, 12], [2.5697546703594093e-06, 10], [2.469555031325637e-06, 6]], "generated": ["Nicole Kidman is a follower of Scientology.", "Thus, she knows some Scientologists.", "So the answer is yes."], "conf_comments": ["(A)\nThus, N", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6005663526443343, 0.46823091484180607, 0.3283189087412981, 0.2699083893159502], "conf": [0.9656297814759772, 0.8473323322858626, 0.6033032300329407, 0.4090679018865824], "prob": [[0.00037887045119444634, 8], [6.677228998190284e-06, 11], [7.850065349959444e-08, 12], [7.697693429958646e-08, 6]], "generated": ["Tom Cruise is a scientologist.", "Tom Cruise is married to Nicole Kidman.", "Thus, Nicole Kidman knows scientologists.", "So the answer is yes."], "conf_comments": ["(A)\nNicole Kidman", "(A)\nThus, N", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6005663526443343, 0.46594711412048306, 0.3025807473029583, 0.24840422338162732], "conf": [0.9656297814759772, 0.8495054477724286, 0.6535769370981445, 0.44437390331785126], "prob": [[0.00037887045119444634, 8], [5.829669047917129e-06, 11], [2.121207477289916e-10, 17], [2.012261398170407e-10, 6]], "generated": ["Tom Cruise is a scientologist.", "Tom Cruise was married to Nicole Kidman.", "Thus, Nicole Kidman knew at least one scientologist during her marriage.", "So the answer is yes."], "conf_comments": ["(A)\nNicole Kidman", "(A)\nThus, N", "(A)\nNicole Kidman", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6474979838296772, 0.43767347057577916, 0.3475382865575907], "conf": [0.9074271797403244, 0.5988230832147625, 0.3793431042316808], "prob": [[2.0198113389223065e-05, 14], [5.632490737956973e-08, 16], [5.476810512411926e-08, 6]], "generated": ["There is no evidence that Nicole Kidman knows a Scientologist.", "Thus, Nicole Kidman doesn’t know any Scientologists.", "So the answer is no."], "conf_comments": ["(A)\nNicole Kidman", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6720257411389221, 0.4032726764046242, 0.3284949284391501], "conf": [0.9096123607312873, 0.6403125856115648, 0.43086383601687317], "prob": [[0.003692527251807907, 8], [2.318006957985716e-06, 11], [2.1310083881746022e-06, 6]], "generated": ["Nicole Kidman is Scientologist.", "Thus, she probably knows some Scientologists.", "So the answer is yes."], "conf_comments": ["(A)\nThus, N", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 45.09480404853821}
{"index": 48, "question": "Was John Lennon known to be a good friend to Sasha Obama?", "answer": "no", "explanation": "No. John Lennon died in 1980. Sasha Obama was born in 2001.", "generated": [{"finished": true, "length": 4, "score": [0.9138961342342389, 0.8411163039044375, 0.6095919477310469, 0.5034603842387378], "conf": [0.9826865805151772, 0.8634882250293717, 0.6357978314461381, 0.43682029111322596], "prob": [[0.14208357032764793, 12], [0.0882155654067649, 13], [0.00020184228244136837, 18], [0.0001932958606886374, 6]], "generated": ["John Lennon died in 1980.", "Sasha Obama was born in 2001.", "Thus, it was impossible for John Lennon to know Sasha Obama.", "So the answer is no."], "conf_comments": ["(A)\nSasha Obama", "(A)\nThus, John", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.9138961342342389, 0.8411163039044375, 0.4946928946136358, 0.4065946921417111], "conf": [0.9826865805151772, 0.8634882250293717, 0.6143941904971384, 0.42143479340151874], "prob": [[0.14208357032764793, 12], [0.0882155654067649, 13], [0.00013380189906868223, 9], [0.00012208855159417603, 6]], "generated": ["John Lennon died in 1980.", "Sasha Obama was born in 2001.", "Thus, they could not be friends.", "So the answer is no."], "conf_comments": ["(A)\nSasha Obama", "(A)\nThus, John", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.8234577196973004, 0.6136716783049321, 0.5086371115131426], "conf": [0.9547398146363012, 0.7060883153778154, 0.48706872541085283], "prob": [[0.0005379559146131645, 22], [9.869360213508118e-07, 22], [9.628646581034912e-07, 6]], "generated": ["John Lennon died in 1980, which is before Sasha Obama was born.", "Thus, it was impossible for John Lennon to be a good friend of Sasha Obama.", "So the answer is no."], "conf_comments": ["(A)\nThus, John", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.8234577196973004, 0.6090011360637414, 0.49690193824189793], "conf": [0.9547398146363012, 0.7005861221550599, 0.4695308813293553], "prob": [[0.0005379559146131645, 22], [1.5076087923550174e-06, 20], [1.4484514083500295e-06, 6]], "generated": ["John Lennon died in 1980, which is before Sasha Obama was born.", "Thus, John Lennon could not have been a good friend of Sasha Obama.", "So the answer is no."], "conf_comments": ["(A)\nThus, John", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7452350603948356, 0.6623901639208762, 0.4456800656157781, 0.3149386557206308, 0.25878670794006214], "conf": [0.9725981903090123, 0.841010274460586, 0.6964412011089641, 0.4885866529074202, 0.33119641594974614], "prob": [[4.429637625009059e-06, 22], [7.962726779592777e-07, 19], [5.067923507413393e-11, 16], [5.6467374121827946e-14, 20], [5.5147442030311226e-14, 6]], "generated": ["The date of John Lennon’s death is December 8th, 1980.", "Sasha Obama was born on June 10th, 2001.", "This implies that John Lennon died before Sasha Obama was born.", "Thus, John Lennon could not possibly be a good friend to Sasha Obama.", "So the answer is no."], "conf_comments": ["(A)\nSasha Obama", "(A)\nThus, John", "(A)\nThus, John", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 50.84136700630188}
{"index": 49, "question": "Would the tunnels at CERN fit onto the High Speed 1 rails?", "answer": "yes", "explanation": "Yes. High Speed 1 (HS1), legally the Channel Tunnel Rail Link (CTRL), is a 67-mile (108 km) high-speed railway. The CERN collider is contained in a circular tunnel, with a circumference of 26.7 kilometres (16.6 mi). The circumference of a circle is the length of the enclosing boundary.", "generated": [{"finished": true, "length": 4, "score": [0.7219325426670964, 0.5160925077028834, 0.4019877847935058, 0.33230219538916433], "conf": [0.9782376438872801, 0.6885455432365254, 0.4960649489347971, 0.34021337268736473], "prob": [[6.374240903853397e-06, 19], [2.0042538841537604e-08, 18], [6.445363494202769e-10, 20], [6.306880937736187e-10, 6]], "generated": ["The total length of the tunnels at CERN is about 28 km.", "The total length of the High Speed 1 rails is also about 28 km.", "Thus, the tunnels at CERN fit onto the High Speed 1 rails.", "So the answer is yes."], "conf_comments": ["(A)\nThe total length of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6609241460854646, 0.4805743838305179, 0.4161046506979706], "conf": [0.863339401790316, 0.6680973879156586, 0.5031494919475594], "prob": [[8.712382919714354e-11, 34], [1.9973412027118548e-14, 22], [1.9436233021802787e-14, 6]], "generated": ["The length of the tunnels at CERN is almost 27 km and the length of the High Speed 1 rails is around 75 km.", "Thus, the tunnels at CERN cannot be fitted onto the High Speed 1 rails.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7737405033779811, 0.5996431198687954, 0.37884465332611833, 0.2710104708942071, 0.23119869757371247], "conf": [0.9736398498765845, 0.8543310030789185, 0.6462930325794871, 0.48913909406318706, 0.3574760116169652], "prob": [[0.00025672578168533134, 17], [4.196773772049744e-08, 23], [9.097350753105205e-15, 24], [7.938883472937408e-18, 18], [7.742193160512839e-18, 6]], "generated": ["The diameter of the High Speed 1 rails is 1435 mm.", "In comparison, the diameter of the tunnels at CERN is about 3000 mm.", "Thus, the tunnels at CERN is around half the size of the High Speed 1 rails.", "Thus, the tunnels would not fit onto the High Speed 1 rails.", "So the answer is no."], "conf_comments": ["(A)\nThe diameter of the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7737405033779811, 0.5996431198687954, 0.37884465332611833, 0.3022458696493947], "conf": [0.9736398498765845, 0.8543310030789185, 0.6462930325794871, 0.42698872306732133], "prob": [[0.00025672578168533134, 17], [4.196773772049744e-08, 23], [9.097350753105205e-15, 24], [7.274177589213125e-15, 6]], "generated": ["The diameter of the High Speed 1 rails is 1435 mm.", "In comparison, the diameter of the tunnels at CERN is about 3000 mm.", "Thus, the tunnels at CERN is around half the size of the High Speed 1 rails.", "So the answer is no."], "conf_comments": ["(A)\nThe diameter of the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6609241460854646, 0.4555298962243599, 0.38913185068838757], "conf": [0.863339401790316, 0.6587601308455808, 0.48269322378621826], "prob": [[8.712382919714354e-11, 34], [4.1497075266645405e-15, 21], [4.048674438036359e-15, 6]], "generated": ["The length of the tunnels at CERN is almost 27 km and the length of the High Speed 1 rails is around 75 km.", "Thus, the tunnels at CERN does not fit onto High Speed 1 rails.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 51.43607020378113}
{"index": 50, "question": "Could an elephant easily defeat a male macaque?", "answer": "yes", "explanation": "Yes. Male macaques range from 16 to 28 inches tall with a weight between 12.13 to 39.7 pounds. Elephants are between 7 to 11 feet tall and weigh several thousand pounds. Elephants contain large, sharp tusks that can injure or kill other animals.", "generated": [{"finished": true, "length": 3, "score": [0.6544282272433333, 0.43933763331537407, 0.3647716012892599], "conf": [0.9468769215487836, 0.6859628514254235, 0.4751748128893139], "prob": [[1.4997788075921182e-05, 14], [7.548808635689496e-09, 16], [7.332176072254998e-09, 6]], "generated": ["Male elephants are usually larger than male macaques.", "Thus, a male elephant would easily defeat a male macaque.", "So the answer is yes."], "conf_comments": ["(A)\nMale eleph", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6544282272433333, 0.4023313651637028, 0.32680362970610144], "conf": [0.9468769215487836, 0.6731180313630202, 0.44853649834333903], "prob": [[1.4997788075921182e-05, 14], [1.7284466081867574e-10, 18], [1.6287422173659374e-10, 6]], "generated": ["Male elephants are usually larger than male macaques.", "Thus, an elephant could win in a fight against a male macaque.", "So the answer is yes."], "conf_comments": ["(A)\nMale eleph", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6292393908377361, 0.4592979471097304, 0.21784248497913866, 0.17726419495067494], "conf": [0.9624863568677945, 0.8512801728810685, 0.6479208472801647, 0.434642676834224], "prob": [[9.662374869172108e-06, 13], [1.0541701505979971e-09, 18], [2.728004808703698e-20, 20], [2.5230388438651905e-20, 6]], "generated": ["The average male macaque weighs around 8 kg.", "The average male elephant weighs around 4,400 kg.", "Thus, it was difficult for a male macaque to overcome or defeat an elephant.", "So the answer is no."], "conf_comments": ["(A)\nThe average eleph", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.673122953032768, 0.3612699662668691, 0.226240887625238, 0.17290006581924575], "conf": [0.8453795217705332, 0.6580034744771613, 0.43497805902582043, 0.2746668928662158], "prob": [[1.0990455275245593e-06, 22], [3.4960824393850763e-16, 22], [1.7184170378175e-20, 19], [1.0759288163509543e-20, 6]], "generated": ["An elephant is a herbivore while the male macaque is an omnivore.", "Animals which are herbivores are generally more peaceful than animals which are omnivores.", "Thus, it is less likely that a male macaque would attack an elephant.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, an", "(A)\nAn elephant", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6292393908377361, 0.4592979471097304, 0.19224548443386702, 0.09479752206129945, 0.07618983590718731], "conf": [0.9624863568677945, 0.8512801728810685, 0.691156406562378, 0.4617585375694106, 0.30101544716348394], "prob": [[9.662374869172108e-06, 13], [1.0541701505979971e-09, 18], [5.002455733286801e-19, 14], [1.933318109979982e-30, 26], [1.830041172702808e-30, 6]], "generated": ["The average male macaque weighs around 8 kg.", "The average male elephant weighs around 4,400 kg.", "Elephants are known for their strength and their horns.", "Thus, when an elephant strikes a male macaque, it can easily defeat and kill the male macaque.", "So the answer is yes."], "conf_comments": ["(A)\nThe average eleph", "(A)\nThus, the", "(A)\nThus, an", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6292393908377361, 0.4592979471097304, 0.19224548443386702, 0.09479752206129945, 0.04814908167003225], "conf": [0.9624863568677945, 0.8512801728810685, 0.691156406562378, 0.4617585375694106, 0.27693002310558906], "prob": [[9.662374869172108e-06, 13], [1.0541701505979971e-09, 18], [5.002455733286801e-19, 14], [1.933318109979982e-30, 26], [1.2247993524096858e-32, 6]], "generated": ["The average male macaque weighs around 8 kg.", "The average male elephant weighs around 4,400 kg.", "Elephants are known for their strength and their horns.", "Thus, when an elephant strikes a male macaque, it can easily defeat and kill the male macaque.", "So the answer is no."], "conf_comments": ["(A)\nThe average eleph", "(A)\nThus, the", "(A)\nThus, an", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 51.08687973022461}
{"index": 51, "question": "Do the Ubuntu people speak Ubuntu?", "answer": "no", "explanation": "No. Ubuntu is a of free and open-source software used on computers. The Ubuntu people are a tribe that lives in Africa. The Ubuntu people derive their language from Nguni Bantu. As of 2019 only 10% of households in Africa have a computer.", "generated": [{"finished": true, "length": 4, "score": [0.7321347551380895, 0.5106811501438967, 0.37415186938910866, 0.31525732986255445], "conf": [0.9640750767795216, 0.7146576961248732, 0.5298806928398619, 0.3774565713182217], "prob": [[0.016424733911810055, 7], [0.0005656417451634447, 8], [2.23719108906141e-05, 10], [2.1926887655382757e-05, 6]], "generated": ["Ubuntu is an operating system.", "People do not speak operating systems.", "Thus, Ubuntu people do not speak Ubuntu.", "So the answer is no."], "conf_comments": ["(A)\nThe Ubuntu people are", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6368321512248378, 0.47617287668837227, 0.39686218298175524], "conf": [0.953889902864698, 0.7073880878032993, 0.4931055271715864], "prob": [[0.0010676125854728773, 8], [4.774546092419989e-05, 11], [4.674552874418854e-05, 6]], "generated": ["The Ubuntu people speak Xhosa.", "Thus, the Ubuntu people do not speak Ubuntu.", "So the answer is no."], "conf_comments": ["(A)\nUbuntu is a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7321347551380895, 0.5106811501438967, 0.24689997046265647, 0.2040840122530437], "conf": [0.9640750767795216, 0.7146576961248732, 0.5105141736580754, 0.35139105704253076], "prob": [[0.016424733911810055, 7], [0.0005656417451634447, 8], [7.959180707738245e-09, 10], [7.614235651310333e-09, 6]], "generated": ["Ubuntu is an operating system.", "People do not speak operating systems.", "Thus, Ubuntu does not speak any language.", "So the answer is no."], "conf_comments": ["(A)\nThe Ubuntu people are", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6843195453127873, 0.538231340738794, 0.25747567671190114, 0.20296932183071992], "conf": [0.9389398423614203, 0.857629667116678, 0.6030007851670369, 0.3852408239612796], "prob": [[5.3447094775432965e-11, 34], [7.092689336202832e-14, 17], [3.8781263173862183e-23, 19], [3.284473080318792e-23, 6]], "generated": ["Ubuntu means “I-am-what-I-am-because-of-who-we-all-are” in Xhosa and Zulu.", "Xhosa and Zulu are two of the eleven official languages of South Africa.", "Thus it is not surprising that the Ubuntu people speak one of the eleven South African languages.", "So the answer is yes."], "conf_comments": ["(A)\nUbuntu is also", "(A)\nUbuntu is also", "(A)\nThe Ubuntu people are", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6843195453127873, 0.538231340738794, 0.21403015489520008, 0.15644414641751173], "conf": [0.9389398423614203, 0.857629667116678, 0.5629775223339599, 0.32038889971265366], "prob": [[5.3447094775432965e-11, 34], [7.092689336202832e-14, 17], [1.1723938810547692e-33, 32], [8.027260122554477e-34, 6]], "generated": ["Ubuntu means “I-am-what-I-am-because-of-who-we-all-are” in Xhosa and Zulu.", "Xhosa and Zulu are two of the eleven official languages of South Africa.", "Thus, there is the possibility of Ubuntu-speaking South Africans, and perhaps some people in other countries that speak some of the eleven official languages.", "So the answer is yes."], "conf_comments": ["(A)\nUbuntu is also", "(A)\nUbuntu is also", "(A)\nHowever, Ubuntu is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 4, "score": [0.6843195453127873, 0.538231340738794, 0.21403015489520008, 0.11094981407855868], "conf": [0.9389398423614203, 0.857629667116678, 0.5629775223339599, 0.3762312916441685], "prob": [[5.3447094775432965e-11, 34], [7.092689336202832e-14, 17], [1.1723938810547692e-33, 32], [1.2955889099307164e-37, 10]], "generated": ["Ubuntu means “I-am-what-I-am-because-of-who-we-all-are” in Xhosa and Zulu.", "Xhosa and Zulu are two of the eleven official languages of South Africa.", "Thus, there is the possibility of Ubuntu-speaking South Africans, and perhaps some people in other countries that speak some of the eleven official languages.", "Thus, at least some people speak Ubuntu.", "So the answer is yes."], "conf_comments": ["(A)\nUbuntu is also", "(A)\nUbuntu is also", "(A)\nHowever, Ubuntu is", "(A)\nSo the answer is"], "info": [[[0.00011050798975213964, 10], [0.9640825815542831, 6]], 0.6682883005424629, "(A)\nSo the answer is", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 49.69462490081787}
{"index": 52, "question": "Is cycling a high-risk activity for pelvis fractures?", "answer": "no", "explanation": "No. Cycling is a low-impact activity . Stress fractures in a pelvic bone often develop as a result of repetitive, high-impact activity that puts stress on the pelvis, such as long-distance running or ballet.", "generated": [{"finished": true, "length": 4, "score": [0.6516810903718304, 0.30165032228535293, 0.25086948666247644, 0.21302346607806832], "conf": [0.9549614142188898, 0.7653524285859681, 0.5699125908238967, 0.41201099279889103], "prob": [[5.2633981260741065e-06, 15], [2.5608076827802134e-16, 18], [6.298857934199995e-17, 19], [6.200315229281955e-17, 6]], "generated": ["Cycling is the second most common cause of pelvis fracture.", "This indicates that pelvis fracture is a common injury for pelvis cyclists.", "Thus, cycling is a high-risk activity for pelvis fractures.", "So the answer is yes."], "conf_comments": ["(A)\nThe most common cause", "(A)\nThus, cy", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6516810903718304, 0.30165032228535293, 0.22912649780110483, 0.19322276264228894], "conf": [0.9549614142188898, 0.7653524285859681, 0.5744661834579836, 0.4101223849843908], "prob": [[5.2633981260741065e-06, 15], [2.5608076827802134e-16, 18], [1.3279380563663783e-18, 20], [1.297416528539548e-18, 6]], "generated": ["Cycling is the second most common cause of pelvis fracture.", "This indicates that pelvis fracture is a common injury for pelvis cyclists.", "Thus, cycling can be a high-risk activity for pelvis fractures.", "So the answer is yes."], "conf_comments": ["(A)\nThe most common cause", "(A)\nThus, cy", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.4745788074371324, 0.23043592135364543, 0.16945721023520816, 0.1419306535708041], "conf": [0.9187113331954985, 0.6576774012496803, 0.46273810604789517, 0.32549982021299895], "prob": [[4.172912970671024e-11, 17], [1.106713776505353e-22, 24], [9.69590853419378e-25, 18], [9.538704245577928e-25, 6]], "generated": ["The pelvis has many bones and joints, which makes it fragile.", "The bike riding position puts the rider in a high-risk position for pelvis fractures.", "Thus, it is a high-risk activity for pelvis fractures.", "So the answer is yes."], "conf_comments": ["(A)\nCycling is a", "(A)\nThus, cy", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.4951621912870928, 0.23466467861531556, 0.12979155020151134, 0.10213497634861679, 0.08509451115692628], "conf": [0.9472951155743752, 0.6761012216767426, 0.45625020028395863, 0.32508083405079835, 0.22633782921648524], "prob": [[3.4922570129196414e-07, 11], [3.135709727789431e-18, 22], [3.923510384521054e-26, 23], [2.3715472683536036e-27, 20], [2.328958555818821e-27, 6]], "generated": ["Cycling is a popular activity in the modern world.", "Yet, the frequency of pelvis fractures related to cycling has been reported to be low.", "This indicates that cycling is either low-risk or non-risk for pelvis fractures.", "Thus, cycling is not a high-risk activity for pelvis fractures.", "So the answer is no."], "conf_comments": ["(A)\nCycling is a", "(A)\nThus, cy", "(A)\nThus, cy", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6382654665605274, 0.3287895186601004, 0.17405813227728656, 0.13579522884774411], "conf": [0.939492888270312, 0.7681871241503594, 0.43890135832531446, 0.27709983566755914], "prob": [[8.467007637461203e-10, 25], [1.4220802199164712e-19, 20], [2.224773839566924e-26, 22], [1.7862906157768097e-26, 6]], "generated": ["Pelvis fractures account for less than 1% of all fractures, so they are uncommon.", "Cycling is one of the most popular and common activities, so many people engage in it.", "Based on these information, cycling is a high-risk activity for pelvis fractures.", "So the answer is yes."], "conf_comments": ["(A)\nCycling is a", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 54.34602403640747}
{"index": 53, "question": "Is a platypus immune from cholera?", "answer": "yes", "explanation": "Yes. Cholera is a bacteria that damages the small intestines in humans. The intestines are part of the stomach of humans. A platypus does not have a stomach.", "generated": [{"finished": true, "length": 4, "score": [0.5706794891157951, 0.30738658107623557, 0.2468754938804442, 0.20153055834495115], "conf": [0.9626836586103815, 0.6648368633289898, 0.45678964349331164, 0.3049842862721849], "prob": [[2.5716566767746534e-07, 14], [5.589206364214033e-16, 23], [2.035682793216692e-16, 16], [2.012324118580132e-16, 6]], "generated": ["Cholera is a viral infection transmitted via water.", "A platypus is aquatic, so it could contract cholera by drinking contaminated water.", "Thus, a platypus is not immune from cholera.", "So the answer is no."], "conf_comments": ["(A)\nPlatypuses", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6721323897964044, 0.42510590183392666, 0.2298711781707006, 0.17791358664383414], "conf": [0.9697361947521321, 0.7388207823441411, 0.4143929281705402, 0.255725657066966], "prob": [[5.030752461981685e-08, 22], [3.1960957781382773e-12, 15], [1.5441215643027954e-23, 40], [1.2918283143939559e-23, 6]], "generated": ["Cholera is a type of bacteria that causes severe gastrointestinal infections.", "A platypus is a duck-billed mammal.", "Thus, it is unlikely for a platypus to be immune from cholera because there are no known cases of duck-billed mammals catching cholera.", "So the answer is no."], "conf_comments": ["(A)\nPlatypuses", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5706794891157951, 0.30738658107623557, 0.20565839723165533, 0.1650441856998258], "conf": [0.9626836586103815, 0.6648368633289898, 0.43147635983895605, 0.2807933894797552], "prob": [[2.5716566767746534e-07, 14], [5.589206364214033e-16, 23], [1.4663879121826395e-18, 16], [1.377574268351948e-18, 6]], "generated": ["Cholera is a viral infection transmitted via water.", "A platypus is aquatic, so it could contract cholera by drinking contaminated water.", "Therefore, a platypus is not immune to cholera.", "So the answer is no."], "conf_comments": ["(A)\nPlatypuses", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7205923288938832, 0.3084184072793124, 0.1632599396012601, 0.1202455387692265, 0.10122160574845641], "conf": [0.9677813340144676, 0.8177934986767161, 0.597149405620828, 0.4389692767693355, 0.3123873247654526], "prob": [[6.037353897362465e-07, 23], [1.5195643872986237e-23, 25], [2.8511303147113557e-37, 33], [2.2052511038577927e-39, 16], [2.149578827602584e-39, 6]], "generated": ["Cholera is an infectious disease that is caused by ingesting cholera bacteria.", "According to a veterinary article, there is no antibiotic currently registered for the treatment of cholera.", "Since there is no treatment, the only way to prevent cholera for this platypus is to prevent the ingestion of cholera bacteria.", "Thus, a platypus is not immune from cholera.", "So the answer is no."], "conf_comments": ["(A)\nThe platypus", "(A)\nThus, a", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5620168711963568, 0.3080855662804412, 0.16983971335651185, 0.13490886958088252], "conf": [0.9458081506194591, 0.8025596261468522, 0.5038028741092088, 0.3205571935775422], "prob": [[4.607637415767125e-14, 28], [3.5361743309429875e-22, 18], [6.649983345376691e-27, 15], [6.323627795844851e-27, 6]], "generated": ["Cholera usually spread by ingestion of contaminated food or water or by ingestion of contaminated fecal matter.", "The duckbilled platypus is a carnivorous mammal.", "Thus, a platypus is vulnerable to cholera.", "So the answer is no."], "conf_comments": ["(A)\nPlatypuses", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7205923288938832, 0.3084184072793124, 0.1632599396012601, 0.1202455387692265, 0.039855731166318395], "conf": [0.9677813340144676, 0.8177934986767161, 0.597149405620828, 0.4389692767693355, 0.290562426983723], "prob": [[6.037353897362465e-07, 23], [1.5195643872986237e-23, 25], [2.8511303147113557e-37, 33], [2.2052511038577927e-39, 16], [4.609860381310592e-44, 6]], "generated": ["Cholera is an infectious disease that is caused by ingesting cholera bacteria.", "According to a veterinary article, there is no antibiotic currently registered for the treatment of cholera.", "Since there is no treatment, the only way to prevent cholera for this platypus is to prevent the ingestion of cholera bacteria.", "Thus, a platypus is not immune from cholera.", "So the answer is no.\\"], "conf_comments": ["(A)\nThe platypus", "(A)\nThus, a", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.68645453453064}
{"index": 54, "question": "Would a moose hypothetically be too much for a minotaur to devour whole?", "answer": "yes", "explanation": "Yes. A minotaur was a mythological beast with the head of a bull and body of a human. Human stomachs process about three to four pounds of food a day. A moose can weigh up to 1500 pounds. Bulls can consume around 33 pounds of food a day.", "generated": [{"finished": true, "length": 6, "score": [0.8129884280262988, 0.4375730520120572, 0.22137769861593345, 0.14445135861842784, 0.10907505258158504, 0.07927189945467236], "conf": [0.9588633768593304, 0.8022464732703939, 0.7062038784498331, 0.593326852222313, 0.4883427960310743, 0.3321015104368402], "prob": [[0.00019207724602261065, 23], [1.9743908041554375e-10, 13], [4.913411560018288e-34, 44], [1.510206438370068e-45, 39], [8.853434444106508e-48, 14], [1.9433963651912237e-48, 6]], "generated": ["A minotaur is a mythical creature with the body of a man and the head of a bull.", "A bull is a term for an adult male domestic cattle.", "According to the USDA, a bull weighs around 900-1,900 lbs, but more specifically, a bull around 1,490 lbs.", "In comparison, a moose weighs 880-1,600 lbs, and more specifically, a moose about 1,140 lbs.", "Thus, a moose is heavier than a bull.", "So the answer is yes."], "conf_comments": ["(A)\nA moose is", "(A)\nA moose is", "(A)\nA moose is", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.8129884280262988, 0.4375730520120572, 0.22137769861593345, 0.14445135861842784, 0.10907505258158504, 0.0745889444631239], "conf": [0.9588633768593304, 0.8022464732703939, 0.7062038784498331, 0.593326852222313, 0.4883427960310743, 0.307737560619203], "prob": [[0.00019207724602261065, 23], [1.9743908041554375e-10, 13], [4.913411560018288e-34, 44], [1.510206438370068e-45, 39], [8.853434444106508e-48, 14], [1.4783110083707523e-48, 6]], "generated": ["A minotaur is a mythical creature with the body of a man and the head of a bull.", "A bull is a term for an adult male domestic cattle.", "According to the USDA, a bull weighs around 900-1,900 lbs, but more specifically, a bull around 1,490 lbs.", "In comparison, a moose weighs 880-1,600 lbs, and more specifically, a moose about 1,140 lbs.", "Thus, a moose is heavier than a bull.", "So the answer is no."], "conf_comments": ["(A)\nA moose is", "(A)\nA moose is", "(A)\nA moose is", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.8129884280262988, 0.4375730520120572, 0.22137769861593345, 0.14372966537738738, 0.08735437466095074, 0.07079091047397126], "conf": [0.9588633768593304, 0.8022464732703939, 0.7062038784498331, 0.5357958504235293, 0.35023364658080725, 0.2332970687540663], "prob": [[0.00019207724602261065, 23], [1.9743908041554375e-10, 13], [4.913411560018288e-34, 44], [1.1912741846227926e-39, 22], [8.023735178355375e-49, 37], [7.368552061954903e-49, 6]], "generated": ["A minotaur is a mythical creature with the body of a man and the head of a bull.", "A bull is a term for an adult male domestic cattle.", "According to the USDA, a bull weighs around 900-1,900 lbs, but more specifically, a bull around 1,490 lbs.", "On the other hand, the largest moose ever recorded weighed 1,800 lbs.", "Thus, it is impossible for a moose weighing 1,800 lbs to be too much for a bull minotaur to devour whole.", "So the answer is no."], "conf_comments": ["(A)\nA moose is", "(A)\nA moose is", "(A)\nA moose is", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8129884280262988, 0.4850282026520784, 0.21393124439097824, 0.12160144393158616, 0.09487938749040864], "conf": [0.9588633768593304, 0.7123730663481382, 0.5516478816848345, 0.3580082296007353, 0.22207250937222947], "prob": [[0.00019207724602261065, 23], [3.734051842311775e-11, 21], [3.741796365519501e-20, 15], [1.320820132139796e-25, 18], [1.1804038709332616e-25, 6]], "generated": ["A minotaur is a mythical creature with the body of a man and the head of a bull.", "A moose is a real animal with the body of bull and the head of a deer.", "Bulls and moose both belong to the same family and genus.", "Thus, a moose won’t be too much for a minotaur.", "So the answer is no."], "conf_comments": ["(A)\nA moose is", "(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 9, "score": [0.8129884280262988, 0.4375730520120572, 0.22137769861593345, 0.14445135861842784, 0.08719751801908791, 0.04238942213351153, 0.020987134140018272, 0.00720011245290206, 0.005422696267730229], "conf": [0.9588633768593304, 0.8022464732703939, 0.7062038784498331, 0.593326852222313, 0.44476755093669507, 0.3348417573239157, 0.25497036350292795, 0.16023762771248654, 0.10397530112346001], "prob": [[0.00019207724602261065, 23], [1.9743908041554375e-10, 13], [4.913411560018288e-34, 44], [1.510206438370068e-45, 39], [7.964683053008023e-57, 36], [2.255131723977909e-64, 15], [8.676109925024115e-70, 11], [4.5790153047200027e-85, 21], [2.0431018561830792e-85, 6]], "generated": ["A minotaur is a mythical creature with the body of a man and the head of a bull.", "A bull is a term for an adult male domestic cattle.", "According to the USDA, a bull weighs around 900-1,900 lbs, but more specifically, a bull around 1,490 lbs.", "In comparison, a moose weighs 880-1,600 lbs, and more specifically, a moose about 1,140 lbs.", "Thus, a bull and a moose are within 300 lbs of each other, and both are close to 1,000 lbs.", "Additionally, both moose and bull are herbivorous.", "Herbivores are animals that eats plants.", "Thus, a minotaur would not have any advantages or ability devouring a moose.", "So the answer is no."], "conf_comments": ["(A)\nA moose is", "(A)\nA moose is", "(A)\nA moose is", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": [], "9": []}, "run_time": 122.16494560241699}
{"index": 55, "question": "Are there bones in an anchovy pizza?", "answer": "yes", "explanation": "Yes. Anchovies used on pizza are typically packed whole in oil or water.  Anchovies on pizza are not usually cut or filleted in any way.", "generated": [{"finished": true, "length": 4, "score": [0.6268590744243785, 0.47093675623909137, 0.40928975046875127, 0.352472384574431], "conf": [0.926360045270089, 0.7830967672508041, 0.6288967944699997, 0.4674265616406555], "prob": [[6.402069968033309e-09, 22], [7.522590201083134e-11, 11], [2.998862707167782e-11, 15], [2.959942504306213e-11, 6]], "generated": ["Anchovy pizza contains cheese, tomato paste, oregano, and anchovies.", "There are no bones in these ingredients.", "Thus, there are no bones in an anchovy pizza.", "So the answer is no."], "conf_comments": ["(A)\nBones are not", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6944409628895918, 0.44752478644175175, 0.37079870482062294, 0.3139277747166525], "conf": [0.9598842146811158, 0.8060014952914948, 0.6103845733586181, 0.43843988971729814], "prob": [[0.004058957382772865, 8], [2.9385981087422847e-05, 7], [7.436756847275661e-06, 14], [7.342482374754087e-06, 6]], "generated": ["Anchovies have bones.", "Pizza has anchovies.", "Thus, there are bones in an anchovy pizza.", "So the answer is yes."], "conf_comments": ["(A)\nPizza is made", "(A)\nThus, there", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6268590744243785, 0.37149563881459735, 0.28477958335313985, 0.24512552123102252], "conf": [0.926360045270089, 0.751977620258084, 0.5927010389765542, 0.44037124006469114], "prob": [[6.402069968033309e-09, 22], [6.330755232957327e-17, 22], [7.738340704853576e-19, 15], [7.608594651354831e-19, 6]], "generated": ["Anchovy pizza contains cheese, tomato paste, oregano, and anchovies.", "None of these ingredients possess bones, so an anchovy pizza does not contain bones.", "Thus, there are no bones in an anchovy pizza.", "So the answer is no."], "conf_comments": ["(A)\nBones are not", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6268590744243785, 0.47093675623909137, 0.2693034929383933, 0.2245362719655685], "conf": [0.926360045270089, 0.7830967672508041, 0.568663033267653, 0.399646698250321], "prob": [[6.402069968033309e-09, 22], [7.522590201083134e-11, 11], [9.687111994905873e-17, 17], [9.074050333056548e-17, 6]], "generated": ["Anchovy pizza contains cheese, tomato paste, oregano, and anchovies.", "There are no bones in these ingredients.", "Therefore, an anchovy pizza will not have any bones in it.", "So the answer is no."], "conf_comments": ["(A)\nBones are not", "(A)\nThus, there", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6944409628895918, 0.44752478644175175, 0.2291448299796562, 0.18730273634758463], "conf": [0.9598842146811158, 0.8060014952914948, 0.5856202537192929, 0.3972084563644553], "prob": [[0.004058957382772865, 8], [2.9385981087422847e-05, 7], [6.725516654135919e-12, 15], [6.144972972666497e-12, 6]], "generated": ["Anchovies have bones.", "Pizza has anchovies.", "Thus, there is at least one bone for each pizza.", "So the answer is yes."], "conf_comments": ["(A)\nPizza is made", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 4, "score": [0.6944409628895918, 0.44752478644175175, 0.2291448299796562, 0.1579680166656193], "conf": [0.9598842146811158, 0.8060014952914948, 0.5856202537192929, 0.41296542065488256], "prob": [[0.004058957382772865, 8], [2.9385981087422847e-05, 7], [6.725516654135919e-12, 15], [2.681724309859147e-14, 14]], "generated": ["Anchovies have bones.", "Pizza has anchovies.", "Thus, there is at least one bone for each pizza.", "Therefore, there are bones in an anchovy pizza.", "So the answer is yes."], "conf_comments": ["(A)\nPizza is made", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": [[[0.00398738780642227, 14], [0.9771362339011307, 6]], 0.7051761240020747, "(A)\n# Is the above", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 54.0692355632782}
{"index": 56, "question": "Can a Bengal cat survive eating only pancakes?", "answer": "no", "explanation": "No. Bengal cats are carnivores. Pancakes contain no meat. Carnivores eat only meat to survive. ", "generated": [{"finished": true, "length": 4, "score": [0.6812689921621102, 0.42236804584520704, 0.30677699450721346, 0.260516629025304], "conf": [0.9500589895379545, 0.7469175121244094, 0.5511059332987325, 0.39862583429309856], "prob": [[0.0007742241869010401, 10], [1.235582855515416e-06, 9], [5.763525860078741e-09, 16], [5.660542540516771e-09, 6]], "generated": ["Bengal cats are meat eaters.", "There is no meat in pancakes.", "Thus, a Bengal cat cannot survive on only pancakes.", "So the answer is no."], "conf_comments": ["(A)\nPancakes are", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6812689921621102, 0.3558186231594579, 0.2834898936017964, 0.24264893586475747], "conf": [0.9500589895379545, 0.7778327316687811, 0.5680593637273451, 0.4175528363278875], "prob": [[0.0007742241869010401, 10], [4.34906414144708e-09, 11], [4.011169147235437e-10, 17], [3.932370299931732e-10, 6]], "generated": ["Bengal cats are meat eaters.", "Pancakes, obviously, do not contain meat.", "Thus, a Bengal cat cannot survive eating only pancakes.", "So the answer is no."], "conf_comments": ["(A)\nPancakes are", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5888357765181116, 0.35639634956823485, 0.25950635866943983, 0.21743972067643597], "conf": [0.9429661480665839, 0.8227035512186966, 0.5948201931630104, 0.4191981120304479], "prob": [[9.220793121041789e-14, 30], [1.162704302584665e-18, 13], [8.129506588548385e-21, 16], [7.946081836971144e-21, 6]], "generated": ["A Bengal cat must get at least three of the main nutrients: carbohydrates, fats, and proteins.", "A pancake has all three of those nutrients.", "Thus, a Bengal cat could survive eating pancakes.", "So the answer is yes."], "conf_comments": ["(A)\nPancakes are", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6812689921621102, 0.3558186231594579, 0.25487500069458346, 0.2145948330560696], "conf": [0.9500589895379545, 0.7778327316687811, 0.540297352002609, 0.38752657844466654], "prob": [[0.0007742241869010401, 10], [4.34906414144708e-09, 11], [2.523796947017844e-11, 17], [2.3525926379522255e-11, 6]], "generated": ["Bengal cats are meat eaters.", "Pancakes, obviously, do not contain meat.", "Therefore, a Bengal cat cannot survive eating only pancakes.", "So the answer is no."], "conf_comments": ["(A)\nPancakes are", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6812689921621102, 0.42236804584520704, 0.21999469790636422, 0.18131972458197793], "conf": [0.9500589895379545, 0.7469175121244094, 0.5429654093542388, 0.37062692678721626], "prob": [[0.0007742241869010401, 10], [1.235582855515416e-06, 9], [1.7642342037867181e-16, 23], [1.7138051537544133e-16, 6]], "generated": ["Bengal cats are meat eaters.", "There is no meat in pancakes.", "Thus, a Bengal cannot eat only pancakes and will die of nutritional deficiency.", "So the answer is no."], "conf_comments": ["(A)\nPancakes are", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 52.88675284385681}
{"index": 57, "question": "Can the original name of the zucchini be typed on the top row of a QWERTY keyboard?", "answer": "no", "explanation": "No. The original name for the zucchini in Mexican language or Nahuatl is ayokonetl. The top row of a QWERTY keyboard contains the keys q, w, e, r, t, y, u, i , o, and p.", "generated": [{"finished": true, "length": 4, "score": [0.5683233591823333, 0.39144664101088694, 0.32277937849709937, 0.2636713116110965], "conf": [0.9529196957328614, 0.8202039300064112, 0.6311514925456568, 0.4227096294763175], "prob": [[1.1818421963247344e-09, 19], [7.91281846694506e-15, 20], [2.8003146341030374e-16, 27], [2.7393078732861267e-16, 6]], "generated": ["The zucchini (plural: zucchini) has 7 letters.", "The top row of a QWERTY keyboard has 7 letters (including the spacebar).", "Thus, the original name of the zucchini could be typed on the top row of a QWERTY keyboard.", "So the answer is yes."], "conf_comments": ["(A)\nThe top row of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8270599831819898, 0.4431637623125124, 0.3345120306923991, 0.2619939078848981], "conf": [0.9511091308848587, 0.7994469250338122, 0.4848963830162561, 0.2981302075206593], "prob": [[0.009904015617195356, 14], [2.1537277626988197e-14, 25], [3.9788222670471286e-15, 27], [3.924331858892119e-15, 6]], "generated": ["The original name of the zucchini is Italian squash.", "Italian squash only has one key on the top row of a QWERTY keyboard, which is “J”.", "Thus, the original name of the zucchini cannot be typed on the top row of a QWERTY keyboard.", "So the answer is no."], "conf_comments": ["(A)\nThe top row of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6596635239957248, 0.37293974879158825, 0.2657174192765, 0.21505215850376952], "conf": [0.9427870427388152, 0.7687207798316612, 0.5199228266669584, 0.3416091443710632], "prob": [[4.1724907354682995e-07, 19], [8.438764293233532e-13, 14], [6.472765080297456e-16, 25], [6.35377150132029e-16, 6]], "generated": ["Zucchini is a variation of the Italian word zucca, which means gourd.", "The top row of a QWERTY keyboard is qwert.", "Thus, the name of zucchini cannot be typed on the top row of a QWERTY keyboard.", "So the answer is no."], "conf_comments": ["(A)\nThe top row of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5903747587394735, 0.37029223404277767, 0.2926385839306914], "conf": [0.7676457030411711, 0.44209923463398965, 0.27755050899935607], "prob": [[1.2146837252681851e-09, 26], [8.837529116378976e-14, 25], [8.567242492848364e-14, 6]], "generated": ["The top row of a QWERTY keyboard is the letter row, while the zucchini is a vegetable.", "Thus, it is impossible to type the zucchini’s original name on a QWERTY keyboard.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6596635239957248, 0.37293974879158825, 0.21541250564651662, 0.17138437481664917], "conf": [0.9427870427388152, 0.7687207798316612, 0.5443405558195035, 0.34636015597479536], "prob": [[4.1724907354682995e-07, 19], [8.438764293233532e-13, 14], [6.81220733977091e-25, 37], [6.603104938999025e-25, 6]], "generated": ["Zucchini is a variation of the Italian word zucca, which means gourd.", "The top row of a QWERTY keyboard is qwert.", "Thus, the original name of the zucchini cannot be typed on the top row of a QWERTY keyboard (except for a keyboard that has the Italian alphabet).", "So the answer is no."], "conf_comments": ["(A)\nThe top row of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 50.53802752494812}
{"index": 58, "question": "Does The Hague border multiple bodies of water?", "answer": "no", "explanation": "No. The Hague is in the Netherlands.  The Hague is in the Western part of the Netherlands.  The Netherlands borders the North Sea to its west. ", "generated": [{"finished": true, "length": 4, "score": [0.8094948136785844, 0.421740316451949, 0.3029570652874678, 0.26614586225088616], "conf": [0.9629302844308221, 0.8045587870696923, 0.6834736077984412, 0.5289558018967233], "prob": [[0.03129678028295062, 9], [7.022221007617621e-17, 30], [1.0766586475459181e-19, 13], [1.0586583272621235e-19, 6]], "generated": ["The Hague is located in the Netherlands.", "According to Wikipedia, the Kingdom of the Netherlands borders Germany, Belgium and the North Sea (which is an inlet of the Atlantic Ocean).", "Thus, The Hague also borders multiple bodies of water.", "So the answer is yes."], "conf_comments": ["(A)\nThe Netherlands is a", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6417049033357414, 0.31158442238019923, 0.2653959736426857, 0.2332910347907072], "conf": [0.9582657285850928, 0.8196435453497924, 0.6925078819449612, 0.5360346257638975], "prob": [[3.658048533487402e-09, 23], [9.44514590075256e-29, 35], [1.5179537050933914e-29, 12], [1.5020846948093475e-29, 6]], "generated": ["The Hague, also referred to as Den Haag in Dutch, is a coastal city in the Netherlands.", "Since it is a coastal city, it is bordered by several bodies of water: North Sea, The Hague Channel, and the Wester Scheveninge Lake.", "Thus, The Hague borders multiple bodies of water.", "So the answer is yes."], "conf_comments": ["(A)\nIt is located on", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8094948136785844, 0.421740316451949, 0.24541427456162607, 0.2173620684281844], "conf": [0.9629302844308221, 0.8045587870696923, 0.7001009917634461, 0.5509205663590266], "prob": [[0.03129678028295062, 9], [7.022221007617621e-17, 30], [3.295026159784337e-22, 13], [3.233675018848908e-22, 6]], "generated": ["The Hague is located in the Netherlands.", "According to Wikipedia, the Kingdom of the Netherlands borders Germany, Belgium and the North Sea (which is an inlet of the Atlantic Ocean).", "Thus, there are multiple bodies of water that border it.", "So the answer is yes."], "conf_comments": ["(A)\nThe Netherlands is a", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6417049033357414, 0.31158442238019923, 0.22465118405389506, 0.19630840335606545], "conf": [0.9582657285850928, 0.8196435453497924, 0.6834360814724997, 0.5237532875234697], "prob": [[3.658048533487402e-09, 23], [9.44514590075256e-29, 35], [1.1920020284694095e-32, 19], [1.1664496273304833e-32, 6]], "generated": ["The Hague, also referred to as Den Haag in Dutch, is a coastal city in the Netherlands.", "Since it is a coastal city, it is bordered by several bodies of water: North Sea, The Hague Channel, and the Wester Scheveninge Lake.", "Thus, it is true that The Hague is bordered by multiple bodies of water.", "So the answer is yes."], "conf_comments": ["(A)\nIt is located on", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8094948136785844, 0.40023816075634877, 0.2447874617779336, 0.1952658310026441], "conf": [0.9629302844308221, 0.8527880839784411, 0.5503801197758235, 0.3526511605573703], "prob": [[0.03129678028295062, 9], [4.34148949540803e-15, 23], [7.039705844476891e-19, 16], [6.75311362686936e-19, 6]], "generated": ["The Hague is located in the Netherlands.", "The Netherlands has about 4,154km of coastline according to the CIA world factbook.", "Thus, The Hague is not bordered by multiple bodies of water.", "So the answer is no."], "conf_comments": ["(A)\nThe Netherlands is a", "(A)\nThus, The", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 52.59842920303345}
{"index": 59, "question": "Do people in middle school usually get breast exams?", "answer": "no", "explanation": "No. Women should begin giving themselves breast exams at the age of 20. Middle school students are usually preteens or young teens.", "generated": [{"finished": true, "length": 3, "score": [0.5800077770779052, 0.42598217718853715, 0.36717239354929326], "conf": [0.9475564007825376, 0.7864601909208494, 0.5865536604855442], "prob": [[2.2893641194922978e-17, 37], [3.5675339228177676e-20, 15], [3.4859823343561845e-20, 6]], "generated": ["According to the American Cancer Society, breast cancer is rare in teenagers and breast exams are often not recommended until the age of 20 in the United States.", "Thus, people in middle school would not usually get breast exams.", "So the answer is no."], "conf_comments": ["(A)\nThus, people", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6228573280229049, 0.42975861778210017, 0.3049426316205515, 0.26008101171312337], "conf": [0.9677012176967128, 0.8844142496726033, 0.6991855926630475, 0.5101176341077501], "prob": [[2.7701428715740784e-06, 14], [1.501629776705937e-08, 8], [6.686012618518056e-11, 12], [6.567287353441597e-11, 6]], "generated": ["People usually get breast examinations during adulthood.", "Middle schoolers are not adults.", "Thus, they usually do not get breast exams.", "So the answer is no."], "conf_comments": ["(A)\nMiddle school is for", "(A)\nThus, people", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5800077770779052, 0.37162380024892916, 0.29960153000140366, 0.2494186187557283], "conf": [0.9475564007825376, 0.7946274660086774, 0.5974067932934498, 0.41964783519367616], "prob": [[2.2893641194922978e-17, 37], [2.4900572675299273e-22, 16], [2.804308374729867e-23, 15], [2.58675264947713e-23, 6]], "generated": ["According to the American Cancer Society, breast cancer is rare in teenagers and breast exams are often not recommended until the age of 20 in the United States.", "This can indicate that most people in middle school would not get breast exams.", "Thus, people in middle school usually do not get breast exams.", "So the answer is no."], "conf_comments": ["(A)\nThus, people", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6228573280229049, 0.42975861778210017, 0.3016114594988521, 0.24786728617637793], "conf": [0.9677012176967128, 0.8844142496726033, 0.629219237077995, 0.4267157674407088], "prob": [[2.7701428715740784e-06, 14], [1.501629776705937e-08, 8], [1.2602764748744737e-10, 13], [1.2294252267794752e-10, 6]], "generated": ["People usually get breast examinations during adulthood.", "Middle schoolers are not adults.", "Thus, people in middle school rarely get breast exams.", "So the answer is no."], "conf_comments": ["(A)\nMiddle school is for", "(A)\nThus, people", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6093571355407403, 0.38091923911092823, 0.2906554144669153], "conf": [0.9487857061287526, 0.6565139337261867, 0.4199534219685011], "prob": [[2.3433962774633104e-13, 31], [1.3926639356227682e-16, 13], [7.918617832399954e-17, 6]], "generated": ["The American Academy of Pediatrics (AAP) suggests that teens between 13-18 years of age should get breast exams.", "Thus, people in middle school might get breast exams.", "So the answer is yes."], "conf_comments": ["(A)\nThe average age of", "(A)\nHowever, the A", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 47.06774091720581}
{"index": 60, "question": "Did people in Korea under Japanese Rule watch a lot of Iron Chef?", "answer": "no", "explanation": "No. The first televisions were sold in 1946. Iron Chef started airing in 1993.", "generated": [{"finished": true, "length": 4, "score": [0.8683299333513347, 0.6518866436952344, 0.459638941558161, 0.3849869933512429], "conf": [0.9796698747947671, 0.8899481573426801, 0.6494128716262667, 0.4577566230659414], "prob": [[0.005318581590978038, 20], [6.659646199884954e-06, 14], [3.091126593002917e-09, 20], [3.004590029483364e-09, 6]], "generated": ["Korea was under Japanese rule from 1910 to 1945.", "Iron Chef began airing in 1992.", "Thus, it was impossible for people under Japanese rule to watch a lot of Iron Chef.", "So the answer is no."], "conf_comments": ["(A)\nIron Chef", "(A)\nThus, people", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8138282232246259, 0.6091205057382244, 0.38354586766123466, 0.3153226062870734], "conf": [0.9757630362230002, 0.781824942315454, 0.5452368976801988, 0.37075845497654436], "prob": [[0.0004309457813540447, 20], [1.6409295902133498e-07, 22], [1.1611587309642593e-12, 21], [1.1197220557355738e-12, 6]], "generated": ["Iron Chef aired in Japan from 1993-1999.", "Korea under Japanese Rule began in 1910 and ended in 1945.", "Thus, it was not possible for the people of Korea under Japanese Rule to watch Iron Chef.", "So the answer is no."], "conf_comments": ["(A)\nKorea was", "(A)\nThus, people", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7596007781875084, 0.6178907572728543, 0.34727878451796357, 0.2830336285173671], "conf": [0.9718344327101133, 0.8384168721095795, 0.58632526439601, 0.3944711380665538], "prob": [[1.0440647474487118e-05, 22], [3.9738335778892276e-08, 21], [4.579977936655868e-16, 23], [4.2415156290950797e-16, 6]], "generated": ["The Iron Chef TV show ran from 1993 to 1999 in Japan.", "Korea was colonized by Japan from 1910 to 1945.", "Thus, it makes sense that people in Korea under Japanese rule did not watch any episodes of Iron Chef.", "So the answer is no."], "conf_comments": ["(A)\nKorea was", "(A)\nThus, people", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.689710932767088, 0.44142955438053155, 0.3709916762512721], "conf": [0.923500151745339, 0.6783510266535872, 0.48216435494163223], "prob": [[1.1716953968807379e-09, 31], [5.716405596582615e-14, 17], [5.5044313495448217e-14, 6]], "generated": ["Iron Chef debuted in 1993, while Japan officially occupied Korea from 1910 until 1945.", "Thus, it was impossible for people in Korea to have watched Iron Chef.", "So the answer is no."], "conf_comments": ["(A)\nThus, people", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8138282232246259, 0.6091205057382244, 0.3521164847013468, 0.21106706221438998, 0.17512060019869016], "conf": [0.9757630362230002, 0.781824942315454, 0.5547846657609367, 0.3886495016696606, 0.2696922203150177], "prob": [[0.0004309457813540447, 20], [1.6409295902133498e-07, 22], [4.329211606834473e-12, 14], [6.872263424407666e-18, 20], [6.549968888634392e-18, 6]], "generated": ["Iron Chef aired in Japan from 1993-1999.", "Korea under Japanese Rule began in 1910 and ended in 1945.", "Thus, there was no overlap between these two periods of time.", "Therefore, it was impossible to watch Iron Chef if you were in Korea under Japanese Rule.", "So the answer is no."], "conf_comments": ["(A)\nKorea was", "(A)\nThus, people", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 53.32073926925659}
{"index": 61, "question": "Would  bald eagle deliver an urgent message before B-52?", "answer": "no", "explanation": "No. A bald eagle can travel up to 99 MPH. The B-52 is a US air bomber that can travel up to 650 MPH.", "generated": [{"finished": true, "length": 3, "score": [0.5815157075534011, 0.4535065601795025, 0.3799427931578593], "conf": [0.8308894166394137, 0.6035829954802154, 0.42564859221630874], "prob": [[4.002637565224279e-17, 42], [9.598565040701221e-19, 21], [9.331183484557677e-19, 6]], "generated": ["Bald eagles can fly at speeds of up to 75 km/h, which is slower than B-52 which have a max speed of 964 km/h.", "Thus, a bald eagle could not deliver an urgent message before a B-52.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5815157075534011, 0.42424662752628417, 0.35151351782785983], "conf": [0.8308894166394137, 0.577515348842612, 0.39817530278356483], "prob": [[4.002637565224279e-17, 42], [2.5126068363472504e-19, 19], [2.448744069926039e-19, 6]], "generated": ["Bald eagles can fly at speeds of up to 75 km/h, which is slower than B-52 which have a max speed of 964 km/h.", "Thus, bald eagles cannot deliver an urgent message before B-52.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6450841001133483, 0.4061547575431618, 0.2726485554390305, 0.22260435611952262], "conf": [0.9196621274150972, 0.7913231002028064, 0.5850219003980273, 0.39312376429819496], "prob": [[1.3972507212352147e-06, 17], [1.0443459238893539e-19, 39], [7.227117986675705e-25, 24], [6.88632629492936e-25, 6]], "generated": ["A bald eagle would deliver a message more quickly than a B-52.", "Bald eagles can fly at speeds of up to 27 mph, which is faster than the average speed of a B-52 of 16 mph.", "Thus, if the message is urgent, the bald eagle would deliver it before the B-52.", "So the answer is yes."], "conf_comments": ["(A)\nA bald eagle", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6658087293057551, 0.3940736289580305, 0.2559330288463119, 0.2060743734769657], "conf": [0.9644086708690509, 0.7178888171533206, 0.42251611704248876, 0.27667752048729766], "prob": [[4.089707255965838e-05, 13], [2.571612414117344e-12, 22], [6.20871001981246e-16, 25], [5.847761171738258e-16, 6]], "generated": ["Bald eagles can fly 30 mph.", "On the other hand, a B-52 reaches a maximum speed of 600 mph.", "Thus, it is highly unlikely for a bald eagle to deliver an urgent message before a B-52.", "So the answer is no."], "conf_comments": ["(A)\nB-52", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6658087293057551, 0.4394491173783933, 0.24354983715976028, 0.19722881203294493], "conf": [0.9644086708690509, 0.6880918259780606, 0.4696777315819259, 0.31094493761227726], "prob": [[4.089707255965838e-05, 13], [4.092166134674972e-08, 14], [1.0540150022492089e-14, 19], [9.957168386986753e-15, 6]], "generated": ["Bald eagles can fly 30 mph.", "The B-52 can fly 500 mph.", "Thus, the bald eagle would not be able to deliver the urgent message before.", "So the answer is no."], "conf_comments": ["(A)\nB-52", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": false, "length": 4, "score": [0.6658087293057551, 0.4394491173783933, 0.24354983715976028, 0.054832491307189375], "conf": [0.9644086708690509, 0.6880918259780606, 0.4696777315819259, 0.27698354308698986], "prob": [[4.089707255965838e-05, 13], [4.092166134674972e-08, 14], [1.0540150022492089e-14, 19], [1.472476410319975e-28, 13]], "generated": ["Bald eagles can fly 30 mph.", "The B-52 can fly 500 mph.", "Thus, the bald eagle would not be able to deliver the urgent message before.", "There are also 5-6 hours of difference in time.", "So the answer is likely no."], "conf_comments": ["(A)\nB-52", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[1.3970165578078044e-14, 13], [0.00014634855383150884, 7]], 0.5897310527243416, "(A)\nSo the answer is", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 48.22547101974487}
{"index": 62, "question": "Does Snoopy look like Chance from Homeward Bound?", "answer": "no", "explanation": "No. Chance from Homeward Bound is a golden retriever.  Snoopy is black and white. Golden Retrievers are yellow in color.", "generated": [{"finished": true, "length": 4, "score": [0.6498473838192669, 0.5279757212116593, 0.4043788254282775, 0.33378169150309894], "conf": [0.9658840407255251, 0.8441296789428142, 0.5953340051352782, 0.4073161875363471], "prob": [[4.879475183466139e-05, 12], [5.168219038748714e-06, 8], [2.712058994539516e-07, 16], [2.6446153105209657e-07, 6]], "generated": ["Chance from Homeward Bound is a dog character.", "Snoopy is a dog character.", "Thus, Snoopy looks like Chance from Homeward Bound.", "So the answer is yes."], "conf_comments": ["(A)\nSnoopy is", "(A)\nThus, S", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6498473838192669, 0.5332411887599289, 0.26327018527648277, 0.20172456074831163, 0.1692662628520897], "conf": [0.9658840407255251, 0.8647748670943425, 0.6351857873983947, 0.4585501392716516, 0.3241155911734159], "prob": [[4.879475183466139e-05, 12], [3.7548459840604946e-06, 9], [2.2235639903514828e-12, 13], [2.28854356497244e-13, 11], [2.2357341112965995e-13, 6]], "generated": ["Chance from Homeward Bound is a dog character.", "Snoopy is also a dog character.", "So Snoopy and Chance look similar to each other.", "Thus, Snoopy looks like Chance.", "So the answer is yes."], "conf_comments": ["(A)\nSnoopy is", "(A)\nThus, S", "(A)\n# Is the above", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6810070524484461, 0.3991162698730697, 0.2618551251650385, 0.21053473984158633], "conf": [0.9706103118747875, 0.8049890972986681, 0.5182266194929249, 0.33710576180449886], "prob": [[0.0012981499505579095, 9], [6.549801647499441e-06, 6], [1.563621947246762e-08, 15], [1.5059317235781108e-08, 6]], "generated": ["Snoopy is a cartoon character.", "Chance is a dog.", "Thus, Snoopy and Chance do not look alike.", "So the answer is no."], "conf_comments": ["(A)\nChance is a", "(A)\nSnoopy is", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6498473838192669, 0.5332411887599289, 0.26327018527648277, 0.20617064374281824], "conf": [0.9658840407255251, 0.8647748670943425, 0.6351857873983947, 0.47622829047776644], "prob": [[4.879475183466139e-05, 12], [3.7548459840604946e-06, 9], [2.2235639903514828e-12, 13], [4.455912400776199e-13, 8]], "generated": ["Chance from Homeward Bound is a dog character.", "Snoopy is also a dog character.", "So Snoopy and Chance look similar to each other.", "Thus, the answer is yes."], "conf_comments": ["(A)\nSnoopy is", "(A)\nThus, S", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6498473838192669, 0.5279757212116593, 0.30484310443682416, 0.15470290686144755, 0.12047075129598703], "conf": [0.9658840407255251, 0.8441296789428142, 0.6211171138380943, 0.40740586796663564, 0.25193378048179915], "prob": [[4.879475183466139e-05, 12], [5.168219038748714e-06, 8], [9.175401638905754e-09, 8], [1.7719151568697917e-16, 19], [1.5757449212633049e-16, 6]], "generated": ["Chance from Homeward Bound is a dog character.", "Snoopy is a dog character.", "Thus, they both are dogs.", "Since Snoopy looks like a dog, he would also look like Chance as well.", "So the answer is yes."], "conf_comments": ["(A)\nSnoopy is", "(A)\nThus, S", "(A)\nSnoopy is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 58.840137243270874}
{"index": 63, "question": "Was Dioskourides a lapidary?", "answer": "yes", "explanation": "Yes. A lapidary is a person or machine who cuts gemstones; classically, it refers to a person who engraves gemstones. Dioskourides put his signature on a Roman amethyst ringstone with a portrait of Demosthenes circa late 1st century BC. Artists sign their work.", "generated": [{"finished": true, "length": 4, "score": [0.709656934927778, 0.48492572619709917, 0.40408032858845927, 0.33967643769373446], "conf": [0.9677169993186031, 0.7887392813949713, 0.5864383708873604, 0.4158097229616534], "prob": [[5.561961556388595e-05, 15], [2.1179341520996924e-07, 10], [8.129377399741057e-08, 14], [7.965192546086759e-08, 6]], "generated": ["Dioskourides was a Greek physician during the first century.", "A lapidary is a stonecutter.", "Thus, Dioskourides was not a lapidary.", "So the answer is no."], "conf_comments": ["(A)\nHe wrote a book", "(A)\nThus, Dios", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.709656934927778, 0.48492572619709917, 0.3616138457155404, 0.29134434299709977], "conf": [0.9677169993186031, 0.7887392813949713, 0.5689219098238635, 0.3833637600131825], "prob": [[5.561961556388595e-05, 15], [2.1179341520996924e-07, 10], [1.1649886934941723e-09, 20], [9.309145118672406e-10, 6]], "generated": ["Dioskourides was a Greek physician during the first century.", "A lapidary is a stonecutter.", "Since Dioskourides was not a stonecutter, he was not a lapidary.", "So the answer is no."], "conf_comments": ["(A)\nHe wrote a book", "(A)\nThus, Dios", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7045965914997409, 0.46708439116623207, 0.3900188907480197], "conf": [0.9745730531885469, 0.7439645091362034, 0.5238894734548479], "prob": [[1.3846293627375013e-06, 20], [6.077695448229364e-10, 14], [5.726610807829305e-10, 6]], "generated": ["A lapidary is an expert in minerals, gems, and crystals.", "Thus, Dioskourides was not a lapidary.", "So the answer is no."], "conf_comments": ["(A)\nDioskour", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7020607780436796, 0.4195921577316616, 0.2781035766874148, 0.2367007996123874], "conf": [0.9700594337625394, 0.5652875227358496, 0.43176178909158536, 0.316351908118761], "prob": [[0.0011468610027662704, 10], [1.2122445548126024e-06, 14], [2.7619133348701662e-09, 11], [2.579706709298686e-09, 6]], "generated": ["A lapidary is a stone cutter.", "According to Wikipedia, Dioskourides was a physician.", "Thus, he was not a stone cutter.", "So the answer is no."], "conf_comments": ["(A)\nDioskour", "(A)\nThus, Dios", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7045965914997409, 0.3556729526347838, 0.2746459214394627, 0.2205112537625341], "conf": [0.9745730531885469, 0.7270168817761382, 0.5085024515974542, 0.32882044110397324], "prob": [[1.3846293627375013e-06, 20], [7.553090966065743e-17, 22], [8.088532285075799e-18, 14], [7.939011633214687e-18, 6]], "generated": ["A lapidary is an expert in minerals, gems, and crystals.", "Dioskourides is known for his treaties on plants, metals, and minerals.", "Thus, Dioskourides was not a lapidary.", "So the answer is no."], "conf_comments": ["(A)\nDioskour", "(A)\nThus, Dios", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 48.60207986831665}
{"index": 64, "question": "Do most middle class families have butlers?", "answer": "no", "explanation": "No. Butlers make about $60,000 per year on average for their work. Middle class income is between $48,000 and $145,000.", "generated": [{"finished": true, "length": 4, "score": [0.6772328434827599, 0.44709956104898746, 0.3616450232887297, 0.2933128971692493], "conf": [0.9689330801682761, 0.8130089187297176, 0.6261468290501845, 0.41338248444827186], "prob": [[7.145115997051971e-08, 22], [3.9417574487340255e-14, 22], [4.7312184443827486e-15, 13], [4.629154615509408e-15, 6]], "generated": ["The median household income of middle class families in the United States is about $70,000.", "In the United States, the median annual income of butlers is about $40,000.", "Thus, most middle class families do not have butlers.", "So the answer is no."], "conf_comments": ["(A)\nThe average annual sal", "(A)\nThus, most", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7164709157164995, 0.3660716687418691, 0.27367470965677837, 0.21445666109262873], "conf": [0.9266024517114918, 0.7139793543047277, 0.4624428523937161, 0.2848624142497155], "prob": [[0.0008356881240786336, 12], [4.915076883597744e-08, 9], [7.228881576229892e-09, 13], [7.09360698472394e-09, 6]], "generated": ["Most middle class families don’t have butlers.", "Only rich families can afford a butler.", "Thus, most middle class families do not have butlers.", "So the answer is no."], "conf_comments": ["(A)\nThus, most", "(A)\nThus, most", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7164709157164995, 0.3347305039703301, 0.2159153512059704, 0.17338445321029003, 0.13763038327178084], "conf": [0.9266024517114918, 0.7803111719550647, 0.5493950143493791, 0.3777711111618282, 0.2389464473301697], "prob": [[0.0008356881240786336, 12], [9.775513742329262e-07, 5], [1.773594905368102e-09, 12], [7.217448920596319e-10, 14], [7.053468107090717e-10, 6]], "generated": ["Most middle class families don’t have butlers.", "Butlers are expensive.", "Most middle class families can’t afford butlers.", "Thus, most middle class families don’t have butlers.", "So the answer is no."], "conf_comments": ["(A)\nThus, most", "(A)\nThus, most", "(A)\nThus, most", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6772328434827599, 0.44709956104898746, 0.24041619617976273, 0.1662983481987457], "conf": [0.9689330801682761, 0.8130089187297176, 0.5659276317789022, 0.35249768022027483], "prob": [[7.145115997051971e-08, 22], [3.9417574487340255e-14, 22], [6.608719715280666e-23, 23], [1.3577997242241498e-23, 6]], "generated": ["The median household income of middle class families in the United States is about $70,000.", "In the United States, the median annual income of butlers is about $40,000.", "Thus, the income for most middle class families in the United States is less than that of a butler.", "So the answer is no."], "conf_comments": ["(A)\nThe average annual sal", "(A)\nThus, most", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6772328434827599, 0.3587706176363056, 0.20515263494806657, 0.16601237579320932], "conf": [0.9689330801682761, 0.8074876763724658, 0.629875100916586, 0.4296255296490446], "prob": [[7.145115997051971e-08, 22], [1.0886771754686246e-19, 25], [1.6571016414613188e-29, 26], [1.297465130644007e-29, 6]], "generated": ["The median household income of middle class families in the United States is about $70,000.", "According to PayScale, the average base salary for a butler is about $$44,000.", "This is much lower than the mean income of middle class families, which indicates that most middle class families do not have butlers.", "So the answer is no."], "conf_comments": ["(A)\nThe average annual sal", "(A)\nThus, most", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 52.049280405044556}
{"index": 65, "question": "Was Hillary Clinton's deputy chief of staff in 2009 baptised?", "answer": "no", "explanation": "No. Huma Abedin was Hillary Clinton's deputy chief of staff in 2009. Huma Abedin is an American Muslim. Baptism is a Christian tradition.", "generated": [{"finished": true, "length": 3, "score": [0.7908558934771241, 0.6757423349493143, 0.5863822345217077], "conf": [0.9012459432778877, 0.7608326132663707, 0.5748711679482268], "prob": [[6.496869224770211e-07, 39], [4.756588576459276e-08, 18], [4.6601860713678467e-08, 6]], "generated": ["According to Wikipedia, Huma Abedin, Hillary Clinton's deputy chief of staff in 2009, was baptised in 2009.", "Thus, Huma Abedin was baptised in 2009.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.7908558934771241, 0.5861058354341188], "conf": [0.9012459432778877, 0.7082922523561487], "prob": [[6.496869224770211e-07, 39], [7.568910930362633e-08, 6]], "generated": ["According to Wikipedia, Huma Abedin, Hillary Clinton's deputy chief of staff in 2009, was baptised in 2009.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6952945297196449, 0.46553819166458504, 0.3243841474876628, 0.2533868051567073], "conf": [0.9339618376283656, 0.7685821459081531, 0.5343191547129971, 0.33024885825703415], "prob": [[9.866846601981096e-07, 21], [2.508848848750638e-13, 25], [2.218332869595328e-17, 26], [2.0534101626626785e-17, 6]], "generated": ["Hillary Clinton didn’t become deputy chief of staff in 2009.", "Instead, in 2009, Hillary Clinton became the Secretary of State under the Obama administration.", "Thus, Hillary Clinton’s deputy chief of staff in 2009 was unbaptised.", "So the answer is no."], "conf_comments": ["(A)\nShe became deputy", "(A)\nThus, Hill", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6952945297196449, 0.4482682063944066, 0.3014831557345278, 0.23407121803676423], "conf": [0.9339618376283656, 0.7772323708797411, 0.5098814639827697, 0.31221523807123513], "prob": [[9.866846601981096e-07, 21], [9.535498060977708e-10, 10], [3.8771534906937797e-13, 21], [3.5287697034480005e-13, 6]], "generated": ["Hillary Clinton didn’t become deputy chief of staff in 2009.", "Instead, she became the Secretary of State.", "Thus, she didn’t have any deputy chief of staff in 2009.", "So the answer is no."], "conf_comments": ["(A)\nShe became deputy", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6952945297196449, 0.46553819166458504, 0.354633951492871, 0.1862415865551396, 0.12862011420989386], "conf": [0.9339618376283656, 0.7685821459081531, 0.6338568965737187, 0.39606385112054504, 0.269485598017361], "prob": [[9.866846601981096e-07, 21], [2.508848848750638e-13, 25], [3.155779880809713e-16, 19], [1.2759792353524657e-22, 18], [7.436976750993947e-24, 8]], "generated": ["Hillary Clinton didn’t become deputy chief of staff in 2009.", "Instead, in 2009, Hillary Clinton became the Secretary of State under the Obama administration.", "Hillary Clinton’s deputy chief of staff was Huma Abedin.", "I could not find any evidence whether Huma Abedin was baptised or not.", "Thus, the answer is unknown."], "conf_comments": ["(A)\nShe became deputy", "(A)\nThus, Hill", "(A)\nHuma Abed", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": null}, {"finished": true, "length": 5, "score": [0.6952945297196449, 0.46553819166458504, 0.354633951492871, 0.1862415865551396, 0.11158134368931485], "conf": [0.9339618376283656, 0.7685821459081531, 0.6338568965737187, 0.39606385112054504, 0.28192576776542444], "prob": [[9.866846601981096e-07, 21], [2.508848848750638e-13, 25], [3.155779880809713e-16, 19], [1.2759792353524657e-22, 18], [7.732365517957157e-32, 31]], "generated": ["Hillary Clinton didn’t become deputy chief of staff in 2009.", "Instead, in 2009, Hillary Clinton became the Secretary of State under the Obama administration.", "Hillary Clinton’s deputy chief of staff was Huma Abedin.", "I could not find any evidence whether Huma Abedin was baptised or not.", "Since there is no evidence to support that Huma Abedin was baptised in 2009, the answer is don’t know."], "conf_comments": ["(A)\nShe became deputy", "(A)\nThus, Hill", "(A)\nHuma Abed", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 48.512208461761475}
{"index": 66, "question": "Does the New York Public Library sell Alpo products?", "answer": "no", "explanation": "No. The New York Public Library is a public lending library system in New York City. Alpo produces pet food and related products.", "generated": [{"finished": true, "length": 4, "score": [0.7889355834962336, 0.4381384109437276, 0.3761427448180053, 0.3208710277856426], "conf": [0.9610510697495567, 0.6830075198247166, 0.5324206403214348, 0.3882126735583593], "prob": [[0.020046086679528567, 9], [1.3767622485734845e-08, 17], [5.938254335917518e-09, 15], [5.868182761559849e-09, 6]], "generated": ["Alpo is a brand of dog food.", "The New York Public Library is a public library, which does not sell any food.", "Thus, the New York Public Library does not sell Alpo products.", "So the answer is no."], "conf_comments": ["(A)\nThe New York Public", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.8558558813410222, 0.4871471621536277, 0.39083130215777906], "conf": [0.9552288827518542, 0.812467533901185, 0.6383507364935368], "prob": [[0.04133576697068529, 12], [6.97767189449732e-06, 9], [1.4156319696585216e-06, 8]], "generated": ["The New York Public Library does not sell Alpo products.", "It is a library and not a store.", "Thus, the answer is no."], "conf_comments": ["(A)\nThe New York Public", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6400458578928571, 0.4322212978812327, 0.36931782565451715], "conf": [0.9524060142083852, 0.7705210320194962, 0.5658162354231748], "prob": [[2.021161995863024e-08, 21], [2.0793826905147586e-11, 12], [2.0087147279439356e-11, 6]], "generated": ["The New York Public Library does not sell food, since it is a library and not a supermarket.", "Thus, they don’t sell Alpo products.", "So the answer is no."], "conf_comments": ["(A)\nAlpo is a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6575482808581671, 0.5001936628524232, 0.27724792696806416, 0.23668105784480767], "conf": [0.9654635733882928, 0.8320902043623508, 0.6137862523345751, 0.45043327199916294], "prob": [[0.0007245604191661911, 9], [2.0088783987112952e-05, 9], [1.6472882671245298e-11, 16], [1.579909092230664e-11, 6]], "generated": ["New York Public Library is a public library.", "Alpo is a brand of dog food.", "Thus, it is not a public library that sells Alpo products.", "So the answer is no."], "conf_comments": ["(A)\nAlpo is a", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6575482808581671, 0.5001936628524232, 0.2306557612469322, 0.19120226335780857], "conf": [0.9654635733882928, 0.8320902043623508, 0.55965758413663, 0.3931715964020931], "prob": [[0.0007245604191661911, 9], [2.0088783987112952e-05, 9], [6.319163726503001e-17, 23], [5.534084923394058e-17, 6]], "generated": ["New York Public Library is a public library.", "Alpo is a brand of dog food.", "Thus, they are completely different and it seems unreasonable for New York Public Library to sell Alpo.", "So the answer is no."], "conf_comments": ["(A)\nAlpo is a", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": false, "length": 4, "score": [0.6575482808581671, 0.5001936628524232, 0.27724792696806416, 0.08443377143920815], "conf": [0.9654635733882928, 0.8320902043623508, 0.6137862523345751, 0.3689209405933064], "prob": [[0.0007245604191661911, 9], [2.0088783987112952e-05, 9], [1.6472882671245298e-11, 16], [2.2967887878733262e-29, 22]], "generated": ["New York Public Library is a public library.", "Alpo is a brand of dog food.", "Thus, it is not a public library that sells Alpo products.", "There is probably no public library that sells Alpo products because there are not many benefits to the public.", "Thus, the answer is no."], "conf_comments": ["(A)\nAlpo is a", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[1.3942846760406728e-18, 22], [0.03990806084383115, 8]], 0.6010576795913758, "(A)\nSo the answer is", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 47.86277151107788}
{"index": 67, "question": "Did Solomon make up bigger percentage of Islamic prophets than Kings of Judah?", "answer": "no", "explanation": "No. According to The Quran, Solomon was one of 25 prophets. According to some Islamic hadiths, there have been as many as 124,000 prophets. Solomon was one of 20 Kings of Judah.", "generated": [{"finished": true, "length": 3, "score": [0.613158323470669, 0.4531095848314038, 0.3704389537328517], "conf": [0.9199425067097504, 0.665971123639493, 0.4482838064993381], "prob": [[8.972669408043523e-13, 31], [1.370729781243622e-15, 23], [1.3137923610099023e-15, 6]], "generated": ["The percentage of the prophets in Islam is about 25%, and the percentage of the kings of Judah is about 6.6%.", "Thus, Solomon made up a bigger percentage of the Islamic prophets than kings of Judah.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.613158323470669, 0.39121901856862673, 0.3210999533560947], "conf": [0.9199425067097504, 0.6720266865558159, 0.45604438166403416], "prob": [[8.972669408043523e-13, 31], [1.4443710902175273e-15, 11], [1.3822883396774497e-15, 6]], "generated": ["The percentage of the prophets in Islam is about 25%, and the percentage of the kings of Judah is about 6.6%.", "Thus, Solomon made up a bigger percentage.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5985784959441092, 0.3776768170298888, 0.26791054426763916, 0.15000005818111778, 0.11831700989493318], "conf": [0.9516896794646195, 0.8219425896792346, 0.6760887071596341, 0.4351990565326295, 0.27708049483060904], "prob": [[2.153850118180536e-05, 11], [4.21063695996074e-10, 14], [1.8910027972819216e-12, 11], [2.524118246849039e-19, 22], [2.1982178887610907e-19, 6]], "generated": ["Solomon is mentioned in the Qur'an.", "Solomon is considered to be one of the Islamic prophets.", "Kings of Judah are mentioned in the Bible.", "Thus, Solomon made up a greater percentage of both Islamic prophets and Kings of Judah.", "So the answer is yes."], "conf_comments": ["(A)\nSolomon is also", "(A)\nSolomon is also", "(A)\nThus, Sol", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.5985784959441092, 0.3776768170298888, 0.26791054426763916, 0.13053477240134514, 0.06992097162859996, 0.047696656692833225, 0.03824959419229671], "conf": [0.9516896794646195, 0.8219425896792346, 0.6760887071596341, 0.5273996318631013, 0.39057357886115923, 0.2625058569050972, 0.17020442717144924], "prob": [[2.153850118180536e-05, 11], [4.21063695996074e-10, 14], [1.8910027972819216e-12, 11], [1.1932524017101379e-18, 12], [6.057979790081432e-22, 8], [2.6862675246346247e-25, 21], [2.557554273618333e-25, 6]], "generated": ["Solomon is mentioned in the Qur'an.", "Solomon is considered to be one of the Islamic prophets.", "Kings of Judah are mentioned in the Bible.", "David is considered to be from the Kings of Judah.", "David was an Islamic prophet.", "Thus, Solomon made up a bigger proportion of Islamic prophets than Kings of Judah.", "So the answer is yes."], "conf_comments": ["(A)\nSolomon is also", "(A)\nSolomon is also", "(A)\nThus, Sol", "(A)\nSolomon is considered", "(A)\nThus, Sol", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6878414024885763, 0.37598299904693633, 0.18833286595578577, 0.1356635941761722], "conf": [0.9563984678916664, 0.8623779177284965, 0.5466501337641144, 0.3050527023301779], "prob": [[0.00043422547945307316, 11], [3.664687966927652e-14, 21], [1.2564366076809366e-24, 26], [8.120719613299603e-25, 6]], "generated": ["Solomon was one of the prophets of Islam.", "In the Bible, there were a total of 25 rulers of Israel and Judah.", "Thus, the total percentage of the prophets of Islam was the same as the rulers of Israel and Judah.", "So the answer is no."], "conf_comments": ["(A)\nSolomon was also", "(A)\nOf these, ", "(A)\nSo the answer is", "(A), but the previous step is"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 58.75232219696045}
{"index": 68, "question": "Would Taylor Swift refer to Snoopy as oppa?", "answer": "yes", "explanation": "Yes. Oppa is a Korean word used by women to address a man who is 10 or more years older than her. Snoopy is 47 years old. Taylor Swift is 30 years old.", "generated": [{"finished": true, "length": 4, "score": [0.6198984814449513, 0.4128051440565366, 0.22308444235989236, 0.18294055947374901], "conf": [0.9624206996272147, 0.802359486166066, 0.5897393351685248, 0.3984456218326064], "prob": [[4.1727180073372984e-07, 16], [4.024521504123022e-10, 11], [2.4760457910964276e-15, 13], [2.4076509084965903e-15, 6]], "generated": ["Oppa is a term used to refer to a man’s older brother.", "Taylor Swift and Snoopy are not related.", "Thus, she would NOT refer to him as oppa.", "So the answer is no."], "conf_comments": ["(A)\nTaylor Swift is", "(A)\nThus, Taylor", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.655042047062983, 0.43065837006496505, 0.22388097023137057, 0.17984528068608477], "conf": [0.9776802658069996, 0.828775900251154, 0.5477675041191188, 0.3725832392298536], "prob": [[0.0006040509406698384, 9], [2.473893616366458e-08, 15], [4.2224987120050724e-16, 20], [3.0788576925415305e-16, 6]], "generated": ["Taylor Swift is an American pop star.", "Snoopy is a beagle from the Peanuts comics.", "It is thus highly improbable for Taylor Swift to refer to Snoopy as oppa.", "So the answer is no."], "conf_comments": ["(A)\nSnoopy is", "(A)\nThus, Taylor", "(A)\nOppa is a", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6198984814449513, 0.4128051440565366, 0.21751056944504443, 0.17437985922608237], "conf": [0.9624206996272147, 0.802359486166066, 0.5616128275745098, 0.36221088865243384], "prob": [[4.1727180073372984e-07, 16], [4.024521504123022e-10, 11], [2.37661754332399e-17, 18], [2.3281121040617495e-17, 6]], "generated": ["Oppa is a term used to refer to a man’s older brother.", "Taylor Swift and Snoopy are not related.", "Thus, a Taylor Swift couldn't refer to Snoopy as oppa.", "So the answer is no."], "conf_comments": ["(A)\nTaylor Swift is", "(A)\nThus, Taylor", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.655042047062983, 0.43065837006496505, 0.22388097023137057, 0.14274227273231974, 0.11266725259500558], "conf": [0.9776802658069996, 0.828775900251154, 0.5477675041191188, 0.3255541638948242, 0.20566150546978287], "prob": [[0.0006040509406698384, 9], [2.473893616366458e-08, 15], [4.2224987120050724e-16, 20], [3.100294437876013e-19, 19], [2.8521053698266375e-19, 6]], "generated": ["Taylor Swift is an American pop star.", "Snoopy is a beagle from the Peanuts comics.", "It is thus highly improbable for Taylor Swift to refer to Snoopy as oppa.", "Thus, it is unlikely that Taylor Swift would refer to Snoopy as oppa.", "So the answer is no."], "conf_comments": ["(A)\nSnoopy is", "(A)\nThus, Taylor", "(A)\nOppa is a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5949741335248517, 0.3342846384124835, 0.20873725828331421, 0.16911071528703145], "conf": [0.9677293263866934, 0.8724160307405204, 0.6161098441106858, 0.40747412710462555], "prob": [[0.00011726824168844507, 9], [9.280744549971438e-09, 9], [3.8206478597717844e-13, 17], [3.650391889426312e-13, 6]], "generated": ["Taylor Swift doesn't speak Korean.", "Oppa is Korean for “older brother”.", "Thus, Taylor Swift will probably never refer to Snoopy as oppa.", "So the answer is no."], "conf_comments": ["(A)\nSnoopy is", "(A)\nThus, Taylor", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.78945851325989}
{"index": 69, "question": "Did Confederate States Army influence West Point fashion?", "answer": "yes", "explanation": "Yes. The Confederate States Army was clad in cadet gray uniforms. West Point uniforms are cadet gray and white. Confederate States Army uniforms contained Generally, the uniform jacket of the Confederate soldier was single breasted, made of gray or brown fabric, with a six to nine button front and hat.  West Point uniforms contain a standing collar, white trousers, and black shakos (known as a \"tarbucket hat\" in U.S. Army nomenclature).", "generated": [{"finished": true, "length": 4, "score": [0.6678264154978452, 0.4266338213084147, 0.32118031451219253, 0.2707838096172584], "conf": [0.9754989901131149, 0.8943117568779179, 0.7197877296790319, 0.5131261605128827], "prob": [[6.64828744777114e-10, 27], [1.0857785944956542e-18, 25], [5.634014146012625e-21, 15], [5.535857795547045e-21, 6]], "generated": ["West Point has been a military academy and training center for the United States Army since its founding in 1802.", "Confederate States Army did not get formed until the outbreak of the American Civil War in 1861.", "Thus, West Point fashion was not influenced by Confederate States Army.", "So the answer is no."], "conf_comments": ["(A)\nThe Confederate States", "(A)\nThus, Confeder", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7672010465116261, 0.5760004289000076, 0.3578945805357481, 0.21646266710978657, 0.1662111407894448], "conf": [0.9755023704246327, 0.8707858162685629, 0.6568947633983582, 0.40723625152120524, 0.247900937778057], "prob": [[0.0003086318239559466, 16], [4.926502563139521e-06, 9], [2.131775031290583e-10, 15], [4.6049352582861877e-14, 16], [3.801531548210249e-14, 6]], "generated": ["The Confederate States Army was an army of the Confederate States of America.", "West Point is the United States Military Academy.", "Thus, the Confederate States Army was not from the United States.", "Therefore, the Confederate States Army probably did not influence West Point fashion.", "So the answer is no."], "conf_comments": ["(A)\nWest Point is a", "(A)\nThe Confederate States", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6678264154978452, 0.4266338213084147, 0.26976849251393453, 0.2295541724441469], "conf": [0.9754989901131149, 0.8943117568779179, 0.6916647309007117, 0.5035703029786779], "prob": [[6.64828744777114e-10, 27], [1.0857785944956542e-18, 25], [2.0454498686524096e-22, 13], [1.9793817116529633e-22, 6]], "generated": ["West Point has been a military academy and training center for the United States Army since its founding in 1802.", "Confederate States Army did not get formed until the outbreak of the American Civil War in 1861.", "Thus, Confederate States Army cannot influence West Point fashion.", "So the answer is no."], "conf_comments": ["(A)\nThe Confederate States", "(A)\nThus, Confeder", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7672010465116261, 0.5760004289000076, 0.3578945805357481, 0.16990182453635133, 0.1363062132147986], "conf": [0.9755023704246327, 0.8707858162685629, 0.6568947633983582, 0.462814765903601, 0.3018941398726271], "prob": [[0.0003086318239559466, 16], [4.926502563139521e-06, 9], [2.131775031290583e-10, 15], [2.3363443188716777e-14, 8], [2.15607224030777e-14, 6]], "generated": ["The Confederate States Army was an army of the Confederate States of America.", "West Point is the United States Military Academy.", "Thus, the Confederate States Army was not from the United States.", "Hence, there was no influence.", "So the answer is no."], "conf_comments": ["(A)\nWest Point is a", "(A)\nThe Confederate States", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7672010465116261, 0.5760004289000076, 0.23724302945093648, 0.16439191043596893], "conf": [0.9755023704246327, 0.8707858162685629, 0.4985406918562893, 0.2952666602611445], "prob": [[0.0003086318239559466, 16], [4.926502563139521e-06, 9], [4.738187677779805e-25, 36], [1.3451578110157862e-25, 6]], "generated": ["The Confederate States Army was an army of the Confederate States of America.", "West Point is the United States Military Academy.", "Thus, it is very likely that the Confederate States Army and West Point were on opposite sides of the American Civil War, and may have fought each other to influence fashion.", "So the answer is yes."], "conf_comments": ["(A)\nWest Point is a", "(A)\nThe Confederate States", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 58.76246356964111}
{"index": 70, "question": "Does conservatism repulse Blaire White?", "answer": "no", "explanation": "No. Blaire White is a Youtuber. Blaire White posts content that leans towards conservative politics.", "generated": [{"finished": true, "length": 4, "score": [0.7105578170089586, 0.45542338039787145, 0.3834297911364098, 0.31422331486279], "conf": [0.9485197354909404, 0.6761225526575947, 0.5136106611592193, 0.3462721034410744], "prob": [[0.0018261006727091772, 10], [8.140727737827515e-07, 14], [3.0882864669455193e-07, 14], [3.017500575288674e-07, 6]], "generated": ["Blaire White is a conservative person.", "Blaire White is not repulsed by conservatism.", "Thus, conservatism does not repulse Blaire White.", "So the answer is no."], "conf_comments": ["(A)\nThus, conserv", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6736864204179741, 0.42797959631840665, 0.3577867252898903], "conf": [0.9530999556665485, 0.7393751863734501, 0.5239038128252743], "prob": [[6.989308279708132e-06, 16], [7.435186863170649e-10, 14], [6.845192454450531e-10, 6]], "generated": ["Blaire White is a political commentator who is a political conservative.", "Thus, she is not repulsed by conservativism.", "So the answer is no."], "conf_comments": ["(A)\nThus, conserv", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6982098767625294, 0.4443511903876817, 0.27867500427255437], "conf": [0.9660795296253744, 0.829005823708321, 0.5490595546461381], "prob": [[0.0010705105722803683, 10], [5.874232673187775e-07, 10], [1.0381543054523684e-11, 21]], "generated": ["Blaire White is a YouTuber.", "Blaire White is also a conservative.", "Thus, conservatism doesn’t repulse Blaire White, so the answer is no."], "conf_comments": ["(A)\nShe is a conserv", "(A)\nThus, Bla", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6808534240988235, 0.34959000197801465, 0.22115849712761945, 0.14444123821354865, 0.11623074842060445], "conf": [0.969565930552854, 0.8813306191969885, 0.7976134540454323, 0.6098529753738378, 0.4306314431834772], "prob": [[6.821639581601419e-05, 13], [2.4170169095045133e-11, 12], [1.563149273747317e-14, 9], [2.5466403505212278e-17, 11], [1.514405062804563e-17, 6]], "generated": ["Blaire White is a right-wing internet personality.", "This indicates that she leans right on the political spectrum.", "Conservatism also leans right.", "Thus, Blaire White could be conservative.", "So the answer is yes."], "conf_comments": ["(A)\nThus, conserv", "(A)\nThus, conserv", "(A)\nThus, conserv", "(A)\nHowever, Blaire", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6808534240988235, 0.34959000197801465, 0.20909005515295448, 0.16834944551464803], "conf": [0.969565930552854, 0.8813306191969885, 0.6780241966186192, 0.4553285062771366], "prob": [[6.821639581601419e-05, 13], [2.4170169095045133e-11, 12], [2.4561868689610282e-14, 9], [1.9875738587542403e-14, 6]], "generated": ["Blaire White is a right-wing internet personality.", "This indicates that she leans right on the political spectrum.", "Thus, she supports conservatism.", "So the answer is no."], "conf_comments": ["(A)\nThus, conserv", "(A)\nThus, conserv", "(A)\nConservatism", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 49.91485047340393}
{"index": 71, "question": "Do all parts of the aloe vera plant taste good?", "answer": "no", "explanation": "No. There is a layer of yellow latex liquid between the outside of an aloe leaf and the gel inside. The latex inside aloe tastes very bitter.", "generated": [{"finished": true, "length": 4, "score": [0.6215020565397904, 0.27374031823238587, 0.21927634911487542, 0.18105583069812128], "conf": [0.8798706933814957, 0.7326980860893366, 0.5694355966246658, 0.3894062693872814], "prob": [[9.874793585215903e-06, 14], [3.4992018071660317e-22, 26], [1.631371503045929e-23, 16], [1.6019597249002723e-23, 6]], "generated": ["All parts of the aloe vera plant do NOT taste good.", "Almost every part of an aloe plant is toxic, except for the gel-like substance in the leaf itself.", "Thus, not all parts of the aloe vera plant taste good.", "So the answer is no."], "conf_comments": ["(A)\nThe aloe ver", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6215020565397904, 0.32016476653136994, 0.1963215935185094, 0.14744786549579222], "conf": [0.8798706933814957, 0.7466669312878985, 0.45232055297877877, 0.25999935901990606], "prob": [[9.874793585215903e-06, 14], [2.3103750522411603e-19, 27], [6.957716806891983e-23, 17], [6.21378253349549e-23, 6]], "generated": ["All parts of the aloe vera plant do NOT taste good.", "The aloe vera plant has a bitter, soapy, or unappetizing taste that is not suitable for human consumption.", "Thus, some parts of the aloe vera plant does not taste good.", "So the answer is no."], "conf_comments": ["(A)\nThe aloe ver", "(A)\nThus, all", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5005998170963217, 0.2614326408515851, 0.16980042595942646, 0.14050920083963872], "conf": [0.9615629264956439, 0.777825908777427, 0.580684461609871, 0.40331394415269306], "prob": [[1.180489870226468e-10, 17], [2.895556177084901e-17, 14], [9.797918626746066e-21, 14], [8.997202451248125e-21, 6]], "generated": ["Thick, outer skin of the aloe vera plant tastes bitter.", "Only inner fillets of the aloe vera plant taste good.", "Thus, parts of the aloe vera plant taste good.", "So the answer is yes."], "conf_comments": ["(A)\nThe inner gel of", "(A)\nThus, all", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5005998170963217, 0.2614326408515851, 0.22791654476394113, 0.11705543480262952], "conf": [0.9615629264956439, 0.777825908777427, 0.6337370286132157, 0.45441969752810474], "prob": [[1.180489870226468e-10, 17], [2.895556177084901e-17, 14], [9.519595630640199e-18, 16], [8.677652766629427e-21, 7]], "generated": ["Thick, outer skin of the aloe vera plant tastes bitter.", "Only inner fillets of the aloe vera plant taste good.", "Thus, not all parts of the aloe vera plant taste good.", "So the answer is no."], "conf_comments": ["(A)\nThe inner gel of", "(A)\nThus, all", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6215020565397904, 0.27374031823238587, 0.14479070315632384, 0.11178022387610778], "conf": [0.8798706933814957, 0.7326980860893366, 0.5256391964227203, 0.3287695235728451], "prob": [[9.874793585215903e-06, 14], [3.4992018071660317e-22, 26], [5.9395086788467164e-30, 19], [4.446509640168408e-30, 6]], "generated": ["All parts of the aloe vera plant do NOT taste good.", "Almost every part of an aloe plant is toxic, except for the gel-like substance in the leaf itself.", "Thus there is a part of an aloe vera plant that does NOT taste good.", "So the answer is no."], "conf_comments": ["(A)\nThe aloe ver", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6215020565397904, 0.27374031823238587, 0.14479070315632384, 0.09996142248499848], "conf": [0.8798706933814957, 0.7326980860893366, 0.5256391964227203, 0.3213973111259489], "prob": [[9.874793585215903e-06, 14], [3.4992018071660317e-22, 26], [5.9395086788467164e-30, 19], [1.3326759136256461e-30, 6]], "generated": ["All parts of the aloe vera plant do NOT taste good.", "Almost every part of an aloe plant is toxic, except for the gel-like substance in the leaf itself.", "Thus there is a part of an aloe vera plant that does NOT taste good.", "So the answer is yes."], "conf_comments": ["(A)\nThe aloe ver", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 53.185675382614136}
{"index": 72, "question": "Does an individual oceanographer study many sciences?", "answer": "yes", "explanation": "Yes. Study of the oceans involve many fields or science. To properly study their specific topic of research, an oceanographer must understand how their science interacts with the other involved sciences.", "generated": [{"finished": true, "length": 5, "score": [0.6989709602735723, 0.5582102728171778, 0.2949903842341573, 0.22155253109934883, 0.18759172543834085], "conf": [0.9657988018650965, 0.9001420021753249, 0.7120772567274278, 0.5458493005556266, 0.3928522526858867], "prob": [[0.00028078844932059616, 12], [6.322799750438911e-06, 10], [1.298904206539369e-13, 17], [4.449281542891235e-15, 11], [4.3470485898268e-15, 6]], "generated": ["A person who studies oceanography is an oceanographer.", "Oceanography is the study of the ocean.", "Since oceanographers study the ocean, it can be said that they study science.", "Thus, an oceanographer studies many sciences.", "So the answer is yes."], "conf_comments": ["(A)\nOceanography is", "(A)\nThus, an", "(A)\nThus, an", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6989709602735723, 0.5582102728171778, 0.2949903842341573, 0.21673427369035303, 0.18363886082962583], "conf": [0.9657988018650965, 0.9001420021753249, 0.7120772567274278, 0.5482560104278771, 0.3964531845092903], "prob": [[0.00028078844932059616, 12], [6.322799750438911e-06, 10], [1.298904206539369e-13, 17], [3.1037822703102964e-16, 17], [2.9722417910835645e-16, 6]], "generated": ["A person who studies oceanography is an oceanographer.", "Oceanography is the study of the ocean.", "Since oceanographers study the ocean, it can be said that they study science.", "Thus, it can be said that an individual oceanographer studies many sciences.", "So the answer is yes."], "conf_comments": ["(A)\nOceanography is", "(A)\nThus, an", "(A)\nThus, an", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.7861941879434324, 0.3659787667773203, 0.3112264252605645], "conf": [0.9653117419049155, 0.7660960120501908, 0.5672726952117713], "prob": [[0.0074184428949599325, 11], [7.081037431798338e-12, 16], [6.144573179638569e-12, 6]], "generated": ["An oceanographer is someone who studies the ocean.", "Thus, an individual oceanographer studies more than one science at least.", "So the answer is yes."], "conf_comments": ["(A)\nOceanography is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7861941879434324, 0.31884776057668435, 0.21184178140333418, 0.17958702079852834], "conf": [0.9653117419049155, 0.8253420833356544, 0.6291625361580953, 0.4553718054512612], "prob": [[0.0074184428949599325, 11], [2.6095383964694456e-14, 16], [7.202415734312389e-18, 15], [6.902690486015103e-18, 6]], "generated": ["An oceanographer is someone who studies the ocean.", "Some examples of sciences would include biology, Earth science, and chemistry.", "Thus, an individual oceanographer would study at least three sciences.", "So the answer is yes."], "conf_comments": ["(A)\nOceanography is", "(A)\nThus, an", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6923383240779409, 0.3212815782935674, 0.19102642610183487, 0.15235423554336855], "conf": [0.9620631470584619, 0.799729526908859, 0.5930930723711684, 0.3930971056402891], "prob": [[0.00011658190811315469, 13], [6.529956909294529e-20, 26], [4.285211152261199e-24, 13], [3.348445533755215e-24, 6]], "generated": ["An oceanographer is someone who specializes in oceanography.", "Oceanography is a branch of a multidisciplinary field such as physical sciences, earth sciences and biological sciences.", "Therefore, an oceanographer does not study many sciences.", "So the answer is no."], "conf_comments": ["(A)\nOceanography is", "(A)\nThus, an", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 55.45947504043579}
{"index": 73, "question": "Did Kurt Cobain's music genre survive after his death?", "answer": "yes", "explanation": "Yes. Kurt Cobain was the lead singer of Nirvana. Nirvana's music is classified as Grunge rock. Kurt Cobain died on April 5, 1994. Some of the major Grunge rock bands included Alice in Chains, Pearl Jam, and Soundgarden. Alice in Chains and Pearl Jam released their latest albums in 2018 and 2020 respectively.", "generated": [{"finished": true, "length": 4, "score": [0.8480240607275397, 0.457496948632125, 0.3297476445949636, 0.26814825624193683], "conf": [0.9829545356254226, 0.8637443231622642, 0.6084619683537092, 0.4044724334613449], "prob": [[0.02351770926807377, 12], [4.099155283689405e-08, 12], [1.7060517621879666e-10, 18], [1.653402405544292e-10, 6]], "generated": ["Kurt Cobain died in 1994.", "Pearl Jam started in 1990.", "Thus, Kurt Cobain's music genre did not survive after his death.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, the", "(A)\nThus, Pear", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7477548182183092, 0.35337313297942247, 0.29557077228357176, 0.25273537641804916], "conf": [0.9722888712623646, 0.8301244328629133, 0.6656332148947905, 0.4877129710938729], "prob": [[4.7316897037125494e-05, 18], [5.946051962765624e-15, 17], [6.705864687104695e-16, 16], [6.621150459824746e-16, 6]], "generated": ["Kurt Cobain is the lead singer of the alternative rock band Nirvana.", "According to the AllMusic genre page, alternative rock is alive and well today.", "Thus, Kurt Cobain’s music genre survived after his death.", "So the answer is yes."], "conf_comments": ["(A)\nHe died in ", "(A)\nThus, Kurt", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7477548182183092, 0.35337313297942247, 0.24683646340080312, 0.20235174631930986], "conf": [0.9722888712623646, 0.8301244328629133, 0.6360882658385789, 0.4868012456733944], "prob": [[4.7316897037125494e-05, 18], [5.946051962765624e-15, 17], [2.765949955697349e-18, 17], [9.779850279477244e-19, 8]], "generated": ["Kurt Cobain is the lead singer of the alternative rock band Nirvana.", "According to the AllMusic genre page, alternative rock is alive and well today.", "This can indicate that the music genre survived after Kurt Cobain’s death.", "Thus, the answer is yes."], "conf_comments": ["(A)\nHe died in ", "(A)\nThus, Kurt", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6446003468851775, 0.3056863192074709, 0.2259134736074015, 0.1836788333545194], "conf": [0.9668199166517756, 0.8795695007329642, 0.674001482073156, 0.44703294897528834], "prob": [[3.970322426703431e-05, 12], [3.1226348922556286e-14, 15], [7.038327894675666e-17, 18], [6.899238311441773e-17, 6]], "generated": ["The music genre that Kurt Cobain played is grunge.", "By 2002, grunge lost its prominence.", "Thus, the music genre did not survive after Kurt Cobain’s death.", "So the answer is no."], "conf_comments": ["(A)\nGrunge is a", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6446003468851775, 0.3056863192074709, 0.2284026449985338, 0.13664501107301777, 0.11333501324934281], "conf": [0.9668199166517756, 0.8795695007329642, 0.764682321195235, 0.5829916831880895, 0.40725262520976635], "prob": [[3.970322426703431e-05, 12], [3.1226348922556286e-14, 15], [1.5353011767698802e-16, 12], [1.8839330163999704e-22, 18], [1.7183043618600145e-22, 6]], "generated": ["The music genre that Kurt Cobain played is grunge.", "By 2002, grunge lost its prominence.", "Kurt Cobain died in 1994.", "Thus, Cobain's music genre was dead by the time of his death.", "So the answer is no."], "conf_comments": ["(A)\nGrunge is a", "(A)\nThus, the", "(A)\nThus, Kurt", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6446003468851775, 0.3056863192074709, 0.2284026449985338, 0.13664501107301777, 0.04704189202894375], "conf": [0.9668199166517756, 0.8795695007329642, 0.764682321195235, 0.5829916831880895, 0.38042631347461664], "prob": [[3.970322426703431e-05, 12], [3.1226348922556286e-14, 15], [1.5353011767698802e-16, 12], [1.8839330163999704e-22, 18], [3.723355603274117e-26, 5]], "generated": ["The music genre that Kurt Cobain played is grunge.", "By 2002, grunge lost its prominence.", "Kurt Cobain died in 1994.", "Thus, Cobain's music genre was dead by the time of his death.", "The answer is no."], "conf_comments": ["(A)\nGrunge is a", "(A)\nThus, the", "(A)\nThus, Kurt", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 54.192683696746826}
{"index": 74, "question": "Did Immanuel Kant ever meet the 14th president of the United States?", "answer": "no", "explanation": "No. Immanuel Kant died on Feb 12, 1804. Franklin Pierce was the 14th president of the United States. Franklin PIerce was born Nov 23, 1804.", "generated": [{"finished": true, "length": 4, "score": [0.8764475895804016, 0.756736594553758, 0.5108830428975902, 0.42347111212413635], "conf": [0.979410890500275, 0.8437092673345024, 0.6597111006559415, 0.45707823582992085], "prob": [[0.02050178003965325, 16], [0.00047779681296535547, 26], [1.455898884224521e-07, 15], [1.384644636197092e-07, 6]], "generated": ["Immanuel Kant lived from 1724-1804.", "Franklin Pierce, the 14th president of the United States, was born in 1804.", "Thus, Immanuel Kant did not live to meet Franklin Pierce.", "So the answer is no."], "conf_comments": ["(A)\nFranklin Pier", "(A)\nThus, Im", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.852010659428034, 0.7486802460886796, 0.5035650385034974, 0.3796251672416045, 0.32403605087309484], "conf": [0.9706439507190061, 0.8230482785301678, 0.6852464900141956, 0.5447296163219868, 0.3979063268632257], "prob": [[0.0002932533060910586, 28], [5.746599571007755e-06, 42], [1.8032583280389908e-10, 17], [1.5689760035640321e-13, 21], [1.5448174777765563e-13, 6]], "generated": ["Immanuel Kant (1724-1804) lived from 1724 to 1804.", "Franklin Pierce (1804-1869) was the 14th president of the United States and lived from 1804 to 1869.", "Franklin Pierce was born two years after Immanuel Kant’s death.", "Thus, Immanuel Kant didn’t meet the 14th president of the United States.", "So the answer is no."], "conf_comments": ["(A)\nFranklin Pier", "(A)\nThus, Im", "(A)\nThus, Im", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8374141576778918, 0.6947911638514177, 0.467300274356827, 0.36943308200723163, 0.3179077196252157], "conf": [0.9836839548882849, 0.8155622612680397, 0.6714266540860938, 0.5573033999021005, 0.4136584429020201], "prob": [[0.024169914518771027, 11], [0.00013231694770744892, 28], [3.0255481457034e-08, 14], [7.822004008292374e-11, 21], [7.712591361725387e-11, 6]], "generated": ["Immanuel Kant died in 1804.", "Franklin Pierce was the 14th president of the United States, and he was born in 1804.", "This means that Immanuel Kant could not have met Franklin Pierce.", "Thus, Immanuel Kant did not ever meet the 14th president of the United States.", "So the answer is no."], "conf_comments": ["(A)\nFranklin Pier", "(A)\nThus, Im", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8593396029046589, 0.7830958634931254, 0.4709929291953847, 0.36414868709416753, 0.3096275664140183], "conf": [0.9751514586529677, 0.8464019544580506, 0.7093810897492313, 0.5718221539736511, 0.41449574941576645], "prob": [[0.002913460657739485, 21], [0.0008446691097174792, 28], [1.5238217191863263e-08, 13], [3.124997972936307e-10, 13], [3.0762778987882076e-10, 6]], "generated": ["Immanuel Kant (1724-1804) was a German philosopher.", "Franklin Pierce (1804-1869) was the 14th president of the United States.", "Immanuel Kant had already died before Franklin Pierce was born.", "Thus, Immanuel Kant and Franklin Pierce never met.", "So the answer is no."], "conf_comments": ["(A)\nFranklin Pier", "(A)\nThus, Im", "(A)\nThus, Im", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8374141576778918, 0.6947911638514177, 0.467300274356827, 0.3813202257609837], "conf": [0.9836839548882849, 0.8155622612680397, 0.6714266540860938, 0.48716644921164065], "prob": [[0.024169914518771027, 11], [0.00013231694770744892, 28], [3.0255481457034e-08, 14], [1.8074112187895237e-08, 6]], "generated": ["Immanuel Kant died in 1804.", "Franklin Pierce was the 14th president of the United States, and he was born in 1804.", "This means that Immanuel Kant could not have met Franklin Pierce.", "So the answer is no."], "conf_comments": ["(A)\nFranklin Pier", "(A)\nThus, Im", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 66.64803886413574}
{"index": 75, "question": "Would eliminating competition in the Japanese bulk carrier market be profitable for a steel company?", "answer": "yes", "explanation": "Yes. 62% of bulk carriers are built in Japan. Bulk carrier hulls are made of steel.", "generated": [{"finished": true, "length": 5, "score": [0.5032069733811365, 0.3512649226047655, 0.2144970270882334, 0.07774672060927185, 0.058431782621178145], "conf": [0.929526434200275, 0.7413550366341949, 0.5658653714952001, 0.39277995656940423, 0.2596914100890443], "prob": [[0.00011133222739853824, 7], [4.928903739521163e-07, 11], [5.187142039315279e-12, 16], [1.805661026200282e-26, 20], [7.020875981464952e-27, 6]], "generated": ["Steel is a raw material.", "Bulk carriers are ships that transport raw materials.", "Thus, steel companies would be affected by the Japanese bulk carrier market.", "Since it is a market, it is only profitable to reduce competition in order to increase profit.", "So the answer is yes."], "conf_comments": ["(A)\nBulk carriers", "(A)\nThus, elimin", "(A)\nEliminating", "(A)\nThus, elimin", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.44531797252212313, 0.19335488193258255, 0.11889845601894539, 0.08967580184747852], "conf": [0.873320910161666, 0.5195088417410152, 0.28226955335124737, 0.16205655516211148], "prob": [[2.2003856937248627e-10, 15], [2.329262236580561e-23, 26], [1.1513583458019932e-26, 21], [1.0893909322699861e-26, 6]], "generated": ["Steel companies are usually related to the automobile industry or construction industry.", "Thus, steel companies do not typically engage in the Japanese bulk carrier market, which is in the shipping sector.", "Thus, eliminating competition in the Japanese bulk carrier market is not profitable for steel companies.", "So the answer is no."], "conf_comments": ["(A)\nThe Japanese bulk car", "(A)\nThus, elimin", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5032069733811365, 0.3512649226047655, 0.13858285018029834, 0.06311990246456654, 0.0489366481680132], "conf": [0.929526434200275, 0.7413550366341949, 0.48123418095295184, 0.3093682607783119, 0.19074995097571462], "prob": [[0.00011133222739853824, 7], [4.928903739521163e-07, 11], [8.868872201620387e-24, 27], [1.6281691718219795e-38, 30], [1.3975979665375148e-38, 6]], "generated": ["Steel is a raw material.", "Bulk carriers are ships that transport raw materials.", "This can indicate that the Japanese bulk carrier market is dominated by steel companies because they need steam ships to transport their raw materials.", "Thus, eliminating competition in a market that is dominated by steel companies, who are your competitors, is probably not very profitable.", "So the answer is no."], "conf_comments": ["(A)\nBulk carriers", "(A)\nThus, elimin", "(A)\nThus, elimin", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.44531797252212313, 0.19335488193258255, 0.1137365543252094, 0.08718846826057945], "conf": [0.873320910161666, 0.5195088417410152, 0.28658664588233446, 0.17101825100468424], "prob": [[2.2003856937248627e-10, 15], [2.329262236580561e-23, 26], [8.13971046838446e-28, 22], [7.423311115029654e-28, 6]], "generated": ["Steel companies are usually related to the automobile industry or construction industry.", "Thus, steel companies do not typically engage in the Japanese bulk carrier market, which is in the shipping sector.", "Therefore, eliminating competition in the Japanese bulk carrier market will not be profitable for steel companies.", "So the answer is no."], "conf_comments": ["(A)\nThe Japanese bulk car", "(A)\nThus, elimin", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5032069733811365, 0.3512649226047655, 0.13858285018029834, 0.06311990246456654, 0.0357744286074869], "conf": [0.929526434200275, 0.7413550366341949, 0.48123418095295184, 0.3093682607783119, 0.18138735558187297], "prob": [[0.00011133222739853824, 7], [4.928903739521163e-07, 11], [8.868872201620387e-24, 27], [1.6281691718219795e-38, 30], [2.4125158070276032e-40, 7]], "generated": ["Steel is a raw material.", "Bulk carriers are ships that transport raw materials.", "This can indicate that the Japanese bulk carrier market is dominated by steel companies because they need steam ships to transport their raw materials.", "Thus, eliminating competition in a market that is dominated by steel companies, who are your competitors, is probably not very profitable.", "So the answer is probably not."], "conf_comments": ["(A)\nBulk carriers", "(A)\nThus, elimin", "(A)\nThus, elimin", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5032069733811365, 0.3512649226047655, 0.2144970270882334, 0.07599859803329638, 0.029791205923558512], "conf": [0.929526434200275, 0.7413550366341949, 0.5658653714952001, 0.3877486428043871, 0.2289132488844191], "prob": [[0.00011133222739853824, 7], [4.928903739521163e-07, 11], [5.187142039315279e-12, 16], [2.8034578666249837e-25, 18], [3.23778640657921e-35, 17]], "generated": ["Steel is a raw material.", "Bulk carriers are ships that transport raw materials.", "Thus, steel companies would be affected by the Japanese bulk carrier market.", "If competition is eliminated, companies would lose competition, but they can increase their prices.", "In conclusion, profit would increase for a steel company, so the answer is yes."], "conf_comments": ["(A)\nBulk carriers", "(A)\nThus, elimin", "(A)\nEliminating", "(A)\nThus, elimin", "(A)\n# Is the above"], "info": null}, {"finished": false, "length": 5, "score": [0.5032069733811365, 0.3512649226047655, 0.2144970270882334, 0.07599859803329638, 0.023887868779693566], "conf": [0.929526434200275, 0.7413550366341949, 0.5658653714952001, 0.3877486428043871, 0.23015874681624232], "prob": [[0.00011133222739853824, 7], [4.928903739521163e-07, 11], [5.187142039315279e-12, 16], [2.8034578666249837e-25, 18], [2.694574115612404e-39, 18]], "generated": ["Steel is a raw material.", "Bulk carriers are ships that transport raw materials.", "Thus, steel companies would be affected by the Japanese bulk carrier market.", "If competition is eliminated, companies would lose competition, but they can increase their prices.", "So, eliminating competition would probably make profits, because companies can increase their prices.", "So the answer is no."], "conf_comments": ["(A)\nBulk carriers", "(A)\nThus, elimin", "(A)\nEliminating", "(A)\nThus, elimin", "(A)\nSo the answer is"], "info": [[[9.61160910492419e-15, 18], [0.012507247100241122, 6]], 0.5935771822478143, "(A)\nSo the answer is", 5]}, {"finished": false, "length": 5, "score": [0.5032069733811365, 0.3512649226047655, 0.2144970270882334, 0.07774672060927185, 0.01983858998032556], "conf": [0.929526434200275, 0.7413550366341949, 0.5658653714952001, 0.39277995656940423, 0.21832636498806052], "prob": [[0.00011133222739853824, 7], [4.928903739521163e-07, 11], [5.187142039315279e-12, 16], [1.805661026200282e-26, 20], [2.657945391955886e-42, 17]], "generated": ["Steel is a raw material.", "Bulk carriers are ships that transport raw materials.", "Thus, steel companies would be affected by the Japanese bulk carrier market.", "Since it is a market, it is only profitable to reduce competition in order to increase profit.", "However, eliminating competition altogether is not viable since there would no market participants.", "Thus, eliminating competition would be profitable for a steel company.", "So the answer is yes."], "conf_comments": ["(A)\nBulk carriers", "(A)\nThus, elimin", "(A)\nEliminating", "(A)\nThus, elimin", "(A)\nSo the answer is"], "info": [[[1.4720068459078928e-16, 17], [0.001062851491126281, 15], [0.9273641468037193, 6]], 0.5558490481412389, "(A)\nSo the answer is", 5]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 61.75511956214905}
{"index": 76, "question": "Would a geographer use biochemistry in their work?", "answer": "no", "explanation": "No. A geographer is a scientist whose area of study is geography, the study of Earth's natural environment and human society. Biochemistry is the study of chemical processes within and relating to living organisms.", "generated": [{"finished": true, "length": 4, "score": [0.7363923594406516, 0.35616175587202625, 0.2648754946521093, 0.2134015008535062], "conf": [0.9596989971897953, 0.6122773223073756, 0.4594645226378596, 0.30362842895042796], "prob": [[0.0005985295334389455, 13], [3.1483313379017615e-12, 19], [1.7592541315579253e-14, 17], [1.579989310485681e-14, 6]], "generated": ["Biochemistry is the study of biological molecules.", "In general, the job of a geographer is not to study biological molecules.", "Thus, a geographer may not use biochemistry in their work.", "So the answer is no."], "conf_comments": ["(A)\nGeography is the", "(A)\nThus, a", "(A)\nHowever, a ge", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7341061782394475, 0.4533252554459721, 0.2563463005858548, 0.21261202846158211], "conf": [0.966821941223443, 0.8106608230522192, 0.5997978263224496, 0.4234065908555863], "prob": [[0.0005014485564872746, 13], [1.6794865577905827e-09, 16], [3.6861328601189074e-21, 32], [3.156341200212229e-21, 6]], "generated": ["Geography is the study of the physical features of the earth.", "Biochemistry is chemistry applied to the study of living organisms.", "Since one is the study of physical features and the other is the study of living objects, a geographer would not use biochemistry in their work.", "So the answer is no."], "conf_comments": ["(A)\nBiochemistry", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7341061782394475, 0.3049874179548299, 0.2527673755492951, 0.20625851639203538], "conf": [0.966821941223443, 0.692675215242005, 0.5132126524109881, 0.34260131602008037], "prob": [[0.0005014485564872746, 13], [3.04140286877014e-18, 23], [8.393676157478728e-19, 17], [8.265947068424452e-19, 6]], "generated": ["Geography is the study of the physical features of the earth.", "Geography is not a science, so does not include biology, which encompasses biochemistry.", "Thus, a geographer would not use biochemistry in their work.", "So the answer is no."], "conf_comments": ["(A)\nBiochemistry", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7341061782394475, 0.3049874179548299, 0.23591500992468914, 0.19290799235107112], "conf": [0.966821941223443, 0.692675215242005, 0.5051303257664475, 0.3386214345030836], "prob": [[0.0005014485564872746, 13], [3.04140286877014e-18, 23], [1.9059128635106123e-19, 14], [1.8766046922797574e-19, 6]], "generated": ["Geography is the study of the physical features of the earth.", "Geography is not a science, so does not include biology, which encompasses biochemistry.", "Thus, a geographer would not use biochemistry.", "So the answer is no."], "conf_comments": ["(A)\nBiochemistry", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7341061782394475, 0.4533252554459721, 0.2563463005858548, 0.19257264879030794], "conf": [0.966821941223443, 0.8106608230522192, 0.5997978263224496, 0.45418275537447644], "prob": [[0.0005014485564872746, 13], [1.6794865577905827e-09, 16], [3.6861328601189074e-21, 32], [3.507888285663013e-22, 8]], "generated": ["Geography is the study of the physical features of the earth.", "Biochemistry is chemistry applied to the study of living organisms.", "Since one is the study of physical features and the other is the study of living objects, a geographer would not use biochemistry in their work.", "Thus, the answer is no."], "conf_comments": ["(A)\nBiochemistry", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.4555345398412678, 0.30683842493489283, 0.2572295347460589], "conf": [0.8568732993169332, 0.6609301427025869, 0.47170717420501074], "prob": [[4.814157266748154e-13, 20], [2.0125222661670754e-17, 19], [1.8347301950965463e-17, 6]], "generated": ["Geographers study the form of the earth, while biochemist study molecular biology.", "Thus, it would not be common for a geographer to use biochemistry.", "So the answer is no."], "conf_comments": ["(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 50.35156178474426}
{"index": 77, "question": "Is most coffee produced South of the Equator?", "answer": "yes", "explanation": "Yes. The countries with the highest coffee production are in South America. Almost all of South America is in the Southern Hemisphere.", "generated": [{"finished": true, "length": 3, "score": [0.691689291984523, 0.49169879167804825, 0.42546380924938926], "conf": [0.9397051661981699, 0.7780558380556057, 0.5846958007089292], "prob": [[1.2604963063026279e-13, 44], [1.2541224450953512e-16, 14], [1.2268359202733463e-16, 6]], "generated": ["Around 70% of coffee is grown between of the Tropic of Cancer (23.5 degrees North latitude) and the Tropic of Capricorn (23.5 degrees South latitude).", "Thus, most coffee is produced in South of the Equator.", "So the answer is yes."], "conf_comments": ["(A)\nThus, most", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6172265784467821, 0.43616532005670244, 0.37079508196622507, 0.3162432133580762], "conf": [0.9451293486676061, 0.8269258654255396, 0.6535058048873897, 0.47666251172358165], "prob": [[7.416070546361359e-06, 13], [1.5523460028581122e-08, 11], [4.856963679580083e-09, 13], [4.777960539767683e-09, 6]], "generated": ["Most countries that produce coffee are located in the Tropics.", "The Tropics are in the Southern Hemisphere.", "Thus, most coffee is produced South of the Equator.", "So the answer is yes."], "conf_comments": ["(A)\nThe Tropics are", "(A)\nThus, most", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6172265784467821, 0.43616532005670244, 0.3036339716964385, 0.2546248757852523], "conf": [0.9451293486676061, 0.8269258654255396, 0.6525910449378028, 0.4618021777479541], "prob": [[7.416070546361359e-06, 13], [1.5523460028581122e-08, 11], [1.033587848500952e-11, 15], [9.955517305623976e-12, 6]], "generated": ["Most countries that produce coffee are located in the Tropics.", "The Tropics are in the Southern Hemisphere.", "Thus, most countries that produce coffee are south of the Equator.", "So the answer is yes."], "conf_comments": ["(A)\nThe Tropics are", "(A)\nThus, most", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5667873749416653, 0.32054555991362155, 0.2544397456094294, 0.21591113605683157], "conf": [0.9556705213972658, 0.7903488357270307, 0.612268991652389, 0.4419481912395691], "prob": [[1.288959740648677e-11, 23], [4.830071588197283e-19, 18], [3.2917255479936553e-20, 13], [3.244368098621547e-20, 6]], "generated": ["In the world, over 70% of all coffee beans are produced in the southern hemisphere.", "In the southern hemisphere, coffee beans are mainly produced in South American countries.", "Thus, most coffee is produced south of the Equator.", "So the answer is yes."], "conf_comments": ["(A)\nThe Equator is", "(A)\nThus, most", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6172265784467821, 0.43200907107932707, 0.29571892138018896], "conf": [0.9451293486676061, 0.8366531912461654, 0.6742990119343656], "prob": [[7.416070546361359e-06, 13], [1.998004979564932e-08, 10], [2.6080752333061706e-10, 8]], "generated": ["Most countries that produce coffee are located in the Tropics.", "The Tropics lie south of the Equator.", "Thus, the answer is yes."], "conf_comments": ["(A)\nThe Tropics are", "(A)\nThus, most", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 47.61387038230896}
{"index": 78, "question": "Is it hard to get a BLT in Casablanca?", "answer": "yes", "explanation": "Yes. A BLT is a sandwich consisting of bacon, lettuce and tomato. Casablanca is predominantly Muslim. Islam forbids the consumption of pork and pork products.", "generated": [{"finished": true, "length": 4, "score": [0.6607096824489805, 0.4631932649906494, 0.32294559716520843, 0.2484845950582107], "conf": [0.8454165833585561, 0.6247035524424173, 0.39690013982809047, 0.24699145734577863], "prob": [[1.776910937766177e-08, 27], [5.101101747154448e-12, 20], [5.382980163802753e-14, 17], [3.9907995412547973e-14, 6]], "generated": ["[Casablanca Subreddit](https://www.reddit.com/r/Casablanca/)", "[BLT Reddit](https://www.reddit.com/r/BLT/)", "Thus, it is hard to get a BLT in Casablanca.", "So the answer is yes."], "conf_comments": ["(A)\nThe Casablan", "(A)\nThus, it", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6836546849619117, 0.3539901631715083, 0.2779336692616543, 0.2137476214070488], "conf": [0.9489239389860806, 0.7664981651487497, 0.5447897507054713, 0.32360593136215515], "prob": [[1.4335126411840153e-06, 19], [1.032513952155159e-14, 17], [9.183158090299278e-16, 17], [8.949285181157756e-16, 6]], "generated": ["BLT is acronym for bacon, lettuce, and tomato sandwich.", "You can get a BLT in any Burger King in Casablanca.", "Thus, it is easy to get a BLT in Casablanca.", "So the answer is no."], "conf_comments": ["(A)\nCasablan", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6607096824489805, 0.4631932649906494, 0.2658048777518326, 0.20275363014346237], "conf": [0.8454165833585561, 0.6247035524424173, 0.43969116705271694, 0.25922350512139325], "prob": [[1.776910937766177e-08, 27], [5.101101747154448e-12, 20], [5.887918375275685e-18, 18], [5.440893975924675e-18, 6]], "generated": ["[Casablanca Subreddit](https://www.reddit.com/r/Casablanca/)", "[BLT Reddit](https://www.reddit.com/r/BLT/)", "Thus, a BLT in Casablanca should be relatively easy to get.", "So the answer is no."], "conf_comments": ["(A)\nThe Casablan", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7375673528956393, 0.49306593639251795, 0.2624073168768523, 0.16818774195254232, 0.13008345954030542], "conf": [0.9616034258678131, 0.8325582992587492, 0.5706108042366481, 0.33006687188784195, 0.204391732049005], "prob": [[0.0033579458227953086, 10], [2.2170379296959582e-10, 25], [7.98982200985604e-19, 22], [2.376345511090318e-21, 17], [1.9314084516739676e-21, 6]], "generated": ["A BLT is a type of sandwich.", "According to Wikipedia, “A BLT is a sandwich consisting of bacon, lettuce, and tomato.”", "According to Google, “Casablanca is known for its history, culture, and economy.”", "Thus, it is unlikely to get a BLT in Casablanca.", "So the answer is no."], "conf_comments": ["(A)\nCasablan", "(A)\nCasablan", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7380109628649342, 0.35924215784603275, 0.2526598074025727, 0.1921349236574341], "conf": [0.9548321313796185, 0.6641741519864827, 0.4346813917431386, 0.26811914058000547], "prob": [[7.167640571215836e-05, 17], [3.679973548350957e-15, 22], [2.3834189756518434e-17, 18], [1.6184585858443043e-17, 6]], "generated": ["\"BLT\" stands for \"bacon, lettuce, and tomato\".", "Unfortunately I was unable to find a sandwich shop named \"BLT\" in Casablanca.", "Thus, it might be hard to get a BLT in Casablanca.", "So the answer is yes."], "conf_comments": ["(A)\nCasablan", "(A)\nThus, it", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 52.768937826156616}
{"index": 79, "question": "Could largest ship from Voyages of Christopher Columbus haul Statue of Liberty?", "answer": "no", "explanation": "No. The largest ship Christopher Columbus used was the Santa Maria. The Santa Maria has a cargo capacity of 108 tons. The Statue of Liberty weighs 225 tons.", "generated": [{"finished": true, "length": 5, "score": [0.6836127317872307, 0.4199370897923913, 0.2794056374750695, 0.2151396533842544, 0.18900741434812535], "conf": [0.9681567877353229, 0.8161831460974257, 0.6656543167872844, 0.5340338161927501, 0.4134745166110998], "prob": [[2.0236836738460535e-06, 18], [4.713695040452781e-13, 19], [7.162563787930992e-23, 37], [1.111567258557511e-26, 29], [1.0908281166809093e-26, 6]], "generated": ["The largest ship from Voyages of Christopher Columbus is 70 feet long.", "The pedestal of the Statue of Liberty is around 42 feet tall.", "In other words, the largest ship from Voyages of Christopher Columbus is less than 3/7 as long as the pedestal of the Statue of Liberty.", "Thus, it would be impossible to haul the Statue of Liberty using the largest ship from Voyages of Christopher Columbus.", "So the answer is no."], "conf_comments": ["(A)\nThe Statue of", "(A)\nThe Statue of", "(A)\nThe Statue of", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6836127317872307, 0.4199370897923913, 0.2794056374750695, 0.20910176273305303, 0.17784382358621484], "conf": [0.9681567877353229, 0.8161831460974257, 0.6656543167872844, 0.5211651176204143, 0.40222203746377905], "prob": [[2.0236836738460535e-06, 18], [4.713695040452781e-13, 19], [7.162563787930992e-23, 37], [1.1818130088955487e-26, 26], [8.012697607967885e-27, 6]], "generated": ["The largest ship from Voyages of Christopher Columbus is 70 feet long.", "The pedestal of the Statue of Liberty is around 42 feet tall.", "In other words, the largest ship from Voyages of Christopher Columbus is less than 3/7 as long as the pedestal of the Statue of Liberty.", "So it is impossible for the largest ship from Voyages of Christopher Columbus to haul the Statue of Liberty.", "So the answer is no."], "conf_comments": ["(A)\nThe Statue of", "(A)\nThe Statue of", "(A)\nThe Statue of", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6836127317872307, 0.4428606008602319, 0.2983203647524599, 0.2478948951596742], "conf": [0.9681567877353229, 0.7556876015252444, 0.5627928650091873, 0.39072899706758324], "prob": [[2.0236836738460535e-06, 18], [3.414943040961679e-10, 14], [2.786548288457071e-14, 19], [2.697245028222959e-14, 6]], "generated": ["The largest ship from Voyages of Christopher Columbus is 70 feet long.", "The Stature of Liberty is 305 feet tall.", "Thus, the ship would not be able to haul the Statute of Liberty.", "So the answer is no."], "conf_comments": ["(A)\nThe Statue of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.5842983742382067, 0.3897454548606932, 0.23467580889050713, 0.1730215428670469, 0.12563992602348045, 0.1067637057166838], "conf": [0.972972750752441, 0.9145044805413638, 0.8284218731312405, 0.7115267659786371, 0.534162177135468, 0.3898060162764059], "prob": [[5.280789465574395e-08, 16], [1.4124805654957189e-11, 11], [1.5297294040696942e-17, 15], [1.6013086580186658e-20, 15], [2.3377730454710707e-24, 25], [2.1943279279033e-24, 6]], "generated": ["The Santa Maria (ship) is the largest ship used by Christopher Columbus.", "It had a capacity of 100 tons.", "One ton is equivalent to 907.185 kg.", "The height of the Statue of Liberty is 46 meters.", "Thus, the largest ship from the Voyages of Christopher Columbus cannot haul the Statue of Liberty.", "So the answer is no."], "conf_comments": ["(A)\nThe Santa Maria (", "(A)\nThe Statue of", "(A)\nThe Statue of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.5842983742382067, 0.3897454548606932, 0.2562346306249778, 0.14657071472672453, 0.08609741604911182, 0.0736651682964764], "conf": [0.972972750752441, 0.9145044805413638, 0.8015905152730957, 0.5829425000955035, 0.4533514302497804, 0.337845510823195], "prob": [[5.280789465574395e-08, 16], [1.4124805654957189e-11, 11], [3.5009384498497134e-16, 15], [6.190110950605482e-27, 31], [3.146637607214185e-32, 15], [2.827564808557969e-32, 6]], "generated": ["The Santa Maria (ship) is the largest ship used by Christopher Columbus.", "It had a capacity of 100 tons.", "The height of Statue of Liberty is 305 ft.", "If we assume the weight of Statue of Liberty is around 300 tons, it is about 3 times the capacity of Santa Maria.", "Thus, Santa Maria would have trouble carrying Statue of Liberty.", "So the answer is no."], "conf_comments": ["(A)\nThe Santa Maria (", "(A)\nThe Statue of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 6, "score": [0.5842983742382067, 0.3897454548606932, 0.2562346306249778, 0.14657071472672453, 0.08609741604911182, 0.03932655424336721], "conf": [0.972972750752441, 0.9145044805413638, 0.8015905152730957, 0.5829425000955035, 0.4533514302497804, 0.3453126010581154], "prob": [[5.280789465574395e-08, 16], [1.4124805654957189e-11, 11], [3.5009384498497134e-16, 15], [6.190110950605482e-27, 31], [3.146637607214185e-32, 15], [7.513007654713226e-47, 26]], "generated": ["The Santa Maria (ship) is the largest ship used by Christopher Columbus.", "It had a capacity of 100 tons.", "The height of Statue of Liberty is 305 ft.", "If we assume the weight of Statue of Liberty is around 300 tons, it is about 3 times the capacity of Santa Maria.", "Thus, Santa Maria would have trouble carrying Statue of Liberty.", "Even if it was theoretically possible, it required 3 Santa Maria’s to haul Statue of Liberty.", "So the answer is no."], "conf_comments": ["(A)\nThe Santa Maria (", "(A)\nThe Statue of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[2.3876304146014206e-15, 26], [0.763822517296968, 6]], 0.7616885665671342, "(A)\nSo the answer is", 6]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 69.73617887496948}
{"index": 80, "question": "Would Jolly Green Giant's largest monument look impressive next to Pyrenees?", "answer": "no", "explanation": "No. The Jolly Green Giant monument in Blue Earth, Minnesota is 55.5 feet tall. The Pyrenees mountains are 11,168 feet high. The Pyrenees mountains are 305 miles wide.", "generated": [{"finished": true, "length": 4, "score": [0.6576735655821402, 0.38403213527643626, 0.3129244529995065, 0.264020736505122], "conf": [0.9662766070758126, 0.6601485437225044, 0.5257494501111164, 0.376534082674343], "prob": [[2.0903776345328392e-08, 22], [2.389078264776056e-15, 23], [3.036627085755058e-17, 24], [2.928337319039094e-17, 6]], "generated": ["According to the source, Jolly Green Giant's largest monument is 70 feet tall.", "The source also said that the height of the Pyrenees is around 10,000 feet.", "Thus, Jolly Green Giant's largest monument would not look impressive next to the Pyrenees.", "So the answer is no."], "conf_comments": ["(A)\nThe Pyrenees", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7194310771229092, 0.45441564791735006, 0.2860698142950603, 0.24139710896032618], "conf": [0.9768163381453033, 0.7469803390849786, 0.582109222475819, 0.4162088645760553], "prob": [[0.0001375044087551562, 14], [1.6842639535073999e-12, 28], [1.7148558101417462e-17, 17], [1.6730422195991247e-17, 6]], "generated": ["The Jolly Green Giant monument is 49 feet tall.", "In comparison, the Pyrenees mountains are about 15,000 feet tall, which is much larger than the monument.", "Thus, the monument would not look at all impressive next to the mountains.", "So the answer is no."], "conf_comments": ["(A)\nThe Pyrenees", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7194310771229092, 0.45441564791735006, 0.25730786762771696, 0.21467054547495437], "conf": [0.9768163381453033, 0.7469803390849786, 0.5940518865737408, 0.42101473249202065], "prob": [[0.0001375044087551562, 14], [1.6842639535073999e-12, 28], [5.0488363411053665e-18, 14], [4.5309215167925204e-18, 6]], "generated": ["The Jolly Green Giant monument is 49 feet tall.", "In comparison, the Pyrenees mountains are about 15,000 feet tall, which is much larger than the monument.", "Thus, it would look really small next to Pyrenees.", "So the answer is no."], "conf_comments": ["(A)\nThe Pyrenees", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6576735655821402, 0.3579136453997681, 0.22504741758419924, 0.18535102856889868], "conf": [0.9662766070758126, 0.8326034140998044, 0.6415652052279842, 0.4401354195875231], "prob": [[2.0903776345328392e-08, 22], [1.1071465484568259e-17, 20], [3.2293681767997277e-25, 26], [3.0178389362555e-25, 6]], "generated": ["According to the source, Jolly Green Giant's largest monument is 70 feet tall.", "In comparison, the Pyrenees Mountains have an elevation of about 400 feet.", "Thus, the largest monument of Jolly Green Giant would look unimpressive compared to the Pyrenees mountains.", "So the answer is no."], "conf_comments": ["(A)\nThe Pyrenees", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6576735655821402, 0.3579136453997681, 0.23275253311992947, 0.1673049199950189], "conf": [0.9662766070758126, 0.8326034140998044, 0.6367193223403073, 0.39439812952874714], "prob": [[2.0903776345328392e-08, 22], [1.1071465484568259e-17, 20], [2.2645897795878825e-24, 26], [7.628518372963024e-25, 6]], "generated": ["According to the source, Jolly Green Giant's largest monument is 70 feet tall.", "In comparison, the Pyrenees Mountains have an elevation of about 400 feet.", "Thus, the Pyrenees Mountains are about 70 times higher than Jolly Green Giant's largest monument.", "So the answer is no."], "conf_comments": ["(A)\nThe Pyrenees", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6576735655821402, 0.3579136453997681, 0.23275253311992947, 0.16428292215770052], "conf": [0.9662766070758126, 0.8326034140998044, 0.6367193223403073, 0.43052947937086894], "prob": [[2.0903776345328392e-08, 22], [1.1071465484568259e-17, 20], [2.2645897795878825e-24, 26], [3.6227335356970336e-25, 6]], "generated": ["According to the source, Jolly Green Giant's largest monument is 70 feet tall.", "In comparison, the Pyrenees Mountains have an elevation of about 400 feet.", "Thus, the Pyrenees Mountains are about 70 times higher than Jolly Green Giant's largest monument.", "So the answer is yes."], "conf_comments": ["(A)\nThe Pyrenees", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 53.33687114715576}
{"index": 81, "question": "Did Karl Marx influence the communist party of China?", "answer": "yes", "explanation": "Yes. Communist ideology is the foundation of communist party of China. Marx produced a political pamphlet that has since come to be commonly known as the communist manifesto. ", "generated": [{"finished": true, "length": 3, "score": [0.6406685079653911, 0.5268786714325788, 0.4403687432261964], "conf": [0.8475359242375574, 0.6175416550359524, 0.4323015515896499], "prob": [[2.439390893376527e-07, 21], [9.261553351642866e-08, 13], [9.145996303245082e-08, 6]], "generated": ["Karl Marx is the creator of communism and communism influenced the communist party of China.", "Thus, Karl Marx influenced the communist party of China.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Karl", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8210623877779392, 0.5161080237268599, 0.4055141408726617, 0.32425136999487797], "conf": [0.9853495105631731, 0.8764262917680032, 0.6084386459880693, 0.39076809361488457], "prob": [[0.015373831488742353, 11], [1.2068959642310883e-10, 23], [2.0754424699065387e-11, 15], [2.0202572151453203e-11, 6]], "generated": ["Karl Marx died in 1883.", "In 1921, the communist party of China founded by Chen Duxiu in Shanghai.", "Thus, Karl Marx did not influence the communist party of China.", "So the answer is no."], "conf_comments": ["(A)\nThe Communist Party", "(A)\nThus, Karl", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8210623877779392, 0.5161080237268599, 0.35979650854831907, 0.29433404947357017], "conf": [0.9853495105631731, 0.8764262917680032, 0.6366500524468539, 0.43914339893493676], "prob": [[0.015373831488742353, 11], [1.2068959642310883e-10, 23], [1.944582646756681e-13, 16], [1.6217938821703425e-13, 6]], "generated": ["Karl Marx died in 1883.", "In 1921, the communist party of China founded by Chen Duxiu in Shanghai.", "Therefore, Karl Marx could not have influenced the communist party of China.", "So the answer is no."], "conf_comments": ["(A)\nThe Communist Party", "(A)\nThus, Karl", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 7, "score": [0.8063862250524788, 0.530838914081275, 0.37610255524755626, 0.3097833382976491, 0.1887546494071882, 0.12391494614599564, 0.10290056577591766], "conf": [0.9694693893393987, 0.8504525735909493, 0.6971484084367928, 0.5015822275211054, 0.3526980851578351, 0.25753030647494957, 0.1783454229701179], "prob": [[0.01842981597863244, 10], [2.7942405152078614e-08, 19], [1.0925833046150788e-11, 16], [3.795020625837617e-12, 18], [9.558278287944442e-21, 31], [3.513840651343611e-24, 15], [3.425383205390866e-24, 6]], "generated": ["Karl Marx was a German philosopher.", "Mao Zedong founded the Communist party of China in 1921.", "Karl Marx lived from 1818-1883.", "Mao Zedong lived from 1893-1976.", "Since Karl Marx died in 1883, he could not have influenced the Chinese Communist party which was founded in 1921.", "Thus, Karl Marx does not influence the Communist party of China.", "So the answer is no."], "conf_comments": ["(A)\nThe Communist Party", "(A)\nThus, Karl", "(A)\nThus, Karl", "(A)\nThus, Karl", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8210623877779392, 0.5161080237268599, 0.35979650854831907, 0.26812003203610923], "conf": [0.9853495105631731, 0.8764262917680032, 0.6366500524468539, 0.4656536091686957], "prob": [[0.015373831488742353, 11], [1.2068959642310883e-10, 23], [1.944582646756681e-13, 16], [2.1472664543834753e-14, 8]], "generated": ["Karl Marx died in 1883.", "In 1921, the communist party of China founded by Chen Duxiu in Shanghai.", "Therefore, Karl Marx could not have influenced the communist party of China.", "Thus, the answer is no."], "conf_comments": ["(A)\nThe Communist Party", "(A)\nThus, Karl", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 55.810189723968506}
{"index": 82, "question": "Does Soylent use Pea for their source of protein? ", "answer": "no", "explanation": "No. Soylent is a meal replacement drink that offers 20mg protein. The protein in Soylent is derived from Soy.", "generated": [{"finished": true, "length": 4, "score": [0.6287895384403709, 0.3994268829363993, 0.3321153549576979, 0.2895419996803154], "conf": [0.9562752385996452, 0.780656537445842, 0.6033827602074329, 0.4601934849109833], "prob": [[0.0008539287500835625, 8], [4.4385172555311146e-08, 14], [8.332865758477323e-09, 15], [8.161694711756957e-09, 6]], "generated": ["Peas are a source of protein.", "Soylent uses Pea for some of their source of protein.", "Thus, Soylent uses Pea for their source of protein.", "So the answer is yes."], "conf_comments": ["(A)\nSoylent is", "(A)\nThus, So", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.577810122014254, 0.45521698854207476, 0.3853075406166456], "conf": [0.9150479128801273, 0.7409591981601014, 0.5341370531234114], "prob": [[7.409184115808492e-07, 14], [1.0520324938548241e-08, 16], [1.0137882272895521e-08, 6]], "generated": ["Soylent’s protein source is made from yellow pea protein", "Thus, Soylent does use Pea for their source of protein.", "So the answer is yes."], "conf_comments": ["(A)\nYellow pea", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6287895384403709, 0.3994268829363993, 0.2959993508998072, 0.25372438610991815], "conf": [0.9562752385996452, 0.780656537445842, 0.5790716252873803, 0.4280785644484612], "prob": [[0.0008539287500835625, 8], [4.4385172555311146e-08, 14], [2.6768681585104706e-10, 17], [2.580698218869044e-10, 6]], "generated": ["Peas are a source of protein.", "Soylent uses Pea for some of their source of protein.", "Thus, Soylent uses Pea for some of their source of protein.", "So the answer is yes."], "conf_comments": ["(A)\nSoylent is", "(A)\nThus, So", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.577810122014254, 0.43884198630802895], "conf": [0.9150479128801273, 0.7220136772702747], "prob": [[7.409184115808492e-07, 14], [1.1309337573658769e-07, 6]], "generated": ["Soylent’s protein source is made from yellow pea protein", "So the answer is yes."], "conf_comments": ["(A)\nYellow pea", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5727063778229683, 0.30406719855370395, 0.2720397858115368], "conf": [0.9125307532161427, 0.7862168508588501, 0.6328093483151279], "prob": [[6.021836324265186e-15, 32], [7.906260038254233e-19, 8], [7.647897868003511e-19, 6]], "generated": ["According to the ingredients list on their website, the source of protein is Sustainable Certified Organic Pea Protein Isolate.", "Thus, they do use it.", "So the answer is yes."], "conf_comments": ["(A)\nThus, So", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6287895384403709, 0.3994268829363993, 0.2959993508998072, 0.1617714098694449], "conf": [0.9562752385996452, 0.780656537445842, 0.5790716252873803, 0.43189379712406784], "prob": [[0.0008539287500835625, 8], [4.4385172555311146e-08, 14], [2.6768681585104706e-10, 17], [1.7711107348028555e-13, 8]], "generated": ["Peas are a source of protein.", "Soylent uses Pea for some of their source of protein.", "Thus, Soylent uses Pea for some of their source of protein.", "Thus, the answer is yes."], "conf_comments": ["(A)\nSoylent is", "(A)\nThus, So", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 42.76370906829834}
{"index": 83, "question": "Do most religious people in Quebec refer to the Quran?", "answer": "no", "explanation": "No. Christianity is the major religion in Quebec. Christians refer to the Bible as their book of reference.", "generated": [{"finished": true, "length": 3, "score": [0.5571657829460516, 0.3789186633106308, 0.30056585486649806], "conf": [0.9583305987965167, 0.6286203892929102, 0.40327487105979726], "prob": [[5.500864565732864e-12, 23], [1.0207776147566348e-14, 18], [9.08612475282684e-15, 6]], "generated": ["As of 2021, 75% of the population in Quebec said they have no religion.", "Thus, most religious people in Quebec probably do not refer to the Quran.", "So the answer is no."], "conf_comments": ["(A)\nThus, most", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6152479463783437, 0.4037537792014425, 0.22211676353807758, 0.1628965537719662, 0.13094151562358275], "conf": [0.9612490260985068, 0.8341812478714258, 0.5901827701029319, 0.3648285416066257, 0.23686038510513327], "prob": [[8.040603641464607e-09, 20], [3.614768197932693e-12, 11], [1.1905782357124755e-20, 23], [1.117527451504238e-21, 17], [1.085980583399668e-21, 6]], "generated": ["In 2011, Quebec was 80% Christian and 6% Muslim.", "Thus, most religious people in Quebec were Christian.", "The Quran is the Islamic holy book and thus only Muslims would refer to the Quran.", "Thus, most religious people in Quebec did not refer to the Quran.", "So the answer is no."], "conf_comments": ["(A)\nThus, most", "(A)\nSince most religious people", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6152479463783437, 0.4037537792014425, 0.22211676353807758, 0.16216461733112658, 0.13043482501069828], "conf": [0.9612490260985068, 0.8341812478714258, 0.5901827701029319, 0.3656092881860223, 0.2377555357142088], "prob": [[8.040603641464607e-09, 20], [3.614768197932693e-12, 11], [1.1905782357124755e-20, 23], [9.246516486008153e-22, 17], [8.964934996224088e-22, 6]], "generated": ["In 2011, Quebec was 80% Christian and 6% Muslim.", "Thus, most religious people in Quebec were Christian.", "The Quran is the Islamic holy book and thus only Muslims would refer to the Quran.", "Thus, most religious people in Quebec do not refer to the Quran.", "So the answer is no."], "conf_comments": ["(A)\nThus, most", "(A)\nSince most religious people", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5387961324150948, 0.3526711451570134, 0.29246267473681176], "conf": [0.9673281903824889, 0.7184013286858011, 0.5139209477890054], "prob": [[1.0538456115693593e-11, 21], [9.1503303801633e-16, 17], [7.222172368293857e-16, 6]], "generated": ["As of 2019, only 6.3% of inhabitants in Quebec were religious.", "Therefore, most religious people in Quebec would not refer to the Quran.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThe Quran", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5387961324150948, 0.3526711451570134, 0.26369070724752186], "conf": [0.9673281903824889, 0.7184013286858011, 0.5455891749516638], "prob": [[1.0538456115693593e-11, 21], [9.1503303801633e-16, 17], [7.889440102530673e-17, 8]], "generated": ["As of 2019, only 6.3% of inhabitants in Quebec were religious.", "Therefore, most religious people in Quebec would not refer to the Quran.", "Thus, the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThe Quran", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 48.4317786693573}
{"index": 84, "question": "Does Family Guy take place on the American West Coast?", "answer": "no", "explanation": "No. Family Guy takes place in the fictional town of Quahog, Rhode Island. Rhode Island is a state on the American East Coast.", "generated": [{"finished": true, "length": 4, "score": [0.6495421361349788, 0.4384569429490131, 0.2958743248521379, 0.2558664685850719], "conf": [0.9119409469237754, 0.8156556287635462, 0.6578876686450099, 0.49347199426842947], "prob": [[2.0181750948258983e-07, 20], [1.2107352839650814e-10, 11], [4.046408042989422e-14, 14], [3.974460926010151e-14, 6]], "generated": ["From the wikipedia page, Family Guy takes place in Quahog, Rhode Island.", "Quahog is not on the American West Coast.", "Thus, Family Guy is not based on the American West Coast.", "So the answer is no."], "conf_comments": ["(A)\nRhode Island", "(A)\nThus, Family", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6389829600346684, 0.3707901346199429, 0.3112627932250261, 0.2557634965804792], "conf": [0.9511217657915462, 0.7792975335716739, 0.6062339700213027, 0.41029339655869407], "prob": [[5.709806188916133e-07, 17], [6.385755382365958e-14, 18], [1.4492021682949872e-14, 15], [1.4286953219231035e-14, 6]], "generated": ["Family Guy occurs in a fictional town in Rhode Island called Quahog.", "The closest major city to Rhode Island is Boston, which is in the East Coast.", "Thus, Family Guy does not take place on the American West Coast.", "So the answer is no."], "conf_comments": ["(A)\nRhode Island", "(A)\nThus, Family", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6389829600346684, 0.3707901346199429, 0.27909302571769345, 0.23095375371921872], "conf": [0.9511217657915462, 0.7792975335716739, 0.6236548198755011, 0.42819836784933235], "prob": [[5.709806188916133e-07, 17], [6.385755382365958e-14, 18], [3.5910541877072157e-16, 15], [3.5345172627854857e-16, 6]], "generated": ["Family Guy occurs in a fictional town in Rhode Island called Quahog.", "The closest major city to Rhode Island is Boston, which is in the East Coast.", "Thus, the show does not take place on the American West Coast.", "So the answer is no."], "conf_comments": ["(A)\nRhode Island", "(A)\nThus, Family", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.631872257702934, 0.433538141270227, 0.28876435461385846, 0.18800523710671102, 0.10173896321798186, 0.05995961469596412], "conf": [0.9611266632739496, 0.8620488033200252, 0.7429697937375564, 0.6230193956883162, 0.39804823708820736, 0.22391583355011818], "prob": [[2.3418496819349837e-08, 20], [1.6238466848165307e-14, 22], [2.0306266034468324e-19, 17], [2.4103297813446047e-25, 20], [8.481824777072354e-33, 22], [4.699514647324538e-34, 6]], "generated": ["The first episode of Family Guy aired in January 27, 1999.", "From 1999 to 2001, Family Guy took place in Rhode Island.", "From 2001 to present, it takes place in Quahog.", "Quahog is a fictional town which is based on Cranston in Rhode Island.", "As Cranston is in the East Coast, Quahog must be on the East Coast as well.", "So the answer is no."], "conf_comments": ["(A)\nThe first episode of", "(A)\nIn 20", "(A)\nQuahog is", "(A)\nThus, Family", "(A)\nThus, Family", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.6974146480055213, 0.38515598362678016], "conf": [0.9649028937978504, 0.7570621538401536], "prob": [[3.447289919062455e-05, 15], [1.0462606842033618e-25, 50]], "generated": ["Family Guy is a television series created by Seth MacFarlane.", "Seth MacFarlane is from Kent, Connecticut (about 85 miles from New York City) and the show Family Guy is based off of a short film he made when he was a student at Rhode Island School of Design (in"], "conf_comments": ["(A)\nThe show is set", "(A)\nProvidence,"], "info": null}, {"finished": false, "length": 6, "score": [0.631872257702934, 0.433538141270227, 0.28876435461385846, 0.18800523710671102, 0.10173896321798186, 0.04401940468397002], "conf": [0.9611266632739496, 0.8620488033200252, 0.7429697937375564, 0.6230193956883162, 0.39804823708820736, 0.2815643583708674], "prob": [[2.3418496819349837e-08, 20], [1.6238466848165307e-14, 22], [2.0306266034468324e-19, 17], [2.4103297813446047e-25, 20], [8.481824777072354e-33, 22], [1.8559462777862958e-41, 15]], "generated": ["The first episode of Family Guy aired in January 27, 1999.", "From 1999 to 2001, Family Guy took place in Rhode Island.", "From 2001 to present, it takes place in Quahog.", "Quahog is a fictional town which is based on Cranston in Rhode Island.", "As Cranston is in the East Coast, Quahog must be on the East Coast as well.", "Additionally, most of Rhode Island is also on the East Coast.", "With these observations, family Guy must be placed on the American East Coast.", "So the answer is no."], "conf_comments": ["(A)\nThe first episode of", "(A)\nIn 20", "(A)\nQuahog is", "(A)\nThus, Family", "(A)\nThus, Family", "(A)\nThus, Family"], "info": [[[2.1881450354918875e-09, 15], [4.315760257088162e-14, 15], [0.3599338093405557, 6]], 0.707362405196315, "(A)\nThus, Family", 6]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 53.24251866340637}
{"index": 85, "question": "Are lengths measured in metres in the UK?", "answer": "yes", "explanation": "Yes. Metres are part of the metric system. The UK uses the metric system for measurements.", "generated": [{"finished": true, "length": 3, "score": [0.5278188202815832, 0.4358800920504653, 0.37034434664810284], "conf": [0.9007830562035797, 0.7219411276018304, 0.5227619093938106], "prob": [[2.0731998204030238e-10, 19], [2.9869926735015585e-11, 12], [2.9328204179059756e-11, 6]], "generated": ["In the UK, the unit of length is the metric system measurement of ‘metre’.", "Thus, lengths are measured in metres in the UK.", "So the answer is yes."], "conf_comments": ["(A)\nIn the UK,", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.54557669721542, 0.42591246019771895, 0.3015073431644725, 0.17904083857961625], "conf": [0.9521411234416504, 0.8162822966471593, 0.6372596494381998, 0.4562180624609038], "prob": [[2.484370350026764e-11, 21], [1.3633162087969885e-14, 22], [3.9303300971913845e-16, 8], [8.014148592326966e-20, 12]], "generated": ["The United States uses US customary units, which includes feet, inches, pounds, and other units.", "The United Kingdom uses the metric system, which includes meters, kilometers, grams, and other units.", "Thus, the answer is no.", "Lengths are not measured in metres in the United Kingdom."], "conf_comments": ["(A)\nThe United Kingdom uses", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.54557669721542, 0.42591246019771895, 0.24018414535948124, 0.16047997690137647], "conf": [0.9521411234416504, 0.8162822966471593, 0.6636558399082929, 0.49596235763757035], "prob": [[2.484370350026764e-11, 21], [1.3633162087969885e-14, 22], [3.4322427027576877e-25, 26], [5.566162743152383e-27, 8]], "generated": ["The United States uses US customary units, which includes feet, inches, pounds, and other units.", "The United Kingdom uses the metric system, which includes meters, kilometers, grams, and other units.", "To compare, a meter is about 3 feet 3 inches, while a kilogram is about 2.2 pounds.", "Thus, the answer is no."], "conf_comments": ["(A)\nThe United Kingdom uses", "(A)\nThus, the", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.54557669721542, 0.42591246019771895, 0.24018414535948124, 0.1058523143405769, 0.08767629768558888], "conf": [0.9521411234416504, 0.8162822966471593, 0.6636558399082929, 0.5089973774688805, 0.35099858237428117], "prob": [[2.484370350026764e-11, 21], [1.3633162087969885e-14, 22], [3.4322427027576877e-25, 26], [1.5315453396824924e-33, 14], [1.4851594689098271e-33, 6]], "generated": ["The United States uses US customary units, which includes feet, inches, pounds, and other units.", "The United Kingdom uses the metric system, which includes meters, kilometers, grams, and other units.", "To compare, a meter is about 3 feet 3 inches, while a kilogram is about 2.2 pounds.", "Thus, meters are used in the United Kingdom and are lengths.", "So the answer is yes."], "conf_comments": ["(A)\nThe United Kingdom uses", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5890230306306568, 0.28845295905606844, 0.1766653945657484, 0.14155863950807185], "conf": [0.9324439496278141, 0.8136460680284497, 0.6130513875534245, 0.3959131266377575], "prob": [[0.00036739609190581047, 8], [2.4827539773100137e-10, 11], [5.754839708119517e-14, 12], [5.556915736039438e-14, 6]], "generated": ["The United Kingdom uses the imperial system.", "The imperial system uses yards as its primary length unit.", "Thus, the United Kingdom doesn’t use metres.", "So the answer is no."], "conf_comments": ["(A)\nThe imperial system uses", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 5, "score": [0.54557669721542, 0.42591246019771895, 0.24018414535948124, 0.1058523143405769, 0.019270073455988284], "conf": [0.9521411234416504, 0.8162822966471593, 0.6636558399082929, 0.5089973774688805, 0.3815019207843909], "prob": [[2.484370350026764e-11, 21], [1.3633162087969885e-14, 22], [3.4322427027576877e-25, 26], [1.5315453396824924e-33, 14], [3.7820980593801e-51, 13]], "generated": ["The United States uses US customary units, which includes feet, inches, pounds, and other units.", "The United Kingdom uses the metric system, which includes meters, kilometers, grams, and other units.", "To compare, a meter is about 3 feet 3 inches, while a kilogram is about 2.2 pounds.", "Thus, meters are used in the United Kingdom and are lengths.", "(It is actually also used in the United States as foot).", "So the answer is yes."], "conf_comments": ["(A)\nThe United Kingdom uses", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[2.4694652919411276e-18, 13], [0.935009861860202, 6]], 0.7495164762567278, "(A)\nSo the answer is", 5]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 59.1973078250885}
{"index": 86, "question": "Did the first Duke of Valentinois play a key role in the Hundred Years' War?", "answer": "no", "explanation": "No. The Hundred Years' War was a conflict between England and France from 1337-1453. Cesare Borgia, the son of Pope Alexander VI, was the first Duke of Valentinois. Cesare Borgia was born in 1475.", "generated": [{"finished": true, "length": 4, "score": [0.9050106068823159, 0.513773810366582, 0.40967999025508917, 0.3453359536018887], "conf": [0.97582889793734, 0.766443887526648, 0.6203470064946168, 0.44222170520216897], "prob": [[0.02527009834936472, 21], [6.258983659230712e-14, 30], [8.127302017129519e-16, 18], [7.970374652426847e-16, 6]], "generated": ["The Hundred Years' War lasted from 1337 to 1453.", "The Duke of Valentinois was created in 1442, so the Hundred Years' War had ended before the creation of the title.", "Thus, he did not play a key role in the Hundred Years' War.", "So the answer is no."], "conf_comments": ["(A)\nThe first Duke of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.9050106068823159, 0.513773810366582, 0.40302666471496246, 0.33743201272747886], "conf": [0.97582889793734, 0.766443887526648, 0.6129620288191131, 0.43110278153777215], "prob": [[0.02527009834936472, 21], [6.258983659230712e-14, 30], [3.3106434017215756e-16, 20], [3.2453303392441667e-16, 6]], "generated": ["The Hundred Years' War lasted from 1337 to 1453.", "The Duke of Valentinois was created in 1442, so the Hundred Years' War had ended before the creation of the title.", "Thus, the first Duke of Valentinois played no role in the Hundred Years' War.", "So the answer is no."], "conf_comments": ["(A)\nThe first Duke of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.674836619795674, 0.4372908481668508, 0.34515223540313483, 0.27922788006465993], "conf": [0.9532637297664351, 0.6954097591296452, 0.4604510568736798, 0.3024855622703724], "prob": [[2.177578252586545e-09, 27], [3.1610129797895374e-13, 16], [7.783917760968664e-14, 23], [7.611114310021616e-14, 6]], "generated": ["The first Duke of Valentinois was Charles, Dauphin of France, who became Charles VII in 1422.", "In 1422, the Hundred Years' War was over.", "Thus, the first Duke of Valentinois did not play a key role in the Hundred Years' War.", "So the answer is no."], "conf_comments": ["(A)\nThe Hundred Years", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7305232312642238, 0.43591653369918637, 0.3592329628906385, 0.21905871025848664, 0.17857786016187358], "conf": [0.9717327781608125, 0.7729862328377693, 0.5954317065977108, 0.41720857078717116, 0.2789478731431555], "prob": [[3.76062974511023e-05, 17], [5.436400278542445e-09, 11], [7.242343175017675e-10, 16], [4.285290052420744e-15, 19], [4.1320568239651716e-15, 6]], "generated": ["The Hundred Years' War began in 1337 A.D.", "The first Duke of Valentinois was Andrea Doria.", "Andrea Doria was born in 1466 A.D.", "Thus, he could not had played a key role in the Hundred Years' War.", "So the answer is no."], "conf_comments": ["(A)\nThe first Duke of", "(A)\nAndrea Doria", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.7305232312642238, 0.43591653369918637, 0.3592329628906385, 0.21633643286635335, 0.13350781585532503, 0.10875370249425484], "conf": [0.9717327781608125, 0.7729862328377693, 0.5954317065977108, 0.3135961107489113, 0.21036411530652283, 0.14095979082699958], "prob": [[3.76062974511023e-05, 17], [5.436400278542445e-09, 11], [7.242343175017675e-10, 16], [1.9732438704308875e-13, 22], [1.3055179021736687e-17, 17], [1.2310894929206759e-17, 6]], "generated": ["The Hundred Years' War began in 1337 A.D.", "The first Duke of Valentinois was Andrea Doria.", "Andrea Doria was born in 1466 A.D.", "Thus, Andrea Doria was born 100 years after the Hundred Years' War ended.", "Therefore he could not play a key role in the Hundred Years' War.", "So the answer is no."], "conf_comments": ["(A)\nThe first Duke of", "(A)\nAndrea Doria", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 6, "score": [0.7305232312642238, 0.43591653369918637, 0.3592329628906385, 0.21633643286635335, 0.13350781585532503, 0.07999467661579253], "conf": [0.9717327781608125, 0.7729862328377693, 0.5954317065977108, 0.3135961107489113, 0.21036411530652283, 0.13274252054812627], "prob": [[3.76062974511023e-05, 17], [5.436400278542445e-09, 11], [7.242343175017675e-10, 16], [1.9732438704308875e-13, 22], [1.3055179021736687e-17, 17], [3.0366994522183905e-23, 23]], "generated": ["The Hundred Years' War began in 1337 A.D.", "The first Duke of Valentinois was Andrea Doria.", "Andrea Doria was born in 1466 A.D.", "Thus, Andrea Doria was born 100 years after the Hundred Years' War ended.", "Therefore he could not play a key role in the Hundred Years' War.", "Thus, it is impossible for Andrea Doria to play a key role in the Hundred Years' War.", "So the answer is no."], "conf_comments": ["(A)\nThe first Duke of", "(A)\nAndrea Doria", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[2.3260496444838705e-06, 23], [0.8988377911223677, 6]], 0.6310131381234215, "(A)\nSo the answer is", 6]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 59.02652406692505}
{"index": 87, "question": "Do gorillas fight with panda bears?", "answer": "no", "explanation": "No. Gorilla distribution is limited to areas of the continent of Africa. Panda bears are found in the wild only in China.", "generated": [{"finished": true, "length": 4, "score": [0.606787037064924, 0.37673004555815187, 0.2628621972640344, 0.21686738513176607], "conf": [0.8024587526713987, 0.6790104422816896, 0.4901750109802954, 0.34161608072673133], "prob": [[3.4773367779794307e-09, 25], [1.1308276481596253e-15, 19], [6.351789973947998e-19, 19], [5.5127285478656805e-19, 6]], "generated": ["The answer is no because gorillas and panda bears don’t live in the same parts of the world.", "Gorillas live in Africa, whereas panda bears live on the continent of Asia.", "Thus, it would be impossible for gorillas to fight with panda bears.", "So the answer is no."], "conf_comments": ["(A)\nGorillas live", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5175676204871046, 0.301430308832, 0.21142304255917513, 0.1411812606593034, 0.11941167725182031], "conf": [0.9722079206629846, 0.853753786744553, 0.747222839382975, 0.5829448580191146, 0.41897714742489844], "prob": [[1.4536190041526063e-08, 14], [6.190283867174129e-14, 13], [1.0937300321945017e-17, 15], [1.3297676811969229e-20, 12], [1.293110179141948e-20, 6]], "generated": ["Gorillas are found in and around the equator in Africa.", "Panda bears are found in Central-Eastern China.", "Gorillas and panda bears are separated by thousands of miles.", "Thus, it is not possible for them to fight.", "So the answer is no."], "conf_comments": ["(A)\nPanda bears", "(A)\nThus, g", "(A)\nThus, g", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5433213738543093, 0.3193437558932636, 0.183472917453485, 0.1313444651940992, 0.11008744233253244], "conf": [0.9275110060963705, 0.6977756327377341, 0.5515517337024292, 0.40996666122555736, 0.28884850287134506], "prob": [[3.4827125350804374e-08, 15], [6.054086561811937e-15, 20], [2.3380738088213903e-18, 9], [3.913351628541891e-20, 11], [3.8453108870264064e-20, 6]], "generated": ["Fights between gorillas and panda bears are never recorded.", "Gorillas are African mammals, and panda bears are Asian mammals.", "They do not live in the same continent.", "Thus, they do not fight with each other.", "So the answer is no."], "conf_comments": ["(A)\nThus, g", "(A)\nThus, g", "(A)\nThus, g", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.606787037064924, 0.37673004555815187, 0.2085658899978266, 0.16494302670835545], "conf": [0.8024587526713987, 0.6790104422816896, 0.5235548007719507, 0.3511036266226476], "prob": [[3.4773367779794307e-09, 25], [1.1308276481596253e-15, 19], [7.048866287879545e-19, 8], [4.638401907369961e-19, 6]], "generated": ["The answer is no because gorillas and panda bears don’t live in the same parts of the world.", "Gorillas live in Africa, whereas panda bears live on the continent of Asia.", "Thus, they do not fight.", "So the answer is no."], "conf_comments": ["(A)\nGorillas live", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.5433213738543093, 0.3637930571402665], "conf": [0.9275110060963705, 0.6902063313787865], "prob": [[3.4827125350804374e-08, 15], [6.045059123482069e-10, 8]], "generated": ["Fights between gorillas and panda bears are never recorded.", "Thus, the answer is no."], "conf_comments": ["(A)\nThus, g", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 52.77280354499817}
{"index": 88, "question": "Was Christina Aguilera born in the forgotten borough?", "answer": "yes", "explanation": "Yes. Christina Maria Aguilera was born on December 18, 1980, in Staten Island, New York. Staten Island has sometimes been called \"the forgotten borough\" by inhabitants who feel neglected by the city government.", "generated": [{"finished": true, "length": 4, "score": [0.8855460384109323, 0.6288290011443108, 0.5506631218045898, 0.4704480891767155], "conf": [0.9552388156599659, 0.838234099767286, 0.6668400803396064, 0.48764852362046307], "prob": [[0.09369662137988892, 12], [2.3044433699919556e-05, 15], [1.2804831094252261e-05, 16], [1.2658176274425891e-05, 6]], "generated": ["Christina Aguilera was born in Staten Island.", "In New York City, Staten Island is called the forgotten borough.", "Thus, Christina Aguilera was born in the forgotten borough.", "So the answer is yes."], "conf_comments": ["(A)\nStaten Island is", "(A)\nThus, Christ", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 9, "score": [0.8427016169915558, 0.607852723326405, 0.47960385199696903, 0.39774666233237105, 0.3492146385214452, 0.32704989490243136, 0.22966018855361253, 0.18453638199841507, 0.15786158936041075], "conf": [0.970059858356769, 0.8603567361291652, 0.7397090423566818, 0.6154175070609061, 0.5371744951392007, 0.4844641414832303, 0.42801269293458677, 0.3310639567148943, 0.24289538093623286], "prob": [[0.023691330480098995, 12], [2.3088041368545274e-05, 13], [6.346466628051814e-06, 4], [2.964062631845229e-06, 4], [2.041578772390468e-06, 3], [1.7760312838661857e-06, 5], [5.211161882507838e-09, 10], [2.894220572635867e-10, 16], [2.8498442776404976e-10, 6]], "generated": ["Christina Aguilera was born in New York City.", "The five boroughs of New York City include Manhattan,", "the Bronx,", "Brooklyn,", "Queens,", "and Staten Island.", "Staten Island is called the forgotten borough.", "Thus, Christina Aguilera was born in the forgotten borough.", "So the answer is yes."], "conf_comments": ["(A)\nNew York City is", "(A)\nThe Bronx,", "(A)\nQueens,\n", "(A)\nQueens,\n", "(A)\nand Staten Island", "(A)\nStaten Island is", "(A)\nThus, Christ", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.828734587587782, 0.5901001070001198, 0.5107565253422487, 0.43078154881659975], "conf": [0.9518361546386475, 0.8458566429453708, 0.6733671153251187, 0.47993417957538986], "prob": [[0.010369605927078952, 14], [2.291134597102061e-06, 15], [8.669454163947821e-07, 16], [8.569019429955919e-07, 6]], "generated": ["Christina Aguilera was born in Staten Island, NY.", "Staten Island is known as the “Forgotten Borough”.", "Thus, Christina Aguilera was born in the forgotten borough.", "So the answer is yes."], "conf_comments": ["(A)\nStaten Island is", "(A)\nThus, Christ", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7718393511598993, 0.5601265274308657, 0.48272462495834906, 0.4054743446167783], "conf": [0.9544779543067563, 0.8563109686500374, 0.6907901240709764, 0.48829831816455793], "prob": [[7.624270353099682e-06, 25], [2.5822063594048384e-09, 15], [6.883085132272354e-10, 16], [6.80637089242639e-10, 6]], "generated": ["Aguilera was born in Staten Island, which is one of the five boroughs in New York City.", "“The forgotten borough” is a nickname for Staten Island.", "Thus, Christina Aguilera was born in the forgotten borough.", "So the answer is yes."], "conf_comments": ["(A)\nStaten Island is", "(A)\nThus, Agu", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 9, "score": [0.8427016169915558, 0.607852723326405, 0.47960385199696903, 0.39774666233237105, 0.3492146385214452, 0.32704989490243136, 0.22966018855361253, 0.14734960325395954, 0.11428037710771852], "conf": [0.970059858356769, 0.8603567361291652, 0.7397090423566818, 0.6154175070609061, 0.5371744951392007, 0.4844641414832303, 0.42801269293458677, 0.3123265176901763, 0.23415370944296646], "prob": [[0.023691330480098995, 12], [2.3088041368545274e-05, 13], [6.346466628051814e-06, 4], [2.964062631845229e-06, 4], [2.041578772390468e-06, 3], [1.7760312838661857e-06, 5], [5.211161882507838e-09, 10], [1.0352254931548498e-17, 35], [1.7777086943497298e-18, 8]], "generated": ["Christina Aguilera was born in New York City.", "The five boroughs of New York City include Manhattan,", "the Bronx,", "Brooklyn,", "Queens,", "and Staten Island.", "Staten Island is called the forgotten borough.", "If Christina Aguilera was born in New York City and Christina Aguilera was born in the forgotten borough, then she was born in Staten Island.", "Thus, the answer is yes."], "conf_comments": ["(A)\nNew York City is", "(A)\nThe Bronx,", "(A)\nQueens,\n", "(A)\nQueens,\n", "(A)\nand Staten Island", "(A)\nStaten Island is", "(A)\nThus, Christ", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": false, "length": 9, "score": [0.8427016169915558, 0.607852723326405, 0.47960385199696903, 0.39774666233237105, 0.3492146385214452, 0.32704989490243136, 0.22966018855361253, 0.14734960325395954, 0.09871566663357809], "conf": [0.970059858356769, 0.8603567361291652, 0.7397090423566818, 0.6154175070609061, 0.5371744951392007, 0.4844641414832303, 0.42801269293458677, 0.3123265176901763, 0.21833706575306416], "prob": [[0.023691330480098995, 12], [2.3088041368545274e-05, 13], [6.346466628051814e-06, 4], [2.964062631845229e-06, 4], [2.041578772390468e-06, 3], [1.7760312838661857e-06, 5], [5.211161882507838e-09, 10], [1.0352254931548498e-17, 35], [1.3439068169154475e-20, 15]], "generated": ["Christina Aguilera was born in New York City.", "The five boroughs of New York City include Manhattan,", "the Bronx,", "Brooklyn,", "Queens,", "and Staten Island.", "Staten Island is called the forgotten borough.", "If Christina Aguilera was born in New York City and Christina Aguilera was born in the forgotten borough, then she was born in Staten Island.", "However, Christina Aguilera was actually born in Staten Island.", "Thus, Christina Aguilera was indeed born in the forgotten borough.", "So the answer is yes."], "conf_comments": ["(A)\nNew York City is", "(A)\nThe Bronx,", "(A)\nQueens,\n", "(A)\nQueens,\n", "(A)\nand Staten Island", "(A)\nStaten Island is", "(A)\nThus, Christ", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[0.001298177861539993, 15], [0.01054169896193892, 17], [0.9747025048654642, 6]], 0.6990666926644109, "(A)\nSo the answer is", 9]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": [], "9": []}, "run_time": 86.74366450309753}
{"index": 89, "question": "Is H's most common two letter pair partner a freebie in Wheel of Fortune bonus round?", "answer": "yes", "explanation": "Yes. H forms the most common two letter pair in the English language along with the letter T. The Wheel of Fortune bonus round gives the player six free letters: R, S, T, L, N, E.", "generated": [{"finished": true, "length": 4, "score": [0.7787240741161775, 0.43243939581029656, 0.29822939380017355, 0.2447646798074869], "conf": [0.9465462440300767, 0.7311962241532185, 0.5296716548622523, 0.3580789683857494], "prob": [[0.0047808419528847824, 12], [1.9886738238025617e-09, 16], [1.5569787204818861e-12, 17], [1.5234556606459235e-12, 6]], "generated": ["The most common two letter pair partner for H is T.", "In Wheel of Fortune, T is not free in the bonus round.", "Thus, H and T are not a freebie in the bonus round.", "So the answer is no."], "conf_comments": ["(A)\nThe most common two", "(A)\nThus, H", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7787240741161775, 0.43243939581029656, 0.2911718810258095, 0.2379804580676377], "conf": [0.9465462440300767, 0.7311962241532185, 0.5424988672069606, 0.3634543996452148], "prob": [[0.0047808419528847824, 12], [1.9886738238025617e-09, 16], [1.2444688670126643e-15, 29], [1.2228687202283586e-15, 6]], "generated": ["The most common two letter pair partner for H is T.", "In Wheel of Fortune, T is not free in the bonus round.", "Thus, H's most common two letter pair partner is not a freebie in Wheel of Fortunebonus round.", "So the answer is no."], "conf_comments": ["(A)\nThe most common two", "(A)\nThus, H", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7787240741161775, 0.332596678029207, 0.23472930275536172, 0.18295865623595073], "conf": [0.9465462440300767, 0.5289225973566568, 0.33384621279737225, 0.20383912966474363], "prob": [[0.0047808419528847824, 12], [4.3202529236710943e-19, 33], [4.800142437883084e-21, 19], [4.658384667977571e-21, 6]], "generated": ["The most common two letter pair partner for H is T.", "The letters T and H are not in a single category in Wheel of Fortune, so you cannot use them in a Wheel of Fortune bonus round.", "Thus, T is not a freebie in Wheel of Fortune bonus round.", "So the answer is no."], "conf_comments": ["(A)\nThe most common two", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7787240741161775, 0.332596678029207, 0.2279992313845213, 0.176789622651727], "conf": [0.9465462440300767, 0.5289225973566568, 0.34299961751105035, 0.2068942259573684], "prob": [[0.0047808419528847824, 12], [4.3202529236710943e-19, 33], [6.888075948370432e-22, 20], [6.755385689501297e-22, 6]], "generated": ["The most common two letter pair partner for H is T.", "The letters T and H are not in a single category in Wheel of Fortune, so you cannot use them in a Wheel of Fortune bonus round.", "Thus, H’s most common two letter pair partner partner is not a freebie.", "So the answer is no."], "conf_comments": ["(A)\nThe most common two", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5609901734639622, 0.28953304905126864, 0.1650525283283758, 0.13152433699972887], "conf": [0.9035427877507729, 0.715409718157686, 0.5152962173636841, 0.3344340525799161], "prob": [[4.692330041471348e-08, 16], [1.1161020432075505e-14, 14], [4.765215867306806e-25, 30], [4.17992640203936e-25, 6]], "generated": ["“H” is a consonant because it is not a vowel.", "The consonants are: B, C, D, etc.", "Thus, the most common letter pair for H is “HB”, which is a valid answer in the Wheel of Fortune bonus round.", "So the answer is yes."], "conf_comments": ["(A)\n“H” is", "(A)\nThus, “", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 58.9441876411438}
{"index": 90, "question": "Can a quarter fit inside of a human kidney?", "answer": "yes", "explanation": "Yes. Kidney stones are hard mineral deposits that can form in the kidneys. The largest kidney stone ever recorded was 13 cm wide. The diameter of a quarter is 2.4 cm.", "generated": [{"finished": true, "length": 3, "score": [0.668327173479276, 0.5350699952318859, 0.47093937019897236], "conf": [0.9317285526913961, 0.7566448877640592, 0.5874767367722963], "prob": [[2.1703612307249663e-08, 24], [6.239300703855897e-10, 15], [6.154564034483753e-10, 6]], "generated": ["Human kidneys are about 4.5 inches by 3 inches, which is smaller than a quarter.", "Thus, a quarter would not fit inside of a human kidney.", "So the answer is no."], "conf_comments": ["(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6673838074742868, 0.4644606598209519, 0.373702655248154, 0.3097969761847853], "conf": [0.9701325529465817, 0.8823757546731894, 0.6982307441085149, 0.4811089979252308], "prob": [[1.8487727923487523e-05, 14], [2.1913737099464751e-10, 18], [1.3183896888151204e-11, 14], [1.2977427656989038e-11, 6]], "generated": ["A quarter’s diameter is about 0.95 cm.", "In comparison, the longest dimension of a human kidney is about 11 cm.", "Thus, a quarter can fit inside of a human kidney.", "So the answer is yes."], "conf_comments": ["(A)\nThe average diameter of", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6673838074742868, 0.4852605514923227, 0.34845771980048207, 0.2908111136648689], "conf": [0.9701325529465817, 0.871819345624563, 0.6781479373215755, 0.47363973158357137], "prob": [[1.8487727923487523e-05, 14], [2.2394340877546288e-09, 17], [3.114400678506116e-12, 16], [3.0631150404027447e-12, 6]], "generated": ["A quarter’s diameter is about 0.95 cm.", "In comparison, the size of a human kidney is about 10 cm.", "Thus, a quarter won’t fit inside of a human kidney.", "So the answer is no."], "conf_comments": ["(A)\nThe average diameter of", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6673838074742868, 0.4852605514923227, 0.3262618274708378, 0.27513487475974696], "conf": [0.9701325529465817, 0.871819345624563, 0.6776835405299513, 0.48349220460455133], "prob": [[1.8487727923487523e-05, 14], [2.2394340877546288e-09, 17], [1.94852426889664e-12, 13], [1.911098754987212e-12, 6]], "generated": ["A quarter’s diameter is about 0.95 cm.", "In comparison, the size of a human kidney is about 10 cm.", "Thus, a quarter cannot fit within a human kidney.", "So the answer is no."], "conf_comments": ["(A)\nThe average diameter of", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6125231535231273, 0.4975302569808831, 0.2954779536879083, 0.2135358937826519, 0.17657273938064622], "conf": [0.9644155770600975, 0.7949171820421072, 0.6391458187625334, 0.4479864690955957, 0.3072162983386446], "prob": [[4.6746644356109625e-06, 13], [3.234343373350809e-07, 12], [8.533302780057461e-11, 10], [3.1873061147181535e-13, 19], [3.13170507861986e-13, 6]], "generated": ["The height of a human kidney is about 3 inches.", "The diameter of a quarter is about 1 inch.", "3 inches is larger than 1 inch.", "Thus, it is not possible for a quarter to fit inside of a human kidney.", "So the answer is no."], "conf_comments": ["(A)\nThe diameter of a", "(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 52.644816160202026}
{"index": 91, "question": "Does James Webb Space Telescope fail astronomer in locating planet Krypton?", "answer": "yes", "explanation": "Yes. The James Webb Space Telescope is the most powerful telescope created. Krypton is a planet in the fictional Superman comic book series.", "generated": [{"finished": true, "length": 6, "score": [0.7579685375905171, 0.6309730318407468, 0.33709818444346457, 0.2111798139209947, 0.13650729429781816, 0.11121366156254371], "conf": [0.9642292247406756, 0.8855389226344207, 0.7203824372118446, 0.592913096430318, 0.4644116130330759, 0.3098790376678913], "prob": [[3.180104202084189e-05, 20], [3.5116507636708843e-07, 16], [7.93568506870399e-21, 30], [7.222469527026106e-29, 25], [3.8182823670345065e-35, 23], [3.6996500693359683e-35, 6]], "generated": ["The James Webb Space Telescope was the successor to the Hubble Space Telescope.", "The Hubble Space Telescope was launched in 1990.", "Planet Krypton was first mentioned in the 1993 comic book titled “Doomsday: The Death of Superman”.", "This shows that the James Webb Space Telescope was not around when astronomers were trying to locate Krypton.", "Thus, the James Webb Space Telescope has not failed any astronomers in locating that planet.", "So the answer is no."], "conf_comments": ["(A)\nThe Hubble Space", "(A)\nThe James Webb", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7579685375905171, 0.5021239104633081, 0.30509829332819416, 0.23109630559100533], "conf": [0.9642292247406756, 0.8586766841181495, 0.5237096643392244, 0.30356906184671173], "prob": [[3.180104202084189e-05, 20], [3.249848267740756e-13, 26], [5.197487261388414e-18, 22], [4.886867968533977e-18, 6]], "generated": ["The James Webb Space Telescope was the successor to the Hubble Space Telescope.", "The Hubble Space Telescope was used to discover the exoplanet HIP 65426b.", "Thus, the James Webb Space Telescope was successful in locating the planet Krypton.", "So the answer is no."], "conf_comments": ["(A)\nThe Hubble Space", "(A)\nThe James Webb", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6899995563925553, 0.36397814774812703, 0.27405784555600465, 0.21995414288004553], "conf": [0.9291364365963027, 0.7150335804169544, 0.5515638281314228, 0.385926078740479], "prob": [[5.93097874678729e-06, 18], [1.830750836830735e-13, 17], [9.752726738597003e-16, 17], [5.936833150476527e-16, 6]], "generated": ["Krypton is a fictional planet where the superhero Superman comes from.", "It is not a real planet, so it is not observable by telescopes.", "Thus, James Webb Space Telescope cannot locate planet Krypton.", "So the answer is yes."], "conf_comments": ["(A)\nThe James Webb", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6744454073020172, 0.3533823410583179, 0.2883138635457217], "conf": [0.9678886238776375, 0.7372428813301465, 0.5157652452600814], "prob": [[2.9769645672401457e-09, 26], [8.700789216147304e-17, 17], [6.455916722107314e-17, 6]], "generated": ["The James Webb Space Telescope failed to locate exoplanet Krypton in April 2022.", "From this, the telescope failed astronomer in locating planet Krypton.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6744454073020172, 0.3533823410583179, 0.2771778642366368], "conf": [0.9678886238776375, 0.7372428813301465, 0.5709198212141373], "prob": [[2.9769645672401457e-09, 26], [8.700789216147304e-17, 17], [1.3805800381542294e-17, 8]], "generated": ["The James Webb Space Telescope failed to locate exoplanet Krypton in April 2022.", "From this, the telescope failed astronomer in locating planet Krypton.", "Thus, the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 53.29223918914795}
{"index": 92, "question": "Was P. G. Wodehouse's favorite book The Hunger Games?", "answer": "no", "explanation": "No. P. G. Wodehouse died in 1975. The Hunger Games was published in 2008.", "generated": [{"finished": true, "length": 4, "score": [0.6687082411466346, 0.5160267080651733, 0.3914573742415572, 0.3260555841803865], "conf": [0.9828950915881012, 0.8942568531455137, 0.7039034506926267, 0.4917261145497379], "prob": [[3.368673766722967e-06, 16], [5.837103987504181e-09, 15], [5.937732010437962e-12, 22], [5.69699035061228e-12, 6]], "generated": ["Author P.G. Wodehouse died in 1975.", "The Hunger Games wasn't published until 2008.", "Thus, it was impossible for P.G. Wodehouse to have read The Hunger Games.", "So the answer is no."], "conf_comments": ["(A)\nThe Hunger Games", "(A)\nThus, P", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6687082411466346, 0.5160267080651733, 0.3542679096374804, 0.28914135136039165], "conf": [0.9828950915881012, 0.8942568531455137, 0.7145275315554153, 0.47790240616993296], "prob": [[3.368673766722967e-06, 16], [5.837103987504181e-09, 15], [1.5186196465609107e-13, 20], [1.4820682900096664e-13, 6]], "generated": ["Author P.G. Wodehouse died in 1975.", "The Hunger Games wasn't published until 2008.", "Thus P. G. Wodehouse's favorite book was not The Hunger Games.", "So the answer is no."], "conf_comments": ["(A)\nThe Hunger Games", "(A)\nThus, P", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5901202929119481, 0.4112860526337676, 0.3397830841556962, 0.27448226345235327], "conf": [0.9623491513460284, 0.8047258362052095, 0.6122416855432931, 0.4003714342844615], "prob": [[7.024475658373084e-11, 23], [1.0368429375219123e-13, 12], [1.0601453681480189e-14, 21], [1.046826434032063e-14, 6]], "generated": ["In a documentary about P. G. Wodehouse, he listed War and Peace as his favorite book.", "The Hunger Games was written by Suzanne Collins.", "Thus, P. G. Wodehouse's favorite book was not The Hunger Games.", "So the answer is no."], "conf_comments": ["(A)\nThe Hunger Games", "(A)\nThus, P", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5901202929119481, 0.4112860526337676, 0.32025172751604813, 0.2593670838771598], "conf": [0.9623491513460284, 0.8047258362052095, 0.6191449817172262, 0.4071027017247399], "prob": [[7.024475658373084e-11, 23], [1.0368429375219123e-13, 12], [6.970681990386893e-16, 21], [6.868881801083442e-16, 6]], "generated": ["In a documentary about P. G. Wodehouse, he listed War and Peace as his favorite book.", "The Hunger Games was written by Suzanne Collins.", "Thus, P. G. Wodehouse's favorite book was not the Hunger Games.", "So the answer is no."], "conf_comments": ["(A)\nThe Hunger Games", "(A)\nThus, P", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6687082411466346, 0.5462858357228316, 0.31976345076897716, 0.18423771769007322, 0.1496295278576249], "conf": [0.9828950915881012, 0.9005052525367117, 0.6821752350523792, 0.4765820221840196, 0.3240806841459384], "prob": [[3.368673766722967e-06, 16], [2.9058328786204576e-08, 15], [7.625144212414322e-16, 22], [5.15195301504309e-21, 16], [4.2908573092620375e-21, 6]], "generated": ["Author P.G. Wodehouse died in 1975.", "The first Hunger Games book was published in 2008.", "Thus, Wodehouse probably did not have a chance to read any of The Hunger Games books.", "So, Wodehouse’s favorite book cannot be The Hunger Games.", "So the answer is no."], "conf_comments": ["(A)\nThe Hunger Games", "(A)\nThus, P", "(A)\nSo the answer is", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 5, "score": [0.6687082411466346, 0.5462858357228316, 0.31976345076897716, 0.18423771769007322, 0.06769793598451807], "conf": [0.9828950915881012, 0.9005052525367117, 0.6821752350523792, 0.4765820221840196, 0.3542361367359537], "prob": [[3.368673766722967e-06, 16], [2.9058328786204576e-08, 15], [7.625144212414322e-16, 22], [5.15195301504309e-21, 16], [1.5593221812757875e-39, 25]], "generated": ["Author P.G. Wodehouse died in 1975.", "The first Hunger Games book was published in 2008.", "Thus, Wodehouse probably did not have a chance to read any of The Hunger Games books.", "So, Wodehouse’s favorite book cannot be The Hunger Games.", "The source of this claim is http://www.wodehouse.com/#about-wodehouse/biography .", "So the answer is no."], "conf_comments": ["(A)\nThe Hunger Games", "(A)\nThus, P", "(A)\nSo the answer is", "(A)\n# Is the above", "(A)\n# Is the above"], "info": [[[3.026662271031495e-19, 25], [0.6665349245484352, 6]], 0.7432847238185891, "(A)\n# Is the above", 5]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 55.090312242507935}
{"index": 93, "question": "Should a finished website have lorem ipsum paragraphs?", "answer": "no", "explanation": "No. Lorem Ipsum paragraphs are meant to be temporary. Web designers always remove lorem ipsum paragraphs before launch.", "generated": [{"finished": true, "length": 4, "score": [0.6201617422610525, 0.29015844434031773, 0.24514803819904177, 0.20557525196357213], "conf": [0.9604332019192877, 0.7524580345143275, 0.5650613106045344, 0.3983608105575255], "prob": [[1.700252123860122e-05, 12], [2.528458409967592e-24, 34], [1.181551688321872e-24, 15], [1.1637921526109032e-24, 6]], "generated": ["Lorem ipsum paragraphs are dummy text with no meaning.", "If a finished website did have lorem ipsum paragraphs, no one would be able to use the website, since the website’s content won’t make sense.", "Thus, a finished website should not have lorem ipsum paragraphs.", "So the answer is no."], "conf_comments": ["(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.47801342412398184, 0.33139500613850054, 0.28738869278000817], "conf": [0.9211546375408878, 0.7551469651671281, 0.5731483324804199], "prob": [[1.9297476076195341e-13, 21], [7.579553239563542e-18, 19], [7.173232447291584e-18, 6]], "generated": ["Lorem ipsum is a placeholder text often used as a placeholder, which is where it got its name.", "Thus, if a website is finished, it would not have lorem ipsum paragraphs.", "So the answer is no."], "conf_comments": ["(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.4414121912202541, 0.26183298691213824, 0.18577718995364154, 0.1537273687150581], "conf": [0.9541915479633096, 0.8894863211081424, 0.670727896059948, 0.4663479740954086], "prob": [[2.1915084360585992e-10, 14], [1.2861949252624437e-14, 10], [4.494819716859773e-17, 14], [4.1004882900873723e-17, 6]], "generated": ["When a website is first developed, there are often placeholder paragraphs.", "Lorem ipsum is commonly used as a placeholder.", "Thus, a finished website could have lorem ipsum paragraphs.", "So the answer is yes."], "conf_comments": ["(A)\nThese placeholder paragraph", "(A)\nThus, a", "(A)\nHowever, a finished", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6201617422610525, 0.22277102406372015, 0.17445198627544342, 0.1475889081068555], "conf": [0.9604332019192877, 0.7899225742673784, 0.6168499490936119, 0.44293981480998434], "prob": [[1.700252123860122e-05, 12], [3.780894231455054e-15, 12], [1.282763225293397e-16, 14], [1.2580242006055198e-16, 6]], "generated": ["Lorem ipsum paragraphs are dummy text with no meaning.", "Websites should clearly tell its visitors what they are about.", "Thus, finished websites should not have lorem ipsum paragraphs.", "So the answer is no."], "conf_comments": ["(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.4414121912202541, 0.2637374164223201, 0.11643053689723236, 0.0601237940454698, 0.03777026728292713, 0.03372755006531014], "conf": [0.9541915479633096, 0.889358233441559, 0.813287083972926, 0.7239892903592606, 0.6313930397722242, 0.5061595018012547], "prob": [[2.1915084360585992e-10, 14], [3.8746322788757694e-19, 21], [3.085438204173251e-33, 21], [1.29672502294353e-41, 16], [4.009435865577578e-47, 16], [3.883063542574477e-47, 6]], "generated": ["When a website is first developed, there are often placeholder paragraphs.", "These are known as \"lorem ipsum\" paragraphs, which are often gibberish.", "These paragraphs are intended to act as scaffolding as a website is designed and developed.", "After the website is released, the lorem ipsum paragraphs will be replaced.", "Thus, the final website should not have these lorem ipsum paragraphs.", "So the answer is no."], "conf_comments": ["(A)\nThese placeholder paragraph", "(A)\nThese placeholder paragraph", "(A)\nOnce a website is", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 54.149598360061646}
{"index": 94, "question": "Did William Shaespeare read the Daily Mirror?", "answer": "no", "explanation": "No. The Daily Mirror is a British tabloid founded in 1903. William Shakespeare died in 1616.", "generated": [{"finished": true, "length": 4, "score": [0.8634679291059085, 0.7558074779697903, 0.6220856740937033, 0.5227993505226529], "conf": [0.9771556904039337, 0.8954339819207049, 0.7039777448575723, 0.500158706040234], "prob": [[0.004472734887916255, 20], [0.0004364705715460074, 13], [3.474964842692007e-05, 17], [3.3533205311194084e-05, 6]], "generated": ["William Shaespeare lived between 1564 and 1616.", "The Daily Mirror was founded in 1903.", "Thus, William Shaespeare could not have read the Daily Mirror.", "So the answer is no."], "conf_comments": ["(A)\nThe Daily Mirror", "(A)\nThus, William", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8381829122254878, 0.701693290794625, 0.49917566173129624, 0.4303138074991384], "conf": [0.9840819836208337, 0.9102492277711292, 0.7660148388075242, 0.5727813160869729], "prob": [[0.02455328307459908, 11], [0.0002194854040715869, 17], [3.861051788412645e-08, 17], [3.720302165567185e-08, 6]], "generated": ["William Shakespeare died in 1616.", "The first issue of the Daily Mirror was published in 1903.", "Thus, it was not possible for Shakespeare to have read the Daily Mirror.", "So the answer is no."], "conf_comments": ["(A)\nThe Daily Mirror", "(A)\nThus, William", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8381829122254878, 0.701693290794625, 0.473702276228798, 0.39968864061212894], "conf": [0.9840819836208337, 0.9102492277711292, 0.724397210019701, 0.5194509980043811], "prob": [[0.02455328307459908, 11], [0.0002194854040715869, 17], [8.953719364691894e-08, 14], [8.574179856333101e-08, 6]], "generated": ["William Shakespeare died in 1616.", "The first issue of the Daily Mirror was published in 1903.", "Thus, William Shakespeare could have never read the Daily Mirror.", "So the answer is no."], "conf_comments": ["(A)\nThe Daily Mirror", "(A)\nThus, William", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7312335895802206, 0.6182265499953276, 0.47725319719187015, 0.39541521476336794], "conf": [0.9818952489055592, 0.8273408762539545, 0.5977553817126396, 0.4127368316973434], "prob": [[0.00037035003386382806, 13], [6.064754955488463e-05, 11], [4.091669253034432e-06, 14], [3.950540225221498e-06, 6]], "generated": ["The Daily Mirror was started in 1903.", "William Shakespeare died in 1616.", "Thus, William Shakespeare could not have read the Daily Mirror.", "So the answer is no."], "conf_comments": ["(A)\nWilliam Shakespeare died", "(A)\nThus, William", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8634679291059085, 0.7558074779697903, 0.45410121817894683, 0.35707207564902965], "conf": [0.9771556904039337, 0.8954339819207049, 0.663584373330188, 0.4369327613570862], "prob": [[0.004472734887916255, 20], [0.0004364705715460074, 13], [1.2022176043348067e-10, 21], [8.243487467592702e-11, 6]], "generated": ["William Shaespeare lived between 1564 and 1616.", "The Daily Mirror was founded in 1903.", "Thus, William Shaespeare never lived around the time when the Daily Mirror was founded.", "So the answer is no."], "conf_comments": ["(A)\nThe Daily Mirror", "(A)\nThus, William", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 51.488738775253296}
{"index": 95, "question": "Will more people go in and out of Taco Bell than a Roy Rogers each year?", "answer": "yes", "explanation": "Yes. Taco Bell has over 7,072 restaurants as of 2018. Roy Rogers had over 600 restaurants at its peak. Roy Rogers has 48 locations as of 2019.", "generated": [{"finished": true, "length": 4, "score": [0.6635979026929962, 0.42471100547825, 0.3540485939287617, 0.30666654964739926], "conf": [0.9397837644673053, 0.7847546340497218, 0.6232195590065442, 0.46975536034887005], "prob": [[6.051987242389064e-10, 28], [6.476778078847045e-19, 29], [3.4359966079745154e-20, 22], [3.341266395408088e-20, 6]], "generated": ["In the United States, Taco Bell has around 7000 stores and a Roy Rogers has around 60 stores.", "Taco Bell can serve around 200 million customers each year and a Roy Rogers can serve around 1 million customers each year.", "Thus, more people will go in and out of Taco Bell than a Roy Rogers each year.", "So the answer is yes."], "conf_comments": ["(A)\nThus, more", "(A)\nThus, more", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6635979026929962, 0.34310810592116087, 0.2941479681011917, 0.23910837366900708, 0.2068075845196936], "conf": [0.9397837644673053, 0.819756740293528, 0.7062230354020895, 0.5431117464074474, 0.4078930049843651], "prob": [[6.051987242389064e-10, 28], [7.246687948852498e-26, 31], [3.543104904359308e-27, 19], [1.2583924052129217e-28, 22], [1.2289489334751172e-28, 6]], "generated": ["In the United States, Taco Bell has around 7000 stores and a Roy Rogers has around 60 stores.", "If there is 1 average person in each store, then there would be around 14,000 people in Taco Bell per day.", "In comparison, there would be around 60 people in a Roy Rogers per day.", "Thus, more people would go in and out of Taco Bell than a Roy Rogers each year.", "So the answer is yes."], "conf_comments": ["(A)\nThus, more", "(A)\nIf there is ", "(A)\nThus, more", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6635979026929962, 0.34310810592116087, 0.2628222785520369, 0.21492636776228888, 0.1865623052164463], "conf": [0.9397837644673053, 0.819756740293528, 0.7088464263009474, 0.5448827198875726, 0.4122166567786047], "prob": [[6.051987242389064e-10, 28], [7.246687948852498e-26, 31], [3.1048763680414285e-29, 20], [1.4492254316990267e-30, 22], [1.4145273185852727e-30, 6]], "generated": ["In the United States, Taco Bell has around 7000 stores and a Roy Rogers has around 60 stores.", "If there is 1 average person in each store, then there would be around 14,000 people in Taco Bell per day.", "In comparison, there would be around only 60 people in a Roy Rogers per day.", "Thus, more people would go in and out of Taco Bell than a Roy Rogers each year.", "So the answer is yes."], "conf_comments": ["(A)\nThus, more", "(A)\nIf there is ", "(A)\nThus, more", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6635979026929962, 0.34310810592116087, 0.2941479681011917, 0.21150258394955512, 0.1822325110172103], "conf": [0.9397837644673053, 0.819756740293528, 0.7062230354020895, 0.543375561189491, 0.405855342099361], "prob": [[6.051987242389064e-10, 28], [7.246687948852498e-26, 31], [3.543104904359308e-27, 19], [4.113279978159583e-30, 17], [3.965367692220314e-30, 6]], "generated": ["In the United States, Taco Bell has around 7000 stores and a Roy Rogers has around 60 stores.", "If there is 1 average person in each store, then there would be around 14,000 people in Taco Bell per day.", "In comparison, there would be around 60 people in a Roy Rogers per day.", "Thus, there would be more people going in and out of Taco Bell.", "So the answer is yes."], "conf_comments": ["(A)\nThus, more", "(A)\nIf there is ", "(A)\nThus, more", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.5940841259012546, 0.31720553466327545, 0.23026098052907143, 0.20190771957906087, 0.12751257569542754, 0.10776417803333978], "conf": [0.9645428018186074, 0.8604304727982507, 0.7486417671452769, 0.6050953271402391, 0.4505905503584968, 0.3246606486224001], "prob": [[1.03273826452615e-07, 16], [1.5763178449816067e-27, 40], [5.162480886966344e-31, 16], [6.664843201145632e-32, 41], [8.768883395633968e-37, 18], [8.319853683510623e-37, 6]], "generated": ["Roy Rogers has 250 restaurants in the United States.", "Assuming that the number of daily customers at Roy Rogers is at most 100, then in one year, there would be 36500 customers for Roy Rogers.", "Taco Bell has 2000 restaurants in the United States.", "Assuming that the number of daily customers at Taco Bell is at most 100, then in one year, there would be 365000 customers for Taco Bell.", "Thus, more people will go in and out than a Roy Rogers each year.", "So the answer is yes."], "conf_comments": ["(A)\nTaco Bell has", "(A)\nTaco Bell has", "(A)\nAssuming that the", "(A)\nThus, more", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 65.8078408241272}
{"index": 96, "question": "Is it safe to wear sandals in snow?", "answer": "no", "explanation": "No. Sandals have open toes and don't completely cover the feet. Snow is very cold and direct exposure to skin can cause hypothermia. The feet need to be completely covered to walk through snow safely.", "generated": [{"finished": true, "length": 3, "score": [0.5548854871632283, 0.4734176187473953, 0.41606543782464706], "conf": [0.9254974517343583, 0.7479899583558757, 0.5792129788996528], "prob": [[1.124055162321321e-12, 25], [2.5980617824422023e-13, 14], [2.5585921383893064e-13, 6]], "generated": ["Wearing sandals in snow may cause extreme discomfort to the feet and may result in frostbite.", "Thus, it is not safe to wear sandals in snow.", "So the answer is no."], "conf_comments": ["(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5548854871632283, 0.39275871207433977, 0.3424236928719441], "conf": [0.9254974517343583, 0.7204725108028924, 0.5501026351810993], "prob": [[1.124055162321321e-12, 25], [2.350855331846557e-15, 14], [2.2883570912932626e-15, 6]], "generated": ["Wearing sandals in snow may cause extreme discomfort to the feet and may result in frostbite.", "Thus, it is NOT safe to wear sandals in snow.", "So the answer is no."], "conf_comments": ["(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.4980268831289688, 0.38968153755938423, 0.33458699751236953], "conf": [0.9393456072086026, 0.7144982211991086, 0.5300239037893393], "prob": [[2.114474525967032e-09, 15], [1.0128363542896438e-10, 14], [9.758076274799633e-11, 6]], "generated": ["Sandals do not provide any traction to an icy surface.", "Thus, it is not safe to wear sandals in snow.", "So the answer is no."], "conf_comments": ["(A)\nThus, it", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5188136554644156, 0.32478639175224616, 0.1981500739133092, 0.15039385977419564, 0.12278877858062581], "conf": [0.9071139558725884, 0.8058286884844862, 0.6246250561938236, 0.46058990848985454, 0.31287194230914905], "prob": [[4.659414850074336e-07, 12], [4.927413858889231e-12, 14], [5.3459246587515163e-20, 25], [1.317266306108813e-21, 15], [1.1762598432536929e-21, 6]], "generated": ["The safety of sandals in snow depends on the snow.", "If the snow is dry, then wearing sandals is fine.", "However, if the snow is wet, then wearing sandals could make your feet colder, which can be unsafe.", "Thus, it is not always safe to wear sandals in snow.", "So the answer is no."], "conf_comments": ["(A)\nIf the snow is", "(A)\nIf the snow is", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.4980268831289688, 0.30360046201323504, 0.2442797311360037], "conf": [0.9393456072086026, 0.7077930005483064, 0.4965093724199118], "prob": [[2.114474525967032e-09, 15], [2.5911784293358404e-14, 16], [1.6009960399399453e-14, 6]], "generated": ["Sandals do not provide any traction to an icy surface.", "Thus, wearing sandals in snow can increase the chance of falling.", "So the answer is no."], "conf_comments": ["(A)\nThus, it", "(A)\nThus, it", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5188136554644156, 0.32478639175224616, 0.1981500739133092, 0.15039385977419564, 0.09295199978473587], "conf": [0.9071139558725884, 0.8058286884844862, 0.6246250561938236, 0.46058990848985454, 0.3149259849946351], "prob": [[4.659414850074336e-07, 12], [4.927413858889231e-12, 14], [5.3459246587515163e-20, 25], [1.317266306108813e-21, 15], [2.2377722770704378e-23, 7]], "generated": ["The safety of sandals in snow depends on the snow.", "If the snow is dry, then wearing sandals is fine.", "However, if the snow is wet, then wearing sandals could make your feet colder, which can be unsafe.", "Thus, it is not always safe to wear sandals in snow.", "So the answer is it depends."], "conf_comments": ["(A)\nIf the snow is", "(A)\nIf the snow is", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 43.625606060028076}
{"index": 97, "question": "Was Nikola Tesla's home country involved in the American Civil War?", "answer": "no", "explanation": "No. Nikola Tesla was born in the Austrian Empire. The American Civil War was a domestic American conflict.", "generated": [{"finished": true, "length": 4, "score": [0.7339113585220062, 0.5777240390942041, 0.48602900239717794, 0.4162085023899941], "conf": [0.9567580270142962, 0.8322263264532762, 0.6568627801469004, 0.48307193220206135], "prob": [[5.808553731370404e-08, 29], [2.555625389949703e-10, 16], [2.589126430517076e-11, 21], [2.545162440474687e-11, 6]], "generated": ["Nikola Tesla was born and raised in Croatia, which was part of Austro-Hungary when he was born.", "Austro-Hungary was never involved in the American Civil War.", "Thus, Nikola Tesla's home country was not involved in the American Civil War.", "So the answer is no."], "conf_comments": ["(A)\nAustro-", "(A)\nThus, Nik", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7339113585220062, 0.6017612396099083, 0.40666615816114016, 0.35048313911957274, 0.3024331918193365], "conf": [0.9567580270142962, 0.8599982218415392, 0.6860525477005157, 0.5453658326442941, 0.4071238615508126], "prob": [[5.808553731370404e-08, 29], [2.330867213336744e-10, 19], [2.888892141625113e-13, 12], [6.947234745039269e-14, 21], [6.841133432697081e-14, 6]], "generated": ["Nikola Tesla was born and raised in Croatia, which was part of Austro-Hungary when he was born.", "The American Civil War took place from 1861 to 1865.", "Croatia was not involved in the American Civil War.", "Thus, Nikola Tesla's home country was not involved in the American Civil War.", "So the answer is no."], "conf_comments": ["(A)\nAustro-", "(A)\nThus, Nik", "(A)\nThus, Nik", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6559813847179418, 0.4665582743715562, 0.4052492376345697, 0.3452557518430315], "conf": [0.9320617291756542, 0.7547793437327378, 0.6085554693608185, 0.4428894838981667], "prob": [[3.9308727773436064e-11, 31], [3.0607282524894556e-16, 25], [7.58623643894744e-17, 21], [7.465858862879528e-17, 6]], "generated": ["Nikola Tesla's home country is Serbia, and Serbia was a province of Austro-Hungary at that time.", "The American Civil War occurred from 1861 to 1865, so Serbia was not involved.", "Thus, Nikola Tesla's home country was not involved in the American Civil War.", "So the answer is no."], "conf_comments": ["(A)\nAustro-", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7339113585220062, 0.5777240390942041, 0.39915717829961095, 0.33192166053767], "conf": [0.9567580270142962, 0.8322263264532762, 0.618717885851309, 0.42968755101046635], "prob": [[5.808553731370404e-08, 29], [2.555625389949703e-10, 16], [1.4948738920423727e-14, 22], [1.4566115885606898e-14, 6]], "generated": ["Nikola Tesla was born and raised in Croatia, which was part of Austro-Hungary when he was born.", "Austro-Hungary was never involved in the American Civil War.", "Thus, Nikola Tesla’s home country has not been involved in the American Civil War.", "So the answer is no."], "conf_comments": ["(A)\nAustro-", "(A)\nThus, Nik", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7339113585220062, 0.6017612396099083, 0.40666615816114016, 0.28000682833693624, 0.24202421664557952], "conf": [0.9567580270142962, 0.8599982218415392, 0.6860525477005157, 0.554776793222062, 0.41557908523487996], "prob": [[5.808553731370404e-08, 29], [2.330867213336744e-10, 19], [2.888892141625113e-13, 12], [3.897957413351029e-18, 21], [3.836251915284054e-18, 6]], "generated": ["Nikola Tesla was born and raised in Croatia, which was part of Austro-Hungary when he was born.", "The American Civil War took place from 1861 to 1865.", "Croatia was not involved in the American Civil War.", "Thus, Nikola Tesla'a home country was not involved in the American Civil War.", "So the answer is no."], "conf_comments": ["(A)\nAustro-", "(A)\nThus, Nik", "(A)\nThus, Nik", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.13933873176575}
{"index": 98, "question": "Did Ada Lovelace die tragically young for her era?", "answer": "no", "explanation": "No. Ada Lovelace died at the age of 36 in 1852. The life expectancy in the 1800s was between 35 and 39 years old. Tuberculosis was one of the leading causes of death in the 1800s and a vaccine was not made until the 1900s.", "generated": [{"finished": true, "length": 4, "score": [0.8253657588436439, 0.5040940059517992, 0.4305585375518362, 0.3729221083576893], "conf": [0.9648698735213079, 0.8412142834293888, 0.6790480945702885, 0.510624477121926], "prob": [[0.005399608042002992, 15], [1.5714470068733438e-15, 34], [3.443590065934652e-16, 15], [3.394970368180564e-16, 6]], "generated": ["Ada Lovelace was 36 years old when she died.", "This could be considered young considering that the average life expectancy for women in 1852 (the year Ada Lovelace died) was 43.", "Thus, Ada Lovelace died tragically young for her era.", "So the answer is yes."], "conf_comments": ["(A)\nThe average life expect", "(A)\nThus, Ada", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8253657588436439, 0.47010271919518676, 0.4118060392641814, 0.3601602978748983], "conf": [0.9648698735213079, 0.8159544768936217, 0.6817625645439734, 0.5227561341183532], "prob": [[0.005399608042002992, 15], [1.7477539712561688e-10, 18], [4.8749529408697655e-11, 15], [4.8040996864767056e-11, 6]], "generated": ["Ada Lovelace was 36 years old when she died.", "According to this website, 36 years old was considered young for her time.", "Thus, Ada Lovelace died tragically young for her era.", "So the answer is yes."], "conf_comments": ["(A)\nThe average life expect", "(A)\nThus, Ada", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8253657588436439, 0.47010271919518676, 0.37293414872649744, 0.3236611134391017], "conf": [0.9648698735213079, 0.8159544768936217, 0.6586791377078441, 0.4977188188357634], "prob": [[0.005399608042002992, 15], [1.7477539712561688e-10, 18], [8.809066649350034e-12, 12], [8.641107942864882e-12, 6]], "generated": ["Ada Lovelace was 36 years old when she died.", "According to this website, 36 years old was considered young for her time.", "Thus, Ada Lovelace died tragically young.", "So the answer is yes."], "conf_comments": ["(A)\nThe average life expect", "(A)\nThus, Ada", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6500497339800031, 0.568309112480388, 0.314818361200522, 0.2699647600320586], "conf": [0.9699591412972705, 0.9063211992657766, 0.7824007986848522, 0.5868286598729933], "prob": [[2.1845073304084496e-09, 24], [1.311646209864865e-10, 14], [6.746349098500796e-17, 14], [5.991573667692083e-17, 6]], "generated": ["In the 1840s, the average life expectancy in England was around 35 years old.", "Ada Lovelace died at the age of 36.", "Hence, she did die at a young age for her era.", "So the answer is yes."], "conf_comments": ["(A)\nAda Lovel", "(A)\nThus, Ada", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6500497339800031, 0.568309112480388, 0.3160128386073361, 0.26262903343562316], "conf": [0.9699591412972705, 0.9063211992657766, 0.7931740615746704, 0.5606261674200981], "prob": [[2.1845073304084496e-09, 24], [1.311646209864865e-10, 14], [4.962070187144034e-16, 12], [4.3200605617270165e-16, 6]], "generated": ["In the 1840s, the average life expectancy in England was around 35 years old.", "Ada Lovelace died at the age of 36.", "Thus, she died young in comparison to the average.", "So the answer is yes."], "conf_comments": ["(A)\nAda Lovel", "(A)\nThus, Ada", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 53.04221510887146}
{"index": 99, "question": "Can an Asian black bear use chopsticks?", "answer": "no", "explanation": "No. Asian black bear are a species of bear found in asia.  Asian black bear don't have opposable thumbs. Chopsticks are eating utensils use requires opposable thumbs.", "generated": [{"finished": true, "length": 4, "score": [0.5691597135202247, 0.3340833160925336, 0.26588272595288354, 0.2253763144529625], "conf": [0.9498272152381061, 0.8619628984130343, 0.6889794205398501, 0.49621656896806476], "prob": [[4.533920495529928e-10, 20], [5.9784011885473e-24, 33], [2.301054758518522e-25, 14], [2.2685830536718675e-25, 6]], "generated": ["Chopsticks are the typical utensils for East Asian and Southeast Asian countries.", "Asian black bears are mainly found in the East Asian countries of China, North Korea, South Korea, etc, where chopsticks are commonly used.", "Thus, Asian black bears could use chopsticks.", "So the answer is yes."], "conf_comments": ["(A)\nAsian black be", "(A)\nThus, Asian", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5825527494851758, 0.32998155522556455, 0.2415400146309378, 0.20363660377887735], "conf": [0.9497352430965214, 0.7639790261041676, 0.55311919567118, 0.39673050155970585], "prob": [[1.5484010593948332e-06, 13], [6.355686007059652e-13, 16], [2.817541761964065e-15, 18], [2.668150253194862e-15, 6]], "generated": ["Asian black bears eat their food using their paws.", "This indicates that an Asian black bear would have trouble using chopsticks.", "Thus, it is unlikely that an Asian black bear could use chopsticks.", "So the answer is no."], "conf_comments": ["(A)\nChopsticks", "(A)\nThus, an", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 9, "score": [0.6594284106766002, 0.4032419973586714, 0.32691212073156345, 0.1777236454750754, 0.0675158750402639, 0.04957583806529921, 0.03775383478550153, 0.02479003270199827, 0.018708334078164034], "conf": [0.9526319137826146, 0.8300486115040429, 0.6823156180159142, 0.5182842458400502, 0.37331200134168013, 0.2887014753498916, 0.20996319546409178, 0.11639638555491784, 0.0665697808941639], "prob": [[3.735359154283402e-05, 13], [3.3972555879284406e-09, 11], [1.4825545222092257e-10, 14], [1.0512256240362166e-16, 15], [1.1429956252317816e-30, 20], [1.2070321006527903e-33, 19], [3.226445232802127e-35, 16], [9.559267431088547e-37, 14], [9.32169772446321e-37, 6]], "generated": ["Chopsticks are eating utensils for humans.", "Humans are classified as mammals.", "Asian black bears are also classified as mammals.", "Mammals can be divided into placental and marsupial.", "Asian black bears are classified as an umbekatimorpha (bears).", "Umbekatimorpha are classified as a placental mammal.", "Humans are also classified as a placental mammal.", "Thus, Asian black bears can use chopsticks.", "So the answer is yes."], "conf_comments": ["(A)\nAsian black be", "(A)\nAsian black be", "(A)\nThus, Asian", "(A)\nAsian black be", "(A)\nThus, Asian", "(A)\nThus, Asian", "(A)\nThus, Asian", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5825527494851758, 0.32998155522556455, 0.25731694751111334], "conf": [0.9497352430965214, 0.7639790261041676, 0.5647689783448571], "prob": [[1.5484010593948332e-06, 13], [6.355686007059652e-13, 16], [1.9686601715122542e-13, 6]], "generated": ["Asian black bears eat their food using their paws.", "This indicates that an Asian black bear would have trouble using chopsticks.", "So the answer is no."], "conf_comments": ["(A)\nChopsticks", "(A)\nThus, an", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 8, "score": [0.6594284106766002, 0.4032419973586714, 0.32691212073156345, 0.1777236454750754, 0.1129348721817372, 0.047448478375422785, 0.03270057929921143, 0.026198098698996936], "conf": [0.9526319137826146, 0.8300486115040429, 0.6823156180159142, 0.5182842458400502, 0.4015534094761335, 0.2698520136180848, 0.16449585770588296, 0.10591536129972558], "prob": [[3.735359154283402e-05, 13], [3.3972555879284406e-09, 11], [1.4825545222092257e-10, 14], [1.0512256240362166e-16, 15], [3.1156038856707875e-21, 16], [1.5997050248204142e-30, 16], [4.863602307669789e-32, 14], [4.7720619365222896e-32, 6]], "generated": ["Chopsticks are eating utensils for humans.", "Humans are classified as mammals.", "Asian black bears are also classified as mammals.", "Mammals can be divided into placental and marsupial.", "Asian black bears and humans belong to placental mammals.", "Only one species of animal, the koala, uses chopsticks.", "Thus, Asian black bears cannot use chopsticks.", "So the answer is no."], "conf_comments": ["(A)\nAsian black be", "(A)\nAsian black be", "(A)\nThus, Asian", "(A)\nAsian black be", "(A)\nThus, Asian", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": [], "9": []}, "run_time": 288.29217410087585}
{"index": 100, "question": "Can someone sell their time through the Toronto Star?", "answer": "yes", "explanation": "Yes. The Toronto Star has a classifieds section. Readers can advertise their own labor or services and thus their time .", "generated": [{"finished": true, "length": 4, "score": [0.8368388500357298, 0.40154368125392864, 0.31117185733623537, 0.2620154894805437], "conf": [0.9783058038660993, 0.6805635551227863, 0.49170971452885004, 0.34997977078090314], "prob": [[0.09630819999711783, 7], [1.3866478835930973e-05, 8], [8.657453536913576e-07, 15], [8.458728677727013e-07, 6]], "generated": ["The Toronto Star is a newspaper.", "It is not possible to sell time.", "Thus, it is not possible to sell time through the Toronto Star.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8368388500357298, 0.40154368125392864, 0.2926833478969996, 0.24590139154944557], "conf": [0.9783058038660993, 0.6805635551227863, 0.49404396327137945, 0.34983926234485463], "prob": [[0.09630819999711783, 7], [1.3866478835930973e-05, 8], [1.7540241797173703e-07, 14], [1.720970927384588e-07, 6]], "generated": ["The Toronto Star is a newspaper.", "It is not possible to sell time.", "Thus, someone can not sell their time through the Toronto Star.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6542439496291745, 0.3226055077658607, 0.24795676251957127, 0.21856998282334714], "conf": [0.9700660690333106, 0.8235096569558618, 0.6661223449743768, 0.5187234834784965], "prob": [[1.060458367729418e-05, 14], [1.128386283054547e-11, 11], [1.0122868529903944e-13, 15], [9.990492131095351e-14, 6]], "generated": ["The Toronto Star is a newspaper that publishes local and international news.", "It does not sell or rent people's time.", "Thus, someone can't sell their time through the Toronto Star.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.759410325512878, 0.33405187799001906, 0.2283307231144465, 0.18635801283445202], "conf": [0.9747970541902446, 0.8642970388974316, 0.5920525650997066, 0.39575159430094586], "prob": [[0.015007567338332499, 8], [7.721798799348976e-08, 8], [3.6388927669300307e-10, 14], [3.5645067948380453e-10, 6]], "generated": ["Toronto Star is a newspaper.", "One can sell their time by working.", "Thus, a person can sell their time through the Toronto Star.", "So the answer is yes."], "conf_comments": ["(A)\nThus, it", "(A)\nThus, someone", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5979484314464795, 0.2953786471703402, 0.2259729347527999], "conf": [0.97057061514176, 0.6825701655290869, 0.49701175410703496], "prob": [[2.3009342262345893e-06, 13], [8.438245773481641e-13, 14], [1.4700583319855317e-13, 8]], "generated": ["The Toronto Star is a newspaper for the Canadian province of Ontario.", "Therefore, the Toronto Star should not be used to sell time.", "Thus, the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 49.71811580657959}
{"index": 101, "question": "Were deaths from Apollo 13 mission eclipsed by other space missions?", "answer": "yes", "explanation": "Yes. Apollo 13 was a failed Moon landing in which the crew had faulty oxygen resources. All of the Apollo 13 crew returned safely to earth. The Challenger was a space shuttle that malfunctioned, killing all seven passengers. The Space Shuttle Columbia disaster had seven casualties.", "generated": [{"finished": true, "length": 4, "score": [0.7310491308367565, 0.503977675069845, 0.35763726622734937, 0.26265412995683846], "conf": [0.9531118400554983, 0.8432642141184834, 0.6054647491732091, 0.3932082215952644], "prob": [[9.54966747534795e-05, 16], [4.411578690586189e-16, 42], [3.0346159965922295e-22, 40], [9.958616133364095e-23, 6]], "generated": ["The number of deaths during Apollo 13 mission was 0.", "In comparison, the number of deaths in the Challenger (1986) and Colubmbia (2003) space missions were 7 and 8, respectively.", "Thus, the deaths from Apollo 13 mission are less than the deaths from Challenger (1986) and Colubmbia (2003).", "So the answer is yes."], "conf_comments": ["(A)\nThe number of death", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6277822438855278, 0.469304570642702, 0.31715793241928636, 0.2517165531486115], "conf": [0.9561193757187291, 0.798614385884334, 0.5280045390108176, 0.33697672511764365], "prob": [[5.788186054036774e-10, 24], [1.3948901624233408e-12, 15], [9.277001034578829e-17, 26], [8.575678420816158e-17, 6]], "generated": ["According to NASA, all astronauts from the Apollo 13 mission survived and returned home safely.", "Thus, no one died from the Apollo 13 mission.", "Thus, the deaths from the Apollo 13 mission are not eclipsed by other space missions.", "So the answer is no."], "conf_comments": ["(A)\nThus, no", "(A)\nIn comparison, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7310491308367565, 0.503977675069845, 0.3342185635622182, 0.2507034543124348], "conf": [0.9531118400554983, 0.8432642141184834, 0.6301840138720823, 0.3737944828355818], "prob": [[9.54966747534795e-05, 16], [4.411578690586189e-16, 42], [1.8601833540636975e-20, 19], [1.3555630412149936e-20, 6]], "generated": ["The number of deaths during Apollo 13 mission was 0.", "In comparison, the number of deaths in the Challenger (1986) and Colubmbia (2003) space missions were 7 and 8, respectively.", "Thus, Apollo 13 mission has less deaths than other space missions.", "So the answer is no."], "conf_comments": ["(A)\nThe number of death", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.7310491308367565, 0.4878392085607417], "conf": [0.9531118400554983, 0.7881435786583342], "prob": [[9.54966747534795e-05, 16], [6.437679342604859e-18, 49]], "generated": ["The number of deaths during Apollo 13 mission was 0.", "In comparison, the number of deaths during the launch of Apollo 1 launch was 3, the number of deaths during the launch of Soyuz 11 spaceflight was 3, and the number of deaths"], "conf_comments": ["(A)\nThe number of death", "(A)\nduring the launch"], "info": null}, {"finished": true, "length": 4, "score": [0.7009857978147471, 0.40402886000415456, 0.2770472737610492, 0.22008517622689241], "conf": [0.9645466917659596, 0.6489009282873877, 0.4268116376634409, 0.2720071007697711], "prob": [[0.00015568163052392405, 13], [7.979104419679514e-09, 14], [1.8095990489909208e-12, 25], [1.7059305878228966e-12, 6]], "generated": ["There were no fatalities from Apollo 13 mission.", "However, there have been many fatalities from other space missions.", "Thus, deaths from the Apollo 13 mission are not eclipsed by other space missions.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, there", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 48.283700942993164}
{"index": 102, "question": "Can you purchase a dish with injera at Taco Bell?", "answer": "no", "explanation": "No. Taco Bell serves a variety of Mexican and Tex-Mex foods that include tacos, burritos, quesadillas, and nachos. Injera is a sour fermented flatbread with a slightly spongy texture, traditionally made out of teff flour. Injera is part of Ethiopian cuisine.", "generated": [{"finished": true, "length": 3, "score": [0.7453342495629782, 0.5268450803769227, 0.4497164242977877], "conf": [0.9499191751637637, 0.7432091408272682, 0.5432690234011963], "prob": [[0.0016002433353303192, 12], [7.820857144868354e-07, 17], [7.67186214474214e-07, 6]], "generated": ["Taco Bell does not serve dishes with injera.", "Thus, you cannot purchase a dish with injera at Taco bell.", "So the answer is no."], "conf_comments": ["(A)\nInjera is", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6902880023435859, 0.426615934148218, 0.36299616186477435, 0.3078658118287804], "conf": [0.9421765855091602, 0.768819970759373, 0.5962007438971825, 0.4297523360733162], "prob": [[1.3117476437737232e-09, 30], [6.623106662372533e-13, 10], [2.0597610132405312e-13, 17], [2.0341154823600198e-13, 6]], "generated": ["Taco Bell’s main menu consists of tacos, burritos, nachos, quesadillas, and grillers.", "There is no dish that contains injera.", "Thus, you cannot purchase a dish with injera at Taco Bell.", "So the answer is no."], "conf_comments": ["(A)\nInjera is", "(A)\nThus, you", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6902880023435859, 0.4171131521322454, 0.3559958567698676, 0.30534747723740713], "conf": [0.9421765855091602, 0.8090267706623012, 0.644719891689528, 0.47520124748539405], "prob": [[1.3117476437737232e-09, 30], [1.1522969272954758e-16, 19], [2.5010444874633398e-17, 17], [2.4732904239860415e-17, 6]], "generated": ["Taco Bell’s main menu consists of tacos, burritos, nachos, quesadillas, and grillers.", "None of these contain injera, which is usually served in Ethiopian cuisine.", "Thus, you cannot purchase a dish with injera at Taco Bell.", "So the answer is no."], "conf_comments": ["(A)\nInjera is", "(A)\nThus, you", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.679239863097451, 0.46518217392208405, 0.3967148520620534], "conf": [0.9576778155873188, 0.7551240070366178, 0.5521758433415165], "prob": [[8.418417684122569e-06, 16], [3.4773199928793465e-09, 15], [3.3663142395415835e-09, 6]], "generated": ["Taco Bell does not sell injera, a popular Ethiopian bread.", "Thus, you can’t purchase a dish with injera.", "So the answer is no."], "conf_comments": ["(A)\nInjera is", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.679239863097451, 0.5078974668321169], "conf": [0.9576778155873188, 0.7762959990333282], "prob": [[8.418417684122569e-06, 16], [4.313335107864015e-07, 8]], "generated": ["Taco Bell does not sell injera, a popular Ethiopian bread.", "Thus, the answer is no."], "conf_comments": ["(A)\nInjera is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 42.6443247795105}
{"index": 103, "question": "Is xenophobia hypothetically unimportant between Saladin and Ali Askari?", "answer": "yes", "explanation": "Yes. Xenophobia is the dislike of someone that is foreign or from a different background. Saladin was a Kurdish leader that became sultan of Egypt. Ali Askari was a Kurdish politician.", "generated": [{"finished": true, "length": 5, "score": [0.747079814635443, 0.3833639039630357, 0.24035494862010207, 0.18387940296487948, 0.1533110762625709], "conf": [0.9421082295408603, 0.8253911498217277, 0.5818062295368305, 0.4383712063890511, 0.31039312759026444], "prob": [[8.946010927106344e-08, 31], [1.3199903848726269e-15, 15], [2.0703694711562395e-19, 15], [6.028332194945638e-21, 14], [5.39836304138721e-21, 6]], "generated": ["The dictionary definition of xenophobia is “fear and hatred of strangers or foreigners or of anything that is strange or foreign.”", "Saladin was a native of Iraq and so was Ali Askari.", "Both of them were not strangers or foreigners to each other.", "Thus, there was no xenophobia between them.", "So the answer is no."], "conf_comments": ["(A)\nSaladin was a", "(A)\nThus, Sal", "(A)\nThus, x", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.747079814635443, 0.41722413668235114, 0.27755667420296803, 0.14247026238690919, 0.11455112556235378], "conf": [0.9421082295408603, 0.8652011585765949, 0.6811180656448163, 0.499216572149999, 0.34620669945105315], "prob": [[8.946010927106344e-08, 31], [2.1061632095535432e-13, 12], [1.179401121479418e-16, 13], [3.314432463087159e-32, 35], [2.1748633361584854e-32, 6]], "generated": ["The dictionary definition of xenophobia is “fear and hatred of strangers or foreigners or of anything that is strange or foreign.”", "Saladin was born in Persia and died in Egypt.", "Ali Askari was born in Egypt and died in Turkey.", "So it can be inferred that neither person expressed xenophobia and xenophobia was hypothetically unimportant between Saladin and Ali Askari.", "So the answer is yes."], "conf_comments": ["(A)\nSaladin was a", "(A)\nAli Askari", "(A)\nThus, Sal", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.646434560119348, 0.47494887300112415, 0.29132915242150537, 0.1478018029118482, 0.11367798453125914], "conf": [0.9722480917647811, 0.7785373016880504, 0.638194158782453, 0.45940686661240193, 0.29395526357400303], "prob": [[4.2798945364859885e-12, 31], [1.607971426197931e-15, 20], [6.451910221776512e-20, 13], [4.60509891124107e-27, 16], [2.8753850487761878e-27, 6]], "generated": ["Saladin was the Sultan of the Abbasid Caliphate in the mid 12th century and died in 1193.", "Ali Askari was the 11th Imam and died in 868.", "These two people lived over 500 years apart.", "Thus, xenophobia was hypothetically of little importance.", "So the answer is yes."], "conf_comments": ["(A)\nAli Askari", "(A)\nThus, Sal", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.646434560119348, 0.32934756745956917, 0.21767745805284278, 0.11723640796331836, 0.08995430786986656], "conf": [0.9722480917647811, 0.7654478118565673, 0.6482204478619753, 0.36395046997510655, 0.21828510741021323], "prob": [[4.2798945364859885e-12, 31], [1.4964839544066855e-26, 30], [3.7605011295797625e-31, 16], [2.5849928368142993e-36, 18], [2.3125011867748525e-36, 6]], "generated": ["Saladin was the Sultan of the Abbasid Caliphate in the mid 12th century and died in 1193.", "Ali Askari was a Shia religious leader who preached mostly in Iraq from 1892 to 1974.", "Thus, Saladin and Ali Askari were living in different time periods.", "Hence, xenophobia was not hypothetically unimportant between them.", "So the answer is no."], "conf_comments": ["(A)\nAli Askari", "(A)\nThus, Sal", "(A)\nThus, Sal", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.646434560119348, 0.47494887300112415, 0.29132915242150537, 0.12416016645382796, 0.05212691166321504, 0.02992273649749968, 0.02241409982513978], "conf": [0.9722480917647811, 0.7785373016880504, 0.638194158782453, 0.4668075651046142, 0.29068198811199836, 0.17075861311945553, 0.09842906298975392], "prob": [[4.2798945364859885e-12, 31], [1.607971426197931e-15, 20], [6.451910221776512e-20, 13], [5.12767786337624e-32, 20], [6.522543905623631e-48, 29], [1.1060772466719871e-52, 19], [9.40981295328399e-53, 6]], "generated": ["Saladin was the Sultan of the Abbasid Caliphate in the mid 12th century and died in 1193.", "Ali Askari was the 11th Imam and died in 868.", "These two people lived over 500 years apart.", "During that time period, the Ottoman Empire did not exist in 1453.", "As of 2010, the United States had the highest immigration level in the world with 40 million immigrants.", "Thus, xenophobia is still important between Saladin and Ali Askari.", "So the answer is no."], "conf_comments": ["(A)\nAli Askari", "(A)\nThus, Sal", "(A)\nThus, it", "(A)\nThus, x", "(A)\nThus, x", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.646434560119348, 0.47494887300112415, 0.29132915242150537, 0.12416016645382796, 0.05212691166321504, 0.02992273649749968, 0.019418751834277534], "conf": [0.9722480917647811, 0.7785373016880504, 0.638194158782453, 0.4668075651046142, 0.29068198811199836, 0.17075861311945553, 0.10365370699119394], "prob": [[4.2798945364859885e-12, 31], [1.607971426197931e-15, 20], [6.451910221776512e-20, 13], [5.12767786337624e-32, 20], [6.522543905623631e-48, 29], [1.1060772466719871e-52, 19], [1.2337074357955277e-53, 6]], "generated": ["Saladin was the Sultan of the Abbasid Caliphate in the mid 12th century and died in 1193.", "Ali Askari was the 11th Imam and died in 868.", "These two people lived over 500 years apart.", "During that time period, the Ottoman Empire did not exist in 1453.", "As of 2010, the United States had the highest immigration level in the world with 40 million immigrants.", "Thus, xenophobia is still important between Saladin and Ali Askari.", "So the answer is yes."], "conf_comments": ["(A)\nAli Askari", "(A)\nThus, Sal", "(A)\nThus, it", "(A)\nThus, x", "(A)\nThus, x", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 73.91174340248108}
{"index": 104, "question": "Does Sockington enjoy onions?", "answer": "no", "explanation": "No. Sockington is a domestic cat. Onions can cause toxicity in cats by breaking down their red blood cells.", "generated": [{"finished": true, "length": 2, "score": [0.6406078446934825, 0.5127952652605858], "conf": [0.9213027490774658, 0.7145933997583884], "prob": [[0.0015497337999309923, 8], [0.0003362266247981706, 8]], "generated": ["Sockington likes onions.", "Thus, the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5881768327463989, 0.3112446215914842, 0.27218433817985443, 0.23642374913541747], "conf": [0.9660065091316334, 0.8766947561259991, 0.7133052691247931, 0.5403693843787862], "prob": [[0.00027057409112757856, 8], [1.6545379269576325e-14, 20], [7.867252330689037e-15, 12], [7.67833187331934e-15, 6]], "generated": ["Sockington has a twitter account.", "On the account’s ‘About’ section, he states that he does not like onions.", "Thus, Sockington does not enjoy onions.", "So the answer is no."], "conf_comments": ["(A)\nSockington'", "(A)\nThus, So", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5137707730636627, 0.38509934909120846, 0.31625316102746964], "conf": [0.7477787284392059, 0.5100475304452654, 0.3454921601203763], "prob": [[1.1242037313358662e-10, 22], [1.0966697601406996e-11, 12], [1.0682076116446448e-11, 6]], "generated": ["Sockington is a very cute cat and it is unlikely that he likes to eat onions.", "Thus, Sockington does not enjoy onions.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.5137707730636627, 0.3662188194603919], "conf": [0.7477787284392059, 0.5057697286552265], "prob": [[1.1242037313358662e-10, 22], [1.140111781790243e-11, 8]], "generated": ["Sockington is a very cute cat and it is unlikely that he likes to eat onions.", "Thus, the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6406078446934825, 0.3029010860082306, 0.16056087441141037, 0.12048381475419168], "conf": [0.9213027490774658, 0.6019470503724713, 0.37699193337097914, 0.24476264058348451], "prob": [[0.0015497337999309923, 8], [4.676429093019796e-10, 14], [3.443900120913934e-13, 9], [8.292768186746745e-14, 10]], "generated": ["Sockington likes onions.", "I don’t know if Sockington likes onions.", "Thus, I’m not sure.", "So the answer is I don’t know."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, So", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6406078446934825, 0.3029010860082306, 0.16056087441141037, 0.08233855887200589], "conf": [0.9213027490774658, 0.6019470503724713, 0.37699193337097914, 0.23561890377285885], "prob": [[0.0015497337999309923, 8], [4.676429093019796e-10, 14], [3.443900120913934e-13, 9], [1.9113690473161567e-15, 6]], "generated": ["Sockington likes onions.", "I don’t know if Sockington likes onions.", "Thus, I’m not sure.", "So the answer is neutral."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, So", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 39.9227352142334}
{"index": 105, "question": "Was Raphael's paintings influenced by the country of Guam?", "answer": "no", "explanation": "No. Raphael was a European painter that died in 1520. Magellan's 1521 voyage was Europe's first trip to Guam.", "generated": [{"finished": true, "length": 4, "score": [0.8778269444126574, 0.5639849093049792, 0.41710145941850413, 0.32800912842493535], "conf": [0.9752774595310314, 0.8827820177104408, 0.6139114495347782, 0.38128579326452494], "prob": [[0.007103660107751189, 21], [2.3568261584674617e-09, 19], [5.0520389954358115e-11, 16], [4.924112875584867e-11, 6]], "generated": ["Raphael was born in 1483 and died in 1520.", "Guam was ceded to the USA from the Spanish Empire in 1898.", "Thus, Raphael was not influenced by the country of Guam.", "So the answer is no."], "conf_comments": ["(A)\nGuam was discovered", "(A)\nThus, Rap", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8778269444126574, 0.5639849093049792, 0.3905514106805051, 0.3004667755158924], "conf": [0.9752774595310314, 0.8827820177104408, 0.5755789902296311, 0.3423770369212547], "prob": [[0.007103660107751189, 21], [2.3568261584674617e-09, 19], [1.2706254243569065e-11, 17], [1.2332136147221078e-11, 6]], "generated": ["Raphael was born in 1483 and died in 1520.", "Guam was ceded to the USA from the Spanish Empire in 1898.", "Thus, Raphael’s paintings were not influenced by Guam.", "So the answer is no."], "conf_comments": ["(A)\nGuam was discovered", "(A)\nThus, Rap", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7353700571625371, 0.5851494572543893, 0.38927575757752686, 0.29716348621098054], "conf": [0.977562883298838, 0.8823233178956164, 0.5731325674308674, 0.33564526819857565], "prob": [[2.352931347799844e-05, 18], [1.6449424601549182e-07, 14], [2.4163779364674245e-10, 17], [2.3456825913326273e-10, 6]], "generated": ["Raphael painted from 1500 to 1520.", "Guam was colonized by Spain in 1565.", "Thus, Raphael’s paintings was not influenced by Guam.", "So the answer is no."], "conf_comments": ["(A)\nGuam was colon", "(A)\nThus, Rap", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7697489507141657, 0.5351795974525777, 0.34092694777513494, 0.26354217822448894], "conf": [0.9684737394816593, 0.8644542674798041, 0.538399234638825, 0.32329490275848316], "prob": [[8.822605847444503e-05, 19], [1.6468378994402155e-08, 14], [1.5617156695429305e-13, 27], [1.516704711820252e-13, 6]], "generated": ["Raphael was born 1483 and died 1520.", "Guam was conquered by Spain in 1565.", "Thus, Raphael’s paintings were not influenced by the country of Guam because Guam did not exist yet.", "So the answer is no."], "conf_comments": ["(A)\nGuam was discovered", "(A)\nThus, Rap", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7473531597950906, 0.5460518751448658, 0.3416155314759143, 0.24806832278733065], "conf": [0.9645774981452248, 0.8188598147454574, 0.4937804961990116, 0.29139237795922157], "prob": [[9.250146329140096e-05, 17], [2.224414429794381e-07, 13], [9.296304944420052e-11, 18], [4.732049290332841e-11, 6]], "generated": ["Raphael is an Italian painter from the 1500s.", "Guam became a US territory in 1898.", "So Raphael’s paintings were not influenced by the country of Guam.", "So the answer is no."], "conf_comments": ["(A)\nGuam is a", "(A)\nThus, Rap", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 53.72951674461365}
{"index": 106, "question": "Could Oscar Wilde have operated a motor vehicle?", "answer": "yes", "explanation": "Yes. Motor vehicles were in use by the 1890s. Oscar Wilde lived until 1900.", "generated": [{"finished": true, "length": 4, "score": [0.8385528022729224, 0.6158013919011467, 0.4432279013542693, 0.37087630043195596], "conf": [0.9694241610339732, 0.8360910048752705, 0.6829804542044168, 0.49462249150133], "prob": [[0.008094776196726838, 15], [1.0807864381237115e-06, 19], [1.839897634525356e-09, 14], [1.5025330500159397e-09, 6]], "generated": ["Oscar Wilde was born in the year 1854.", "According to Wikipedia, the first motor vehicle was invented in 1885.", "This indicates that Oscar Wilde could not have operated a motor vehicle.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, the", "(A)\nThus, Oscar", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8385528022729224, 0.6158013919011467, 0.4432279013542693, 0.34214874828050057], "conf": [0.9694241610339732, 0.8360910048752705, 0.6829804542044168, 0.5241295348829216], "prob": [[0.008094776196726838, 15], [1.0807864381237115e-06, 19], [1.839897634525356e-09, 14], [2.4319610785907107e-10, 8]], "generated": ["Oscar Wilde was born in the year 1854.", "According to Wikipedia, the first motor vehicle was invented in 1885.", "This indicates that Oscar Wilde could not have operated a motor vehicle.", "Thus, the answer is no."], "conf_comments": ["(A)\nIn comparison, the", "(A)\nThus, Oscar", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.9339779925795961, 0.5398943373646744, 0.405055667556712, 0.3209556134338692], "conf": [0.9725732215617656, 0.8601379847189466, 0.604050866261357, 0.3805545795277354], "prob": [[0.1018053368364599, 21], [1.230123450711942e-07, 14], [6.929998819662789e-09, 13], [6.789466740316516e-09, 6]], "generated": ["Oscar Wilde was born in 1854 and died in 1900.", "The earliest motor vehicle was invented in 1863.", "Thus, Oscar Wilde could have operated a motor vehicle.", "So the answer is yes."], "conf_comments": ["(A)\nThe first motor vehicle", "(A)\nThus, Oscar", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8934383146595952, 0.49182032975378054, 0.33702056934982455, 0.2761358097374772], "conf": [0.976044943126697, 0.798639202827577, 0.6172153305860965, 0.416860532752762], "prob": [[0.07321014965274537, 13], [4.65616394093168e-10, 19], [2.6438389769560084e-13, 15], [2.5498017911881766e-13, 6]], "generated": ["Oscar Wilde was born in 1854.", "The first patent for the internal combustion engine is around 1859.", "Thus, Oscar Wilde was not able to operate a motor vehicle.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, the", "(A)\nThus, Oscar", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6753888961323187, 0.43698551786085954, 0.36649055530520525], "conf": [0.9438864515570061, 0.7282781088113642, 0.5160304430277416], "prob": [[5.449113378272952e-08, 23], [1.9237987024467315e-11, 13], [1.8409421245219182e-11, 6]], "generated": ["Oscar Wilde was born in 1854, which is way before motors were invented.", "Thus, Oscar Wilde did not operate a motor vehicle.", "So the answer is no."], "conf_comments": ["(A)\nThus, Oscar", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.9339779925795961, 0.5398943373646744, 0.3459955710811292, 0.2617367251485751], "conf": [0.9725732215617656, 0.8601379847189466, 0.5539704925359097, 0.3213721912434455], "prob": [[0.1018053368364599, 21], [1.230123450711942e-07, 14], [9.19663947405534e-11, 16], [8.472833799375835e-11, 6]], "generated": ["Oscar Wilde was born in 1854 and died in 1900.", "The earliest motor vehicle was invented in 1863.", "Thus, it is possible that Oscar Wilde could operate a motor vehicle.", "So the answer is yes."], "conf_comments": ["(A)\nThe first motor vehicle", "(A)\nThus, Oscar", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.9339779925795961, 0.5398943373646744, 0.3459955710811292, 0.19136192420723666], "conf": [0.9725732215617656, 0.8601379847189466, 0.5539704925359097, 0.31504125461698657], "prob": [[0.1018053368364599, 21], [1.230123450711942e-07, 14], [9.19663947405534e-11, 16], [2.227225870293975e-12, 6]], "generated": ["Oscar Wilde was born in 1854 and died in 1900.", "The earliest motor vehicle was invented in 1863.", "Thus, it is possible that Oscar Wilde could operate a motor vehicle.", "So the answer is maybe."], "conf_comments": ["(A)\nThe first motor vehicle", "(A)\nThus, Oscar", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 51.17805504798889}
{"index": 107, "question": "Does ontology require a scalpel?", "answer": "no", "explanation": "No. A scalpel is used during surgery. Ontology is a philosophical domain, not a medical one.", "generated": [{"finished": true, "length": 4, "score": [0.7961028295910554, 0.3580832728065426, 0.30658061246301777, 0.26032868366878775], "conf": [0.9643262506184135, 0.6920544144657974, 0.5550358071724789, 0.40119278216851506], "prob": [[0.02287858250103384, 9], [1.2916097134968703e-10, 15], [4.3895285528224314e-11, 12], [4.324672669971886e-11, 6]], "generated": ["Ontology is the study of existence.", "A scalpel does not need to be used in an ontological study.", "Thus, ontology does not require a scalpel.", "So the answer is no."], "conf_comments": ["(A)\nA scalpel is", "(A)\nThus, ont", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7343667156320622, 0.38921862957684866, 0.26200928163308845, 0.2282488620918434], "conf": [0.8856244271923032, 0.7364342590718942, 0.6281281204650185, 0.47849544463968896], "prob": [[3.9671527866560075e-09, 39], [1.99166838577895e-12, 7], [2.843564552729227e-16, 14], [2.779642877992882e-16, 6]], "generated": ["From the definition of ontology, it “is the philosophical study of the nature of being, becoming, existence, or reality, as well as the basic categories of being and their relations”.", "No mention of a scalpel.", "Thus, the ontology doesn’t require a scalpel.", "So the answer is no."], "conf_comments": ["(A)\nThus, ont", "(A)\nThus, ont", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7783592210004409, 0.32184068962285356, 0.2682009268101422, 0.22563464002928096], "conf": [0.9560592299361922, 0.6861161401185536, 0.5434515315581572, 0.38588588919455896], "prob": [[0.0004284310766410295, 17], [2.009490601611716e-18, 23], [4.145639974805029e-19, 12], [4.065828917713768e-19, 6]], "generated": ["Ontology is a branch of philosophy that deals with the nature of being.", "Since there is no medical component in ontology, it is doubtful for ontology to need a scalpel.", "Thus, ontology does not require a scalpel.", "So the answer is no."], "conf_comments": ["(A)\nThus, ont", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7343667156320622, 0.3324875946855428, 0.2598027502848308, 0.22351855283010483], "conf": [0.8856244271923032, 0.7349213642630353, 0.6073493143901824, 0.4520791444150779], "prob": [[3.9671527866560075e-09, 39], [5.058435395995563e-17, 13], [2.4513506524918622e-18, 10], [2.370215702754292e-18, 6]], "generated": ["From the definition of ontology, it “is the philosophical study of the nature of being, becoming, existence, or reality, as well as the basic categories of being and their relations”.", "Ontology does not involve the manipulation of physical matter.", "Thus, a scalpel is not required.", "So the answer is no."], "conf_comments": ["(A)\nThus, ont", "(A)\nThus, ont", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.7861707753961482, 0.4271965466180137], "conf": [0.9621334212484397, 0.7465361819128693], "prob": [[0.004938350201462924, 12], [2.1718548842592104e-06, 8]], "generated": ["Ontology is the study of the nature of reality.", "Thus, the answer is no."], "conf_comments": ["(A)\nA scalpel is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 47.57639980316162}
{"index": 108, "question": "Was King Arthur at the beheading of Anne Boleyn?", "answer": "no", "explanation": "No. King Arthur was a legendary British leader who, according to medieval histories and romances, led the defence of Britain against Saxon invaders in the late 5th and early 6th centuries. Anne Boleyn was beheaded May 19, 1536.", "generated": [{"finished": true, "length": 3, "score": [0.6826161802066165, 0.5730371927937445, 0.47081587981436146], "conf": [0.8671983200203083, 0.6682531288524196, 0.4523519068849106], "prob": [[4.6758288105596895e-12, 42], [9.360017359973754e-13, 18], [9.20626789782944e-13, 6]], "generated": ["King Arthur is a fictional character and the beheading of Anne Boleyn happened in 1536, while King Arthur lived in the 5th century or 6th century.", "Thus, King Arthur was not at the beheading of Anne Boleyn.", "So the answer is no."], "conf_comments": ["(A)\nThus, King", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6826161802066165, 0.49786099535488504, 0.410997262295068], "conf": [0.8671983200203083, 0.661557610754411, 0.45232518805978394], "prob": [[4.6758288105596895e-12, 42], [4.951319406184507e-15, 19], [4.855033606414772e-15, 6]], "generated": ["King Arthur is a fictional character and the beheading of Anne Boleyn happened in 1536, while King Arthur lived in the 5th century or 6th century.", "Thus, King Arthur was not around during the beheading of Anne Boleyn.", "So the answer is no."], "conf_comments": ["(A)\nThus, King", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7084430796776503, 0.44096644106525595, 0.362905894585167, 0.30053199288362464], "conf": [0.9649450746766679, 0.8086046684689469, 0.620723601115333, 0.4275050232414494], "prob": [[0.0027859029176623635, 9], [2.5362150134772502e-11, 24], [2.6622890077982987e-12, 18], [2.5951214256540987e-12, 6]], "generated": ["King Arthur is a fictional character.", "Anne Boleyn was a queen in the Tudor period who was executed in 1536.", "Thus, King Arthur was not at the beheading of Anne Boleyn.", "So the answer is no."], "conf_comments": ["(A)\nAnne Bole", "(A)\nThus, King", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.7084430796776503, 0.5633096949324, 0.42483427280215186, 0.22970251085301965, 0.17091233551542892, 0.14588910796768131], "conf": [0.9649450746766679, 0.818309830258507, 0.7077917194399318, 0.5541285677650925, 0.42171678620806646, 0.308500415680857], "prob": [[0.0027859029176623635, 9], [4.566010554231935e-05, 14], [4.514289183061249e-09, 22], [3.3585255986662315e-17, 19], [3.060964799552577e-20, 22], [2.9884270790920134e-20, 6]], "generated": ["King Arthur is a fictional character.", "Anne Boleyn was the second wife of King Henry VIII.", "King Henry VIII was the monarch of England from 1509 to 1547.", "Thus, Anne Boleyn was alive and King Arthur was fictional at that time.", "Thus, it was impossible for King Arthur to be at the beheading of Anne Boleyn.", "So the answer is no."], "conf_comments": ["(A)\nAnne Bole", "(A)\nShe was behead", "(A)\nThus, King", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7084430796776503, 0.5633096949324, 0.42483427280215186, 0.22970251085301965, 0.18476892202636253], "conf": [0.9649450746766679, 0.818309830258507, 0.7077917194399318, 0.5541285677650925, 0.3737050213531878], "prob": [[0.0027859029176623635, 9], [4.566010554231935e-05, 14], [4.514289183061249e-09, 22], [3.3585255986662315e-17, 19], [2.619367877706946e-17, 6]], "generated": ["King Arthur is a fictional character.", "Anne Boleyn was the second wife of King Henry VIII.", "King Henry VIII was the monarch of England from 1509 to 1547.", "Thus, Anne Boleyn was alive and King Arthur was fictional at that time.", "So the answer is no."], "conf_comments": ["(A)\nAnne Bole", "(A)\nShe was behead", "(A)\nThus, King", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 56.03153371810913}
{"index": 109, "question": "Would Alexander Graham Bell hypothetically support Nazi eugenics?", "answer": "yes", "explanation": "Yes. Eugenics was the idea of selective breeding or sterilization to rid the human populace of certain traits. Nazis used eugenics to justify mass sterilization and mass murder. Alexander Graham Bell Alexander Graham Bell advocated against the use of sign language and hoped to eradicate deafness through selective breeding.", "generated": [{"finished": true, "length": 5, "score": [0.8115641634218453, 0.44472787201821545, 0.20240105480504886, 0.14967834410661135, 0.11859502614491017], "conf": [0.9351628421177577, 0.7718304846910063, 0.4908590341039864, 0.30743806075681834, 0.19388897724295925], "prob": [[0.010492047537341334, 13], [1.4634707152504434e-14, 27], [4.229173572362469e-32, 36], [5.528773633230319e-33, 15], [5.379549965672378e-33, 6]], "generated": ["Alexander Graham Bell was a proponent of eugenics.", "Nazi eugenics was a German government-sponsored program of selective breeding to produce genetically superior individuals.", "Eugenics and Nazi eugenics are different because the former is focused on improving human qualities while the latter aims to eliminate perceived undesirables.", "Thus, Alexander Graham Bell would not support Nazi eugenics.", "So the answer is no."], "conf_comments": ["(A)\nNazi eugen", "(A)\nThus, Alexander", "(A)\nThus, Alexander", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5688873462775306, 0.32905342253341935, 0.2765944716863036], "conf": [0.9422565931402094, 0.7430826233871494, 0.5398559872322971], "prob": [[2.2847233917075393e-05, 10], [1.1198053347425085e-14, 25], [9.475947192697206e-15, 6]], "generated": ["Alexander Graham Bell was against genetic engineering.", "Thus, he would disagree with Nazi eugenics, which is genetic engineering for racial purification.", "So the answer is no."], "conf_comments": ["(A)\nThus, he", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6393692526783463, 0.4130622302725858, 0.22999603977130778, 0.17970384824304045], "conf": [0.9692667662252924, 0.8563885906782163, 0.6156039691229753, 0.401474485612928], "prob": [[5.6405803436736185e-09, 22], [3.8540927984430846e-16, 22], [2.3832449206755714e-22, 17], [1.60351738478026e-22, 6]], "generated": ["Alexander Graham Bell invented the telephone around 140 years ago (1876).", "Hitler adopted the policy of eugenics around 80 years ago (1933).", "Thus, Alexander Graham Bell and Hitler would not exist in the same time period.", "So the answer is no."], "conf_comments": ["(A)\nNazi eugen", "(A)\nThus, Alexander", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6905220195445602, 0.3202801011442167, 0.1518605848999593, 0.11434807179278164, 0.09810756564935644], "conf": [0.9546475719851297, 0.8680577404433714, 0.6507073106163258, 0.4901493007011105, 0.36755777665540207], "prob": [[8.719260828218882e-13, 40], [1.473887888024331e-26, 22], [2.7054858567636857e-43, 32], [2.8723868069588337e-45, 16], [2.570060817434951e-45, 6]], "generated": ["In 1883, Alexander Graham Bell co-founded the American Association to Promote the Teaching of Speech to the Deaf with his wife and Charles Lafayette.", "The association believed that there should not be any discrimination on the basis of deafness or speech.", "So, we can infer that Alexander Graham Bell probably did not support eugenics, since eugenics discriminated based on physical and mental defects.", "Thus, Alexander Graham Bell probably did not support Nazi eugenics.", "So the answer is no."], "conf_comments": ["(A)\nIn 19", "(A)\nThus, Alexander", "(A)\nThus, Alexander", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8115641634218453, 0.44472787201821545, 0.1788562331735023, 0.1163436657912681], "conf": [0.9351628421177577, 0.7718304846910063, 0.5349524391101383, 0.346711385007454], "prob": [[0.010492047537341334, 13], [1.4634707152504434e-14, 27], [6.058346839125087e-38, 37], [1.8366130346964931e-41, 19]], "generated": ["Alexander Graham Bell was a proponent of eugenics.", "Nazi eugenics was a German government-sponsored program of selective breeding to produce genetically superior individuals.", "Since eugenics is breeding for the purpose of improving genetic qualities, Alexander Graham Bell can support Nazi eugenics since his personal eugenics is consistent.", "Thus, Alexander Graham Bell can support Nazi eugenics and the answer is yes."], "conf_comments": ["(A)\nNazi eugen", "(A)\nThus, Alexander", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 54.56282019615173}
{"index": 110, "question": "Was Harry Truman's presidency unaffected by the twenty-third Amendment to the US Constitution?", "answer": "yes", "explanation": "Yes. The 23rd Amendment to the US Constitution was passed in 1961. Harry Truman was the President from 1945-1953.", "generated": [{"finished": true, "length": 4, "score": [0.8733623079791609, 0.7236732035872078, 0.5880498618222719, 0.45606276030604953], "conf": [0.9723736213850466, 0.7804138238547533, 0.6178867022679845, 0.3949009284774459], "prob": [[0.004788790814153985, 22], [0.00028833902188831655, 18], [2.1439167785935825e-06, 27], [1.4895586132257444e-06, 6]], "generated": ["The twenty-third Amendment was ratified on March 29, 1961.", "Harry Truman left office on January 20, 1953.", "Thus, Harry Truman’s presidency was unaffected by the twenty-third Amendment to the US Constitution.", "So the answer is no."], "conf_comments": ["(A)\nHarry Truman", "(A)\nThus, Harry", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.8128838065989242, 0.6074292936875049, 0.5256075425451212], "conf": [0.965333483898347, 0.8170277761721364, 0.6143206669299733], "prob": [[3.8001730332030714e-07, 39], [2.196242985612866e-12, 29], [2.1415136041074092e-12, 6]], "generated": ["The twenty-third Amendment to the US Constitution was ratified on March 29th, 1961, which is after the presidency of Harry Truman ended.", "Thus, it was impossible that Harry Truman's presidency was affected by the twenty-third Amendment to the US Constitution.", "So the answer is no."], "conf_comments": ["(A)\nThus, Harry", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8733623079791609, 0.7236732035872078, 0.4294735155886605, 0.3603746695180138, 0.30988187284799756], "conf": [0.9723736213850466, 0.7804138238547533, 0.6762799248274538, 0.5722037573438641, 0.43288433859038544], "prob": [[0.004788790814153985, 22], [0.00028833902188831655, 18], [3.932757401449615e-10, 15], [2.499597658712647e-11, 15], [2.1789440461641402e-11, 6]], "generated": ["The twenty-third Amendment was ratified on March 29, 1961.", "Harry Truman left office on January 20, 1953.", "His presidency preceded the ratification of the amendment.", "Thus, his presidency was not affected by the amendment.", "So the answer is no."], "conf_comments": ["(A)\nHarry Truman", "(A)\nThus, Harry", "(A)\nThus, Harry", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8110422238646013, 0.6009041332168013, 0.4595659228484764, 0.36989324578206334], "conf": [0.9719365595286688, 0.8178366542884313, 0.6093579251876609, 0.40096982505080425], "prob": [[5.770657162987527e-05, 25], [6.21072186002107e-08, 16], [3.023347139194997e-10, 22], [2.7529330604505743e-10, 6]], "generated": ["The twenty-third Amendment to the US Constitution was passed on June 16, 1960.", "Harry Truman's presidency ended in 1953.", "Thus, Harry Truman’s presidency was not affected by the twenty-third Amendment.", "So the answer is no."], "conf_comments": ["(A)\nHarry Truman", "(A)\nThus, Harry", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8110422238646013, 0.6009041332168013, 0.4524120306589909, 0.3636872311714171], "conf": [0.9719365595286688, 0.8178366542884313, 0.6326232851254348, 0.42482151807307306], "prob": [[5.770657162987527e-05, 25], [6.21072186002107e-08, 16], [9.072093322849704e-11, 21], [7.205537904808729e-11, 6]], "generated": ["The twenty-third Amendment to the US Constitution was passed on June 16, 1960.", "Harry Truman's presidency ended in 1953.", "Thus, the twenty-third Amendment did not affect Harry Truman’s presidency.", "So the answer is no."], "conf_comments": ["(A)\nHarry Truman", "(A)\nThus, Harry", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 50.38455009460449}
{"index": 111, "question": "Would the trees in Auburn, New York be changing colors in September?", "answer": "yes", "explanation": "Yes. In New York, fall begins during the month of September.  When the weather gets cooler in the fall, leaves begin to change colors. ", "generated": [{"finished": true, "length": 3, "score": [0.5848796796466872, 0.3915545757747134, 0.31752841731183545], "conf": [0.9553307076790152, 0.710579675659739, 0.48285088990887637], "prob": [[1.9112514803137593e-15, 33], [1.6639691673173827e-20, 23], [1.3671850092555678e-20, 6]], "generated": ["According to the NOAA website, the best time to see fall foliage in the State of New York is usually during the months of September and October.", "Thus, it is possible for the trees in Auburn, New York, to change colors in September.", "So the answer is yes."], "conf_comments": ["(A)\nAuburn,", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6527569740917203, 0.37090287372899766, 0.2220963356709908, 0.14515026166854728, 0.1252988961004633], "conf": [0.966919572080965, 0.8820627716679177, 0.7990475817389081, 0.6245133050180924, 0.4675813054271527], "prob": [[0.00012168571243342722, 11], [1.449506925753865e-14, 22], [2.1433405941644903e-19, 12], [1.2099496731224799e-24, 20], [1.1760551941213782e-24, 6]], "generated": ["Auburn, New York is in the USA.", "The USA is in the Northern Hemisphere, which means the seasons of fall and spring are in September.", "Fall is the season for leaves to change their colors.", "Thus, the trees in Auburn, New York would definitely be changing colors during September.", "So the answer is yes."], "conf_comments": ["(A)\nThe USA is in", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.5048959161428064, 0.3113939901886158, 0.21505959861737672, 0.10868466303266666, 0.07874045451247638, 0.06921627256437798], "conf": [0.9711569205365455, 0.9019988495464896, 0.7940715037474966, 0.6836433902730691, 0.5496664121390341, 0.43142974625715863], "prob": [[9.186959957717221e-12, 19], [1.2194765500007349e-15, 10], [9.216707853352447e-22, 23], [1.988604314481239e-34, 24], [9.220691136596821e-38, 18], [8.395018311026004e-38, 6]], "generated": ["September normally marks the beginning of autumn and the end of summer in the United States.", "In autumn, trees start to change colors.", "Auburn is in the state of New York and New York is in the northeastern United States.", "Northeastern United States sees the first signs of autumn and first signs of trees changing colors in late September.", "Thus, the trees in Auburn would most likely be changing colors in September.", "So the answer is yes."], "conf_comments": ["(A)\nAuburn,", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5848796796466872, 0.3915545757747134, 0.25258220342107085], "conf": [0.9553307076790152, 0.710579675659739, 0.43207302330485353], "prob": [[1.9112514803137593e-15, 33], [1.6639691673173827e-20, 23], [1.7092452443615734e-21, 6]], "generated": ["According to the NOAA website, the best time to see fall foliage in the State of New York is usually during the months of September and October.", "Thus, it is possible for the trees in Auburn, New York, to change colors in September.", "So the answer is maybe."], "conf_comments": ["(A)\nAuburn,", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5048959161428064, 0.3113939901886158, 0.18217556027947177, 0.15600269260335237], "conf": [0.9711569205365455, 0.9019988495464896, 0.6904766167547631, 0.5086590039859655], "prob": [[9.186959957717221e-12, 19], [1.2194765500007349e-15, 10], [6.218355411183245e-22, 18], [6.049406111177726e-22, 6]], "generated": ["September normally marks the beginning of autumn and the end of summer in the United States.", "In autumn, trees start to change colors.", "Thus, the trees in Auburn would be changing colors in September for sure.", "So the answer is yes."], "conf_comments": ["(A)\nAuburn,", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.5048959161428064, 0.3113939901886158, 0.21505959861737672, 0.10868466303266666, 0.07874045451247638, 0.04012059211241478], "conf": [0.9711569205365455, 0.9019988495464896, 0.7940715037474966, 0.6836433902730691, 0.5496664121390341, 0.3384197636395362], "prob": [[9.186959957717221e-12, 19], [1.2194765500007349e-15, 10], [9.216707853352447e-22, 23], [1.988604314481239e-34, 24], [9.220691136596821e-38, 18], [5.183908951041208e-40, 6]], "generated": ["September normally marks the beginning of autumn and the end of summer in the United States.", "In autumn, trees start to change colors.", "Auburn is in the state of New York and New York is in the northeastern United States.", "Northeastern United States sees the first signs of autumn and first signs of trees changing colors in late September.", "Thus, the trees in Auburn would most likely be changing colors in September.", "So the answer is no."], "conf_comments": ["(A)\nAuburn,", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 55.974040508270264}
{"index": 112, "question": "Is Black Lives Matter connected with capsaicin?", "answer": "yes", "explanation": "Yes. Black Lives Matter has held numerous protests. Protesters at Black Lives Matter events have had pepper spray used against them by police. Capsaicin is the main ingredient of pepper spray.", "generated": [{"finished": true, "length": 4, "score": [0.6708289225037247, 0.49381655366136584, 0.35874502809355163, 0.28474849833559446], "conf": [0.9694787097866645, 0.8061207748308273, 0.5152521986498381, 0.32590245854370914], "prob": [[2.1557278794950397e-05, 14], [8.244997972893305e-08, 13], [2.1666305613897077e-09, 19], [2.115849561492608e-09, 6]], "generated": ["Black Lives Matter is a human rights and social justice organization.", "Capsaicin is a chemical found in peppers.", "Thus, there is no connection between Black Lives Matter and capsaicin.", "So the answer is no."], "conf_comments": ["(A)\nCapsaic", "(A)\nThus, Black", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7980334096899129, 0.3868203179317612, 0.3056211036387305, 0.24492733141785392], "conf": [0.9642797812786741, 0.5532860808816485, 0.3783688469997506, 0.24352944219659187], "prob": [[0.0013104648608109138, 16], [3.85904026943437e-12, 22], [8.184304272246003e-13, 17], [8.080013466546889e-13, 6]], "generated": ["Capsaicin is a compound found in chili peppers.", "There is no evidence that demonstrates that capsaicin has any connection with Black Lives Matter.", "Thus, Black Lives Matter is not connected with capsaicin.", "So the answer is no."], "conf_comments": ["(A)\nBlack Lives M", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7096582962900083, 0.41236094785906746, 0.25215369425879147, 0.20479193631548165, 0.1682426917285382], "conf": [0.9701799618154128, 0.7572134867639563, 0.5983269074976408, 0.42474918410136064, 0.287198753895764], "prob": [[0.002736632076344105, 9], [7.706275098682644e-10, 18], [4.872268441034813e-15, 16], [1.3978110126126096e-15, 17], [1.3823841292504508e-15, 6]], "generated": ["Black Lives Matter is a movement.", "Capsaicin is a chemical that gives spiciness to hot pepper.", "Capsaicin is chemically different from Black Lives Matter.", "Thus, Black Lives Matter is not connected with capsaicin.", "So the answer is no."], "conf_comments": ["(A)\nCapsaic", "(A)\nThus, Black", "(A)\nThus, Black", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6708289225037247, 0.49381655366136584, 0.24083058060829735, 0.19413834566775207], "conf": [0.9694787097866645, 0.8061207748308273, 0.5208961512447495, 0.3402553823349716], "prob": [[2.1557278794950397e-05, 14], [8.244997972893305e-08, 13], [5.098962969390394e-13, 12], [4.942595029776923e-13, 6]], "generated": ["Black Lives Matter is a human rights and social justice organization.", "Capsaicin is a chemical found in peppers.", "Thus, there’s no connection between these two.", "So the answer is no."], "conf_comments": ["(A)\nCapsaic", "(A)\nThus, Black", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.7980334096899129, 0.3868203179317612, 0.23646498158338317], "conf": [0.9642797812786741, 0.5532860808816485, 0.3520081525191318], "prob": [[0.0013104648608109138, 16], [3.85904026943437e-12, 22], [1.5847185152577277e-13, 6]], "generated": ["Capsaicin is a compound found in chili peppers.", "There is no evidence that demonstrates that capsaicin has any connection with Black Lives Matter.", "So the answer is no."], "conf_comments": ["(A)\nBlack Lives M", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 53.017433166503906}
{"index": 113, "question": "Was Achilles a direct descendent of Gaia?", "answer": "yes", "explanation": "Yes. Achilles was the son of a Nereid.  The Nereids were the 50 daughters of Nereus. Nereus was the eldest son of the union between Gaia and Pontus.", "generated": [{"finished": true, "length": 4, "score": [0.7454510667301323, 0.4938017749072342, 0.4034007475588409, 0.3369015279531021], "conf": [0.9645254278812625, 0.8471279374126301, 0.6477360660957405, 0.45335730902593185], "prob": [[3.105029933439255e-06, 23], [7.2771772011382e-13, 22], [9.456891158923054e-14, 15], [9.261653294731692e-14, 6]], "generated": ["In Greek mythology, Achilles was the son of Peleus and the sea goddess Thetis.", "In Greek mythology, Gaia, also called Gaea, was the goddess of the Earth.", "Thus, Achilles was not a direct descendent of Gaia.", "So the answer is no."], "conf_comments": ["(A)\nIn Greek mythology", "(A)\nThus, Ach", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8031184501292858, 0.5623534661800926, 0.3128042129772028, 0.26014447255231177], "conf": [0.9722211506037808, 0.8220280705382911, 0.5533950331057249, 0.3855113162724824], "prob": [[0.007269921077084532, 12], [3.5339563622429107e-06, 14], [6.066681238595672e-14, 23], [5.810908539587441e-14, 6]], "generated": ["Gaia is the Greek goddess of the Earth.", "Achilles was a Greek warrior in the Trojan War.", "Thus, it is impossible that Achilles was a direct descendent (son or daughter) of Gaia.", "So the answer is no."], "conf_comments": ["(A)\nAchilles was", "(A)\nThus, Ach", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8031184501292858, 0.5623534661800926, 0.3235399163150711, 0.25363680065125926], "conf": [0.9722211506037808, 0.8220280705382911, 0.4711697244835041, 0.2933918428768109], "prob": [[0.007269921077084532, 12], [3.5339563622429107e-06, 14], [1.6214694544894517e-09, 14], [1.4986409383978717e-09, 6]], "generated": ["Gaia is the Greek goddess of the Earth.", "Achilles was a Greek warrior in the Trojan War.", "Thus, there is no relationship between Achilles and Gaia.", "So the answer is no."], "conf_comments": ["(A)\nAchilles was", "(A)\nThus, Ach", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7705039008069653, 0.4705543366436805, 0.2374646742989458, 0.17712935513886732], "conf": [0.971376262469688, 0.8039773641059607, 0.4957926110577411, 0.29008200635286613], "prob": [[0.0027161038121605763, 12], [1.334969590139142e-11, 24], [1.1477262557027073e-19, 21], [8.488133955394525e-20, 6]], "generated": ["Gaia is the Greek goddess of the earth.", "Achilles is the Greek hero and son of Peleus and the sea-nymph Thetis.", "Achilles was not a direct descendent of Gaia because he’s not related to her.", "So the answer is no."], "conf_comments": ["(A)\nAchilles was", "(A)\nThus, Ach", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.7454510667301323, 0.4938017749072342, 0.2672925305662281, 0.1453450024317066, 0.07395890712571952, 0.05883792635357929], "conf": [0.9645254278812625, 0.8471279374126301, 0.6997175349304339, 0.5417509638878469, 0.3451294945910038, 0.21959769507610072], "prob": [[3.105029933439255e-06, 23], [7.2771772011382e-13, 22], [2.2956672679062643e-17, 10], [2.209553185041428e-22, 12], [8.227609789472485e-30, 19], [7.968965632005484e-30, 6]], "generated": ["In Greek mythology, Achilles was the son of Peleus and the sea goddess Thetis.", "In Greek mythology, Gaia, also called Gaea, was the goddess of the Earth.", "She was also the mother of Thetis.", "Thetis was the great aunt of Achilles.", "Thus, Achilles and Gaia were not direct descendents and they are not related.", "So the answer is no."], "conf_comments": ["(A)\nIn Greek mythology", "(A)\nThus, Ach", "(A)\nThus, Ach", "(A)\nThus, Ach", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 58.24707889556885}
{"index": 114, "question": "Does Jerry Seinfeld hang out at the Budweiser Party Deck?", "answer": "no", "explanation": "No. The Budweiser Party Deck is a social gathering spot in Yankee Stadium. Yankee Stadium is home to the New York Yankees baseball team. Jerry Seinfeld is a fan of the New York Mets.", "generated": [{"finished": true, "length": 4, "score": [0.7399040793468177, 0.44637923447809613, 0.3258253397376197, 0.27807496008290467], "conf": [0.9781555011014644, 0.8777888231083246, 0.6939238343491344, 0.5083661719848289], "prob": [[8.925459884050684e-07, 24], [1.2945837640678677e-14, 20], [1.4820058412933876e-18, 23], [1.4314797262997292e-18, 6]], "generated": ["The last episode of “Seinfeld” aired on May 14th, 1998.", "Budweiser began sponsoring the Party Deck in May of 1998.", "Thus, Jerry Seinfeld never had a chance to hang out at the Budweiser Party Deck.", "So the answer is no."], "conf_comments": ["(A)\nThe Budweiser", "(A)\nThus, Jerry", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.7488918125061783, 0.5616791609550207, 0.4010800506406391, 0.23763685509762444, 0.16269243160136607, 0.12922454541476686], "conf": [0.9733235381403229, 0.8648097144817634, 0.727202184677575, 0.42539241415416484, 0.2559508441114781, 0.16206082767338598], "prob": [[8.508949811317206e-05, 17], [2.233842637622935e-07, 13], [5.521577677129402e-10, 12], [1.2159400812437569e-14, 21], [8.236028263826137e-17, 20], [8.059736894364623e-17, 6]], "generated": ["The Budweiser Party Deck is a seating area at Busch Stadium.", "Jerry Seinfeld is a famous comedian.", "Busch Stadium is located in St. Louis, Missouri.", "Jerry Seinfeld was born in Brooklyn, New York and currently lives in New York.", "Thus, Jerry Seinfeld would not hang out at the Budweiser Party Deck.", "So the answer is no."], "conf_comments": ["(A)\nJerry Se", "(A)\nThus, Jerry", "(A)\nSt. Louis is", "(A)\nThus, Jerry", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.6413471479718486, 0.5013998479328552], "conf": [0.8812480419531882, 0.7005650492004722], "prob": [[1.162466925277452e-09, 27], [1.419170819108947e-10, 8]], "generated": ["Jerry Seinfeld hangs out at the Budweiser Party Deck, also called the Bud Light Party Deck.", "Thus, the answer is yes."], "conf_comments": ["(A)\nThe Budweiser", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.7488918125061783, 0.5616791609550207, 0.4010800506406391, 0.23763685509762444, 0.14225881344582442, 0.10865068494759682], "conf": [0.9733235381403229, 0.8648097144817634, 0.727202184677575, 0.42539241415416484, 0.24902224761012667, 0.1557279170725096], "prob": [[8.508949811317206e-05, 17], [2.233842637622935e-07, 13], [5.521577677129402e-10, 12], [1.2159400812437569e-14, 21], [5.714408354799035e-20, 25], [3.763923199624336e-20, 6]], "generated": ["The Budweiser Party Deck is a seating area at Busch Stadium.", "Jerry Seinfeld is a famous comedian.", "Busch Stadium is located in St. Louis, Missouri.", "Jerry Seinfeld was born in Brooklyn, New York and currently lives in New York.", "Therefore, the likelihood of Jerry Seinfeld hanging out at the Budweiser Party Deck is low.", "So the answer is no."], "conf_comments": ["(A)\nJerry Se", "(A)\nThus, Jerry", "(A)\nSt. Louis is", "(A)\nThus, Jerry", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.7488918125061783, 0.5616791609550207, 0.4010800506406391, 0.20583688009038198, 0.1251025927023457, 0.10061078229328271], "conf": [0.9733235381403229, 0.8648097144817634, 0.727202184677575, 0.533163680130744, 0.3530091044514422, 0.22932591086364595], "prob": [[8.508949811317206e-05, 17], [2.233842637622935e-07, 13], [5.521577677129402e-10, 12], [4.231897184977661e-21, 25], [3.7282871225454237e-25, 16], [3.631153531310563e-25, 6]], "generated": ["The Budweiser Party Deck is a seating area at Busch Stadium.", "Jerry Seinfeld is a famous comedian.", "Busch Stadium is located in St. Louis, Missouri.", "St. Louis is about 2165 miles from New York, Jerry Seinfeld’s hometown.", "Thus, Jerry Seinfeld does not hang out at the party deck.", "So the answer is no."], "conf_comments": ["(A)\nJerry Se", "(A)\nThus, Jerry", "(A)\nSt. Louis is", "(A)\nThus, Jerry", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.7488918125061783, 0.5616791609550207, 0.4010800506406391, 0.23763685509762444, 0.14225881344582442, 0.10035961254667446], "conf": [0.9733235381403229, 0.8648097144817634, 0.727202184677575, 0.42539241415416484, 0.24902224761012667, 0.16325694645225777], "prob": [[8.508949811317206e-05, 17], [2.233842637622935e-07, 13], [5.521577677129402e-10, 12], [1.2159400812437569e-14, 21], [5.714408354799035e-20, 25], [6.303471753077963e-21, 8]], "generated": ["The Budweiser Party Deck is a seating area at Busch Stadium.", "Jerry Seinfeld is a famous comedian.", "Busch Stadium is located in St. Louis, Missouri.", "Jerry Seinfeld was born in Brooklyn, New York and currently lives in New York.", "Therefore, the likelihood of Jerry Seinfeld hanging out at the Budweiser Party Deck is low.", "Thus, the answer is no."], "conf_comments": ["(A)\nJerry Se", "(A)\nThus, Jerry", "(A)\nSt. Louis is", "(A)\nThus, Jerry", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 56.284029483795166}
{"index": 115, "question": "Can you hide a basketball in a sand cat's ear?", "answer": "no", "explanation": "No. The diameter of a standard NBA basketball is around 9.5 inches. A sand cat's ear grows to 2.8 inches tall.", "generated": [{"finished": true, "length": 5, "score": [0.5842555819630432, 0.41605672850746583, 0.2680302152254186, 0.19203677006954942, 0.1585180865903933], "conf": [0.94614372685168, 0.7995166390204438, 0.5790261290512498, 0.3935209509186012, 0.26956884715242024], "prob": [[5.037570701615155e-10, 21], [1.0989267799749584e-12, 12], [9.18323310480721e-18, 21], [5.879819874251189e-20, 18], [5.694900126163753e-20, 6]], "generated": ["According to Wikipedia, the Sand Cat has a body length of 18-25 inches.", "A basketball has a diameter of 9.4 inches.", "Thus, the diameter of a basketball is greater than the length of the Sand Cat’s body.", "Thus, it is impossible to hide a basketball in a Sand Cat’s ear.", "So the answer is no."], "conf_comments": ["(A)\nAccording to Wikipedia", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5842555819630432, 0.41605672850746583, 0.2422684727467164, 0.2018634068681512], "conf": [0.94614372685168, 0.7995166390204438, 0.600489054714661, 0.41895694268215206], "prob": [[5.037570701615155e-10, 21], [1.0989267799749584e-12, 12], [1.3589110711591007e-19, 20], [1.319269817195201e-19, 6]], "generated": ["According to Wikipedia, the Sand Cat has a body length of 18-25 inches.", "A basketball has a diameter of 9.4 inches.", "Thus, the basketball can fit in a sand cat's ear and you can hide it.", "So the answer is yes."], "conf_comments": ["(A)\nAccording to Wikipedia", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6445399631419911, 0.356850369336774, 0.22246013929249742, 0.1624172904290823, 0.13260073623359228], "conf": [0.9705754734893055, 0.8179914101370305, 0.6667341680200626, 0.4707040588504304, 0.3150890104344079], "prob": [[6.9276417805127996e-06, 14], [1.1367077928411306e-14, 20], [1.8431273814176853e-20, 18], [1.171988119976554e-22, 18], [1.142287499256009e-22, 6]], "generated": ["A basketball’s circumference is about 9.4 inches.", "According to Wikipedia, a sand cat’s ear is about 4 inches, on average.", "This shows that a basketball is about twice as large as a sand cat’s ear.", "Thus, it is possible to hide a basketball in a sand cat’s ear.", "So the answer is yes."], "conf_comments": ["(A)\nA sand cat’", "(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6445399631419911, 0.356850369336774, 0.21355154768728032, 0.17279059059497123], "conf": [0.9705754734893055, 0.8179914101370305, 0.5859265565328854, 0.39075760443649504], "prob": [[6.9276417805127996e-06, 14], [1.1367077928411306e-14, 20], [5.28139591132414e-24, 31], [4.726847356424442e-24, 6]], "generated": ["A basketball’s circumference is about 9.4 inches.", "According to Wikipedia, a sand cat’s ear is about 4 inches, on average.", "Since a basketball is more than twice the length of an average sand cat’s ear, you can hide a basketball in a sand cat’s ear.", "So the answer is yes."], "conf_comments": ["(A)\nA sand cat’", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.4864848209317538, 0.2990048593190851, 0.1841546270820191, 0.1422177199598423], "conf": [0.9445759628323106, 0.836576463151102, 0.5395250209825121, 0.3269070704761119], "prob": [[9.750277740293102e-07, 10], [8.288151163954063e-11, 11], [3.4594847743212687e-15, 19], [3.146212109017457e-15, 6]], "generated": ["Sand cats are from Asia and Africa.", "In comparison, a basketball is an American invention.", "Thus, it would be hard to find a basketball in a sand cat's ear.", "So the answer is no."], "conf_comments": ["(A)\nBasketballs are", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 58.25868034362793}
{"index": 116, "question": "Could the Spice Girls compete against ŽRK Kumanovo?", "answer": "no", "explanation": "No. The Spice Girls had 5 members. ŽRK Kumanovo is a women's handball club from Kumanovo in the Republic of Macedonia. Handball is a sport played by two teams of seven players each.", "generated": [{"finished": true, "length": 4, "score": [0.8444853215960421, 0.39414839090324927, 0.2918203003754832, 0.23310814139733319], "conf": [0.9600688258431458, 0.704355442155219, 0.4680960543962135, 0.3001021944086585], "prob": [[0.002615994644961256, 20], [1.5805318101245185e-07, 8], [4.071949965102079e-09, 19], [3.958226004460979e-09, 6]], "generated": ["ŽRK Kumanovo is a women’s handball team in North Macedonia.", "The Spice Girls were singers.", "Thus, the Spice Girls could not compete against ŽRK Kumanovo.", "So the answer is no."], "conf_comments": ["(A)\nThe Spice Girls", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7754973087107758, 0.3454014279746468, 0.27405573753514395, 0.22483561473591873], "conf": [0.9605542391724742, 0.7262760344149445, 0.5241471190998688, 0.3536640806747511], "prob": [[8.56591852099568e-05, 20], [1.6458552698083415e-13, 15], [1.4079802149161107e-14, 18], [1.387025792473261e-14, 6]], "generated": ["ŽRK Kumanovo is a women’s handball club located in Macedonia.", "The Spice Girls are a musical group with no known handball members.", "Thus, the Spice Girls cannot compete against ŽRK Kumanovo.", "So the answer is no."], "conf_comments": ["(A)\nThe Spice Girls", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8444853215960421, 0.4522914443841648, 0.20444449096687017, 0.1640378452632789, 0.13645690747411454], "conf": [0.9600688258431458, 0.828790871066988, 0.6189349059938974, 0.4522833700941102, 0.31393445928588054], "prob": [[0.002615994644961256, 20], [1.9200206436614308e-11, 17], [7.827809647928062e-33, 38], [7.048577630770089e-34, 19], [6.920682011222465e-34, 6]], "generated": ["ŽRK Kumanovo is a women’s handball team in North Macedonia.", "Handball is a sport that involves throwing and catching a ball in the hands.", "At least three of the Spice Girls (Mel B, Victoria Beckham, and Mel Chisholm) have been shown to not have the capability of playing handball.", "Thus, the Spice Girls could not compete against ŽRK Kumanovo.", "So the answer is no."], "conf_comments": ["(A)\nThe Spice Girls", "(A)\nThe Spice Girls", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.7754973087107758, 0.4638192965370283, 0.2526806356343116, 0.16009764909887714, 0.07555349493451317, 0.06112721248787639], "conf": [0.9605542391724742, 0.8758617174174014, 0.7238241816155022, 0.5967673960312719, 0.4149491135861445, 0.2767239179020661], "prob": [[8.56591852099568e-05, 20], [4.149491298160883e-12, 18], [3.136551551155759e-27, 34], [2.031861770430345e-34, 23], [8.190940898508267e-44, 19], [7.324585421245074e-44, 6]], "generated": ["ŽRK Kumanovo is a women’s handball club located in Macedonia.", "In 2021, the team won the Macedonian handball championship.", "In 2018, they won the EHF Cup bronze medal and in 2020, they won the EHF Cup silver medal.", "In addition, ŽRK Kumanovo is the top women’s handball club in Macedonia.", "Thus, they are strong and can be expected to compete against the Spice Girls.", "So the answer is yes."], "conf_comments": ["(A)\nThe Spice Girls", "(A)\nThe Spice Girls", "(A)\nThe Spice Girls", "(A)\nThe Spice Girls", "(A)\nThe Spice Girls", "(A)\nThe Spice Girls"], "info": null}, {"finished": true, "length": 6, "score": [0.7754973087107758, 0.4638192965370283, 0.326723071611656, 0.1535118603162084, 0.07144719571338778, 0.0515179001933164], "conf": [0.9605542391724742, 0.8758617174174014, 0.7247939150571827, 0.4392816117200812, 0.2773926380118568, 0.1546614682344236], "prob": [[8.56591852099568e-05, 20], [4.149491298160883e-12, 18], [8.985403403181731e-17, 21], [1.9345373127439867e-33, 38], [8.96641589567763e-49, 33], [5.896187451157475e-49, 6]], "generated": ["ŽRK Kumanovo is a women’s handball club located in Macedonia.", "In 2021, the team won the Macedonian handball championship.", "In comparison, the Spice Girls is a music group formed in the 1990s.", "ŽRK Kumanovo is a sports team at the top of the league in Macedonia and the Spice Girls was a music group from the 1990s.", "Thus, it is easier for a sports team at the top of the league in a country to compete than against a music group that performed decades ago.", "So the answer is no."], "conf_comments": ["(A)\nThe Spice Girls", "(A)\nThe Spice Girls", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 6, "score": [0.7754973087107758, 0.4638192965370283, 0.326723071611656, 0.1535118603162084, 0.07144719571338778, 0.03742640777803897], "conf": [0.9605542391724742, 0.8758617174174014, 0.7247939150571827, 0.4392816117200812, 0.2773926380118568, 0.15245415371455262], "prob": [[8.56591852099568e-05, 20], [4.149491298160883e-12, 18], [8.985403403181731e-17, 21], [1.9345373127439867e-33, 38], [8.96641589567763e-49, 33], [2.0707789307463115e-55, 22]], "generated": ["ŽRK Kumanovo is a women’s handball club located in Macedonia.", "In 2021, the team won the Macedonian handball championship.", "In comparison, the Spice Girls is a music group formed in the 1990s.", "ŽRK Kumanovo is a sports team at the top of the league in Macedonia and the Spice Girls was a music group from the 1990s.", "Thus, it is easier for a sports team at the top of the league in a country to compete than against a music group that performed decades ago.", "Thus, we can say that the Spice Girls could compete against ŽRK Kumanovo.", "So the answer is yes."], "conf_comments": ["(A)\nThe Spice Girls", "(A)\nThe Spice Girls", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[2.309483471254725e-07, 22], [0.922868728449732, 6]], 0.5495969713083595, "(A)\nSo the answer is", 6]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 70.72809672355652}
{"index": 117, "question": "Does the swastika have positive uses?", "answer": "yes", "explanation": "Yes. The swastika is used in the Hindu religion to represent the sun. People practicing Hindu believe the swastika represents prosperity and good luck.", "generated": [{"finished": true, "length": 4, "score": [0.5835754965316942, 0.37202553368728547, 0.2736813444531645, 0.2357098028880098], "conf": [0.9559755753956324, 0.8658366255336445, 0.7059395325469283, 0.525085018271251], "prob": [[1.0015478427149691e-13, 29], [2.960141560689354e-26, 36], [2.165073222948909e-28, 12], [2.1295568799157363e-28, 6]], "generated": ["The swastika is an ancient symbol used by Hindus, Buddhists, and Jains across southeast Asia for hundreds of years.", "In Buddhism, the swastika embodies the four immeasurables: loving-kindness, compassion, joy, and impartiality.", "Thus, the swastika can have positive uses.", "So the answer is yes."], "conf_comments": ["(A)\nIn the 1", "(A)\nIn Hinduism", "(A)\nIn 19", "(A)\nIn 19"], "info": null}, {"finished": true, "length": 4, "score": [0.69454493528461, 0.3683429087898906, 0.2593053509136925, 0.22078890933079565], "conf": [0.9510326823903528, 0.8123037198247415, 0.6446692767402726, 0.46920477103872665], "prob": [[0.0001471188507272239, 13], [7.262038441097778e-10, 11], [1.0484717625519536e-11, 9], [1.0242202680078782e-11, 6]], "generated": ["The swastika is an important symbol in Hinduism.", "It is also used by some Native American cultures.", "Thus, it does have positive uses.", "So the answer is yes."], "conf_comments": ["(A)\nThe swastika", "(A)\nThe swastika", "(A)\nIn addition, it", "(A)\nIn addition, the"], "info": null}, {"finished": true, "length": 3, "score": [0.6191311219771588, 0.37280222232787996, 0.286251453104729], "conf": [0.9564779367812636, 0.6133171874728576, 0.3781003398198278], "prob": [[6.879772595903047e-06, 13], [4.246784161455858e-10, 17], [3.2490701196378663e-10, 6]], "generated": ["The Nazi Party uses the swastika as their symbol.", "Thus, the swastika is used positively by the Nazi Party.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nThe Nazi Party", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.69454493528461, 0.3683429087898906, 0.18307659419163988, 0.1540687123414698], "conf": [0.9510326823903528, 0.8123037198247415, 0.6155507446308972, 0.43802306712631406], "prob": [[0.0001471188507272239, 13], [7.262038441097778e-10, 11], [9.844974085775492e-15, 10], [9.567432170090533e-15, 6]], "generated": ["The swastika is an important symbol in Hinduism.", "It is also used by some Native American cultures.", "Thus, it has positive uses for some.", "So the answer is yes."], "conf_comments": ["(A)\nThe swastika", "(A)\nThe swastika", "(A)\nIn addition, it", "(A)\nIn addition, it"], "info": null}, {"finished": true, "length": 5, "score": [0.5835754965316942, 0.37202553368728547, 0.24539427088931237, 0.1116610113428717, 0.09385418260252436], "conf": [0.9559755753956324, 0.8658366255336445, 0.780696190034149, 0.6400905224457564, 0.47615229657452585], "prob": [[1.0015478427149691e-13, 29], [2.960141560689354e-26, 36], [3.6286355115078667e-34, 25], [2.7886632900641765e-56, 37], [2.0464166282624207e-56, 6]], "generated": ["The swastika is an ancient symbol used by Hindus, Buddhists, and Jains across southeast Asia for hundreds of years.", "In Buddhism, the swastika embodies the four immeasurables: loving-kindness, compassion, joy, and impartiality.", "In Hinduism, the swastika represents the four faces of god Brahma and the four stages of life.", "Though the swastika has negative uses to European cultures due to its use by the Nazi party during World War II, the symbol has positive uses for most cultures.", "So the answer is yes."], "conf_comments": ["(A)\nIn the 1", "(A)\nIn Hinduism", "(A)\nIn Jainism", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 53.382413148880005}
{"index": 118, "question": "Was Black fly upstaged by another insect in Jeff Goldblum's 1986 film?", "answer": "yes", "explanation": "Yes. Jeff Goldnlum starred in the 1986 movie The Fly. The fly used in the movie The Fly was a common Housefly. The Black fly is most closely related to Chironomidae since they both feed on mammals.", "generated": [{"finished": true, "length": 3, "score": [0.668405879525498, 0.512067223305268, 0.4333319334443807], "conf": [0.9002174597590292, 0.7052102073391494, 0.507277742384222], "prob": [[1.6554982639402555e-06, 19], [2.885219053523213e-09, 22], [2.808924608669213e-09, 6]], "generated": ["In the 1986 film, Black fly was upstaged by a fly.", "Thus, in the 1986 film, Black fly was upstaged by another insect.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Black", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6454991899100743, 0.47169211484931806, 0.3776207725621638], "conf": [0.9138335279825442, 0.6610011748143267, 0.42610366025622426], "prob": [[3.135703083018753e-08, 22], [3.3051808291205656e-10, 15], [3.1921562815424643e-10, 6]], "generated": ["In Jeff Goldblum’s 1986 film, Black fly has the most screen time.", "Thus, Black fly was not upstaged by any other insect.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, another", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6454991899100743, 0.4653271087093429, 0.37619682881844324], "conf": [0.9138335279825442, 0.6530409128802239, 0.430056208126612], "prob": [[3.135703083018753e-08, 22], [9.429897242722992e-10, 11], [9.013205855107403e-10, 6]], "generated": ["In Jeff Goldblum’s 1986 film, Black fly has the most screen time.", "Thus, Black fly was not upstaged.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, another", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.625389661718809, 0.3464838004902579, 0.21690056035350055, 0.1681692091254325], "conf": [0.964745113561933, 0.7889812415228068, 0.5207049267684638, 0.3144236583648218], "prob": [[3.239241360251823e-06, 14], [3.5660602396504927e-12, 14], [3.733008819760888e-17, 22], [3.633687137752602e-17, 6]], "generated": ["Black fly is in the 1986 movie The Fly.", "Jeff Goldblum is in the sequel The Fly II.", "Thus, Black fly was not upstaged by a different insect in a Jeff Goldblum film.", "So the answer is no."], "conf_comments": ["(A)\nIn the movie,", "(A)\nThe Fly II was", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6454991899100743, 0.47169211484931806, 0.18109014093631257], "conf": [0.9138335279825442, 0.6610011748143267, 0.44024368257962426], "prob": [[3.135703083018753e-08, 22], [3.3051808291205656e-10, 15], [8.591432491362893e-15, 7]], "generated": ["In Jeff Goldblum’s 1986 film, Black fly has the most screen time.", "Thus, Black fly was not upstaged by any other insect.", "So the answer is \"no\"."], "conf_comments": ["(A)\nIn comparison, another", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 42.99764394760132}
{"index": 119, "question": "Did Linnaeus edit Darwin's draft of Origin of Species?", "answer": "no", "explanation": "No. Linnaeus died in 1778. Origin of Species was published in 1859.", "generated": [{"finished": true, "length": 4, "score": [0.8554465005780013, 0.7268429239113849, 0.4815270158507041, 0.39621318906931285], "conf": [0.977884692679567, 0.8012950871909392, 0.5622150527839859, 0.383970243635523], "prob": [[0.009673173165908254, 16], [0.0018640032958740207, 13], [1.502109209571084e-08, 25], [1.4257065658953405e-08, 6]], "generated": ["Charles Darwin published the Origin of Species in 1859.", "Carl Linnaeus died in 1778.", "Thus, it would be impossible for Linnaeus to edit Darwin's draft in 1859.", "So the answer is no."], "conf_comments": ["(A)\nCarl Linna", "(A)\nThus, Lin", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8554465005780013, 0.7268429239113849, 0.46760608564794337, 0.38027785497116484], "conf": [0.977884692679567, 0.8012950871909392, 0.6224056251147602, 0.416361218820543], "prob": [[0.009673173165908254, 16], [0.0018640032958740207, 13], [7.870766421185662e-08, 16], [7.349994629859617e-08, 6]], "generated": ["Charles Darwin published the Origin of Species in 1859.", "Carl Linnaeus died in 1778.", "Thus, Linnaeus was not able to edit the Origin of Species.", "So the answer is no."], "conf_comments": ["(A)\nCarl Linna", "(A)\nThus, Lin", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.766023648957654, 0.5702566021331296, 0.44157428585060615, 0.26810251569155297, 0.2272102295209214], "conf": [0.9499770332752749, 0.8943193084547191, 0.732398353593776, 0.5573921054595872, 0.4036486727324236], "prob": [[7.695245375970964e-08, 34], [3.2649868413108174e-12, 19], [1.346434699541892e-15, 25], [5.987678174085363e-21, 17], [5.6980473333728584e-21, 6]], "generated": ["Charles Darwin published The Origin of Species by Means of Natural Selection, or The Preservation of Favoured Races in the Struggle for Life.", "He published the book in 1859 and died in 1882.", "In contrast, Carolus Linnaeus was born in 1707 and died in 1778.", "Thus, Linnaeus could not have edited Darwin's draft book.", "So the answer is no."], "conf_comments": ["(A)\nCarl Linna", "(A)\nCarl Linna", "(A)\nThus, Lin", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7441654299021037, 0.4941861516114344, 0.4020245200406941], "conf": [0.9505780019094531, 0.7359480665504117, 0.49062070367112676], "prob": [[4.009388271052014e-06, 23], [9.102284156188025e-11, 19], [8.711678238991871e-11, 6]], "generated": ["Linnaeus was born in 1707 whereas Darwin was born in 1809.", "Thus, it was impossible for Linnaeus to edit Darwin’s drafts.", "So the answer is no."], "conf_comments": ["(A)\nThus, Lin", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7441654299021037, 0.4990806428593628, 0.35591026573223583, 0.28688699444244803], "conf": [0.9505780019094531, 0.7387723805023816, 0.5485364302402442, 0.3584074016707504], "prob": [[4.009388271052014e-06, 23], [4.624529766468481e-12, 25], [2.3876058482749897e-15, 20], [2.308754845940236e-15, 6]], "generated": ["Linnaeus was born in 1707 whereas Darwin was born in 1809.", "Linnaeus died in 1778, which is almost 40 years before Darwin was even born.", "Thus, Linnaeus did not edit Darwin’s draft of the Origin of Species.", "So the answer is no."], "conf_comments": ["(A)\nThus, Lin", "(A)\nThus, Lin", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 53.203776597976685}
{"index": 120, "question": "Do all cancer patients get disability?", "answer": "no", "explanation": "No. All forms of cancer qualify as diagnoses that can result in disability. Disability is not determined by diagnosis, but by degree of impairment. Some cancer patients do not experience major impairment.", "generated": [{"finished": true, "length": 4, "score": [0.6058157653835394, 0.336787441528037, 0.19770898309230228, 0.16425905676551036], "conf": [0.9608512600561764, 0.819480778646666, 0.6547939914688707, 0.4644650049865259], "prob": [[2.0530505686706595e-07, 16], [1.8145464756819196e-14, 16], [7.517431046054062e-19, 12], [6.382850135880308e-19, 6]], "generated": ["Cancer patients can apply for disability benefits from the US Social Security Administration.", "Cancer is listed as a qualifying condition by the US Social Security Administration.", "Thus, cancer patients can get social security disability.", "So the answer is yes."], "conf_comments": ["(A)\nThe US Social Security", "(A)\nThus, all", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5660868962373371, 0.3228268765536772, 0.2549615099997398], "conf": [0.947392845555917, 0.7519245518537243, 0.5354940305057382], "prob": [[8.620953493568488e-16, 32], [1.3288938630372648e-21, 15], [5.998840943694742e-22, 6]], "generated": ["Cancer patients that are not able to work due to severe side effects of cancer can get a Social Security Disability Insurance (SSDI) benefit.", "Hence, cancer patients that are unable to work can get disability.", "So the answer is yes."], "conf_comments": ["(A)\nCancer patients that", "(A)\nThus, all", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6058157653835394, 0.336787441528037, 0.19770898309230228, 0.10357573962788418, 0.0790110506510419], "conf": [0.9608512600561764, 0.819480778646666, 0.6547939914688707, 0.425785896453711, 0.2691439278156937], "prob": [[2.0530505686706595e-07, 16], [1.8145464756819196e-14, 16], [7.517431046054062e-19, 12], [1.0138331650854536e-23, 13], [6.171122005200492e-24, 6]], "generated": ["Cancer patients can apply for disability benefits from the US Social Security Administration.", "Cancer is listed as a qualifying condition by the US Social Security Administration.", "Thus, cancer patients can get social security disability.", "Thus, some cancer patients can get social security disability.", "So the answer is yes."], "conf_comments": ["(A)\nThe US Social Security", "(A)\nThus, all", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6074567087903359, 0.2550899692363129, 0.12964813434321484, 0.1064553298567215], "conf": [0.8989261484395035, 0.6528788571894977, 0.45110458551529764, 0.30828068757396004], "prob": [[2.2891793731486077e-05, 12], [5.772480720635488e-22, 27], [1.1748517526380391e-29, 18], [1.0833836237688973e-29, 6]], "generated": ["Cancer is a disease, not a physical disability.", "Some disabilities are also diseases like blindness, but some disabilities are not diseases like autism and deafness.", "Thus, it is not possible to claim that all cancer patients have a disability.", "So the answer is no."], "conf_comments": ["(A)\nThus, cancer", "(A)\nThus, cancer", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.5660868962373371, 0.2678783966440887], "conf": [0.947392845555917, 0.6442106522056449], "prob": [[8.620953493568488e-16, 32], [1.0995717232968945e-18, 6]], "generated": ["Cancer patients that are not able to work due to severe side effects of cancer can get a Social Security Disability Insurance (SSDI) benefit.", "So the answer is no."], "conf_comments": ["(A)\nCancer patients that", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 5, "score": [0.6058157653835394, 0.336787441528037, 0.19770898309230228, 0.10357573962788418, 0.02670514348592707], "conf": [0.9608512600561764, 0.819480778646666, 0.6547939914688707, 0.425785896453711, 0.27502730191902786], "prob": [[2.0530505686706595e-07, 16], [1.8145464756819196e-14, 16], [7.517431046054062e-19, 12], [1.0138331650854536e-23, 13], [1.1706108289750601e-28, 5]], "generated": ["Cancer patients can apply for disability benefits from the US Social Security Administration.", "Cancer is listed as a qualifying condition by the US Social Security Administration.", "Thus, cancer patients can get social security disability.", "Thus, some cancer patients can get social security disability.", "But not all can.", "So the answer is not necessarily yes."], "conf_comments": ["(A)\nThe US Social Security", "(A)\nThus, all", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[1.1546385236632028e-05, 5], [0.0003034078087833771, 8]], 0.6459286326993859, "(A)\nSo the answer is", 5]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 48.37852334976196}
{"index": 121, "question": "Did the death of Helen Palmer have a significant effect on Dr. Seuss?", "answer": "yes", "explanation": "Yes. Dr. Seuss's real name was Theodor Geisel. Theodor Geisel was married to Helen Palmer at the time of her suicide. Theodor Geisel is quoted having said he considered suicide after the death of his wife.", "generated": [{"finished": true, "length": 5, "score": [0.8526114319800413, 0.6398803458556817, 0.41338931053091454, 0.326438479771772, 0.2685521414841582], "conf": [0.9713257300732512, 0.9029473192840444, 0.7975885790555701, 0.5747341082795594, 0.39207317716524637], "prob": [[0.017295126226821316, 14], [2.094511160431556e-06, 18], [1.2255760411737846e-10, 13], [5.8811153162664906e-12, 21], [5.607716003333242e-12, 6]], "generated": ["Helen Palmer was the first wife of Dr. Seuss.", "They married in 1929 and divorced in 1936.", "Dr. Seuss then married Audrey Stone Geisel.", "Thus, the death of Helen Palmer did not have a significant effect on Dr. Seuss.", "So the answer is no."], "conf_comments": ["(A)\nShe died in ", "(A)\nDr. Seuss", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 7, "score": [0.8358778705093272, 0.5155054839568485, 0.3646461260519625, 0.22576745872779613, 0.12314833932262097, 0.09822553796363076, 0.08103092884948022], "conf": [0.9716571797673174, 0.8763471139957552, 0.7196949758327849, 0.5691459758938744, 0.38127473867617645, 0.28560182472200457, 0.1955261590111516], "prob": [[0.01374180194863491, 13], [4.3476804273336114e-07, 12], [8.015906140213326e-12, 22], [4.68329070811034e-19, 23], [7.220259569952951e-28, 25], [2.3388206379410963e-29, 21], [2.256578182941929e-29, 6]], "generated": ["Helen Palmer was the wife of Dr. Seuss.", "Helen Palmer died around 1967.", "Dr. Seuss was born in 1904, and died in 1991.", "Therefore, the death of Helen Palmer happened 26 years after the birth of Dr. Seuss.", "Given the age gap, Helen Palmer’s death would not have had a significant effect on Dr. Seuss.", "Thus, the death of Helen Palmer did not have a significant effect on Dr. Seuss.", "So the answer is no."], "conf_comments": ["(A)\nShe died in ", "(A)\nDr. Seuss", "(A)\nThus, Dr", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.8358778705093272, 0.5155054839568485, 0.3646461260519625, 0.22576745872779613, 0.12314833932262097, 0.095631900758338], "conf": [0.9716571797673174, 0.8763471139957552, 0.7196949758327849, 0.5691459758938744, 0.38127473867617645, 0.24819311203752975], "prob": [[0.01374180194863491, 13], [4.3476804273336114e-07, 12], [8.015906140213326e-12, 22], [4.68329070811034e-19, 23], [7.220259569952951e-28, 25], [4.563871345425558e-28, 6]], "generated": ["Helen Palmer was the wife of Dr. Seuss.", "Helen Palmer died around 1967.", "Dr. Seuss was born in 1904, and died in 1991.", "Therefore, the death of Helen Palmer happened 26 years after the birth of Dr. Seuss.", "Given the age gap, Helen Palmer’s death would not have had a significant effect on Dr. Seuss.", "So the answer is no."], "conf_comments": ["(A)\nShe died in ", "(A)\nDr. Seuss", "(A)\nThus, Dr", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.8526114319800413, 0.6398803458556817, 0.41338931053091454, 0.2240416177591666, 0.11870007033173718, 0.09537688391355825], "conf": [0.9713257300732512, 0.9029473192840444, 0.7975885790555701, 0.6662529443891928, 0.4471716983383999, 0.29073034868053915], "prob": [[0.017295126226821316, 14], [2.094511160431556e-06, 18], [1.2255760411737846e-10, 13], [5.499304539451239e-22, 25], [8.427071882515366e-29, 18], [8.081392031996037e-29, 6]], "generated": ["Helen Palmer was the first wife of Dr. Seuss.", "They married in 1929 and divorced in 1936.", "Dr. Seuss then married Audrey Stone Geisel.", "Audrey and Dr. Seuss had a successful relationship and were together until she died in 1991.", "Thus, Helen Palmer’s death has no significant effect on Dr Seuss.", "So the answer is no."], "conf_comments": ["(A)\nShe died in ", "(A)\nDr. Seuss", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.8474469943685607, 0.48522254488938066, 0.2921071415800163, 0.17198319698505177, 0.11417836952522928, 0.09521248158062337], "conf": [0.9717355323198203, 0.8977193975057446, 0.755798998112211, 0.6018574205980218, 0.43192389738470316, 0.30141887141514717], "prob": [[0.014503674922222688, 14], [5.784289315217408e-08, 12], [3.4607799678732505e-14, 17], [7.419643857623802e-23, 24], [7.042737936706525e-27, 19], [6.894183886394114e-27, 6]], "generated": ["Helen Palmer was Dr. Seuss’s first wife.", "In 1963, Helen died of cancer.", "Shortly after Helen’s death, Dr. Seuss published The Lorax.", "Since the publication of The Lorax in 1971, it has sold more than 1 million copies.", "Thus, the death of Helen Palmer had a significant effect on Dr. Suess.", "So the answer is yes."], "conf_comments": ["(A)\nShe died in ", "(A)\nDr. Seuss", "(A)\nThe Lorax is", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 80.84751725196838}
{"index": 122, "question": "Can fish get Tonsillitis?", "answer": "no", "explanation": "No. Tonsils are a pair of soft tissue masses located at the rear of the throat. Tonsillitis is the inflammation of tonsils. Fish do not have tonsils. Tonsils are only found in mammals. ", "generated": [{"finished": true, "length": 4, "score": [0.7668935761821316, 0.5928366640728059, 0.48339455902624057, 0.4224820096275603], "conf": [0.9742192960646823, 0.8495312523567424, 0.6657179344901977, 0.5107335892788889], "prob": [[0.0005154684511196261, 15], [2.5073808447078188e-05, 8], [4.846691701530687e-06, 10], [4.721695345360243e-06, 6]], "generated": ["Tonsillitis refers to the inflammation of the tonsils.", "Fish do not have tonsils.", "Thus, fish cannot get tonsillitis.", "So the answer is no."], "conf_comments": ["(A)\nFish do not", "(A)\nThus, fish", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.808202510565259, 0.4967267766042461, 0.32920627006067726, 0.2768366454174565], "conf": [0.9681380746663883, 0.7439066379983493, 0.5691711906782854, 0.4040897567223316], "prob": [[0.0012438056680229646, 17], [4.937184958144673e-11, 24], [1.919752182569779e-13, 10], [1.874568589517834e-13, 6]], "generated": ["Tonsillitis is caused by a viral or bacterial infection.", "Fish are vertebrates and thus are not susceptible to viral or bacterial infections.", "Thus, fish cannot get tonsilitis.", "So the answer is no."], "conf_comments": ["(A)\nFish are not", "(A)\nThus, fish", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6735291186717269, 0.4201941739438558, 0.2998670280843273, 0.2546888001954161], "conf": [0.9683113795707033, 0.8396377590317853, 0.6475653316135559, 0.46892334707511113], "prob": [[1.1818405979375264e-06, 18], [8.740822128415743e-10, 9], [9.098619791178175e-12, 11], [8.892820063109556e-12, 6]], "generated": ["Tonsillitis is a human disease caused by an infection in the tonsils.", "Fish doesn’t have tonsils.", "Thus, a fish cannot get tonsillitis.", "So the answer is no."], "conf_comments": ["(A)\nFish do not", "(A)\nThus, fish", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7668935761821316, 0.5928366640728059, 0.29185476210528427, 0.252064461017325], "conf": [0.9742192960646823, 0.8495312523567424, 0.6677613043525571, 0.5076480216841724], "prob": [[0.0005154684511196261, 15], [2.5073808447078188e-05, 8], [1.851309970724158e-11, 12], [1.651841809774714e-11, 6]], "generated": ["Tonsillitis refers to the inflammation of the tonsils.", "Fish do not have tonsils.", "Hence, fish will never get Tonsillitis.", "So the answer is no."], "conf_comments": ["(A)\nFish do not", "(A)\nThus, fish", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.808202510565259, 0.4967267766042461, 0.28063164107856153, 0.23625713301989557], "conf": [0.9681380746663883, 0.7439066379983493, 0.5937283200519076, 0.42954429606993483], "prob": [[0.0012438056680229646, 17], [4.937184958144673e-11, 24], [2.0660278517591178e-15, 11], [1.8263943049236814e-15, 6]], "generated": ["Tonsillitis is caused by a viral or bacterial infection.", "Fish are vertebrates and thus are not susceptible to viral or bacterial infections.", "Thus, fish generally cannot get tonsillitis.", "So the answer is no."], "conf_comments": ["(A)\nFish are not", "(A)\nThus, fish", "(A)\nHowever, fish can", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 4, "score": [0.7668935761821316, 0.5928366640728059, 0.29185476210528427, 0.11435476120752475], "conf": [0.9742192960646823, 0.8495312523567424, 0.6677613043525571, 0.5089760474174593], "prob": [[0.0005154684511196261, 15], [2.5073808447078188e-05, 8], [1.851309970724158e-11, 12], [2.736272842872667e-23, 17]], "generated": ["Tonsillitis refers to the inflammation of the tonsils.", "Fish do not have tonsils.", "Hence, fish will never get Tonsillitis.", "The disease only occur in animals with tonsils, namely mammals and birds.", "So the answer is no."], "conf_comments": ["(A)\nFish do not", "(A)\nThus, fish", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[1.4780198271186035e-12, 17], [0.5896279826875153, 6]], 0.7622125512513014, "(A)\nSo the answer is", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 47.729262351989746}
{"index": 123, "question": "Would it be impossible to get to Burning Man on the Mayflower?", "answer": "yes", "explanation": "Yes. The Mayflower was a seafaring vessel. Burning Man is held in Black Rock Desert. There are no bodies of water flowing into the Black Rock Desert.", "generated": [{"finished": true, "length": 4, "score": [0.7499239890064977, 0.45984574762965974, 0.3744901290646168, 0.31090441194838464], "conf": [0.9747829820820298, 0.7866908453696129, 0.6370096023580105, 0.4879541436394217], "prob": [[2.8933736739331938e-05, 19], [1.4583639059270016e-12, 22], [4.013536579135614e-14, 18], [2.129938966510916e-14, 6]], "generated": ["Burning Man is held from the last Monday of August to the first Monday of September.", "The Mayflower landed in New England sometime between September and November in 1620.", "Thus, it is impossible to get to Burning Man on the Mayflower.", "So the answer is yes."], "conf_comments": ["(A)\nThe Mayflower", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7499239890064977, 0.45984574762965974, 0.3744901290646168, 0.305151544518162], "conf": [0.9747829820820298, 0.7866908453696129, 0.6370096023580105, 0.4837192043276666], "prob": [[2.8933736739331938e-05, 19], [1.4583639059270016e-12, 22], [4.013536579135614e-14, 18], [1.793682170475823e-14, 6]], "generated": ["Burning Man is held from the last Monday of August to the first Monday of September.", "The Mayflower landed in New England sometime between September and November in 1620.", "Thus, it is impossible to get to Burning Man on the Mayflower.", "So the answer is no."], "conf_comments": ["(A)\nThe Mayflower", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6125842959194804, 0.4019978833176087, 0.28795917093420026, 0.24023498312953132, 0.20351542013637944], "conf": [0.97191591147813, 0.8436483745016247, 0.666664362220461, 0.5246117075726172, 0.3878077450478965], "prob": [[1.2046825714606422e-10, 24], [3.2705298307778555e-15, 15], [4.353112710081239e-20, 26], [4.775718153404065e-21, 18], [3.998531673450508e-21, 6]], "generated": ["Burning Man is a festival with 50,000-70,000 people.", "Mayflower was a ship that transported 102 people.", "The Mayflower would not be able to hold 50,000-70,000 people.", "Thus, it is impossible to get to Burning Man on the Mayflower.", "So the answer is yes."], "conf_comments": ["(A)\nThe Mayflower", "(A)\nThus, it", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6125842959194804, 0.4019978833176087, 0.3147633079555438, 0.26864217095668763], "conf": [0.97191591147813, 0.8436483745016247, 0.7045776169325907, 0.5277808787816967], "prob": [[1.2046825714606422e-10, 24], [3.2705298307778555e-15, 15], [5.697843705086832e-19, 28], [4.817732986354884e-19, 6]], "generated": ["Burning Man is a festival with 50,000-70,000 people.", "Mayflower was a ship that transported 102 people.", "Thus, it is impossible to fit 50,000-70,000 people into Mayflower.", "So the answer is yes."], "conf_comments": ["(A)\nThe Mayflower", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6125842959194804, 0.4019978833176087, 0.3147633079555438, 0.22878622470573254], "conf": [0.97191591147813, 0.8436483745016247, 0.7045776169325907, 0.532824816607675], "prob": [[1.2046825714606422e-10, 24], [3.2705298307778555e-15, 15], [5.697843705086832e-19, 28], [6.624089237975665e-20, 6]], "generated": ["Burning Man is a festival with 50,000-70,000 people.", "Mayflower was a ship that transported 102 people.", "Thus, it is impossible to fit 50,000-70,000 people into Mayflower.", "So the answer is no."], "conf_comments": ["(A)\nThe Mayflower", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.36669373512268}
{"index": 124, "question": "Does Lupita Nyongo have citizenship in paternal Family of Barack Obama's origin country?", "answer": "yes", "explanation": "Yes. Actress Lupita Nyongo has dual citizenship in Kenya and Mexico. Barack Obama's father was born in Nyang’oma Kogelo, Rachuonyo District, Kenya Colony. Barack Obama's father was a Kenyan of the Luo tribe.", "generated": [{"finished": true, "length": 5, "score": [0.8335821926059205, 0.631101016424086, 0.4516090417329295, 0.3358053044450758, 0.27610370905714554], "conf": [0.9576323158221149, 0.7824209688912888, 0.6227569537296823, 0.47013347720711474, 0.31942172024665283], "prob": [[0.040455785442302196, 10], [0.00028301512135518096, 14], [9.154860889742023e-07, 13], [1.0947681542590559e-10, 29], [1.062377391941992e-10, 6]], "generated": ["Lupita Nyongo was born in Mexico.", "Mexico is not the origin country of Barack Obama.", "Barack Obama's origin country is the United States.", "Thus, Lupita Nyongo does not have the citizenship of the paternal Family of Barack Obama's origin country.", "So the answer is no."], "conf_comments": ["(A)\nBarack Obama", "(A)\nThus, L", "(A)\nThus, L", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.678289673527614, 0.5266128263900454, 0.4351729133063962], "conf": [0.9214317852205598, 0.7119395782225001, 0.4879827325692434], "prob": [[5.765185814099264e-08, 24], [9.063553436405451e-11, 26], [8.862896277739692e-11, 6]], "generated": ["Lupita Nyong'o has Kenyan citizenship, which is the origin country for Barack Obama.", "Thus, Lupita Nyongo has citizenship in the paternal family of Barack Obama’s origin country.", "So the answer is yes."], "conf_comments": ["(A)\nThus, L", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8335821926059205, 0.631101016424086, 0.4516090417329295, 0.26755278545398986, 0.2178273729165455], "conf": [0.9576323158221149, 0.7824209688912888, 0.6227569537296823, 0.4167772126720043, 0.27958639569977056], "prob": [[0.040455785442302196, 10], [0.00028301512135518096, 14], [9.154860889742023e-07, 13], [3.272977538889148e-13, 23], [3.0458347138227837e-13, 6]], "generated": ["Lupita Nyongo was born in Mexico.", "Mexico is not the origin country of Barack Obama.", "Barack Obama's origin country is the United States.", "Thus, Lupita Nyongo would not have been born in Barack Obama's origin country.", "So the answer is no."], "conf_comments": ["(A)\nBarack Obama", "(A)\nThus, L", "(A)\nThus, L", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8335821926059205, 0.631101016424086, 0.30782478369539173, 0.24612340468965746], "conf": [0.9576323158221149, 0.7824209688912888, 0.5777609871331701, 0.37159755291196067], "prob": [[0.040455785442302196, 10], [0.00028301512135518096, 14], [1.1405948603328429e-10, 13], [1.0999567040834907e-10, 6]], "generated": ["Lupita Nyongo was born in Mexico.", "Mexico is not the origin country of Barack Obama.", "Thus, she doesn’t have the citizenship there.", "So the answer is no."], "conf_comments": ["(A)\nBarack Obama", "(A)\nThus, L", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6841502212341706, 0.4402557750146082, 0.278837643601581, 0.23883739207900884], "conf": [0.9599860732607203, 0.7462233550607322, 0.6107250930734338, 0.4504921669910038], "prob": [[0.0015571917607190035, 9], [6.425849039025806e-11, 27], [4.843875247071336e-18, 23], [4.689808060201347e-18, 6]], "generated": ["Barack Obama is from Kenya.", "According to wikipedia, Lupita Nyongo is a citizen of Mexico, Kenya, and the United States.", "Thus, she does have citizenship in one of the origins of Barack Obama's family.", "So the answer is yes."], "conf_comments": ["(A)\nLupita Ny", "(A)\nThus, L", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 55.98546385765076}
{"index": 125, "question": "Are there enough Jonny Cash records in the world to give one to each French citizen?", "answer": "yes", "explanation": "Yes. Johnny Cash has sold about 90 million albums. The population of France is around 66 million .", "generated": [{"finished": true, "length": 4, "score": [0.7640223791280445, 0.4810138326125068, 0.36586978945777543, 0.3028168425603019], "conf": [0.9766482619516865, 0.8137913899870142, 0.6327159751440018, 0.43599358941503075], "prob": [[0.0020782018927213166, 12], [3.2329044094242503e-09, 18], [8.758668144309392e-12, 20], [8.453799915283645e-12, 6]], "generated": ["There are about 67 million people living in France.", "The total number of Jonny Cash records ever sold is about 90 million.", "Thus, it is possible to give one Jonny Cash record to each French citizen.", "So the answer is yes."], "conf_comments": ["(A)\nJonny C", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6665583315034143, 0.39467737712996, 0.2742507809200726, 0.23567017799635487], "conf": [0.975924343675864, 0.8447150544201134, 0.6970104128023896, 0.5170580351893761], "prob": [[0.00017412530156355244, 11], [5.572611193396063e-10, 14], [3.606942253146913e-14, 18], [3.5092969617912356e-14, 6]], "generated": ["There are about 68 million people in France.", "Jonny Cash has released about 15 studio albums.", "Thus, there are not enough Jonny Cash records for every French citizen.", "So the answer is no."], "conf_comments": ["(A)\nJonny C", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.786979458989497, 0.42125321039569663, 0.2777918800651655, 0.19408985016700253, 0.16252460579947592], "conf": [0.9793806076731373, 0.8646603555598483, 0.6935602344930869, 0.5214734630569922, 0.36673552755121736], "prob": [[0.002586299492718248, 13], [5.076736126212163e-16, 26], [7.375694629678986e-26, 37], [1.1326111671000998e-28, 15], [1.1126325699636137e-28, 6]], "generated": ["Jonny Cash died in 2003.", "According to the US Census Bureau, the population of France is 65,786,072 people.", "Assuming there is one record per person, it would take 65,786,072 records to give a Jonny Cash record to each citizen.", "Thus, there are enough records to give one to each citizen.", "So the answer is yes."], "conf_comments": ["(A)\nIn 20", "(A)\nThus, there", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7640223791280445, 0.4810138326125068, 0.24493150423977872, 0.20218929156326618], "conf": [0.9766482619516865, 0.8137913899870142, 0.6121842049639927, 0.42488309050249323], "prob": [[0.0020782018927213166, 12], [3.2329044094242503e-09, 18], [2.5545604927614337e-20, 24], [2.2885175018897295e-20, 6]], "generated": ["There are about 67 million people living in France.", "The total number of Jonny Cash records ever sold is about 90 million.", "Thus, the 67 million French people could probably receive a Jonny Cash record if they wanted one.", "So the answer is yes."], "conf_comments": ["(A)\nJonny C", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.786979458989497, 0.5218833860622822, 0.2292194377887475, 0.15660601324968743, 0.1355091017795108], "conf": [0.9793806076731373, 0.8759481459349101, 0.5679407494504038, 0.44963764410559454, 0.3376885895643375], "prob": [[0.002586299492718248, 13], [5.070025112979852e-11, 25], [2.423710456020558e-27, 31], [1.0592862891970544e-31, 19], [1.0399474863363442e-31, 6]], "generated": ["Jonny Cash died in 2003.", "In 2003, the total population of France was 60,850,000.", "The number of albums he released was 19, which is 10 times lower than the number of French citizens in 2003.", "Thus, there could not be enough Jonny Cash records for each French citizen.", "So the answer is no."], "conf_comments": ["(A)\nIn 20", "(A)\nThus, there", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 59.93888211250305}
{"index": 126, "question": "Could someone mistake the smell of your brussels sprouts for a fart?", "answer": "yes", "explanation": "Yes. Brussels Sprouts are cruciferous vegetables. Cruciferous vegetables have a sulfur containing chemical called glucosinolate in them. When you cook brussels sprouts, their smell intensifies. ", "generated": [{"finished": true, "length": 4, "score": [0.6411114687342884, 0.36580735383539337, 0.291611210334187, 0.24338661852244978], "conf": [0.9627284574676453, 0.8380413718451929, 0.6165867185778563, 0.4324827185818801], "prob": [[2.853968913292866e-06, 15], [2.137778490472833e-11, 12], [9.859043323863987e-13, 21], [9.46012765165257e-13, 6]], "generated": ["Brussels sprouts contain sulfur, which can be smelly.", "Similarly, a fart can contain sulfur.", "Thus, someone could mistake the smell of your brussels sprouts for a fart.", "So the answer is yes."], "conf_comments": ["(A)\nFarts also contain", "(A)\nThus, someone", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6767776480309486, 0.33500250588310326, 0.1790322408253718, 0.1385110716226955, 0.11885230175135392], "conf": [0.9694861615510265, 0.7073434974985316, 0.49379401290860814, 0.37919539902500404, 0.28079400973816493], "prob": [[2.7599778058577403e-05, 14], [4.426087006969292e-18, 27], [1.150277083420215e-31, 35], [7.881109403756617e-34, 20], [7.6158077231809856e-34, 6]], "generated": ["Brussels sprouts are vegetables with a strong odor.", "So it is possible that brussels sprouts can smell like farts, given that farts also have an odor.", "If someone could mistake a fart smell for brussels sprouts, it implies that he or she does not know what brussels sprouts smell like.", "Thus, someone could mistake the smell of brussels sprouts for a fart.", "So the answer is yes."], "conf_comments": ["(A)\nFarts are g", "(A)\nThus, it", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6767776480309486, 0.31145284739409984, 0.21849117315922717, 0.17988256999824687], "conf": [0.9694861615510265, 0.7870974367090998, 0.5757708103540545, 0.3933661211949118], "prob": [[2.7599778058577403e-05, 14], [7.096569174328864e-20, 25], [1.7224681360397942e-23, 21], [1.642591553313055e-23, 6]], "generated": ["Brussels sprouts are vegetables with a strong odor.", "Farts are made by anaerobic digestion of intestinal matter, so they can smell bad.", "Thus, someone could mistake a fart for the smell of your brussels sprouts.", "So the answer is yes."], "conf_comments": ["(A)\nFarts are g", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5890223591429091, 0.2775019501894123, 0.18135224646057085, 0.15032051576190458], "conf": [0.971184958984012, 0.6519892951730705, 0.4477344024914464, 0.3090505397765328], "prob": [[2.5138967468554314e-08, 17], [8.74152965528672e-22, 28], [2.5335590780429206e-26, 22], [2.463885944546104e-26, 6]], "generated": ["Brussels sprouts have a distinct odor that is quite stinky.", "Therefore, it is not unlikely that someone can mistake the smell of your brussels sprouts like that of a fart.", "Thus, someone could mistaking the smell of your brussels sprouts for a fart.", "So the answer is yes."], "conf_comments": ["(A)\nFarts have a", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5890223591429091, 0.2775019501894123, 0.2188319712705525], "conf": [0.971184958984012, 0.6519892951730705, 0.42461834537628684], "prob": [[2.5138967468554314e-08, 17], [8.74152965528672e-22, 28], [6.624826377990185e-22, 6]], "generated": ["Brussels sprouts have a distinct odor that is quite stinky.", "Therefore, it is not unlikely that someone can mistake the smell of your brussels sprouts like that of a fart.", "So the answer is yes."], "conf_comments": ["(A)\nFarts have a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 54.80126643180847}
{"index": 127, "question": "Can Stone Cold Steve Austin apply his finisher to a mule deer?", "answer": "yes", "explanation": "Yes. Steve Austin's finisher, The Stone Cold Stunner, involves lifting the opponent and then pulling their head down. The largest male mule deer's are around 330 pounds. Steve Austin has applied his finisher to the wrestler The Big Show. The Big Show weighs 383 pounds.", "generated": [{"finished": true, "length": 3, "score": [0.6392670608095391, 0.42075161491007834, 0.3344063306385259], "conf": [0.8203686193481147, 0.6276627907112773, 0.42399685283772526], "prob": [[1.7772834211214072e-06, 19], [3.8013290827098e-13, 27], [2.541571652760656e-13, 6]], "generated": ["A mule deer is an animal, while Stone Cold Steve Austin is a human being.", "Humans and animals are different life forms, so Stone Cold Steve Austin cannot apply his finisher to a mule deer.", "So the answer is no."], "conf_comments": ["(A)\nStone Cold Steve", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7982978588923184, 0.4654554915669324, 0.24794018489516362, 0.19711588196338392], "conf": [0.9630437905206466, 0.7918811886758385, 0.5345021490311911, 0.3452199932022805], "prob": [[0.00204295313112935, 15], [8.71027698336567e-09, 14], [1.9228178662122607e-17, 23], [1.6887120663296312e-17, 6]], "generated": ["Stone Cold Steve Austin’s finisher is a stunner.", "The mule deer is about the same size as a human.", "Thus, by logic, it is possible for him to apply his finisher to a mule deer.", "So the answer is yes."], "conf_comments": ["(A)\nA stunner", "(A)\nThus, Stone", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8147990788747361, 0.5114779020276435, 0.2718269467841155, 0.15728508981253853, 0.12520147365796475], "conf": [0.9726337569153549, 0.8957553813664475, 0.6495826896457728, 0.4141562954998095, 0.2640554605425154], "prob": [[0.0032529381394329994, 15], [2.5185645900979223e-11, 22], [3.3516159378947435e-23, 29], [3.0898622921419775e-28, 18], [2.9772578094285133e-28, 6]], "generated": ["Stone Cold Steve Austin’s finisher is the Stunner.", "To apply the Stunner, Stone Cold would strike the opponent’s head with his knee.", "To stun a mule deer, the knee of the Stone Cold Steve Austin would have to hit the deer’s head.", "Thus, the Stunner could also be applied to a mule deer.", "So the answer is yes."], "conf_comments": ["(A)\nThe Stunner", "(A)\nA mule de", "(A)\nThus, Stone", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7982978588923184, 0.4654554915669324, 0.21613082778627915, 0.11846338754418685, 0.09014016265271631], "conf": [0.9630437905206466, 0.7918811886758385, 0.6044511437613452, 0.35867773267559777, 0.2179454321071432], "prob": [[0.00204295313112935, 15], [8.71027698336567e-09, 14], [9.110679043788783e-20, 20], [5.646448607928225e-26, 21], [4.2259614114269434e-26, 6]], "generated": ["Stone Cold Steve Austin’s finisher is a stunner.", "The mule deer is about the same size as a human.", "In comparison, Stone Cold Steve Austin is about around 1 cm taller than the average human.", "Therefore, it is unlikely that he can apply a stunner to a mule deer.", "So the answer is no."], "conf_comments": ["(A)\nA stunner", "(A)\nThus, Stone", "(A)\nThus, Stone", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.8147990788747361, 0.3767601325916363, 0.2675058783792031, 0.11711641505840178, 0.06901045264468826, 0.04393032289749482, 0.03278359023213404], "conf": [0.9726337569153549, 0.8805490092794994, 0.7415681382455037, 0.5727278510063311, 0.4071573235495255, 0.2638083874914073, 0.16143126533280852], "prob": [[0.0032529381394329994, 15], [7.196055402449595e-14, 17], [1.9557911544640224e-17, 16], [4.0519932034786705e-27, 16], [4.006642301369329e-41, 45], [1.3731447136118528e-44, 17], [7.802385570323093e-45, 6]], "generated": ["Stone Cold Steve Austin’s finisher is the Stunner.", "Stone Cold Steve Austin weighs around 217.9 kg.", "A mule deer weighs around 72.6 kg.", "A small mule deer weighs around 20.0 kg", "Obviously, 72.6 kg is greater than 20.0 kg while 217.9 kg is much more greater than 72.6 kg and 20.0 kg.", "Stone Cold Steve Austin cannot apply his finisher to a mule deer.", "So the answer is no."], "conf_comments": ["(A)\nThe Stunner", "(A)\nA mule de", "(A)\nThus, Stone", "(A)\nThus, Stone", "(A)\nThus, Stone", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 7, "score": [0.8147990788747361, 0.3767601325916363, 0.2675058783792031, 0.11711641505840178, 0.06901045264468826, 0.04393032289749482, 0.02211424726344669], "conf": [0.9726337569153549, 0.8805490092794994, 0.7415681382455037, 0.5727278510063311, 0.4071573235495255, 0.2638083874914073, 0.17597160778779264], "prob": [[0.0032529381394329994, 15], [7.196055402449595e-14, 17], [1.9557911544640224e-17, 16], [4.0519932034786705e-27, 16], [4.006642301369329e-41, 45], [1.3731447136118528e-44, 17], [2.3347431870061377e-56, 28]], "generated": ["Stone Cold Steve Austin’s finisher is the Stunner.", "Stone Cold Steve Austin weighs around 217.9 kg.", "A mule deer weighs around 72.6 kg.", "A small mule deer weighs around 20.0 kg", "Obviously, 72.6 kg is greater than 20.0 kg while 217.9 kg is much more greater than 72.6 kg and 20.0 kg.", "Stone Cold Steve Austin cannot apply his finisher to a mule deer.", "Even on a smaller mule deer, Stone Cold Steve Austin still cannot apply his finisher due to the difference in body weight.", "So the answer is no."], "conf_comments": ["(A)\nThe Stunner", "(A)\nA mule de", "(A)\nThus, Stone", "(A)\nThus, Stone", "(A)\nThus, Stone", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[1.7002892439974104e-12, 28], [0.557329794914653, 6]], 0.6670432637155039, "(A)\nSo the answer is", 7]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 65.16807174682617}
{"index": 128, "question": "Is the Joker in a healthy romantic relationship?", "answer": "no", "explanation": "No. Healthy relationships are characterized by mutual trust and respect. The Joker is dating Harley Quinn. The Joker frequently abuses and talks down to Harley.", "generated": [{"finished": true, "length": 3, "score": [0.6897093342676046, 0.5632994570195077, 0.4764533499680647], "conf": [0.8898272592386921, 0.6703202368169153, 0.48152217707551853], "prob": [[3.635214364429161e-06, 20], [5.190956088125697e-07, 16], [5.065428665555429e-07, 6]], "generated": ["The Joker is a fictional character who doesn’t have a healthy romantic relationship.", "Thus, the Joker is not in a healthy romantic relationship.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5805245462758459, 0.35966100040886956, 0.30992094546570925, 0.2709466382833823], "conf": [0.9406813739175465, 0.8512920550065213, 0.7108869650124044, 0.5449488246646007], "prob": [[4.346815856480601e-10, 21], [7.178309696582814e-24, 37], [1.2328778188546428e-24, 15], [1.2111085994853055e-24, 6]], "generated": ["Joker is known as a psychopath, sociopath, or a narcissist.", "According to this source, psychopaths, sociopaths, and narcissists cannot have healthy romantic relationships because they do not feel emotional attachment.", "Thus, Joker is not in a healthy romantic relationship.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5805245462758459, 0.35966100040886956, 0.2628309245782739, 0.22844080118942078], "conf": [0.9406813739175465, 0.8512920550065213, 0.7036433554101506, 0.5333926127406793], "prob": [[4.346815856480601e-10, 21], [7.178309696582814e-24, 37], [1.5853065611422432e-26, 14], [1.5527922872093744e-26, 6]], "generated": ["Joker is known as a psychopath, sociopath, or a narcissist.", "According to this source, psychopaths, sociopaths, and narcissists cannot have healthy romantic relationships because they do not feel emotional attachment.", "Thus, the Joker cannot be in a healthy relationship.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6897093342676046, 0.3361882867800779, 0.25714561757564725, 0.20070570717482236], "conf": [0.8898272592386921, 0.5852314707288612, 0.38399674581917637, 0.23502059587030028], "prob": [[3.635214364429161e-06, 20], [1.1048841122898291e-13, 17], [1.7636335646940936e-14, 16], [1.7151622878610856e-14, 6]], "generated": ["The Joker is a fictional character who doesn’t have a healthy romantic relationship.", "As the Joker is fictional, he is not in a healthy relationship.", "Thus, the Joker is not in a healthy romantic relationship.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5251989637164077, 0.2970355545654265, 0.2267117219615296, 0.18830316702009817], "conf": [0.9436553275431125, 0.8016266139724165, 0.6052333186044562, 0.42060062502345713], "prob": [[1.1372245419892864e-07, 13], [1.1418002067642014e-21, 33], [1.3901335528645967e-23, 17], [1.330386405770283e-23, 6]], "generated": ["There are hints that the Joker is a psychopath.", "People with psychopathic tendencies tend to have no empathy for other people, which is an indicator of unhealthy romantic relationships.", "Thus, the Joker is likely not in a healthy romantic relationship.", "So the answer is no."], "conf_comments": ["(A)\nPsychop", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 50.3447961807251}
{"index": 129, "question": "Is the kayak a traditional boat in New Zealand?", "answer": "no", "explanation": "No. Kayaks were developed by native peoples to hunt in northern waters of the Arctic Ocean, North Atlantic, Bering Sea and North Pacific.  New Zealand is in the Southern Hemisphere. The native Maori people of New Zealand arrived there in canoes.", "generated": [{"finished": true, "length": 2, "score": [0.6051314615930486, 0.5026707931092562], "conf": [0.860166967384793, 0.6712124329825588], "prob": [[1.5084022553352569e-05, 13], [5.639518430917325e-06, 8]], "generated": ["A kayak is considered a traditional boat in New Zealand.", "Thus, the answer is yes."], "conf_comments": ["(A)\nNew Zealand is a", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6310729916231123, 0.2590686117420921, 0.1596486245235914, 0.13007816046704646, 0.11020372781628725], "conf": [0.8836136224183212, 0.7702523611334985, 0.6291839763788444, 0.5088998812611908, 0.36668262193248435], "prob": [[4.9376788593714835e-09, 24], [2.6774113765011063e-33, 34], [2.7550267716006824e-39, 18], [1.1687059780638152e-40, 16], [1.1419844244366738e-40, 6]], "generated": ["A kayak is any small, narrow watercraft which is propelled by a double-bladed paddle.", "The word \"kayak\" is first recorded in 1857 for an Inuit boat by early explorers in the territory of the Belcher.", "New Zealand was settled by the Māori people in the 14th century.", "Thus, the kayak is not a traditional boat in New Zealand.", "So the answer is no."], "conf_comments": ["(A)\nThus, a", "(A)\nThe word \"k", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6176592233117455, 0.40782330101272485, 0.187984483208784, 0.13566947170056384, 0.10858817069376794], "conf": [0.9365451778372178, 0.8279835542656907, 0.6080340543536105, 0.4194006091278744, 0.27906361351399295], "prob": [[1.41103227802875e-06, 15], [5.915922616658724e-10, 11], [2.892617815900953e-21, 21], [3.2329336982993636e-23, 16], [2.5748768083945382e-23, 6]], "generated": ["A kayak is a lightweight canoe with a covered deck.", "New Zealand is an island country in Oceania.", "It is surrounded by water, and it is not uncommon for there to be canoes there.", "Thus, the kayak could be a traditional boat in New Zealand.", "So the answer is yes."], "conf_comments": ["(A)\nNew Zealand is an", "(A)\nThus, a", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6176592233117455, 0.40782330101272485, 0.187984483208784, 0.13566947170056384, 0.0898849567728597], "conf": [0.9365451778372178, 0.8279835542656907, 0.6080340543536105, 0.4194006091278744, 0.2575443969672697], "prob": [[1.41103227802875e-06, 15], [5.915922616658724e-10, 11], [2.892617815900953e-21, 21], [3.2329336982993636e-23, 16], [4.312380406154283e-24, 6]], "generated": ["A kayak is a lightweight canoe with a covered deck.", "New Zealand is an island country in Oceania.", "It is surrounded by water, and it is not uncommon for there to be canoes there.", "Thus, the kayak could be a traditional boat in New Zealand.", "So the answer is maybe."], "conf_comments": ["(A)\nNew Zealand is an", "(A)\nThus, a", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6762042296572768, 0.2845373737515065, 0.22267528153215196], "conf": [0.9389874765583976, 0.6346148117755811, 0.4126702107005617], "prob": [[8.658597589135056e-05, 13], [2.9265231774687997e-15, 18], [2.0426161974519958e-15, 6]], "generated": ["Kayaking is a popular recreational sport in New Zealand.", "Thus, there is some evidence that the kayak is traditional there as well.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nHowever, the k", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 5, "score": [0.6176592233117455, 0.40782330101272485, 0.20006036041587968, 0.14066148726347286, 0.062459258935856535], "conf": [0.9365451778372178, 0.8279835542656907, 0.6719017187243401, 0.5277355987830744, 0.3318703048505202], "prob": [[1.41103227802875e-06, 15], [5.915922616658724e-10, 11], [6.275092992111538e-19, 17], [9.604342789907445e-22, 14], [8.66887100154339e-28, 12]], "generated": ["A kayak is a lightweight canoe with a covered deck.", "New Zealand is an island country in Oceania.", "Oceania is a region of the world which contains islands surrounding the Pacific Ocean.", "The Pacific Ocean is the largest and deepest ocean in the world.", "Thus, there is no open ocean in New Zealand.", "Instead, there are mountains and rivers around the country.", "Hence, there is no need for a kayak here as they are suitable for shallow waters.", "So"], "conf_comments": ["(A)\nNew Zealand is an", "(A)\nThus, a", "(A)\nThus, a", "(A)\nThus, the", "(A)\nTherefore, a"], "info": [[[9.025990836825317e-07, 12], [3.4511684310311537e-10, 12], [8.6283315205644105e-16, 22], [0.472240434102621, 1]], 0.6288571504666212, "(A)\nTherefore, a", 5]}, {"finished": false, "length": 5, "score": [0.6176592233117455, 0.40782330101272485, 0.20006036041587968, 0.14066148726347286, 0.03906065628608845], "conf": [0.9365451778372178, 0.8279835542656907, 0.6719017187243401, 0.5277355987830744, 0.34324846589207253], "prob": [[1.41103227802875e-06, 15], [5.915922616658724e-10, 11], [6.275092992111538e-19, 17], [9.604342789907445e-22, 14], [2.4392715877400726e-39, 19]], "generated": ["A kayak is a lightweight canoe with a covered deck.", "New Zealand is an island country in Oceania.", "Oceania is a region of the world which contains islands surrounding the Pacific Ocean.", "The Pacific Ocean is the largest and deepest ocean in the world.", "Therefore, there are few places in world where it is deep enough for the traditional boat.", "Since New Zealand is not a very deep land, it is impossible for a kayak to be a traditional boat in New Zealand.", "So the"], "conf_comments": ["(A)\nNew Zealand is an", "(A)\nThus, a", "(A)\nThus, a", "(A)\nThus, the", "(A)\nThus, the"], "info": [[[2.5397589830959994e-18, 19], [8.067787965643798e-12, 27], [0.8231599954384295, 2]], 0.6504174944490806, "(A)\nThus, the", 5]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 57.65905785560608}
{"index": 130, "question": "Did Boris Yeltsin watch the 2008 Summer Olympics?", "answer": "no", "explanation": "No. The 2008 Summer Olympics were held Aug 08 - 24, 2008. Boris Yeltsin died on Apr 23, 2007.", "generated": [{"finished": true, "length": 4, "score": [0.9151175853830524, 0.7906824211084279, 0.5611720901535666, 0.48862721789376057], "conf": [0.9804136166889317, 0.8752644375763071, 0.7403766724864289, 0.5639789014954008], "prob": [[0.050043178537425945, 19], [0.00047827281947880715, 26], [1.4159133342079964e-10, 29], [1.3764327116874747e-10, 6]], "generated": ["Boris Yeltsin died on April 23, 2007.", "The 2008 Summer Olympics took place from August 8 to August 24, 2008.", "Thus, Boris Yeltsin did not watch the 2008 Summer Olympics since he died before the event even started.", "So the answer is no."], "conf_comments": ["(A)\nThe 20", "(A)\nThus, Bor", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.9181542705869021, 0.6677019792373472, 0.5590595768228259, 0.48668088002402554], "conf": [0.9801697736597607, 0.8673725222780971, 0.7107551447840026, 0.5398264686578803], "prob": [[0.12117699010374429, 14], [3.979973486050927e-08, 29], [1.0998153112096576e-09, 23], [1.0852982494144575e-09, 6]], "generated": ["Boris Yeltsin died in 2007.", "The 2008 Olympics was held in 2008, a year after Boris Yeltsin’s death.", "Thus, it was impossible for Boris Yeltsin to watch the 2008 Olympics.", "So the answer is no."], "conf_comments": ["(A)\nThe 20", "(A)\nThus, Bor", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.9151175853830524, 0.560137360412759, 0.478437601179312, 0.4196724546321479], "conf": [0.9804136166889317, 0.871853947434535, 0.7324181990861799, 0.56574380243368], "prob": [[0.050043178537425945, 19], [3.6877503484464823e-09, 19], [1.9073501284308489e-10, 21], [1.8633236018567903e-10, 6]], "generated": ["Boris Yeltsin died on April 23, 2007.", "The opening ceremony for the the 2008 Summer Olympics was on August 8.", "Thus, Boris Yeltsin did not watch the 2008 Summer Olympics.", "So the answer is no."], "conf_comments": ["(A)\nThe 20", "(A)\nThus, Bor", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.9151175853830524, 0.7906824211084279, 0.48896825213698525, 0.3813669421961365], "conf": [0.9804136166889317, 0.8752644375763071, 0.7326387928607347, 0.5916046920265148], "prob": [[0.050043178537425945, 19], [0.00047827281947880715, 26], [1.4956808967156512e-12, 25], [1.5513274436578728e-13, 8]], "generated": ["Boris Yeltsin died on April 23, 2007.", "The 2008 Summer Olympics took place from August 8 to August 24, 2008.", "This is more than a year after Boris Yeltsin died, so it is impossible that he watch the Olympics.", "Thus, the answer is no."], "conf_comments": ["(A)\nThe 20", "(A)\nThus, Bor", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7481124751680027, 0.559616967410198, 0.4807810527123662], "conf": [0.9197385718959523, 0.7180940845650255, 0.5333885583008303], "prob": [[8.695899821630392e-10, 42], [7.737179270249546e-14, 28], [7.448798502644216e-14, 6]], "generated": ["The 2008 Summer Olympics occurred on 8/8 to 8/24, and Boris Yeltsin died on 4/23, 2007.", "Thus, Boris Yeltsin could not have watched the 2008 Summer Olympics, since he was already dead.", "So the answer is no."], "conf_comments": ["(A)\nThus, Bor", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 54.31495690345764}
{"index": 131, "question": "Did the swallow play a role in a famous film about King Arthur?", "answer": "yes", "explanation": "Yes. Monty Python and the Holy Grail was a famous film about King Arthur. In Monty Python and the Holy Grail, swallows are mentioned several times.", "generated": [{"finished": true, "length": 4, "score": [0.7368918460095303, 0.4768423683032847, 0.3616865832670583, 0.2888966984852998], "conf": [0.9560283939585553, 0.6961102490249093, 0.4784799873256707, 0.3060403083554717], "prob": [[6.663832688293933e-05, 17], [2.636777119012296e-07, 10], [8.97287370203805e-09, 19], [8.838215664198012e-09, 6]], "generated": ["The movie “Monty Python and the Holy Grail” is about King Arthur.", "The swallow does not appear in the movie.", "Thus, the swallow did not play a role in the famous film about King Arthur.", "So the answer is no."], "conf_comments": ["(A)\nThe swallow is", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6541754523726856, 0.3571096544874672, 0.28525073272503826], "conf": [0.9346948049291395, 0.639036474782923, 0.41004013745439544], "prob": [[7.815123449982037e-07, 18], [1.6017740482105965e-11, 13], [1.5484566108249263e-11, 6]], "generated": ["The 2004 film, King Arthur, did not include a swallow.", "Thus, this famous film did not have a swallow.", "So the answer is no."], "conf_comments": ["(A)\nThe 19", "(A)\nThe 19", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.5244130942797318, 0.3688163457854222], "conf": [0.8811603796243135, 0.5974145841970269], "prob": [[2.596458564867871e-08, 15], [2.083534311315839e-09, 8]], "generated": ["Two swallows do not appear in the famous film about King Arthur.", "Thus, the answer is no."], "conf_comments": ["(A)\nThe swallow is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7368918460095303, 0.2403735902425619, 0.13926497979542868, 0.11455177932973759], "conf": [0.9560283939585553, 0.7113789851455861, 0.492597997572297, 0.33445520673717183], "prob": [[6.663832688293933e-05, 17], [1.0893173049875206e-38, 40], [2.710489533463822e-45, 21], [2.653962894546072e-45, 6]], "generated": ["The movie “Monty Python and the Holy Grail” is about King Arthur.", "The 1768 poem “The Legend of the Sparrow” contains the lines “He swallowed a swallow”, and “The swallow plays upon the bridge of Thor”.", "Thus, the swallow has appeared in a movie about King Arthur and it has played a role.", "So the answer is yes."], "conf_comments": ["(A)\nThe swallow is", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7368918460095303, 0.4768423683032847, 0.14328294433295694, 0.044392957057259895], "conf": [0.9560283939585553, 0.6961102490249093, 0.473419884332267, 0.3122888155929029], "prob": [[6.663832688293933e-05, 17], [2.636777119012296e-07, 10], [5.5460848249885375e-31, 27], [3.437224674737156e-40, 11]], "generated": ["The movie “Monty Python and the Holy Grail” is about King Arthur.", "The swallow does not appear in the movie.", "But the song “That’s an awfully big wall” by the four French knights and King Arthur is in the movie.", "So while the answer is no, it almost is."], "conf_comments": ["(A)\nThe swallow is", "(A)\nThus, the", "(A)\nThe song “That", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7368918460095303, 0.4768423683032847, 0.14328294433295694, 0.02220986036860459], "conf": [0.9560283939585553, 0.6961102490249093, 0.473419884332267, 0.3141182468037148], "prob": [[6.663832688293933e-05, 17], [2.636777119012296e-07, 10], [5.5460848249885375e-31, 27], [4.5287510862151337e-41, 7]], "generated": ["The movie “Monty Python and the Holy Grail” is about King Arthur.", "The swallow does not appear in the movie.", "But the song “That’s an awfully big wall” by the four French knights and King Arthur is in the movie.", "So a bird is the answer."], "conf_comments": ["(A)\nThe swallow is", "(A)\nThus, the", "(A)\nThe song “That", "(A)\nThe swallow is"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 45.56266379356384}
{"index": 132, "question": "Was Michael Crichton ever in danger of flunking out of Harvard as an undergraduate?", "answer": "no", "explanation": "No. Scholastic probation or academic dismissal, sometimes known as flunking out, is the termination of students at a higher educational institution as the result of poor academic achievement. Michael Crichton obtained his bachelor's degree in biological anthropology summa cum laude in 1964. Summa cum laude is the highest distinction a person can achieve in college for academic success. Someone who achieves summa cum laude cannot have even a single semester of poor grades.", "generated": [{"finished": true, "length": 5, "score": [0.7183399550037666, 0.542548923284941, 0.464704581888412, 0.33841870673597313, 0.2638119163221557], "conf": [0.9724927721983541, 0.8455292507861041, 0.6648318799920927, 0.43295004927080005, 0.26411843884838937], "prob": [[0.00014022688813722817, 14], [6.904323417447214e-10, 29], [1.6101708741007485e-10, 21], [3.2555191147962305e-12, 19], [3.180791824060927e-12, 6]], "generated": ["Michael Crichton attended Harvard University in 1960.", "According to US news, Princeton University and University of Pennsylvania are ranked as the number 1 and 6 national college, respectively.", "This can indicate that Princeton University is about as academically rigorous as the University of Pennsylvania.", "Thus, Michael Crichton was never in danger of flunking out of Harvard.", "So the answer is no."], "conf_comments": ["(A)\nHe graduated in ", "(A)\nHarvard University is", "(A)\nMichael Crichton", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7183399550037666, 0.542548923284941, 0.464704581888412, 0.32496336197923165, 0.2540306637772091], "conf": [0.9724927721983541, 0.8455292507861041, 0.6648318799920927, 0.42256114338761147, 0.2592210673784037], "prob": [[0.00014022688813722817, 14], [6.904323417447214e-10, 29], [1.6101708741007485e-10, 21], [6.542631633264073e-13, 21], [6.392768562638453e-13, 6]], "generated": ["Michael Crichton attended Harvard University in 1960.", "According to US news, Princeton University and University of Pennsylvania are ranked as the number 1 and 6 national college, respectively.", "This can indicate that Princeton University is about as academically rigorous as the University of Pennsylvania.", "Thus, Michael Crichton would not have been in danger of flunking out of Harvard.", "So the answer is no."], "conf_comments": ["(A)\nHe graduated in ", "(A)\nHarvard University is", "(A)\nMichael Crichton", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.7183399550037666, 0.542548923284941, 0.3972925718622593, 0.1885696361534212, 0.12784530788963802, 0.10366282591020634], "conf": [0.9724927721983541, 0.8455292507861041, 0.6649507421000458, 0.5224687920960432, 0.3788098994471953, 0.2514772551411337], "prob": [[0.00014022688813722817, 14], [6.904323417447214e-10, 29], [3.256611292358333e-13, 20], [3.50538113648746e-19, 11], [9.822735146341297e-24, 23], [9.268876375250667e-24, 6]], "generated": ["Michael Crichton attended Harvard University in 1960.", "According to US news, Princeton University and University of Pennsylvania are ranked as the number 1 and 6 national college, respectively.", "This can indicate that Princeton is about as academically rigorous as the University of Pennsylvania.", "And Michael Crichton graduated from Princeton.", "Thus, it was not likely that Michael Crichton was in danger of flunking out of Harvard.", "So the answer is no."], "conf_comments": ["(A)\nHe graduated in ", "(A)\nHarvard University is", "(A)\nMichael Crichton", "(A)\nThus, Michael", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7812802878912588, 0.48807509607897864, 0.2607120255250681, 0.20059364855946563], "conf": [0.9722322535187198, 0.8994090728153731, 0.7258607264460298, 0.4979054157472759], "prob": [[0.01515630667841101, 9], [3.6270773750753294e-11, 23], [5.277447485131327e-22, 24], [2.180399035958016e-22, 6]], "generated": ["Michael Crichton graduated from Harvard University.", "In 2012, Harvard University’s undergraduate acceptance rate is 6.2%.", "This suggest that, as an undergraduate student, it was unlikely that Michael Crichton would flunk out.", "So the answer is no."], "conf_comments": ["(A)\nHe graduated in ", "(A)\nThus, it", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.7183399550037666, 0.542548923284941, 0.3972925718622593, 0.1885696361534212, 0.10839044751718287, 0.0710763333766568, 0.05587970886104303], "conf": [0.9724927721983541, 0.8455292507861041, 0.6649507421000458, 0.5224687920960432, 0.31779641029836764, 0.19937792474326152, 0.12394245324707223], "prob": [[0.00014022688813722817, 14], [6.904323417447214e-10, 29], [3.256611292358333e-13, 20], [3.50538113648746e-19, 11], [2.0134136156341682e-23, 16], [3.272742140172014e-26, 17], [3.1623003482323894e-26, 6]], "generated": ["Michael Crichton attended Harvard University in 1960.", "According to US news, Princeton University and University of Pennsylvania are ranked as the number 1 and 6 national college, respectively.", "This can indicate that Princeton is about as academically rigorous as the University of Pennsylvania.", "And Michael Crichton graduated from Princeton.", "This can indicate that Princeton is more academically rigorous than Harvard.", "Thus, Michael Crichton would not have flunked out of Harvard.", "So the answer is no."], "conf_comments": ["(A)\nHe graduated in ", "(A)\nHarvard University is", "(A)\nMichael Crichton", "(A)\nThus, Michael", "(A)\nThus, Michael", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 73.11713743209839}
{"index": 133, "question": "Was Surfing popular when pogs came out?", "answer": "yes", "explanation": "Yes. Pogs came out in the 1990's. The 90's saw a rise in 'Big Wave Culture', a practice involving finding the largest possible waves to surf on.", "generated": [{"finished": true, "length": 3, "score": [0.6156579107209248, 0.506496500307791, 0.4254931897343955], "conf": [0.9534992153952989, 0.7510162503098452, 0.5313715938001069], "prob": [[9.426349420301538e-17, 40], [9.694338098379858e-18, 15], [9.54592760680565e-18, 6]], "generated": ["Surfing was already a popular sport in Hawaii and Southern California in the 1930s, well before pogs started to become popular in the 1990s.", "Thus, surfing was not popular when pogs came out.", "So the answer is no."], "conf_comments": ["(A)\nThus, Sur", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.824418923359923, 0.5944472055646196, 0.3774579110099499, 0.31111590627087493], "conf": [0.9765482240786901, 0.7829633581818861, 0.607565850292814, 0.4256693058414455], "prob": [[0.008991428980673658, 13], [1.3554163057572989e-05, 15], [1.4808551704992654e-13, 28], [1.231070458793154e-13, 6]], "generated": ["Pogs were popular in the 1990s.", "Surfing was invented in the 1950s.", "Since Surfing was invented around 20 years before pogs came out, it was not popular when pogs came out.", "So the answer is no."], "conf_comments": ["(A)\nSurfing was", "(A)\nThus, Sur", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.824418923359923, 0.5944472055646196, 0.3190452313852664, 0.25664984048145134, 0.21885529570417087], "conf": [0.9765482240786901, 0.7829633581818861, 0.6134707965533909, 0.44859307604922705, 0.3274712013830611], "prob": [[0.008991428980673658, 13], [1.3554163057572989e-05, 15], [2.7579241345670773e-14, 20], [4.982480919424272e-15, 14], [4.867602392179377e-15, 6]], "generated": ["Pogs were popular in the 1990s.", "Surfing was invented in the 1950s.", "Therefore, at the time pogs came out, Surfing had already been invented.", "Thus, Surfing was popular when pogs came out.", "So the answer is yes."], "conf_comments": ["(A)\nSurfing was", "(A)\nThus, Sur", "(A)\nThus, Sur", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6156579107209248, 0.4579162669178824, 0.3910318684337192], "conf": [0.9534992153952989, 0.8033901216123611, 0.598172451521099], "prob": [[9.426349420301538e-17, 40], [1.712574785550358e-19, 15], [1.5113428656801357e-19, 6]], "generated": ["Surfing was already a popular sport in Hawaii and Southern California in the 1930s, well before pogs started to become popular in the 1990s.", "Thus, Surfing was popular well before pogs came out.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Sur", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.824418923359923, 0.5944472055646196, 0.3190452313852664, 0.2346403252443334], "conf": [0.9765482240786901, 0.7829633581818861, 0.6134707965533909, 0.4375069311010755], "prob": [[0.008991428980673658, 13], [1.3554163057572989e-05, 15], [2.7579241345670773e-14, 20], [5.248567425723356e-15, 6]], "generated": ["Pogs were popular in the 1990s.", "Surfing was invented in the 1950s.", "Therefore, at the time pogs came out, Surfing had already been invented.", "So the answer is yes."], "conf_comments": ["(A)\nSurfing was", "(A)\nThus, Sur", "(A)\nThus, Sur", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 50.61464786529541}
{"index": 134, "question": "Could a student at the University of Houston see a caracal on campus?", "answer": "no", "explanation": "No. The caracal is native to Africa, the Middle East, Central Asia, and India. The University of Houston is located in the United States. The United States is located in North America.", "generated": [{"finished": true, "length": 4, "score": [0.7932235032615825, 0.4307457155756462, 0.3012898743295261, 0.2518041950761589], "conf": [0.9705563779486266, 0.8324794922508935, 0.6053801093143326, 0.42472817630159687], "prob": [[0.005511179534353155, 12], [1.008098000581744e-12, 21], [2.4480822636130024e-16, 21], [2.383798566281396e-16, 6]], "generated": ["The University of Houston is located in Houston, TX.", "According to the United States Fish and Wildlife Service, caracals only breed in Africa.", "Thus, it is not possible to see a caracal on campus at the University of Houston.", "So the answer is no."], "conf_comments": ["(A)\nThe caracal", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6584427199945835, 0.35286498007365336, 0.23491206465960607, 0.1860272517111354], "conf": [0.9735595718173835, 0.7551795843343801, 0.46267413043231775, 0.29421571420573894], "prob": [[1.0652437205536652e-06, 17], [7.069193661493366e-12, 12], [2.0797146845923133e-14, 18], [1.9129994738679255e-14, 6]], "generated": ["The University of Houston enrolls around 43,000 students.", "There are no caracals in the state of Texas.", "Thus, it is unlikely for a student to see a caracal on campus.", "So the answer is no."], "conf_comments": ["(A)\nThe caracal", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6584427199945835, 0.35286498007365336, 0.21776088534548177, 0.1773915425874519], "conf": [0.9735595718173835, 0.7551795843343801, 0.5295257464721826, 0.3529843453499792], "prob": [[1.0652437205536652e-06, 17], [7.069193661493366e-12, 12], [1.195842737403426e-16, 18], [1.1638588884050787e-16, 6]], "generated": ["The University of Houston enrolls around 43,000 students.", "There are no caracals in the state of Texas.", "Thus, a student at UH could not see a caracal on campus.", "So the answer is no."], "conf_comments": ["(A)\nThe caracal", "(A)\nThus, it", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.695839469820197, 0.4006404966361102, 0.2002039579792511, 0.16315188262656713], "conf": [0.965847905707912, 0.759842514801372, 0.4980627326593308, 0.3352899740018681], "prob": [[1.5912925233884567e-05, 16], [2.7921006788376707e-12, 18], [1.0812758525521794e-25, 32], [9.966680778650434e-26, 6]], "generated": ["The University of Houston is on the outskirts of Houston, Texas.", "The caracal is only native to the continents of Africa and Eurasia.", "Thus, it would be very unlikely, if not impossible, for a caracal to appear near the University of Houston or at the University of Houston.", "So the answer is no."], "conf_comments": ["(A)\nThe caracal", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7932235032615825, 0.4307457155756462, 0.174124414509315, 0.13677933071784726], "conf": [0.9705563779486266, 0.8324794922508935, 0.5802331203659249, 0.39486703337853496], "prob": [[0.005511179534353155, 12], [1.008098000581744e-12, 21], [3.2705194229842985e-27, 23], [1.817423857800171e-27, 6]], "generated": ["The University of Houston is located in Houston, TX.", "According to the United States Fish and Wildlife Service, caracals only breed in Africa.", "Since caracals cannot live in America, it's very unlikely for a student to see it on campus.", "So the answer is no."], "conf_comments": ["(A)\nThe caracal", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 4, "score": [0.6584427199945835, 0.35286498007365336, 0.23491206465960607, 0.13239273662440101], "conf": [0.9735595718173835, 0.7551795843343801, 0.46267413043231775, 0.28680468632697015], "prob": [[1.0652437205536652e-06, 17], [7.069193661493366e-12, 12], [2.0797146845923133e-14, 18], [1.232887469283408e-19, 18]], "generated": ["The University of Houston enrolls around 43,000 students.", "There are no caracals in the state of Texas.", "Thus, it is unlikely for a student to see a caracal on campus.", "So a student at the University of Houston could not see a caracal on campus.", "The answer is no."], "conf_comments": ["(A)\nThe caracal", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": [[[5.928156772740638e-06, 18], [0.03820363680287984, 5]], 0.6198848551549293, "(A)\n# Is the above", 4]}, {"finished": false, "length": 4, "score": [0.7932235032615825, 0.4307457155756462, 0.174124414509315, 0.1298229985754779], "conf": [0.9705563779486266, 0.8324794922508935, 0.5802331203659249, 0.43160772098186007], "prob": [[0.005511179534353155, 12], [1.008098000581744e-12, 21], [3.2705194229842985e-27, 23], [9.65076207028592e-30, 20]], "generated": ["The University of Houston is located in Houston, TX.", "According to the United States Fish and Wildlife Service, caracals only breed in Africa.", "Since caracals cannot live in America, it's very unlikely for a student to see it on campus.", "Thus, a student at the University of Houston would not see a caracal on campus.", "So the answer is no."], "conf_comments": ["(A)\nThe caracal", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[0.0029508346602264627, 20], [0.9807357926670536, 6]], 0.7438522652923811, "(A)\nSo the answer is", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 53.6568922996521}
{"index": 135, "question": "Did the Coen brothers ever collaborate with the Brothers Grimm?", "answer": "no", "explanation": "No. The Coen brothers were born in 1954 and 1957. The Brothers Grimm died in 1859 and 1863.", "generated": [{"finished": true, "length": 7, "score": [0.578749419602992, 0.4751040347591231, 0.405778173695297, 0.22648719745694346, 0.17562748155286445, 0.1324651235152397, 0.11208336568467575], "conf": [0.9696187344456948, 0.932938980404679, 0.8621907116266353, 0.7470024345321494, 0.6281907958944798, 0.49725259527468724, 0.3575766500442551], "prob": [[8.359523591559206e-06, 11], [8.15932618862662e-08, 13], [4.771470437225859e-09, 12], [2.882239373156507e-15, 14], [3.361926435791306e-19, 27], [6.319878903247922e-22, 19], [6.155049895100526e-22, 6]], "generated": ["To collaborate means to work together on a project.", "Joel and Ethan Coen are the Coen brothers.", "Jacob and Wilhelm Grimm are the Brothers Grimm.", "The Coen brothers and the Brothers Grimm lived during different centuries.", "The Coen brothers were born in 1954, while the Brothers Grimm were born in 1785.", "Thus, the Coen brothers were never able to collaborate with the Brothers Grimm.", "So the answer is no."], "conf_comments": ["(A)\nThe Coen brothers", "(A)\nThe Brothers Grimm", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.578749419602992, 0.4751040347591231, 0.3401778475512051, 0.2348588570590761, 0.1973170077946545], "conf": [0.9696187344456948, 0.932938980404679, 0.8634058360149145, 0.6686629943828795, 0.47391119050044844], "prob": [[8.359523591559206e-06, 11], [8.15932618862662e-08, 13], [6.812850428800575e-11, 12], [1.77783185923146e-14, 17], [1.7347766576547107e-14, 6]], "generated": ["To collaborate means to work together on a project.", "Joel and Ethan Coen are the Coen brothers.", "Jacob and Wilhelm Grimm are the brothers Grimm.", "Thus, the brothers Grimm have never collaborated with the Coen brothers.", "So the answer is no."], "conf_comments": ["(A)\nThe Coen brothers", "(A)\nThe Brothers Grimm", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.6784266224302741, 0.5130528051567143], "conf": [0.9333957774608117, 0.703615013418303], "prob": [[2.9711103032202735e-06, 18], [3.2607056967667435e-07, 8]], "generated": ["There is no official record of the Coen brothers collaborating with the Brothers Grimm.", "Thus, the answer is no."], "conf_comments": ["(A)\nThe Brothers Grimm", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 7, "score": [0.578749419602992, 0.4751040347591231, 0.3401778475512051, 0.22563959686367713, 0.15336294228566302, 0.11364777515949277, 0.09326837991536255], "conf": [0.9696187344456948, 0.932938980404679, 0.8634058360149145, 0.7787859146558228, 0.6171612731207912, 0.442406192486871, 0.30032285214562826], "prob": [[8.359523591559206e-06, 11], [8.15932618862662e-08, 13], [6.812850428800575e-11, 12], [7.181641313928906e-21, 32], [1.1467895328282733e-27, 29], [5.554607128154712e-30, 20], [5.2982468878547496e-30, 6]], "generated": ["To collaborate means to work together on a project.", "Joel and Ethan Coen are the Coen brothers.", "Jacob and Wilhelm Grimm are the brothers Grimm.", "Joel and Ethan Coen are filmmakers who made movies like Fargo, Miller’s Crossing, and No Country for Old Men.", "Jacob and Wilhelm Grimm are the creators of fairytales like Cinderella, Snow White, and Sleeping Beauty.", "Thus, Joel and Ethan Coen did not collaborate with the brothers Grimm.", "So the answer is no."], "conf_comments": ["(A)\nThe Coen brothers", "(A)\nThe Brothers Grimm", "(A)\nThus, the", "(A)\nJacob and Wilhelm", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6178782787771129, 0.46676269354745253, 0.3038012621365229, 0.24389317535000932], "conf": [0.9564884053966936, 0.8060640033393108, 0.6021141344064266, 0.39622215727165544], "prob": [[0.0002571196785352968, 9], [1.1368629855755143e-05, 8], [4.047568580760751e-09, 14], [3.57236224305268e-09, 6]], "generated": ["The Coen brothers are twin brothers.", "The Brothers Grimm were two brothers.", "Thus, they are not the same as the Coen brothers.", "So the answer is no."], "conf_comments": ["(A)\nThe Brothers Grimm", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 67.17610311508179}
{"index": 136, "question": "Could a hundred thousand lolcats fit on a first generation iPhone?", "answer": "yes", "explanation": "Yes. Lolcat memes are often in jpeg form. The average size of a jpeg is 10 to 30 kb. One gigabyte is 1000000 kb. The first iPhone had 4GB to 16GB of storage .", "generated": [{"finished": true, "length": 5, "score": [0.6476490169111468, 0.4641048437902003, 0.30130696776950444, 0.22430654941423556, 0.1826728669892547], "conf": [0.9689962298263036, 0.8931722983354085, 0.7355696959999091, 0.5540984411682511, 0.3691458351416842], "prob": [[8.109660538358666e-06, 14], [3.747440652326967e-11, 21], [3.595499559636947e-20, 31], [3.08969644333763e-23, 23], [3.007700796540506e-23, 6]], "generated": ["An iPhone 5 has a storage capacity of 32 GB.", "That equates to 32,000,000,000 bytes.", "The average file size of a lolcat is less than 50 kb, which is less than 50,000 bytes.", "Thus, 100,000 lolcats could fit in a first generation iPhone.", "So the answer is yes."], "conf_comments": ["(A)\nA lolcat", "(A)\nA lolcat", "(A)\nThus, ", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6280293568544671, 0.4140925543514108, 0.2555131681082398, 0.19399948795412564, 0.16473569402588922], "conf": [0.9607699495060473, 0.7597863951247021, 0.5658070823856949, 0.42616644712677204, 0.30823597232163985], "prob": [[7.588397317622897e-09, 21], [4.0244812066216344e-16, 28], [1.5671728668755152e-22, 22], [1.272517746580282e-24, 18], [1.2493524113075672e-24, 6]], "generated": ["The size of a lolcat image is 100px x 100px.", "The screen size of the first generation iPhone is about 3.5 inches or 320px x 480px.", "So, 100 thousand lolcats can fit on the screen of the first generation iPhone.", "Thus, a hundred thousand lolcats could fit on a first generation iPhone.", "So the answer is yes."], "conf_comments": ["(A)\nThe size of a", "(A)\nThus, the", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6476490169111468, 0.34748322455415803, 0.2741378989680942, 0.2345167056650663], "conf": [0.9689962298263036, 0.8452185982798339, 0.6734331090430888, 0.4947259563800691], "prob": [[8.109660538358666e-06, 14], [1.0492182015592703e-21, 33], [2.7973519871790567e-24, 24], [2.7338972112280964e-24, 6]], "generated": ["An iPhone 5 has a storage capacity of 32 GB.", "Given each lolcat is around 20KB, then 40,000 lolcats can fit into an iPhone 5.", "Thus, 100,000 lolcats would not fit into an iPhone 5.", "So the answer is no."], "conf_comments": ["(A)\nA lolcat", "(A)\nThus, ", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6280293568544671, 0.4140925543514108, 0.2555131681082398, 0.20523263348638046], "conf": [0.9607699495060473, 0.7597863951247021, 0.5658070823856949, 0.3980003258195602], "prob": [[7.588397317622897e-09, 21], [4.0244812066216344e-16, 28], [1.5671728668755152e-22, 22], [9.32876574659847e-23, 6]], "generated": ["The size of a lolcat image is 100px x 100px.", "The screen size of the first generation iPhone is about 3.5 inches or 320px x 480px.", "So, 100 thousand lolcats can fit on the screen of the first generation iPhone.", "So the answer is yes."], "conf_comments": ["(A)\nThe size of a", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6280293568544671, 0.49191394544957084, 0.2457763099664592, 0.19782218570858257], "conf": [0.9607699495060473, 0.7767222463257213, 0.6210143318652598, 0.4460385531430551], "prob": [[7.588397317622897e-09, 21], [2.310803581230073e-11, 21], [5.31386234777713e-24, 25], [2.8615765326454398e-24, 6]], "generated": ["The size of a lolcat image is 100px x 100px.", "The first generation iPhone has a screen resolution of 320px x 480px.", "Thus, 100 thousand lolcats could fit a little less than 2 times diagonally.", "So the answer is yes."], "conf_comments": ["(A)\nThe size of a", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 59.277968883514404}
{"index": 137, "question": "Are monks forbidden from engaging in warfare?", "answer": "no", "explanation": "No. Monks are members of religious orders that usually take vows of poverty, chastity, and obedience. The Knights Templar were a religious order that fought during the Crusades and captured Jerusalem in 1099. Buddhist Shaolin monks developed very powerful martial arts skills, have defended temples during conquests.", "generated": [{"finished": true, "length": 3, "score": [0.5985137050216799, 0.471625037867253, 0.3877224190331658], "conf": [0.9156329626007276, 0.6829687062981882, 0.4650668986506078], "prob": [[1.8034695933462536e-08, 19], [9.593272121306016e-10, 16], [9.16996635429794e-10, 6]], "generated": ["Most monks are pacifists and usually forbidden from engaging in warfare.", "Thus, most monks are forbidden from engaging in warfare.", "So the answer is yes."], "conf_comments": ["(A)\nHowever, there are", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6328586487160556, 0.4532615649219088, 0.37955118189903664], "conf": [0.9411578846484889, 0.7240400563498953, 0.5189000389615643], "prob": [[1.1566854378417963e-06, 16], [1.76588780885382e-09, 16], [1.5491515305980702e-09, 6]], "generated": ["Monks are often associated with Buddhism, a religion that promotes peace.", "Thus, monks are generally forbidden from engaging in warfare.", "So the answer is yes."], "conf_comments": ["(A)\nThus, mon", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6925509677847393, 0.3065092571759973, 0.23487948581738732, 0.1971668772640594], "conf": [0.9518305170314669, 0.7889974135908077, 0.5899775480490559, 0.4171743520603069], "prob": [[1.5043443358864076e-10, 33], [1.42177653306288e-20, 16], [2.9752303945502306e-22, 16], [2.9140322685234615e-22, 6]], "generated": ["A monk is a person who is devoted to the contemplative life, usually under vows of poverty, chastity, and obedience.", "Usually, people are forbidden from fighting during the contemplative life.", "Thus, a monk is forbidden from engaging in warfare.", "So the answer is yes."], "conf_comments": ["(A)\nThus, a", "(A)\nThus, mon", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5985137050216799, 0.27776162646188424, 0.2188814093660163, 0.18391739759931214], "conf": [0.9156329626007276, 0.8373498239267816, 0.6710550324077186, 0.47704878481677787], "prob": [[1.8034695933462536e-08, 19], [4.703386322099202e-26, 28], [7.942615522126341e-28, 16], [7.622594939335621e-28, 6]], "generated": ["Most monks are pacifists and usually forbidden from engaging in warfare.", "For example, Buddhist monk are forbidden from doing so, for the Buddhist doctrine is one of non-violence.", "Thus, monks are often forbidden from engaging in warfare.", "So the answer is yes."], "conf_comments": ["(A)\nHowever, there are", "(A)\nThus, most", "(A)\nHowever, there are", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6925509677847393, 0.3065092571759973, 0.21652033595616238, 0.17331277991827282], "conf": [0.9518305170314669, 0.7889974135908077, 0.5425793399478955, 0.35995720991572366], "prob": [[1.5043443358864076e-10, 33], [1.42177653306288e-20, 16], [8.40217382713317e-23, 16], [6.817869296376849e-23, 6]], "generated": ["A monk is a person who is devoted to the contemplative life, usually under vows of poverty, chastity, and obedience.", "Usually, people are forbidden from fighting during the contemplative life.", "Thus, monks are probably forbidden from engaging in warfare.", "So the answer is yes."], "conf_comments": ["(A)\nThus, a", "(A)\nThus, mon", "(A)\nHowever, there are", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 48.172399044036865}
{"index": 138, "question": "Is the BBC World Service hosted in Europe?", "answer": "yes", "explanation": "Yes. The BBC World Service is part of the BBC network. The BBC operates in England. England is part of Europe.", "generated": [{"finished": true, "length": 4, "score": [0.5130322579439663, 0.4067889398320448, 0.35423774330136787, 0.30074674132999085], "conf": [0.954572217934035, 0.8123662556431469, 0.6407697438031724, 0.4626478446650375], "prob": [[5.3241125237469303e-08, 13], [6.394227998728713e-09, 7], [3.986606451874115e-09, 12], [3.94625604995403e-09, 6]], "generated": ["The BBC World Service is a radio programme from the United Kingdom.", "The United Kingdom is in Europe.", "Thus, the BBC World Service is hosted in Europe.", "So the answer is yes."], "conf_comments": ["(A)\nThe United Kingdom is", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5398263233434046, 0.4704317294265537, 0.40570658062226567], "conf": [0.886260115696812, 0.72447959754272, 0.5399734655255295], "prob": [[2.359565514939542e-11, 22], [9.75155200249001e-12, 12], [9.629007751686454e-12, 6]], "generated": ["The BBC World Service is a commercial radio station in the United Kingdom, and the UK is part of Europe.", "Thus, the BBC World Service is hosted in Europe.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.6558193100516346, 0.5283185955409126], "conf": [0.940723270620565, 0.7777366688535406], "prob": [[3.6450559781060335e-06, 16], [1.994971394833659e-07, 12]], "generated": ["BBC World Service is a British international broadcaster based in the UK.", "Thus, the BBC World Service is hosted in Europe."], "conf_comments": ["(A)\nThe UK is in", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5398263233434046, 0.38050879473411336, 0.3230658351993668], "conf": [0.886260115696812, 0.6906480023518303, 0.4991953028481884], "prob": [[2.359565514939542e-11, 22], [1.6696100135044667e-13, 11], [1.643042003793659e-13, 6]], "generated": ["The BBC World Service is a commercial radio station in the United Kingdom, and the UK is part of Europe.", "Thus, the BBC World Service hosted in Europe.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6750472056653435, 0.3829665182918794, 0.17981867450229355, 0.15079833998495937, 0.1260639674339375], "conf": [0.9510448526023351, 0.8523153210204478, 0.5223612881231562, 0.38630542076240404, 0.2705578719071533], "prob": [[2.143951322583108e-08, 24], [1.2808110537398058e-10, 5], [1.6719957604918805e-15, 11], [8.696880903101004e-16, 13], [8.584602950117533e-16, 6]], "generated": ["The BBC World Service is a British Broadcasting Corporation (BBC) radio station that broadcasts news internationally.", "Europe is a continent.", "The BBC World Service is not hosted by a continent.", "Thus, the BBC World Service is not hosted in Europe.", "So the answer is no."], "conf_comments": ["(A)\nThe BBC World Service", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 44.778645515441895}
{"index": 139, "question": "Would 1996 leap year baby technically be 1 year old in 2000?", "answer": "yes", "explanation": "Yes. A leap year happens once every 4 years and has 29 days in February. The years 1996, 2000, 2004, 2008, 2012, 2016, and 2020 are the last 7 leap years that have happened. 1996 to 2000 is one leap year.", "generated": [{"finished": true, "length": 4, "score": [0.7641913400473511, 0.49388519644179146, 0.3720035361242477, 0.31397788792788084], "conf": [0.9383033355082387, 0.8516171569446068, 0.656140253297268, 0.4708597213618632], "prob": [[0.00542838447085184, 11], [4.30754878075926e-12, 27], [4.4353241580820465e-16, 30], [4.2440467264967847e-16, 6]], "generated": ["1996 was not a leap year.", "This means that January 1, 1996 and January 1, 2000 were the same date.", "Thus, a 1996 leap year baby would technically be 1 year old in the year 2000.", "So the answer is yes."], "conf_comments": ["(A)\nThus, ", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7641913400473511, 0.49388519644179146, 0.3543459754177564, 0.29020036217370965], "conf": [0.9383033355082387, 0.8516171569446068, 0.6578546031090757, 0.48508485156869197], "prob": [[0.00542838447085184, 11], [4.30754878075926e-12, 27], [1.6872020431438807e-16, 25], [9.556209715402476e-17, 6]], "generated": ["1996 was not a leap year.", "This means that January 1, 1996 and January 1, 2000 were the same date.", "So a 1996 leap year baby would still be 1 year old in 2000.", "So the answer is yes."], "conf_comments": ["(A)\nThus, ", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.711803198555082, 0.42009460648938085, 0.1962076475835502, 0.13002714955595213, 0.10193359777485012, 0.07687800947548443, 0.06529360875606395], "conf": [0.9640186684392362, 0.8808142176995235, 0.7733533627974079, 0.6363333635393024, 0.5761039567079981, 0.47699911086311836, 0.3524946515236256], "prob": [[6.44935664135471e-05, 15], [4.180912554795675e-09, 10], [1.3517826192441747e-20, 19], [8.915947061820074e-26, 19], [5.793265570741342e-28, 13], [1.4261621897796856e-30, 16], [1.23362071992701e-30, 6]], "generated": ["A leap year is a year when February has 29 days.", "Other years, February has 28 days.", "Years with 29 days in February are used to match the calendar with the seasons.", "If a year has 29 days in February, then it is a leap year.", "The year 1996 was a leap year.", "Thus, the year 1996 was a leap year.", "So the answer is yes."], "conf_comments": ["(A)\nIn 19", "(A)\nIn 19", "(A)\nThe year 1", "(A)\nIn 19", "(A)\nThus, the", "(A)\nThe year 2", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.711803198555082, 0.5272772821389037, 0.27029264710462303, 0.1553289560519254, 0.12949325985942817], "conf": [0.9640186684392362, 0.9028150934404124, 0.7913256811327323, 0.6179272274655256, 0.4420643830509504], "prob": [[6.44935664135471e-05, 15], [1.0130676873464325e-10, 25], [7.588459077279692e-25, 27], [3.602304304255446e-40, 41], [3.028503789661973e-40, 6]], "generated": ["A leap year is a year when February has 29 days.", "1996 was a leap year, which means February of 1996 had 29 days.", "Thus, a baby born in 1996 started his/her life 29 days later than a general year.", "In this way, a baby born on February 29th of 1996 would be technically 1 year old once February 29th of 2000 comes.", "So the answer is yes."], "conf_comments": ["(A)\nIn 19", "(A)\nThus, a", "(A)\nIn 20", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.711803198555082, 0.42009460648938085, 0.1962076475835502, 0.15434721328748965, 0.09749526465597182, 0.08350226224827179], "conf": [0.9640186684392362, 0.8808142176995235, 0.7733533627974079, 0.7038885938985538, 0.5832659945548607, 0.43098058241470166], "prob": [[6.44935664135471e-05, 15], [4.180912554795675e-09, 10], [1.3517826192441747e-20, 19], [1.8924380378541887e-24, 23], [5.098041725746193e-33, 27], [4.8801563275731076e-33, 6]], "generated": ["A leap year is a year when February has 29 days.", "Other years, February has 28 days.", "Years with 29 days in February are used to match the calendar with the seasons.", "2000 was not a leap year, so it did not have 29 days in February.", "Thus, a leap year baby would not be a year old in the non-leap year (2000).", "So the answer is no."], "conf_comments": ["(A)\nIn 19", "(A)\nIn 19", "(A)\nThe year 1", "(A)\nThus, ", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 70.24189257621765}
{"index": 140, "question": "Did Clark Gable marry more women once than Richard Burton?", "answer": "yes", "explanation": "Yes. Richard Burton was married to Elizabeth Taylor twice.  Richard Burton married Sally Burton, Suzy Hunt, and Sybil Williams once. Clark Gable was married to five different women, one time each.", "generated": [{"finished": true, "length": 4, "score": [0.793594421564736, 0.6347788468408496, 0.4406043411844195, 0.35797286661841216], "conf": [0.9752449862770456, 0.8186800593076942, 0.5779285827755767, 0.3980685166385582], "prob": [[0.012613478297696424, 10], [0.0008342703273914302, 10], [1.2483213168072214e-05, 11], [9.670312314138607e-06, 6]], "generated": ["Clark Gable was married 5 times.", "Richard Burton was married 6 times.", "Thus, Clark Gable married more women once.", "So the answer is yes."], "conf_comments": ["(A)\nRichard Burton", "(A)\nThus, Richard", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7997518102271214, 0.6978393231036876, 0.4409684028586403, 0.3301956007734532], "conf": [0.9763257539059665, 0.8167334921136085, 0.5828345771376243, 0.36592034873712853], "prob": [[0.022224984302609625, 9], [0.00952521090661708, 9], [1.5410543123587913e-07, 19], [7.818729071086285e-08, 6]], "generated": ["Clark Gable married 5 times.", "Richard Burton married 5 times.", "Thus, both Clark Gable and Richard Burton married the same number of women twice.", "So the answer is yes."], "conf_comments": ["(A)\nRichard Burton", "(A)\nThus, both", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.818229876281426, 0.5350824357348449, 0.4351327445590608], "conf": [0.9252631725276781, 0.5988606553556022, 0.39915316531213013], "prob": [[0.004085560238737621, 17], [1.0276755427035887e-06, 20], [9.803611457635322e-07, 6]], "generated": ["Clark Gable married 5 times and Richard Burton married 5 times.", "Thus, it is not true that Clark Gable married more women once than Richard Burton.", "So the answer is no."], "conf_comments": ["(A)\nClark Gable", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.793594421564736, 0.4775120339805354, 0.3857351565802466, 0.32045607872593423], "conf": [0.9752449862770456, 0.8261469160469322, 0.6009014353149765, 0.4176240549174124], "prob": [[0.012613478297696424, 10], [4.685644078744725e-07, 12], [9.198712176440348e-08, 15], [8.822363272654711e-08, 6]], "generated": ["Clark Gable was married 5 times.", "To compare, Richard Burton was married 5 times.", "Thus, Clark Gable married more women once than Richard Burton.", "So the answer is yes."], "conf_comments": ["(A)\nRichard Burton", "(A)\nThus, Clark", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7997518102271214, 0.6978393231036876, 0.4409684028586403, 0.30998510357498943], "conf": [0.9763257539059665, 0.8167334921136085, 0.5828345771376243, 0.3414950909605057], "prob": [[0.022224984302609625, 9], [0.00952521090661708, 9], [1.5410543123587913e-07, 19], [5.5460430683221586e-08, 6]], "generated": ["Clark Gable married 5 times.", "Richard Burton married 5 times.", "Thus, both Clark Gable and Richard Burton married the same number of women twice.", "So the answer is no."], "conf_comments": ["(A)\nRichard Burton", "(A)\nThus, both", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.793594421564736, 0.6347788468408496, 0.40517288201453006, 0.3096890201395664], "conf": [0.9752449862770456, 0.8186800593076942, 0.5778213757235988, 0.3495026789001867], "prob": [[0.012613478297696424, 10], [0.0008342703273914302, 10], [1.1417848790135346e-06, 12], [9.269751796325992e-07, 6]], "generated": ["Clark Gable was married 5 times.", "Richard Burton was married 6 times.", "Thus, Clark Gable married more women only once.", "So the answer is no."], "conf_comments": ["(A)\nRichard Burton", "(A)\nThus, Richard", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.793594421564736, 0.6347788468408496, 0.4406043411844195, 0.29309486400509877], "conf": [0.9752449862770456, 0.8186800593076942, 0.5779285827755767, 0.3356633505390613], "prob": [[0.012613478297696424, 10], [0.0008342703273914302, 10], [1.2483213168072214e-05, 11], [2.441524661858536e-06, 6]], "generated": ["Clark Gable was married 5 times.", "Richard Burton was married 6 times.", "Thus, Clark Gable married more women once.", "So the answer is no."], "conf_comments": ["(A)\nRichard Burton", "(A)\nThus, Richard", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.793594421564736, 0.6347788468408496, 0.40517288201453006, 0.2775616935798501], "conf": [0.9752449862770456, 0.8186800593076942, 0.5778213757235988, 0.3731554855492928], "prob": [[0.012613478297696424, 10], [0.0008342703273914302, 10], [1.1417848790135346e-06, 12], [1.6812675343654822e-07, 6]], "generated": ["Clark Gable was married 5 times.", "Richard Burton was married 6 times.", "Thus, Clark Gable married more women only once.", "So the answer is yes."], "conf_comments": ["(A)\nRichard Burton", "(A)\nThus, Richard", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 49.23744511604309}
{"index": 141, "question": "Is one blast from double-barreled shotgun likely to kill all squid brains?", "answer": "no", "explanation": "No. A double-barreled shotgun fires two rounds in one single blast. Squids have three brains.", "generated": [{"finished": true, "length": 3, "score": [0.5888739058583351, 0.3949185523144765, 0.3233513943219043], "conf": [0.9325128827782944, 0.6644188723301614, 0.4513172048478838], "prob": [[0.0003656943430482655, 8], [9.29504623730506e-10, 28], [8.590499064126525e-10, 6]], "generated": ["Squid only have one brain.", "Thus, it would be easy for one blast from a double-barreled shotgun to kill all squid brains.", "So the answer is yes."], "conf_comments": ["(A)\nThus, one", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5209978713440542, 0.3179819379770786, 0.2563596123261333], "conf": [0.9510736158694508, 0.7514987015456762, 0.5285858499323264], "prob": [[4.505028200567844e-11, 19], [1.3878174252120738e-18, 23], [8.641206758373597e-19, 6]], "generated": ["The central nervous system of a squid is split into two separate nerve cords.", "Thus, if both nerve cords are destroyed, all squid brains are likely to be killed.", "So the answer is yes."], "conf_comments": ["(A)\nThus, a", "(A)\nA double-bar", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 7, "score": [0.5759037851019814, 0.3187236144147539, 0.18073270270796568, 0.08815582216835173, 0.06830219861223336, 0.04758206783903286, 0.039903567509299444], "conf": [0.9589457214602736, 0.8641112785031396, 0.7220009681820444, 0.5499369324151562, 0.4656882225777397, 0.35054779816569503, 0.25732226181913864], "prob": [[1.0269560106956436e-12, 26], [8.301191826284362e-19, 13], [2.2760736472482948e-23, 11], [6.13310786907882e-30, 13], [3.16628923347537e-33, 22], [1.3060257375285389e-37, 23], [1.0101642428173773e-37, 6]], "generated": ["A blast from a double-barreled shotgun can be equivalent to 16.7 shotgun shells.", "Each shotgun shell is equivalent to 56 pellets.", "Each shotgun pellet can kill a squid.", "So 1 shotgun shell could kill 1 squid.", "In comparison, 16.7 shotgun shells could kill 16.7 squid.", "In other words, 16.7 shotgun shells is likely to kill all squid brains.", "So the answer is yes."], "conf_comments": ["(A)\nA shotgun shell", "(A)\nEach pellet is", "(A)\nThus, one", "(A)\nThus, ", "(A)\nThus, ", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.5759037851019814, 0.3187236144147539, 0.18073270270796568, 0.08815582216835173, 0.06830219861223336, 0.04458016833051743, 0.03519193330797618], "conf": [0.9589457214602736, 0.8641112785031396, 0.7220009681820444, 0.5499369324151562, 0.4656882225777397, 0.3055733481354341, 0.1993899022867072], "prob": [[1.0269560106956436e-12, 26], [8.301191826284362e-19, 13], [2.2760736472482948e-23, 11], [6.13310786907882e-30, 13], [3.16628923347537e-33, 22], [4.1955603526076046e-38, 26], [3.183324481388644e-38, 6]], "generated": ["A blast from a double-barreled shotgun can be equivalent to 16.7 shotgun shells.", "Each shotgun shell is equivalent to 56 pellets.", "Each shotgun pellet can kill a squid.", "So 1 shotgun shell could kill 1 squid.", "In comparison, 16.7 shotgun shells could kill 16.7 squid.", "Thus, one blast from a double-barreled shotgun has a probability of killing all squid brains.", "So the answer is yes."], "conf_comments": ["(A)\nA shotgun shell", "(A)\nEach pellet is", "(A)\nThus, one", "(A)\nThus, ", "(A)\nThus, ", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7569574701215075, 0.4600269027058588, 0.20714863639595962, 0.10784233328117239, 0.08475176386611324], "conf": [0.9604260193953278, 0.8707524276236571, 0.7664721785539412, 0.39256459486083206, 0.2469330113520183], "prob": [[0.00015365393984026864, 17], [5.326499081274555e-10, 14], [2.738985911579135e-18, 13], [9.435841647975225e-26, 27], [8.454465107712613e-26, 6]], "generated": ["A double-barreled shotgun is a gun with two barrels.", "This means that the shotgun can fire two shots at once.", "A brain is a structure inside the head of a squid.", "Thus, two shots from a double-barreled shotgun would probably not kill all brains of a squid.", "So the answer is no."], "conf_comments": ["(A)\nA squid has", "(A)\nA squid has", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.5759037851019814, 0.3187236144147539, 0.18073270270796568, 0.08815582216835173, 0.06830219861223336, 0.04458016833051743, 0.01766806457644378], "conf": [0.9589457214602736, 0.8641112785031396, 0.7220009681820444, 0.5499369324151562, 0.4656882225777397, 0.3055733481354341, 0.1657969301010376], "prob": [[1.0269560106956436e-12, 26], [8.301191826284362e-19, 13], [2.2760736472482948e-23, 11], [6.13310786907882e-30, 13], [3.16628923347537e-33, 22], [4.1955603526076046e-38, 26], [5.020637498622917e-44, 11]], "generated": ["A blast from a double-barreled shotgun can be equivalent to 16.7 shotgun shells.", "Each shotgun shell is equivalent to 56 pellets.", "Each shotgun pellet can kill a squid.", "So 1 shotgun shell could kill 1 squid.", "In comparison, 16.7 shotgun shells could kill 16.7 squid.", "Thus, one blast from a double-barreled shotgun has a probability of killing all squid brains.", "So the answer is it’s hard to say."], "conf_comments": ["(A)\nA shotgun shell", "(A)\nEach pellet is", "(A)\nThus, one", "(A)\nThus, ", "(A)\nThus, ", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 62.51280164718628}
{"index": 142, "question": "Do women often need new shoes during their pregnancy?", "answer": "yes", "explanation": "Yes. Pregnancy can cause swelling in the feet and legs. For safety and comfort, one needs to get new shoes if the size of their feet change.", "generated": [{"finished": true, "length": 4, "score": [0.5271797575506185, 0.30435983000838435, 0.21023167474645443, 0.17202737565776186], "conf": [0.9579221941410763, 0.8409599429087525, 0.60729457892392, 0.41005290340925915], "prob": [[2.993787915165736e-08, 14], [1.4702904068236207e-14, 15], [1.9380445565952498e-17, 16], [1.842951069125338e-17, 6]], "generated": ["Certain parts of the body can swell during pregnancy.", "Shoes are on the feet, which is one of those parts.", "Thus, women might often need new shoes during their pregnancy.", "So the answer is yes."], "conf_comments": ["(A)\nThe feet are one", "(A)\nThus, women", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5271797575506185, 0.30435983000838435, 0.19501566983979401, 0.16227060289453793], "conf": [0.9579221941410763, 0.8409599429087525, 0.6277746590778792, 0.43633655628845136], "prob": [[2.993787915165736e-08, 14], [1.4702904068236207e-14, 15], [1.029857707444856e-18, 16], [1.0062697063636723e-18, 6]], "generated": ["Certain parts of the body can swell during pregnancy.", "Shoes are on the feet, which is one of those parts.", "Thus, women will usually need new shoes during their pregnancy.", "So the answer is yes."], "conf_comments": ["(A)\nThe feet are one", "(A)\nThus, women", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7463529379719512, 0.31116933729388424, 0.19291045485730376, 0.15988226010189024], "conf": [0.9733324478570096, 0.7959726200859387, 0.6206429041066531, 0.4352928739244542], "prob": [[4.653721828017319e-06, 22], [1.346437916366349e-29, 35], [1.958382665357843e-35, 19], [1.7281965881189962e-35, 6]], "generated": ["According to the American Pregnancy Association, pregnancy lasts around 40 weeks.", "While there can be some variation in the length of pregnancy, there is a large amount of overlap (around 5 weeks) in standard pregnancies.", "Thus, some women will need new shoes during some point in their pregnancy.", "So the answer is yes."], "conf_comments": ["(A)\nThe average woman'", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.48748228343517563, 0.3716928437890002], "conf": [0.9100112083017947, 0.6944912099845493], "prob": [[8.375271518140239e-19, 31], [1.6367531257141973e-19, 6]], "generated": ["Pregnant women’s feet will increase by a shoe size or more by the third trimester, which makes them often need new shoes.", "So the answer is yes."], "conf_comments": ["(A)\nThus, women", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.5684167585807651, 0.31147631403828263], "conf": [0.9405576553386003, 0.8336742200774238], "prob": [[5.235600459124996e-10, 20], [1.6376999180142258e-33, 50]], "generated": ["Pregnant women need to exercise regularly to ensure the healthy development of their babies.", "According to a website by the American Pregnancy Association, it is recommended that a woman engages in about 20-30 minutes of moderate exercise (like brisk walking) on 3-5 days each week."], "conf_comments": ["(A)\nExercising", "(A)\nThus, pre"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 43.27701234817505}
{"index": 143, "question": "Are some adherents to Christianity in China historic enemies of Catholic Church?", "answer": "yes", "explanation": "Yes. Christianity in China is comprised of several different groups including: Protestants, Catholics, Evangelicals, and Orthodox Christians. Catholics have been at war with Protestants throughout history. The 1572 St. Bartholomew's Day Massacre saw thousands of Protestants killed by Catholic mobs. English Protestant rulers killed many Irish Catholics during the Reformation.", "generated": [{"finished": true, "length": 5, "score": [0.7722430551944808, 0.5141451788596721, 0.30830616656251086, 0.2587418586240208, 0.22357151518569113], "conf": [0.9712621852803225, 0.9215153256465136, 0.8089705769301281, 0.6529330680937684, 0.4886759307141672], "prob": [[4.4171919227071855e-07, 30], [1.1055723800585752e-14, 23], [9.226096328099758e-25, 26], [6.048929336800148e-26, 20], [5.961590252901882e-26, 6]], "generated": ["The Taiping Rebellion was a civil war in China that started in 1850 and ended in 1864.", "It was led by the Taiping Heavenly Kingdom, a secret society founded by Hong Xiuquan.", "The Taiping Heavenly Kingdom was anti-Christian, so its adherents would have been enemies of Catholicism.", "Thus, some adherents to Christianity in China were historic enemies of the Catholic Church.", "So the answer is yes."], "conf_comments": ["(A)\nThe Taiping", "(A)\nThe Taiping", "(A)\nThus, some", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7722430551944808, 0.5141451788596721, 0.30830616656251086, 0.2017571124331705, 0.17237604432611667], "conf": [0.9712621852803225, 0.9215153256465136, 0.8089705769301281, 0.6392699597358629, 0.4709726594045892], "prob": [[4.4171919227071855e-07, 30], [1.1055723800585752e-14, 23], [9.226096328099758e-25, 26], [8.126631188144753e-30, 19], [7.688096622891683e-30, 6]], "generated": ["The Taiping Rebellion was a civil war in China that started in 1850 and ended in 1864.", "It was led by the Taiping Heavenly Kingdom, a secret society founded by Hong Xiuquan.", "The Taiping Heavenly Kingdom was anti-Christian, so its adherents would have been enemies of Catholicism.", "Thus, some Chinese adherents to Christianity could be historic enemies of Catholic Church.", "So the answer is yes."], "conf_comments": ["(A)\nThe Taiping", "(A)\nThe Taiping", "(A)\nThus, some", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7722430551944808, 0.4578080446324999, 0.32907863311356156, 0.176790602533411, 0.14354951399873594], "conf": [0.9712621852803225, 0.9272576139387348, 0.8694303191461442, 0.6717810706560368, 0.4747749132698087], "prob": [[4.4171919227071855e-07, 30], [9.226650181087093e-16, 20], [3.676315040152151e-20, 17], [1.4341175489447597e-29, 22], [9.452253292038201e-30, 6]], "generated": ["The Taiping Rebellion was a civil war in China that started in 1850 and ended in 1864.", "It was the largest civil war in history and more than 20 million people died in it.", "The leader of the rebellion was a Christian named Hong Xiuquan.", "Thus, some Taiping Rebellion adherents might have been enemies of the Catholic Church.", "So the answer is yes."], "conf_comments": ["(A)\nThe Taiping", "(A)\nThe Taiping", "(A)\nThe Taiping", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.7722430551944808, 0.4578080446324999, 0.32907863311356156, 0.18419347878909564, 0.09733510690894968, 0.08134938798851064], "conf": [0.9712621852803225, 0.9272576139387348, 0.8694303191461442, 0.7281490783721105, 0.5749036153352001, 0.4366953461976357], "prob": [[4.4171919227071855e-07, 30], [9.226650181087093e-16, 20], [3.676315040152151e-20, 17], [7.753591605794974e-31, 25], [2.0568509547412462e-39, 19], [1.243709741030438e-39, 6]], "generated": ["The Taiping Rebellion was a civil war in China that started in 1850 and ended in 1864.", "It was the largest civil war in history and more than 20 million people died in it.", "The leader of the rebellion was a Christian named Hong Xiuquan.", "Although Hong Xiuquan was a Christian, he was not allied with adherents of the Catholic Church.", "The Christian Taiping Rebellion was one of the historic enemies of the Catholic Church.", "So the answer is yes."], "conf_comments": ["(A)\nThe Taiping", "(A)\nThe Taiping", "(A)\nThe Taiping", "(A)\nThus, some", "(A)\nThus, some", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5871441409988899, 0.32242164024366016, 0.27179215357610287], "conf": [0.9646330019985777, 0.6988976220934245, 0.5036371085792078], "prob": [[1.5511561249297876e-06, 13], [1.1324902159803817e-15, 24], [1.0412674340786382e-15, 6]], "generated": ["The Chinese Roman Catholic Church is a part of the Catholic Church.", "Thus, according to this text, some adherents to Christianity in China were historic enemies of Catholic Church.", "So the answer is yes."], "conf_comments": ["(A)\nThe Chinese Roman Catholic", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 6, "score": [0.7722430551944808, 0.4578080446324999, 0.32907863311356156, 0.18419347878909564, 0.09733510690894968, 0.058645118869422044], "conf": [0.9712621852803225, 0.9272576139387348, 0.8694303191461442, 0.7281490783721105, 0.5749036153352001, 0.4587294476473783], "prob": [[4.4171919227071855e-07, 30], [9.226650181087093e-16, 20], [3.676315040152151e-20, 17], [7.753591605794974e-31, 25], [2.0568509547412462e-39, 19], [2.9680119346711252e-46, 20]], "generated": ["The Taiping Rebellion was a civil war in China that started in 1850 and ended in 1864.", "It was the largest civil war in history and more than 20 million people died in it.", "The leader of the rebellion was a Christian named Hong Xiuquan.", "Although Hong Xiuquan was a Christian, he was not allied with adherents of the Catholic Church.", "The Christian Taiping Rebellion was one of the historic enemies of the Catholic Church.", "This demonstrates that some adherents to Christianity in China were enemies of the Catholic Church.", "So the answer is yes."], "conf_comments": ["(A)\nThe Taiping", "(A)\nThe Taiping", "(A)\nThe Taiping", "(A)\nThus, some", "(A)\nThus, some", "(A)\nSo the answer is"], "info": [[[1.4429883350709307e-07, 20], [0.836793970647939, 6]], 0.7979240961633439, "(A)\nSo the answer is", 6]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 65.57148551940918}
{"index": 144, "question": "Is Chinese successor to Chevrolet Cruze name a town far from Milan?", "answer": "no", "explanation": "No. The Chinese successor to Chevrolet Cruze is the Chevrolet Monza. Monza is a city near the north of Milan.", "generated": [{"finished": true, "length": 5, "score": [0.6756011435136928, 0.4612300574733407, 0.22741243710048809, 0.1680306250150896, 0.13956351899277886], "conf": [0.9634927485874012, 0.8490021858418406, 0.6638910671427845, 0.5050532761369426, 0.349633544396902], "prob": [[3.894365816385261e-10, 29], [2.7627917222154623e-14, 15], [1.965405831324822e-24, 20], [1.8514935377000803e-27, 21], [1.813294821211739e-27, 6]], "generated": ["Chevrolet Cruze is a car model built by Chevrolet from 2008 to 2015.", "The successor of this model is the Chevrolet Malibu.", "The town far from Milan, Italy, is Monza, which is a suburb of Milan.", "Thus, the successor to the Chevrolet Cruze is named a town far from Milan.", "So the answer is yes."], "conf_comments": ["(A)\nThe successor to Che", "(A)\nThe Chevro", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7375257238274696, 0.4842224351791576, 0.2726828753014142, 0.1585040942952777, 0.12855276888313077], "conf": [0.9537530105383395, 0.8722146517258217, 0.7191348245447123, 0.5459453158618007, 0.3757225867363781], "prob": [[4.311270861214214e-06, 22], [2.3339172229233694e-09, 10], [3.9770988994937082e-25, 38], [2.1181788924903885e-30, 15], [1.614897952178506e-30, 6]], "generated": ["Currently, the successor of Chevrolet Cruze is the Chevrolet Malibu.", "Milan is the second largest city in Italy.", "The Milan–Malpensa Airport (MXP), which is also known as the Milan-Milano Malpensa Airport, is an international airport located in Milan, Italy.", "Thus, the town farthest from Milan is Malpensa.", "So the answer is yes."], "conf_comments": ["(A)\nThe Chevro", "(A)\nThe Chevro", "(A)\nThe Milan–Mal", "(A)\nThe successor of Che", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7375257238274696, 0.4842224351791576, 0.2726828753014142, 0.1544667163264905, 0.12628709341315456], "conf": [0.9537530105383395, 0.8722146517258217, 0.7191348245447123, 0.5157666814023902, 0.3577296567985761], "prob": [[4.311270861214214e-06, 22], [2.3339172229233694e-09, 10], [3.9770988994937082e-25, 38], [1.6467727034296757e-33, 24], [1.319205109195452e-33, 6]], "generated": ["Currently, the successor of Chevrolet Cruze is the Chevrolet Malibu.", "Milan is the second largest city in Italy.", "The Milan–Malpensa Airport (MXP), which is also known as the Milan-Milano Malpensa Airport, is an international airport located in Milan, Italy.", "Thus, the airport in Milan is about the same name as the successor of Chevrolet Cruze.", "So the answer is yes."], "conf_comments": ["(A)\nThe Chevro", "(A)\nThe Chevro", "(A)\nThe Milan–Mal", "(A)\nThe Milan–Mal", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6756011435136928, 0.4612300574733407, 0.22480652870345982, 0.14755212826226685, 0.120293854400441], "conf": [0.9634927485874012, 0.8490021858418406, 0.6856336738646714, 0.47691315846720905, 0.31998810499649494], "prob": [[3.894365816385261e-10, 29], [2.7627917222154623e-14, 15], [8.68834439750529e-23, 16], [5.45865691739242e-28, 25], [5.1581524504757245e-28, 6]], "generated": ["Chevrolet Cruze is a car model built by Chevrolet from 2008 to 2015.", "The successor of this model is the Chevrolet Malibu.", "The Malibu is a town about 43 miles away from Milan.", "Thus, the name of the successor of the Chevrolet Cruze is similar to a town far from Milan.", "So the answer is yes."], "conf_comments": ["(A)\nThe successor to Che", "(A)\nThe Chevro", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7438606460010228, 0.4298823898096353, 0.22235165062571086, 0.12109690145600623, 0.09182683504985713], "conf": [0.9548929892723842, 0.8582860216975519, 0.6151299077220116, 0.33726846422814777, 0.1950909565808268], "prob": [[3.144421522904363e-05, 19], [8.275106870379341e-08, 6], [2.261410738014834e-13, 13], [7.685521708408819e-17, 13], [7.415560048229024e-17, 6]], "generated": ["The Chinese version of Chevrolet Cruze is called Buick LaCrosse.", "Milan is in Italy.", "Buick LaCrosse was invented in the US.", "Thus, it is not named a town far from Milan.", "So the answer is no."], "conf_comments": ["(A)\nLaCrosse", "(A)\nLaCrosse", "(A)\nThus, Bu", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 72.91808772087097}
{"index": 145, "question": "Could the Dominican Order hypothetically defeat Blessed Gerard's order?", "answer": "no", "explanation": "No. The Dominican Order is a Catholic group of friars that several priestly vows. Blessed Gerard was the founder of the Order of St John of Jerusalem (Knights Hospitaller).  The Order of St John of Jerusalem (Knights Hospitaller) were a well trained Catholic military order that fought in the Crusades.", "generated": [{"finished": true, "length": 5, "score": [0.6837695761884375, 0.5367313050246426, 0.2879773859812066, 0.2190754800468634, 0.18140590302757864], "conf": [0.9027045843621443, 0.7571486205790452, 0.6220957960374162, 0.48981664582629436, 0.33772457830394964], "prob": [[3.7256673209087408e-06, 19], [4.9662380749608184e-08, 14], [2.5612744743749743e-15, 16], [6.36781037212664e-17, 12], [6.158955561347572e-17, 6]], "generated": ["The Dominican Order and Blessed Gerard's order were actually the same order.", "Blessed Gerard was a member of the Dominican Order.", "After Blessed Gerard passed away, his order was named after him.", "Thus, the Dominican Order could not defeat itself.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7609719727250759, 0.5419783798950959, 0.24581057230241918, 0.17466419627988666, 0.14618033667597224], "conf": [0.9234172322846738, 0.7861970654968473, 0.615505794241233, 0.47317963954371217, 0.33521489947354594], "prob": [[5.218188937595536e-07, 31], [2.7815687660862533e-11, 19], [5.461812153146112e-20, 15], [5.253121982449842e-24, 22], [4.907436784185057e-24, 6]], "generated": ["The Dominican Order was founded in 1215 and the Blessed Gerard’s order was founded in 1220.", "Thus, the Dominican Order existed longer than Blessed Gerard's order.", "In addition, the Dominican Order has been the largest Catholic religious order.", "As a result, the Dominican Order could hypothetically defeat Blessed Gerard’s order.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nSince the Dominican", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6837695761884375, 0.5367313050246426, 0.2740713604368851, 0.2129290616990756], "conf": [0.9027045843621443, 0.7571486205790452, 0.46110167842243655, 0.2813344499696387], "prob": [[3.7256673209087408e-06, 19], [4.9662380749608184e-08, 14], [4.405186066634717e-19, 30], [4.1291665317076884e-19, 6]], "generated": ["The Dominican Order and Blessed Gerard's order were actually the same order.", "Blessed Gerard was a member of the Dominican Order.", "Thus, there could not be any kind of hypothetical way for the Dominican Order to defeat Blessed Gerard's order.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7609719727250759, 0.5419783798950959, 0.24581057230241918, 0.157576599769949, 0.12974311912447353], "conf": [0.9234172322846738, 0.7861970654968473, 0.615505794241233, 0.4108825852915777, 0.28314547001236257], "prob": [[5.218188937595536e-07, 31], [2.7815687660862533e-11, 19], [5.461812153146112e-20, 15], [2.9495474225652376e-25, 25], [2.6737167634854966e-25, 6]], "generated": ["The Dominican Order was founded in 1215 and the Blessed Gerard’s order was founded in 1220.", "Thus, the Dominican Order existed longer than Blessed Gerard's order.", "In addition, the Dominican Order has been the largest Catholic religious order.", "Thus, it is very unlikely that Blessed Gerard's order was able to defeat the Dominican Order.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSince the Dominican", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6454165407292132, 0.43542920109796734, 0.2637654802772704], "conf": [0.8735936635606019, 0.707514033065978, 0.40027113683842547], "prob": [[2.433511154016308e-11, 33], [7.590645852349097e-17, 22], [5.6515596244371384e-18, 6]], "generated": ["The Dominican Order was formed in 1215AD and the Blessed Gerard’s order was formed in 1215AD.", "Thus, the Dominican Order formed a year earlier than the Blessed Gerard’s order.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nThe Dominican Order", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 59.77193093299866}
{"index": 146, "question": "Is a pottery kiln inappropriate for use with glass blowing?", "answer": "no", "explanation": "No. Pottery kilns heat from the sides. Glass kilns heat from the top. Glass can be fused in a ceramic pottery kiln without trouble.", "generated": [{"finished": true, "length": 4, "score": [0.5865659434660025, 0.3559419205619119, 0.3057351955555182, 0.2558768453761143], "conf": [0.938953840383857, 0.7788212952426679, 0.6341272991746547, 0.4762006365063676], "prob": [[1.0564066398576848e-07, 16], [3.20919287957913e-23, 44], [4.469665940447524e-24, 20], [2.9432052539161784e-24, 6]], "generated": ["Pottery kilns and glass blowing are different forms of heating.", "While a pottery kiln is used to heat materials in order to make pottery, a glass blowing kiln is used to heat materials in order to make glass objects, such as cups and windows.", "Thus, a pottery kiln is inappropriate for use with glass blowing.", "So the answer is yes."], "conf_comments": ["(A)\nPottery kil", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5865659434660025, 0.3559419205619119, 0.3057351955555182, 0.24056240460800163], "conf": [0.938953840383857, 0.7788212952426679, 0.6341272991746547, 0.4732405634394811], "prob": [[1.0564066398576848e-07, 16], [3.20919287957913e-23, 44], [4.469665940447524e-24, 20], [1.4569002707013113e-24, 6]], "generated": ["Pottery kilns and glass blowing are different forms of heating.", "While a pottery kiln is used to heat materials in order to make pottery, a glass blowing kiln is used to heat materials in order to make glass objects, such as cups and windows.", "Thus, a pottery kiln is inappropriate for use with glass blowing.", "So the answer is no."], "conf_comments": ["(A)\nPottery kil", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5485332600997614, 0.3539209355662889, 0.28329693966864905], "conf": [0.9474334322287769, 0.7743389463775335, 0.5725954582360513], "prob": [[3.429946133086572e-10, 19], [3.19056466153257e-17, 24], [1.3501882780736185e-17, 6]], "generated": ["Pottery kilns are used to dry and oxidize pottery, not glass.", "Since glass is not made of clay, a pottery kiln is inappropriate for glass blowing.", "So the answer is yes."], "conf_comments": ["(A)\nGlass blowing", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6032893041205557, 0.28163348095929513, 0.22347041065953388, 0.18018791701591202], "conf": [0.9706358385864291, 0.7045065279971432, 0.5399455810987702, 0.38709471353866703], "prob": [[7.725723202277066e-06, 12], [5.433866952376638e-21, 29], [1.577706929003619e-22, 18], [8.775876276060886e-23, 6]], "generated": ["Pottery kilns are specifically used for pottery.", "Glass blowing is an extremely hot process, so pottery kilns may not be able to heat up enough to work with this process.", "Thus, a pottery kiln is not appropriate for use with glass blowing.", "So the answer is no."], "conf_comments": ["(A)\nGlass blowing", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5485332600997614, 0.28133068301802777, 0.21561257775595108, 0.17943298212362002], "conf": [0.9474334322287769, 0.7780094810662496, 0.6402898273731407, 0.4814521930690118], "prob": [[3.429946133086572e-10, 19], [4.793817765915253e-23, 26], [2.172890489264652e-25, 16], [1.3265286492030647e-25, 6]], "generated": ["Pottery kilns are used to dry and oxidize pottery, not glass.", "Glass blowing, on the other hand, requires a furnace or furnace-kiln for melting the glass.", "Thus, a pottery kiln cannot be used with glass blowing.", "So the answer is no."], "conf_comments": ["(A)\nGlass blowing", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5485332600997614, 0.28133068301802777, 0.21561257775595108, 0.16834712344462804], "conf": [0.9474334322287769, 0.7780094810662496, 0.6402898273731407, 0.46309177357050874], "prob": [[3.429946133086572e-10, 19], [4.793817765915253e-23, 26], [2.172890489264652e-25, 16], [7.79245587720166e-26, 6]], "generated": ["Pottery kilns are used to dry and oxidize pottery, not glass.", "Glass blowing, on the other hand, requires a furnace or furnace-kiln for melting the glass.", "Thus, a pottery kiln cannot be used with glass blowing.", "So the answer is yes."], "conf_comments": ["(A)\nGlass blowing", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5865659434660025, 0.3559419205619119, 0.19945842721075055, 0.1597515166527593], "conf": [0.938953840383857, 0.7788212952426679, 0.6229605544185542, 0.45480472919370624], "prob": [[1.0564066398576848e-07, 16], [3.20919287957913e-23, 44], [1.574068763822688e-30, 18], [7.243498185448006e-31, 6]], "generated": ["Pottery kilns and glass blowing are different forms of heating.", "While a pottery kiln is used to heat materials in order to make pottery, a glass blowing kiln is used to heat materials in order to make glass objects, such as cups and windows.", "Thus, a pottery kiln is not in proper use for glass blowing.", "So the answer is no."], "conf_comments": ["(A)\nPottery kil", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5865659434660025, 0.3559419205619119, 0.19945842721075055, 0.15943113219793484], "conf": [0.938953840383857, 0.7788212952426679, 0.6229605544185542, 0.44602449190839494], "prob": [[1.0564066398576848e-07, 16], [3.20919287957913e-23, 44], [1.574068763822688e-30, 18], [7.948471617867073e-31, 6]], "generated": ["Pottery kilns and glass blowing are different forms of heating.", "While a pottery kiln is used to heat materials in order to make pottery, a glass blowing kiln is used to heat materials in order to make glass objects, such as cups and windows.", "Thus, a pottery kiln is not in proper use for glass blowing.", "So the answer is yes."], "conf_comments": ["(A)\nPottery kil", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 51.614535331726074}
{"index": 147, "question": "Are there enough people in the Balkans to match the population of Japan?", "answer": "no", "explanation": "No. There are approximately 55 million people in the Balkans. There are more than 125 million people in Japan.", "generated": [{"finished": true, "length": 4, "score": [0.6675128642686967, 0.46418486532791464, 0.3975231650940451, 0.3413773751642438], "conf": [0.9585763012892733, 0.7597421495874934, 0.6088906324358649, 0.45017118488461316], "prob": [[1.0441857324326896e-10, 30], [1.2120204913139102e-15, 23], [2.0554510972625612e-16, 20], [2.024623467576328e-16, 6]], "generated": ["According to World Population Review, the population of the Balkans as of 2020 is 58.5 million people.", "In comparison, the population of Japan as of 2021 is around 127 million people.", "Thus, there are not enough people in the Balkans to match the population of Japan.", "So the answer is no."], "conf_comments": ["(A)\nAccording to World", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6675128642686967, 0.4364406165441804, 0.2993968033773502, 0.2596585003121614, 0.22452196419872056], "conf": [0.9585763012892733, 0.7304206725919351, 0.5375051190333405, 0.42105469007747615, 0.31547011907902134], "prob": [[1.0441857324326896e-10, 30], [3.1364658585677547e-16, 22], [3.209820250004818e-23, 36], [1.424223128082841e-23, 20], [1.4064852211816817e-23, 6]], "generated": ["According to World Population Review, the population of the Balkans as of 2020 is 58.5 million people.", "The population of Japan as of July 2020 is 125.5 million people.", "This means that the population of the Balkans is less than the population of Japan, since 58.5 million is less than 125.5 million.", "Thus, there are not enough people in the Balkans to match the population of Japan.", "So the answer is no."], "conf_comments": ["(A)\nAccording to World", "(A)\nThus, there", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6675128642686967, 0.4364406165441804, 0.2993968033773502, 0.22021135557231636, 0.18252581454993352], "conf": [0.9585763012892733, 0.7304206725919351, 0.5375051190333405, 0.37007536132045604, 0.25531936224658297], "prob": [[1.0441857324326896e-10, 30], [3.1364658585677547e-16, 22], [3.209820250004818e-23, 36], [3.2866532200856523e-25, 19], [3.2048479312624746e-25, 6]], "generated": ["According to World Population Review, the population of the Balkans as of 2020 is 58.5 million people.", "The population of Japan as of July 2020 is 125.5 million people.", "This means that the population of the Balkans is less than the population of Japan, since 58.5 million is less than 125.5 million.", "Thus, there are enough people in the Balkans to match the population of Japan.", "So the answer is yes."], "conf_comments": ["(A)\nAccording to World", "(A)\nThus, there", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6675128642686967, 0.4364406165441804, 0.2751162146539038, 0.2341600136681651], "conf": [0.9585763012892733, 0.7304206725919351, 0.5666050646069903, 0.41819458973393075], "prob": [[1.0441857324326896e-10, 30], [3.1364658585677547e-16, 22], [2.4854691312127727e-22, 21], [2.222179801562762e-22, 6]], "generated": ["According to World Population Review, the population of the Balkans as of 2020 is 58.5 million people.", "The population of Japan as of July 2020 is 125.5 million people.", "Therefore, there is clearly not enough people in the Balkans to match the population of Japan.", "So the answer is no."], "conf_comments": ["(A)\nAccording to World", "(A)\nThus, there", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7171824410221327, 0.38320272005234474, 0.2796445346944182, 0.22789085531123157], "conf": [0.9770234378581735, 0.7533020596975333, 0.5533038226866506, 0.36849828806356105], "prob": [[0.0002385646447734663, 13], [1.5126774475647594e-12, 19], [3.362057532325325e-15, 19], [3.305377497046472e-15, 6]], "generated": ["Japan has a population of about 130 million.", "The total population of the Balkans is estimated to be around 147 million.", "Thus, the population of the Balkans is enough to match the population of Japan.", "So the answer is yes."], "conf_comments": ["(A)\nThe Balkans", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 53.6756432056427}
{"index": 148, "question": "Could Steven Spielberg send emails as a child?", "answer": "no", "explanation": "No. Steven Spielberg was born in 1946. Email did not become available to the general public until 1995.", "generated": [{"finished": true, "length": 4, "score": [0.954129452012541, 0.801496631657837, 0.4403072552197485, 0.3666792569109991], "conf": [0.9808397619638792, 0.9278376883325004, 0.7731436656905565, 0.56837100585533], "prob": [[0.3793281083163712, 13], [0.01509657439681046, 11], [1.5211126826922507e-17, 34], [1.0722526274353667e-17, 6]], "generated": ["Steven Spielberg was born in 1946.", "Email was invented in 1971.", "Since 1946 is prior to 1971, it means that Steven Spielberg was a child, but could not send email at the time.", "So the answer is no."], "conf_comments": ["(A)\nIn 19", "(A)\nThus, Steven", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.954129452012541, 0.5343001516512428, 0.45784792631262994], "conf": [0.9808397619638792, 0.869155841593058, 0.671589405519434], "prob": [[0.3793281083163712, 13], [5.643761484426959e-12, 24], [4.156837067585412e-12, 6]], "generated": ["Steven Spielberg was born in 1946.", "In that time, email systems weren’t invented yet, so he could not send emails as a child.", "So the answer is no."], "conf_comments": ["(A)\nIn 19", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6713406160125491, 0.45956409585058466, 0.3260745238437252, 0.2851097150539614], "conf": [0.9553579799159542, 0.8648252208286723, 0.7365810693571906, 0.5644024331499858], "prob": [[7.313948167203335e-10, 28], [1.0027921225236231e-16, 24], [1.0780835993797581e-19, 13], [1.0636145278155645e-19, 6]], "generated": ["Steven Spielberg was born in 1946 and attended college during 1963 - 1965.", "The first email was sent in 1971 when Steven Spielberg was already in his 20s.", "Thus, Steven Spielberg could not email as a child.", "So the answer is no."], "conf_comments": ["(A)\nEmail was invented", "(A)\nThus, Steven", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.954129452012541, 0.801496631657837, 0.3733573881021047, 0.25104288914802864], "conf": [0.9808397619638792, 0.9278376883325004, 0.5017366464494897, 0.2667601348506194], "prob": [[0.3793281083163712, 13], [0.01509657439681046, 11], [1.840498575649635e-12, 25], [6.958937701264957e-13, 6]], "generated": ["Steven Spielberg was born in 1946.", "Email was invented in 1971.", "Steven Spielberg was an adult by the time email was invented, so of course he sent emails as a child.", "So the answer is no."], "conf_comments": ["(A)\nIn 19", "(A)\nThus, Steven", "(A)\n# Is the above", "(A), but the previous step is"], "info": null}, {"finished": true, "length": 4, "score": [0.954129452012541, 0.801496631657837, 0.3733573881021047, 0.24950883961922718], "conf": [0.9808397619638792, 0.9278376883325004, 0.5017366464494897, 0.27229293077900074], "prob": [[0.3793281083163712, 13], [0.01509657439681046, 11], [1.840498575649635e-12, 25], [5.716187483216921e-13, 6]], "generated": ["Steven Spielberg was born in 1946.", "Email was invented in 1971.", "Steven Spielberg was an adult by the time email was invented, so of course he sent emails as a child.", "So the answer is yes."], "conf_comments": ["(A)\nIn 19", "(A)\nThus, Steven", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 4, "score": [0.8053194696177247, 0.7042887381344581, 0.3759365940474478, 0.16959293162367955], "conf": [0.9743504750375067, 0.7933185232674956, 0.6140250436655047, 0.475234332108511], "prob": [[0.007562307928837413, 12], [0.0033533540787365986, 13], [9.489479914256123e-13, 22], [1.4992984220437654e-18, 10]], "generated": ["The first email was sent in 1971.", "Steven Spielberg was born in 1946.", "This means that he was probably around 25-27 years old when the first email was sent.", "So at the time, he could send emails.", "So the answer is yes."], "conf_comments": ["(A)\nSteven Spielberg", "(A)\nThus, Steven", "(A)\nThus, it", "(A)\n# Is the above"], "info": [[[1.5799584756919683e-06, 10], [0.5566836507588537, 6]], 0.7739657152606285, "(A)\n# Is the above", 4]}, {"finished": false, "length": 4, "score": [0.8053194696177247, 0.7042887381344581, 0.3759365940474478, 0.1161251889877075], "conf": [0.9743504750375067, 0.7933185232674956, 0.6140250436655047, 0.4405117860991655], "prob": [[0.007562307928837413, 12], [0.0033533540787365986, 13], [9.489479914256123e-13, 22], [5.034903311328695e-32, 22]], "generated": ["The first email was sent in 1971.", "Steven Spielberg was born in 1946.", "This means that he was probably around 25-27 years old when the first email was sent.", "At that age, Steven was probably too preoccupied with graduate school or early years of filmmaking.", "As a result, sending an email was probably not on his list of prioritizes.", "Thus, he probably did not send"], "conf_comments": ["(A)\nSteven Spielberg", "(A)\nThus, Steven", "(A)\nThus, it", "(A)\nThus, it"], "info": [[[5.3057737166024437e-20, 22], [1.0336631712143579e-13, 18], [0.01775575397772101, 8]], 0.7174166439034334, "(A)\nThus, it", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 49.84321403503418}
{"index": 149, "question": "Was Charles Manson's body unwanted?", "answer": "no", "explanation": "No. Charles Manson's body was debated over for so long that he had to stay on ice. Charles Manson had four people fighting over his corpse.", "generated": [{"finished": true, "length": 4, "score": [0.7374435704002339, 0.4366751245592213, 0.31245364683601307, 0.2521034935017306], "conf": [0.9622612826840672, 0.7523746412109352, 0.5372477077723521, 0.3506097459651974], "prob": [[1.994275820901785e-06, 23], [3.2623509228735683e-09, 8], [3.0954574007544195e-11, 14], [3.050335056492117e-11, 6]], "generated": ["After Charles Manson died, his body was cremated and his ashes were scattered into the Pacific Ocean.", "This indicates that his body was wanted.", "Thus, the body of Charles Manson was not unwanted.", "So the answer is no."], "conf_comments": ["(A)\nThus, his", "(A)\nThus, his", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7449572925651934, 0.41362867595691233, 0.3496373435781706], "conf": [0.9445640647485212, 0.7197886706779791, 0.5178339802213935], "prob": [[0.00020160501137042112, 16], [3.875436328581588e-09, 12], [3.7195760233097384e-09, 6]], "generated": ["According to Wikipedia, Charles Manson's body was cremated.", "Thus, his body was wanted rather than unwanted.", "So the answer is no."], "conf_comments": ["(A)\nThus, his", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.7374435704002339, 0.4366751245592213, 0.31710859171085504], "conf": [0.9622612826840672, 0.7523746412109352, 0.5432548652983992], "prob": [[1.994275820901785e-06, 23], [3.2623509228735683e-09, 8], [2.640996168402909e-10, 8]], "generated": ["After Charles Manson died, his body was cremated and his ashes were scattered into the Pacific Ocean.", "This indicates that his body was wanted.", "Thus, the answer is no."], "conf_comments": ["(A)\nThus, his", "(A)\nThus, his", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.9008191901862501, 0.40417459538274025, 0.23191499507989505, 0.1829070260465146], "conf": [0.9768824010857062, 0.8195312895569407, 0.5884253392564494, 0.3714651278770845], "prob": [[0.042693380382390234, 17], [7.12871524749352e-14, 19], [1.249412984602305e-19, 17], [1.1433116184580132e-19, 6]], "generated": ["Charles Manson died on November 19, 2017.", "Four months prior, he was sentenced to natural life with the possibility of parole.", "Thus, his body was not unwanted in November 2017.", "So the answer is no."], "conf_comments": ["(A)\nHis body was", "(A)\nThus, he", "(A)\nHowever, he was", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 8, "score": [0.9008191901862501, 0.6605775813319766, 0.33708041951298656, 0.1731747875096112, 0.08911461639485194, 0.03670964989103377, 0.019385301268280464, 0.012423643409681915], "conf": [0.9768824010857062, 0.8133764095448572, 0.656427694153982, 0.5395175421977817, 0.3989179473986588, 0.22311048294301575, 0.12668106725781764, 0.07105493372498847], "prob": [[0.042693380382390234, 17], [2.838370509674801e-06, 22], [1.741659741929957e-22, 33], [8.344888267998337e-36, 27], [1.211832473818055e-53, 40], [6.789056066905901e-61, 14], [6.566242673215001e-65, 13], [1.012369030530948e-65, 6]], "generated": ["Charles Manson died on November 19, 2017.", "Charles Manson's body was cremated on March 9, 2018.", "According to Wikipedia, Charles Manson was from the Baby Boom generation, which was people born from 1946 through 1964.", "The average lifespan for the US is 78.6 years for women and 73.8 years for men.", "The 1946 birth year is before the average lifespan of both women and men, so it was highly plausible for Charles Manson's body to be unwanted.", "However, there is no proof that his body was actually unwanted.", "Thus, his body could have been unwanted or not.", "So the answer is no."], "conf_comments": ["(A)\nHis body was", "(A)\nThus, Charles", "(A)\nThus, Charles", "(A)\nThus, Charles", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 8, "score": [0.9008191901862501, 0.6605775813319766, 0.33708041951298656, 0.1731747875096112, 0.08911461639485194, 0.03670964989103377, 0.019385301268280464, 0.007899787733853268], "conf": [0.9768824010857062, 0.8133764095448572, 0.656427694153982, 0.5395175421977817, 0.3989179473986588, 0.22311048294301575, 0.12668106725781764, 0.0715317544936466], "prob": [[0.042693380382390234, 17], [2.838370509674801e-06, 22], [1.741659741929957e-22, 33], [8.344888267998337e-36, 27], [1.211832473818055e-53, 40], [6.789056066905901e-61, 14], [6.566242673215001e-65, 13], [1.2497033207770182e-68, 7]], "generated": ["Charles Manson died on November 19, 2017.", "Charles Manson's body was cremated on March 9, 2018.", "According to Wikipedia, Charles Manson was from the Baby Boom generation, which was people born from 1946 through 1964.", "The average lifespan for the US is 78.6 years for women and 73.8 years for men.", "The 1946 birth year is before the average lifespan of both women and men, so it was highly plausible for Charles Manson's body to be unwanted.", "However, there is no proof that his body was actually unwanted.", "Thus, his body could have been unwanted or not.", "So the answer is not exactly."], "conf_comments": ["(A)\nHis body was", "(A)\nThus, Charles", "(A)\nThus, Charles", "(A)\nThus, Charles", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": []}, "run_time": 65.14083981513977}
{"index": 150, "question": "Would a Beaver's teeth rival that of a Smilodon?", "answer": "no", "explanation": "No. A beaver has teeth measuring 25 mm on average. The Smilodon was a prehistoric saber-toothed tiger. The Smilodon's teeth were around 11 inches long.", "generated": [{"finished": true, "length": 3, "score": [0.6260315530165167, 0.4317273162084053, 0.3429377124656558], "conf": [0.8963589039479327, 0.5476729904061182, 0.3469990041923518], "prob": [[7.799713528964909e-07, 17], [8.579237381284943e-09, 18], [8.369044463777943e-09, 6]], "generated": ["A Smilodon’s teeth are longer than a beaver’s teeth.", "Thus, a beaver’s teeth cannot rival that of a Smilodon.", "So the answer is no."], "conf_comments": ["(A)\nA beaver’", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5513081975645227, 0.3938999704788035, 0.3189411905341306], "conf": [0.88092838441151, 0.6686200032405649, 0.44889541197262917], "prob": [[2.3042982104434022e-17, 36], [3.4624000656772254e-22, 28], [3.002468035630715e-22, 6]], "generated": ["The weighted average length of Beaver's teeth is roughly 6 mm and the weighted average length of Smilodon's teeth is roughly 68 mm.", "Thus, a Beaver's teeth are less than 1/10 the length of a Smilodon's teeth.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.5999494642898322, 0.3735308282721303, 0.22526651840634468, 0.12204999113658087, 0.09481628563544264, 0.08169614534606696], "conf": [0.9643065781579753, 0.8965796665493095, 0.8132186703561016, 0.6101955460827904, 0.49163731295860963, 0.3665127062350246], "prob": [[3.9501268859269285e-19, 43], [1.72864883776901e-27, 22], [2.2231796396571454e-41, 35], [9.389586667675994e-51, 23], [3.8763526641169e-53, 19], [3.780781852987876e-53, 6]], "generated": ["Smilodon (also known as a sabretooth cat) is an extinct genus of machairodonts that went extinct around 11,700 BP in North America.", "Smilodon is a carnivorous mammal and is known for its long canine teeth.", "The canines of Smilodon were about 7 inches (17.8 cm) and were the longest known canine teeth of any terrestrial mammals.", "In comparison, a North American beaver (Castor canadensis) has teeth about 8 mm in length.", "Thus, a Beaver's teeth do not rival that of a Smilodon.", "So the answer is no."], "conf_comments": ["(A)\nBeavers are", "(A)\nBeavers are", "(A)\nBeavers are", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.5999494642898322, 0.3735308282721303, 0.22526651840634468, 0.12204999113658087, 0.08769422965462675, 0.07473457803914976], "conf": [0.9643065781579753, 0.8965796665493095, 0.8132186703561016, 0.6101955460827904, 0.49315564431008924, 0.36029658909847995], "prob": [[3.9501268859269285e-19, 43], [1.72864883776901e-27, 22], [2.2231796396571454e-41, 35], [9.389586667675994e-51, 23], [1.2013722246103142e-54, 20], [1.1593846975143984e-54, 6]], "generated": ["Smilodon (also known as a sabretooth cat) is an extinct genus of machairodonts that went extinct around 11,700 BP in North America.", "Smilodon is a carnivorous mammal and is known for its long canine teeth.", "The canines of Smilodon were about 7 inches (17.8 cm) and were the longest known canine teeth of any terrestrial mammals.", "In comparison, a North American beaver (Castor canadensis) has teeth about 8 mm in length.", "Thus, the teeth of a beaver does not rival the teeth of a Smilodon.", "So the answer is no."], "conf_comments": ["(A)\nBeavers are", "(A)\nBeavers are", "(A)\nBeavers are", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.5999494642898322, 0.3735308282721303, 0.22526651840634468, 0.08635485883111607, 0.06608141085662707, 0.057622228667538154], "conf": [0.9643065781579753, 0.8965796665493095, 0.8132186703561016, 0.5827344923487645, 0.4773227700719956, 0.3649813627729324], "prob": [[3.9501268859269285e-19, 43], [1.72864883776901e-27, 22], [2.2231796396571454e-41, 35], [9.151207071896159e-54, 18], [1.5563502267510878e-56, 19], [1.504821733436681e-56, 6]], "generated": ["Smilodon (also known as a sabretooth cat) is an extinct genus of machairodonts that went extinct around 11,700 BP in North America.", "Smilodon is a carnivorous mammal and is known for its long canine teeth.", "The canines of Smilodon were about 7 inches (17.8 cm) and were the longest known canine teeth of any terrestrial mammals.", "By comparison, the teeth of the beaver can grow around 1 inch long.", "Thus, the teeth of a beaver could not rival that of a Smilodon.", "So the answer is no."], "conf_comments": ["(A)\nBeavers are", "(A)\nBeavers are", "(A)\nBeavers are", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 65.54787254333496}
{"index": 151, "question": "Is dysphoria around one's pelvis treatable without surgery?", "answer": "yes", "explanation": "Yes. For individuals experiencing dysphoria around having a vagina, packers can relieve symptoms. For people experiencing dysphoria about having a penis, there are tucking underwear and comfort gaffs available.", "generated": [{"finished": true, "length": 4, "score": [0.6215000392150701, 0.3082309198072812, 0.24657629140033036, 0.20199280697331445], "conf": [0.9455961989836728, 0.8057571312779258, 0.6096263287514208, 0.4117408319169834], "prob": [[4.095957636170365e-08, 19], [4.5843197955567514e-21, 24], [1.152576242391799e-22, 22], [1.1089791594915372e-22, 6]], "generated": ["Dysphoria around one's pelvis can be due to anxiety disorders.", "Several types anxiety disorders are treatable without surgery, like posttraumatic stress disorder.", "Thus, it is possible to treat dysphoria around one's pelvis without surgery.", "So the answer is yes."], "conf_comments": ["(A)\nAnxiety dis", "(A)\nThus, d", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.550645683311965, 0.2750371028050102, 0.22532618947328065, 0.18940710422662013], "conf": [0.9242444448819088, 0.716852474637556, 0.5577566738846321, 0.3961142466891827], "prob": [[2.5995510781888827e-13, 26], [4.638029428164164e-30, 34], [2.0829318630034225e-31, 21], [2.0203938870801725e-31, 6]], "generated": ["Pelvic deformity can be caused by trauma, infection, surgery, and congenital defects.", "Treatment of pelvic deformities usually require surgery, but sometimes nonsurgical methods such as physical therapy and orthotics could help.", "Thus, dysphoria around one's pelvis can be treatable without surgery.", "So the answer is yes."], "conf_comments": ["(A)\nPelvic de", "(A)\nThus, d", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6215000392150701, 0.3082309198072812, 0.234015349059403, 0.18832211703231352], "conf": [0.9455961989836728, 0.8057571312779258, 0.6073335265039814, 0.39766196302985846], "prob": [[4.095957636170365e-08, 19], [4.5843197955567514e-21, 24], [7.338230309030546e-24, 24], [6.86996254788334e-24, 6]], "generated": ["Dysphoria around one's pelvis can be due to anxiety disorders.", "Several types anxiety disorders are treatable without surgery, like posttraumatic stress disorder.", "Thus, some types of dysphoria around one's pelvis can be treatable without surgery.", "So the answer is yes."], "conf_comments": ["(A)\nAnxiety dis", "(A)\nThus, d", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.6518589781593912, 0.3148716655801034, 0.15846047434615654, 0.0766409644380153, 0.06315173611632839, 0.0535343027477558], "conf": [0.9588887382692622, 0.8556334654979915, 0.6433455198506038, 0.4794525306283388, 0.38558212754703386, 0.2778817172057364], "prob": [[4.9898360287086836e-06, 15], [1.9916523506433905e-19, 23], [5.058419597782354e-35, 33], [4.361024714920307e-45, 20], [1.4761696474921557e-46, 20], [1.450908666725042e-46, 6]], "generated": ["Dysphoria is a medical term for a feeling of unease.", "Around one’s pelvis is the area around the lower abdomen – typically the groin area.", "The discomfort around the lower abdomen can be due to a variety of causes such as constipation, gastric reflux, etc.", "If surgery is not needed, treatment may include laxatives, antacids, etc.", "Thus, dysphoria around one's pelvis is treatable without surgery.", "So the answer is yes."], "conf_comments": ["(A)\nDysphoria", "(A)\nThus, d", "(A)\nThus, d", "(A)\nThus, d", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.6518589781593912, 0.3148716655801034, 0.15846047434615654, 0.0766409644380153, 0.05704012721628293, 0.048565878770930666], "conf": [0.9588887382692622, 0.8556334654979915, 0.6433455198506038, 0.4794525306283388, 0.3844382312182749, 0.2797418080237463], "prob": [[4.9898360287086836e-06, 15], [1.9916523506433905e-19, 23], [5.058419597782354e-35, 33], [4.361024714920307e-45, 20], [7.456372131165275e-47, 11], [7.290409515889709e-47, 6]], "generated": ["Dysphoria is a medical term for a feeling of unease.", "Around one’s pelvis is the area around the lower abdomen – typically the groin area.", "The discomfort around the lower abdomen can be due to a variety of causes such as constipation, gastric reflux, etc.", "If surgery is not needed, treatment may include laxatives, antacids, etc.", "Thus, it is treatable without surgery.", "So the answer is yes."], "conf_comments": ["(A)\nDysphoria", "(A)\nThus, d", "(A)\nThus, d", "(A)\nThus, d", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 64.79050493240356}
{"index": 152, "question": "Would members of Blue Lives Matter support every element of Grand Theft Auto III?", "answer": "no", "explanation": "No. Blue Lives Matter is a countermovement in the United States that supports police officers and law enforcement personnel. Grand Theft Auto III allows for gratuitous violence against police officers in the game.", "generated": [{"finished": true, "length": 4, "score": [0.6497225034697961, 0.4427216435596908, 0.3189916398786183, 0.25795437288782075], "conf": [0.968113931401645, 0.7686779101385213, 0.5324404068154452, 0.35024999495238723], "prob": [[8.982782333117698e-06, 14], [1.6796354115284526e-09, 16], [3.93908434003489e-12, 21], [3.8011552893154846e-12, 6]], "generated": ["Blue Lives Matter is a political movement to protect police officers.", "Grand Theft Auto III is a video game where players steal cars.", "Thus, Blue Lives Matter members would not support every element of Grand Theft Auto III.", "So the answer is no."], "conf_comments": ["(A)\nGrand Theft", "(A)\nThus, members", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6640087910381771, 0.3475606576735134, 0.27808404534253167, 0.23066224988967052], "conf": [0.9700717928556114, 0.7332681082538911, 0.53583851666052, 0.37065268439034893], "prob": [[7.771867373714048e-05, 12], [2.6986469818848117e-16, 26], [1.4675115558833124e-17, 22], [1.4209788503238847e-17, 6]], "generated": ["Blue Lives Matter aims to protect police officers.", "Grand Theft Auto III is an open-world video game in which players can commit criminal activities in a metropolitan city.", "Thus, members of Blue Lives Matter would not support every element of Grand Theft Auto III.", "So the answer is no."], "conf_comments": ["(A)\nGrand Theft", "(A)\nThus, members", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7390263210883152, 0.34324717132225885, 0.1682507860438885, 0.12112235960210921, 0.09902457112585561], "conf": [0.9693704707312811, 0.7555011433587895, 0.5651353442516684, 0.4040806839399408, 0.27120808807075336], "prob": [[0.0003248032582873933, 14], [2.6851990991909617e-24, 36], [1.2547281781764816e-36, 25], [1.4559447225066261e-39, 21], [1.4202388091735634e-39, 6]], "generated": ["Members of Blue Lives Matter are police supporters.", "Grand Theft Auto (GTA) III is a video game which simulates the life as a gangster, making money by stealing and committing crimes.", "Members of Blue Lives Matter would likely be against and disapprove of everything that GTA III stands for.", "Thus, not all members of Blue Lives Matter would support every element of GTA III.", "So the answer is no."], "conf_comments": ["(A)\nGrand Theft", "(A)\nThus, members", "(A)\nThus, members", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6497225034697961, 0.4427216435596908, 0.1938933401347095, 0.15225642602445347], "conf": [0.968113931401645, 0.7686779101385213, 0.44232296420086814, 0.2761917065184157], "prob": [[8.982782333117698e-06, 14], [1.6796354115284526e-09, 16], [2.4458032555614783e-23, 29], [2.2685455862467208e-23, 6]], "generated": ["Blue Lives Matter is a political movement to protect police officers.", "Grand Theft Auto III is a video game where players steal cars.", "These two concepts have no relationship and, therefor, Blue Lives Matter would not support every element of Grand Theft Auto III.", "So the answer is no."], "conf_comments": ["(A)\nGrand Theft", "(A)\nThus, members", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6640087910381771, 0.3475606576735134, 0.1920301166940905, 0.14643399603432275], "conf": [0.9700717928556114, 0.7332681082538911, 0.5163191310397223, 0.3449441843641799], "prob": [[7.771867373714048e-05, 12], [2.6986469818848117e-16, 26], [3.4242466436016815e-23, 19], [1.48885428705381e-23, 6]], "generated": ["Blue Lives Matter aims to protect police officers.", "Grand Theft Auto III is an open-world video game in which players can commit criminal activities in a metropolitan city.", "Members of Blue lives Matter would not support every element of Grand Theft Auto III.", "So the answer is no."], "conf_comments": ["(A)\nGrand Theft", "(A)\nThus, members", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.692763328552246}
{"index": 153, "question": "Can the Dalai Lama fit in a car?", "answer": "yes", "explanation": "Yes. The Dalai Lama is a person. Cars are designed for people to sit in them.", "generated": [{"finished": true, "length": 4, "score": [0.6325872823086626, 0.3649141531101959, 0.290100816880038, 0.2447166370747141], "conf": [0.9509162788920907, 0.8347794760755701, 0.6756722167499476, 0.48230978502409116], "prob": [[1.6848622523168728e-10, 26], [9.082530758277067e-20, 22], [2.8440519943262957e-21, 14], [2.791078122716029e-21, 6]], "generated": ["According to this website, the Dalai Lama’s height is 5′ 7″ (67 inches).", "Meanwhile, this website says that the average length of a standard car is 157 inches.", "Thus, the Dalai Lama can fit in a car.", "So the answer is yes."], "conf_comments": ["(A)\nAccording to this", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7830271655689781, 0.4082683847452376, 0.20463489133977195, 0.16509569658003206, 0.1386427053785743], "conf": [0.9667922582810747, 0.8558713343394988, 0.7300368467654202, 0.574802600602614, 0.40651830178983533], "prob": [[0.0006847434906235227, 16], [1.478906602971399e-10, 13], [1.6103277999881655e-18, 15], [1.1211119340176603e-19, 14], [1.102087208020849e-19, 6]], "generated": ["The Dalai Lama is about 5 feet and 5 inches tall.", "So he is about 165.1 cm tall.", "A standard car’s interior is about 150 cm wide.", "Thus, the Dalai Lama can fit in a car.", "So the answer is yes."], "conf_comments": ["(A)\nThe average car is", "(A)\nThe average car is", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7830271655689781, 0.4082683847452376, 0.1748446080557289, 0.13270011967488332, 0.11587877689006827], "conf": [0.9667922582810747, 0.8558713343394988, 0.6854839804280808, 0.5602501634043934, 0.4288229948487323], "prob": [[0.0006847434906235227, 16], [1.478906602971399e-10, 13], [4.0256991429611265e-29, 29], [3.0032379705731664e-31, 14], [2.936326605800332e-31, 6]], "generated": ["The Dalai Lama is about 5 feet and 5 inches tall.", "So he is about 165.1 cm tall.", "Using our car size calculator, a sub compact car is about 6 feet, which is about 182.9 cm tall.", "Thus, the Dalai Lama would fit in a car.", "So the answer is yes."], "conf_comments": ["(A)\nThe average car is", "(A)\nThe average car is", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6200281428878336, 0.386915512936508, 0.16435980880041678, 0.13456371543267096, 0.11128213374383289], "conf": [0.9686246005087265, 0.8470928684094449, 0.5972384999262531, 0.4370714065233085, 0.29951196866983154], "prob": [[3.732617707182266e-09, 21], [6.949609535858963e-17, 22], [2.587894596579936e-29, 21], [7.568115995601463e-30, 14], [7.478022300045005e-30, 6]], "generated": ["The Dalai Lama is 4.13 m (13.6 ft) tall.", "A Toyota Corolla has a wheelbase of 2.6 m (8.5 ft).", "According to a physics equation, the wheelbase is sufficient to accommodate the Dalai Lama.", "Thus, the Dalai Lama can fit in a car.", "So the answer is yes."], "conf_comments": ["(A)\nThe average car is", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6200281428878336, 0.386915512936508, 0.16271170416727748, 0.13511585049345784], "conf": [0.9686246005087265, 0.8470928684094449, 0.6172281197752175, 0.4336043485366893], "prob": [[3.732617707182266e-09, 21], [6.949609535858963e-17, 22], [6.022402160698102e-36, 31], [5.386817557063059e-36, 6]], "generated": ["The Dalai Lama is 4.13 m (13.6 ft) tall.", "A Toyota Corolla has a wheelbase of 2.6 m (8.5 ft).", "Thus, it is likely impossible (due to the large difference between the lengths) for the Dalai Lama to physically fit inside a car.", "So the answer is no."], "conf_comments": ["(A)\nThe average car is", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 62.27962875366211}
{"index": 154, "question": "Can actress Dafne Keen win the Eurovision Song Contest finals in 2020?", "answer": "no", "explanation": "No. Contestants must be at least 16 years of age to compete in the finals of Eurovision Song Contest. Dafne Keen is 15 years old in 2020.", "generated": [{"finished": true, "length": 5, "score": [0.7158919264404081, 0.6120751308276439, 0.32974416976882437, 0.24605229087464056, 0.20273733157284393], "conf": [0.9737777049874483, 0.8529424754226436, 0.5655676739823693, 0.3849105915978797, 0.2623054429238875], "prob": [[7.364991505693918e-07, 22], [3.403304843445294e-08, 17], [1.1834226793284371e-14, 18], [6.406155431205187e-17, 26], [6.263089026271349e-17, 6]], "generated": ["The Eurovision Song Contest finals will take place on 12 December 2020.", "Dafne Keen was born on 4 January 2005.", "Dafne Keen will not be 16 by 2020.", "Thus, Dafne Keen will not win the Eurovision Song Contest finals in 2020.", "So the answer is no."], "conf_comments": ["(A)\nDafne Ke", "(A)\nThus, D", "(A)\nThus, D", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.7158919264404081, 0.6120751308276439, 0.3534557200011328, 0.20870373491869346, 0.16821932211441337, 0.14542022022538345], "conf": [0.9737777049874483, 0.8529424754226436, 0.6387203220944788, 0.4323149458450591, 0.3531433608433199, 0.26585737395765147], "prob": [[7.364991505693918e-07, 22], [3.403304843445294e-08, 17], [5.605831776388574e-17, 25], [1.8116045771907195e-24, 26], [1.1747753516206422e-26, 22], [1.1239729794616815e-26, 6]], "generated": ["The Eurovision Song Contest finals will take place on 12 December 2020.", "Dafne Keen was born on 4 January 2005.", "This means that she will only turn 16 years old by the time of the 2020 finals.", "Contestants must be at least 16 years old by 31 December of the year they intend to participate.", "Thus, Dafne Keen could not participate in the 2020 finals.", "So the answer is no."], "conf_comments": ["(A)\nDafne Ke", "(A)\nThus, D", "(A)\nThus, D", "(A)\nThus, D", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7422446131149588, 0.45043464020797835, 0.3196103138520149, 0.273195584571857], "conf": [0.9592967857855256, 0.7680145449080259, 0.5965985550063949, 0.439140974484654], "prob": [[2.652658044672334e-05, 19], [1.8168005939305825e-19, 42], [4.65148023565078e-26, 35], [4.4493367762899684e-26, 6]], "generated": ["Dafne Keen is 14 years old as of 2018.", "According to Eurovision.org, a contestant must be 16 years old or older as of 31 December 2020 to enter Eurovision in 2020.", "Thus, Dafne Keen cannot win the Eurovision Song Contest in 2020 because she will be 16 years old by that time.", "So the answer is no."], "conf_comments": ["(A)\nThe Eurovision Song", "(A)\nThus, D", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7935631716849334, 0.567222623794239, 0.3358838883116053, 0.2718145721622272], "conf": [0.9544103687731232, 0.8239361551590636, 0.47729614577755064, 0.3150058401225903], "prob": [[0.02370558850981852, 9], [4.376014572026597e-05, 12], [8.60371197361357e-09, 17], [8.213051746086788e-09, 6]], "generated": ["Dafne Keen is an actress.", "The Eurovision Song Contest is an annual music competition.", "Thus, it is impossible for Dafne Keen to win the contest.", "So the answer is no."], "conf_comments": ["(A)\nThe Eurovision Song", "(A)\nThus, D", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7422446131149588, 0.49595098295288537, 0.312735226694891, 0.26431448775884936], "conf": [0.9592967857855256, 0.7804958791438369, 0.6139029689464033, 0.4407658830378098], "prob": [[2.652658044672334e-05, 19], [4.02396852560627e-13, 30], [3.404487729666581e-23, 34], [3.3016598881385414e-23, 6]], "generated": ["Dafne Keen is 14 years old as of 2018.", "The Eurovision Song Contest requires a singer to be at least 16 years old by December 31, 2019.", "Thus, actress Dafne Keen cannot win the Eurovision Song Contest in 2020 as she would be ineligible by then.", "So the answer is no."], "conf_comments": ["(A)\nThe Eurovision Song", "(A)\nThus, D", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 62.13609838485718}
{"index": 155, "question": "Does United Airlines have a perfect operation record?", "answer": "no", "explanation": "No. An airline with a perfect operation record has no crashes or other damaging incidents. United Airlines has had over 30 crash incidents over several decades.", "generated": [{"finished": true, "length": 4, "score": [0.47085645042046126, 0.3433825558196847, 0.28842452810713065, 0.2361489289956623], "conf": [0.9362597104918673, 0.7948501313423305, 0.6198347279603019, 0.41652261864798956], "prob": [[1.2831554359015853e-17, 27], [7.219833227678704e-21, 16], [1.9644983321568074e-21, 13], [1.936069881001535e-21, 6]], "generated": ["The airline with the best operation record seems to be Hawaiian Airlines with 0.1 accidents for every million flight hours.", "United Airlines has 1.1 accidents for every million flight hours.", "Thus, United Airlines does not have a perfect operation record.", "So the answer is no."], "conf_comments": ["(A)\nUnited Airlines has", "(A)\nThus, United", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5334414461408907, 0.3655609078992958, 0.20573627524684615, 0.1708152709357795, 0.13711625409175135], "conf": [0.9178527708449905, 0.8435105276010765, 0.6489603816422206, 0.49822940531380683, 0.3218517436689408], "prob": [[1.927579027335772e-13, 25], [9.902040119005679e-21, 25], [1.93738603087168e-28, 20], [4.776197617437718e-29, 13], [4.704031020999806e-29, 6]], "generated": ["In 2017, United Airlines had two high profile incidents which could damage the reputation of the airline.", "In the first incident, a paying passenger was forcibly removed from his seat and dragged off the plane.", "In the second incident, United Airlines accidentally left a dog in the cargo hold and it died.", "Thus, United Airlines does not have a perfect operation record.", "So the answer is no."], "conf_comments": ["(A)\nIn 20", "(A)\nIn the second incident", "(A)\nThus, United", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5334414461408907, 0.3655609078992958, 0.20573627524684615, 0.14342330044645651, 0.11540994391055885], "conf": [0.9178527708449905, 0.8435105276010765, 0.6489603816422206, 0.4999840488198083, 0.3246955120035806], "prob": [[1.927579027335772e-13, 25], [9.902040119005679e-21, 25], [1.93738603087168e-28, 20], [4.849002922151854e-31, 13], [4.764461681538517e-31, 6]], "generated": ["In 2017, United Airlines had two high profile incidents which could damage the reputation of the airline.", "In the first incident, a paying passenger was forcibly removed from his seat and dragged off the plane.", "In the second incident, United Airlines accidentally left a dog in the cargo hold and it died.", "Thus, United Airlines does not have a perfect operations record.", "So the answer is no."], "conf_comments": ["(A)\nIn 20", "(A)\nIn the second incident", "(A)\nThus, United", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5334414461408907, 0.3655609078992958, 0.17653625479253765, 0.14362279190087363, 0.11497342990597735], "conf": [0.9178527708449905, 0.8435105276010765, 0.6223042943850948, 0.48108313546395187, 0.3090895545411926], "prob": [[1.927579027335772e-13, 25], [9.902040119005679e-21, 25], [9.820133557010056e-32, 22], [1.3044213432006932e-32, 13], [1.2844647608954617e-32, 6]], "generated": ["In 2017, United Airlines had two high profile incidents which could damage the reputation of the airline.", "In the first incident, a paying passenger was forcibly removed from his seat and dragged off the plane.", "In the second incident, an employee put a rabbit inside a dog carrier and sent it as cargo.", "Thus, United Airlines does not have a perfect operation record.", "So the answer is no."], "conf_comments": ["(A)\nIn 20", "(A)\nIn the second incident", "(A)\nThus, United", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5525233616874323, 0.2745999230911044, 0.2261136643532859], "conf": [0.8850148576361696, 0.6668384127152005, 0.45429248473281436], "prob": [[3.305747599574615e-13, 27], [5.879514119028982e-21, 16], [5.7143724062254846e-21, 6]], "generated": ["According to a 2017 article, a video showed a passenger being violently dragged off a United Airlines flight.", "Thus, the United Airlines had an operation record that was less than perfect.", "So the answer is no."], "conf_comments": ["(A)\nThus, United", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 60.039408683776855}
{"index": 156, "question": "Can United States Secretary of State do crimes in U.K. without being arrested?", "answer": "yes", "explanation": "Yes. Diplomatic Immunity allows for diplomats in other countries to not be tried for their transgressions. Countries that signed the Vienna Convention on Diplomatic Relations allow for Diplomatic Immunity. All UN member states besides Palau, The Solomon Islands, and South Sudan have signed the Vienna Convention on Diplomatic Relations treaty. The U.K. is one of the original UN member nations.", "generated": [{"finished": true, "length": 4, "score": [0.5438893934380623, 0.26379846496562476, 0.19938527130828035, 0.17208090138178012], "conf": [0.9326070158140014, 0.7809754465932861, 0.6107144431559092, 0.4571110773400083], "prob": [[1.062796262446482e-10, 20], [4.208060362178659e-31, 37], [1.6409757587981783e-34, 25], [1.5939548687381805e-34, 6]], "generated": ["The Secretary of State for Foreign and Commonwealth Affairs reports to Parliament as a member of the Cabinet.", "In the United Kingdom the Cabinet is required by convention to resign immediately if a Cabinet minister is found guilty of a criminal offense, regardless of the nature of the crime.", "Thus, the Secretary of State for Foreign and Commonwealth Affairs cannot commit a crime in U.K. without being arrested.", "So the answer is no."], "conf_comments": ["(A)\nThe Secretary of State", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5258122494377462, 0.29974874059516854, 0.20452220472517887, 0.16691092876559308], "conf": [0.9501328323726236, 0.7907695404406409, 0.6390326808179159, 0.47466508998973017], "prob": [[3.5899046947620487e-22, 40], [9.848163607225064e-40, 43], [8.350976990566835e-44, 17], [4.3399392011477886e-44, 6]], "generated": ["During their travel for work in foreign countries, U.S. officials, including the Secretary of State, enjoy diplomatic immunity, meaning that they can’t be arrested for crimes.", "Since the U.S. is a member of the United Nations, U.S. officials, including the Secretary of State, also receive diplomatic immunity from the Headquarters Agreement with the UN.", "Thus, a Secretary of State can’t be arrested in U.K.", "So the answer is yes."], "conf_comments": ["(A)\nThe United States and", "(A)\nThe United Kingdom is", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5258122494377462, 0.29974874059516854, 0.20452220472517887, 0.16535470785945308], "conf": [0.9501328323726236, 0.7907695404406409, 0.6390326808179159, 0.47760547686511196], "prob": [[3.5899046947620487e-22, 40], [9.848163607225064e-40, 43], [8.350976990566835e-44, 17], [3.737430233818092e-44, 6]], "generated": ["During their travel for work in foreign countries, U.S. officials, including the Secretary of State, enjoy diplomatic immunity, meaning that they can’t be arrested for crimes.", "Since the U.S. is a member of the United Nations, U.S. officials, including the Secretary of State, also receive diplomatic immunity from the Headquarters Agreement with the UN.", "Thus, a Secretary of State can’t be arrested in U.K.", "So the answer is no."], "conf_comments": ["(A)\nThe United States and", "(A)\nThe United Kingdom is", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5258122494377462, 0.3073084616577969, 0.2503012662950768], "conf": [0.9501328323726236, 0.7150513371949906, 0.4795509264376416], "prob": [[3.5899046947620487e-22, 40], [2.7434674517038526e-26, 12], [2.5702601488109148e-26, 6]], "generated": ["During their travel for work in foreign countries, U.S. officials, including the Secretary of State, enjoy diplomatic immunity, meaning that they can’t be arrested for crimes.", "Thus, they could do crimes without being arrested.", "So the answer is yes."], "conf_comments": ["(A)\nThe United States and", "(A)\nHowever, the U", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5258122494377462, 0.29974874059516854, 0.17159379652430706, 0.14652115659750425], "conf": [0.9501328323726236, 0.7907695404406409, 0.6647493233361499, 0.4995705609396822], "prob": [[3.5899046947620487e-22, 40], [9.848163607225064e-40, 43], [5.8259502384747655e-50, 25], [4.858679220067505e-50, 6]], "generated": ["During their travel for work in foreign countries, U.S. officials, including the Secretary of State, enjoy diplomatic immunity, meaning that they can’t be arrested for crimes.", "Since the U.S. is a member of the United Nations, U.S. officials, including the Secretary of State, also receive diplomatic immunity from the Headquarters Agreement with the UN.", "Thus, while Secretary of State would enjoy diplomatic immunity in the U.K. and would not be arrested.", "So the answer is yes."], "conf_comments": ["(A)\nThe United States and", "(A)\nThe United Kingdom is", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5258122494377462, 0.3073084616577969, 0.17874950634215075, 0.1416864664028986], "conf": [0.9501328323726236, 0.7150513371949906, 0.4898050915302799, 0.3128869222960762], "prob": [[3.5899046947620487e-22, 40], [2.7434674517038526e-26, 12], [4.147029021862318e-32, 19], [3.7545215149502075e-32, 6]], "generated": ["During their travel for work in foreign countries, U.S. officials, including the Secretary of State, enjoy diplomatic immunity, meaning that they can’t be arrested for crimes.", "Thus, they could do crimes without being arrested.", "Thus, the U.S. Secretary of State could do crimes without being arrested.", "So the answer is yes."], "conf_comments": ["(A)\nThe United States and", "(A)\nHowever, the U", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 51.737356185913086}
{"index": 157, "question": "Is myocardial infarction a brain problem?", "answer": "no", "explanation": "No. Myocardial infarction is a problem in the heart. The equivalent in the brain would be similar to a stroke.", "generated": [{"finished": true, "length": 3, "score": [0.717006856005831, 0.6147212036995438, 0.5401030260553417], "conf": [0.9557066781745952, 0.8172963477616451, 0.6333240070637839], "prob": [[9.137553676263477e-05, 15], [2.0108434928197205e-05, 10], [1.9655356133249573e-05, 6]], "generated": ["Myocardial infarction involves the heart, not the brain.", "Thus, it is not a brain problem.", "So the answer is no."], "conf_comments": ["(A)\nThus, my", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.717006856005831, 0.6198753289856835, 0.5371819831775271], "conf": [0.9557066781745952, 0.7887360758079397, 0.5942933350143642], "prob": [[9.137553676263477e-05, 15], [1.871155908654506e-05, 16], [1.8344255326653584e-05, 6]], "generated": ["Myocardial infarction involves the heart, not the brain.", "Thus, myocardial infarction is not a brain problem.", "So the answer is no."], "conf_comments": ["(A)\nThus, my", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.8846660838478878, 0.565172290292686, 0.48503751142137114], "conf": [0.9679644439328271, 0.7564404852862947, 0.562511027750669], "prob": [[0.04125629841766028, 15], [2.419938641877655e-06, 15], [2.284548017874569e-06, 6]], "generated": ["Myocardial infarction is also known as a heart attack.", "Thus, it is a heart problem, rather than a brain problem.", "So the answer is no."], "conf_comments": ["(A)\nThe brain is not", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.7326914282474992, 0.5566183950658546, 0.4761782418222655], "conf": [0.9637899174122776, 0.7482537079815997, 0.5523585443641438], "prob": [[0.00015413371385498316, 15], [7.942578807489137e-06, 10], [7.541752029337259e-06, 6]], "generated": ["Myocardial infarction is medically known as heart attack.", "Thus, it is not a brain problem.", "So the answer is no."], "conf_comments": ["(A)\nThus, my", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8846660838478878, 0.6154608837027215, 0.3475608813957608, 0.3020604144950379, 0.2594089072509007], "conf": [0.9679644439328271, 0.8359268220656488, 0.6785380381666207, 0.5404612866951639, 0.3995583859302005], "prob": [[0.04125629841766028, 15], [1.2443898895773406e-05, 14], [6.611051729217583e-11, 13], [2.8264275274128996e-11, 16], [2.7863568869219653e-11, 6]], "generated": ["Myocardial infarction is also known as a heart attack.", "The myocardium is the middle layer of the heart wall.", "The brain is a different organ from the myocardium.", "Thus, myocardial infarction is not a brain problem.", "So the answer is no."], "conf_comments": ["(A)\nThe brain is not", "(A)\nThus, my", "(A)\nThus, my", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 46.06407642364502}
{"index": 158, "question": "Will Chick Fil A be open on Halloween 2021?", "answer": "no", "explanation": "No. Chick Fil A restaurants close on Sundays. Halloween 2021 falls on a Sunday.", "generated": [{"finished": true, "length": 4, "score": [0.6660471681933249, 0.5612441296643524, 0.47699556023904394, 0.4119903182174834], "conf": [0.9693227747396032, 0.8831514318180548, 0.7106513634003153, 0.532887691236254], "prob": [[0.00040310255889429187, 10], [1.2291135813986217e-05, 14], [1.2728239440203999e-06, 21], [1.2341440283852552e-06, 6]], "generated": ["Chick Fil A closes on Sundays.", "Halloween 2021 falls on a Sunday.", "Thus, Chick Fil A will not be open on Halloween 2021.", "So the answer is no."], "conf_comments": ["(A)\nHalloween", "(A)\nThus, Ch", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.625449144417435, 0.4181125309341916, 0.35578161466349006], "conf": [0.9167829519170104, 0.7327238083465315, 0.5342191576736077], "prob": [[5.661570495490065e-10, 25], [5.4096057804990725e-12, 8], [5.1900854128312236e-12, 6]], "generated": ["Chick Fil A’s hours on Halloween 2021 are the same as their regular operating hours.", "Thus, they will be open.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Ch", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6408684048363267, 0.4053087703171068, 0.27920183560411843, 0.2326161312135852], "conf": [0.9649403097869772, 0.7772469982941129, 0.5571128423094079, 0.39308271040662524], "prob": [[1.62099895944425e-08, 21], [4.4608526476469396e-13, 15], [7.72514582252603e-17, 21], [7.003590793794901e-17, 6]], "generated": ["Chick Fil A is a fast-food chain that closes on Christmas and Thanksgiving.", "Halloween is not a holiday on which it closes.", "Thus, Chick Fil A would definitely be open on Halloween 2021.", "So the answer is yes."], "conf_comments": ["(A)\nHalloween", "(A)\nThus, Ch", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6660471681933249, 0.5612441296643524, 0.2577544753275555, 0.21857614910356565], "conf": [0.9693227747396032, 0.8831514318180548, 0.7079885822915917, 0.5248567608477374], "prob": [[0.00040310255889429187, 10], [1.2291135813986217e-05, 14], [3.5573164597135575e-13, 13], [2.963424848881908e-13, 6]], "generated": ["Chick Fil A closes on Sundays.", "Halloween 2021 falls on a Sunday.", "Hence Chick Fil A would be closed for that day.", "So the answer is no."], "conf_comments": ["(A)\nHalloween", "(A)\nThus, Ch", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6627376687577028, 0.3429991318831398, 0.25806153844548024, 0.21315663380185737], "conf": [0.9383791302053409, 0.6953175943928462, 0.5094077000087864, 0.3495587235536209], "prob": [[2.612247585424197e-08, 23], [8.029302386174339e-16, 17], [2.7554910509073245e-18, 22], [2.661815942974521e-18, 6]], "generated": ["According to holiday hours, Chick Fil A is open on Halloween 2020.", "Halloween is on a Saturday, so the hours should be the same.", "Thus, Chick Fil A should be open on Halloween 2021 as well.", "So the answer is yes."], "conf_comments": ["(A)\nThus, it", "(A)\nThus, Ch", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6408684048363267, 0.4053087703171068, 0.27920183560411843, 0.162851763353549], "conf": [0.9649403097869772, 0.7772469982941129, 0.5571128423094079, 0.2948003786271559], "prob": [[1.62099895944425e-08, 21], [4.4608526476469396e-13, 15], [7.72514582252603e-17, 21], [5.456169297810632e-18, 6]], "generated": ["Chick Fil A is a fast-food chain that closes on Christmas and Thanksgiving.", "Halloween is not a holiday on which it closes.", "Thus, Chick Fil A would definitely be open on Halloween 2021.", "So the answer is no."], "conf_comments": ["(A)\nHalloween", "(A)\nThus, Ch", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 50.949585914611816}
{"index": 159, "question": "Was  Godfrey of Bouillon an Islamaphobe?", "answer": "yes", "explanation": "Yes.  Godfrey of Bouillon lead troops during the Prince's Crusade. The Prince's Crusade was an attempt by Europeans to \"take back\" the city of Jerusalem from Islamic hands.", "generated": [{"finished": true, "length": 3, "score": [0.6357956081399899, 0.4186926461824442, 0.331393238479016], "conf": [0.952723311009662, 0.6197766922245889, 0.4000109635104726], "prob": [[4.681233959015186e-07, 17], [7.1212310749415e-10, 16], [5.955453931072109e-10, 6]], "generated": ["Godfrey of Bouillon is a Crusader who fought against Islam.", "Thus, Godfrey of Bouillon was probably an Islamaphobe.", "So the answer is yes."], "conf_comments": ["(A)\nThus, God", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6357956081399899, 0.4478713771166315, 0.24125279084910115, 0.19947861063677236], "conf": [0.952723311009662, 0.7923857398369932, 0.5769343307465677, 0.398862304137306], "prob": [[4.681233959015186e-07, 17], [1.937320047157205e-12, 24], [7.800692530246318e-17, 11], [7.295269793760193e-17, 6]], "generated": ["Godfrey of Bouillon is a Crusader who fought against Islam.", "Islamophobia is the fear or hatred of Islam or Muslims, especially as a political force.", "Thus, he cannot be an Islamaphobe.", "So the answer is no."], "conf_comments": ["(A)\nThus, God", "(A)\nThus, God", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.7438604576618197, 0.3780893976325805, 0.21724941330354064, 0.1258149303543552, 0.08301207191147343, 0.06627101432145098], "conf": [0.9695668344385292, 0.8782468002519797, 0.7091445709899784, 0.4347615532073091, 0.2651303445037927, 0.17044180964673908], "prob": [[1.511281788338511e-07, 28], [1.8873415236589908e-12, 9], [6.892722241431966e-18, 14], [3.9719755258089035e-23, 20], [2.53019596458015e-25, 15], [2.402383442425327e-25, 6]], "generated": ["Godfrey of Bouillon led the First Crusade and died during the Siege of Jerusalem in 1100.", "Jerusalem is located in Palestine.", "Palestine is considered the homeland of the Islamic religion.", "Thus, it can be inferred that Godfrey of Bouillon did not like Islam.", "Thus, Godfrey of Bouillon was an Islamaphobe.", "So the answer is yes."], "conf_comments": ["(A)\nThe First Crus", "(A)\nPalestine is", "(A)\nThus, God", "(A)\nSo the answer is", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7438604576618197, 0.3780893976325805, 0.21724941330354064, 0.1258149303543552, 0.09672938453173627], "conf": [0.9695668344385292, 0.8782468002519797, 0.7091445709899784, 0.4347615532073091, 0.2772884171562962], "prob": [[1.511281788338511e-07, 28], [1.8873415236589908e-12, 9], [6.892722241431966e-18, 14], [3.9719755258089035e-23, 20], [2.5167373626380474e-23, 6]], "generated": ["Godfrey of Bouillon led the First Crusade and died during the Siege of Jerusalem in 1100.", "Jerusalem is located in Palestine.", "Palestine is considered the homeland of the Islamic religion.", "Thus, it can be inferred that Godfrey of Bouillon did not like Islam.", "So the answer is yes."], "conf_comments": ["(A)\nThe First Crus", "(A)\nPalestine is", "(A)\nThus, God", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7438604576618197, 0.4301651019306, 0.24027032614218186, 0.11037422484999176, 0.07837714251611022], "conf": [0.9695668344385292, 0.8205091897708063, 0.6574858870064091, 0.43262861694967364, 0.2788212262841155], "prob": [[1.511281788338511e-07, 28], [7.748777298197589e-19, 28], [2.160052179017009e-25, 16], [3.293454897198224e-40, 30], [7.555257893963267e-41, 6]], "generated": ["Godfrey of Bouillon led the First Crusade and died during the Siege of Jerusalem in 1100.", "Islam was established in 622 and in 1099, the crusaders captured the city of Jerusalem.", "Godfrey of Bouillon was part of the Siege of Jerusalem.", "As he was part of an attack on a city of Islam, there is some evidence that Godfrey of Bouillon was an Islamaphobe.", "So the answer is yes."], "conf_comments": ["(A)\nThe First Crus", "(A)\nThus, God", "(A)\nThus, God", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 61.80497694015503}
{"index": 160, "question": "Is an espresso likely to assuage fear?", "answer": "no", "explanation": "No. Fear raises heart rate. Caffeine raises heart rate. Coffee may also increase symptoms such as jitteriness and nausea .", "generated": [{"finished": true, "length": 4, "score": [0.6029263699325839, 0.36363926008548086, 0.22950687368031597, 0.1913619461355314], "conf": [0.9575648292637582, 0.7588619008610304, 0.5140706200395865, 0.36830124807735637], "prob": [[1.4674651556867464e-09, 21], [5.1526192502056655e-18, 25], [6.189493917872813e-22, 17], [5.167628497561671e-22, 6]], "generated": ["The definition of \"assuage\" is \"lessen (a strong feeling or emotion).\"", "An espresso is caffeine, and caffeine is known to have a stimulant effect.", "Therefore, it is unlikely that an espresso would assuage fear.", "So the answer is no."], "conf_comments": ["(A)\nAn espresso", "(A)\nThus, an", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6224849652838776, 0.34952640285726994, 0.22669511625801086, 0.18575511136646528], "conf": [0.9541910264361056, 0.7406724452580081, 0.4983691698860055, 0.3373281051450326], "prob": [[0.00012196186311174957, 10], [3.6696655580599985e-08, 9], [3.196075911901313e-11, 15], [3.045034657664756e-11, 6]], "generated": ["An espresso is a stimulant.", "In contrast, fear is an emotion.", "Thus, an espresso would probably not assuage fear.", "So the answer is no."], "conf_comments": ["(A)\nFear is a", "(A)\nThus, an", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6224849652838776, 0.34952640285726994, 0.16949968715595023, 0.11874625718930892, 0.09560193147332462], "conf": [0.9541910264361056, 0.7406724452580081, 0.45986952872098846, 0.30196132612149, 0.19675231828894063], "prob": [[0.00012196186311174957, 10], [3.6696655580599985e-08, 9], [1.2117930094418534e-13, 13], [8.597648612875148e-16, 17], [8.3316175598541e-16, 6]], "generated": ["An espresso is a stimulant.", "In contrast, fear is an emotion.", "Hence, an espresso has no effect on fear.", "Thus, it is unlikely that an espresso can assuage fear.", "So the answer is no."], "conf_comments": ["(A)\nFear is a", "(A)\nThus, an", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.6029263699325839, 0.38612905089634375, 0.20809164469614652, 0.08278990050109092, 0.06472232053827032, 0.055364352108897945], "conf": [0.9575648292637582, 0.8206638105982961, 0.7012022497204653, 0.34204912892964306, 0.23986298074754414, 0.17627708255729363], "prob": [[1.4674651556867464e-09, 21], [2.7880022829088007e-16, 21], [5.740586242275221e-21, 10], [4.0379778111802253e-36, 31], [5.1326830284666555e-37, 15], [5.0010636551090944e-37, 6]], "generated": ["The definition of \"assuage\" is \"lessen (a strong feeling or emotion).\"", "In other words, the definition of \"assuage\" is \"lessen (fear).\"", "The espresso is a type of coffee.", "Coffee is a stimulant and drinking it can lead to jitters and insomnia, which can increase fear and anxiety.", "Thus, an espresso is unlikely to assuage fear.", "So the answer is no."], "conf_comments": ["(A)\nAn espresso", "(A)\nAn espresso", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6224849652838776, 0.34952640285726994, 0.16949968715595023, 0.0960745328809476, 0.07813454500671149], "conf": [0.9541910264361056, 0.7406724452580081, 0.45986952872098846, 0.3135193130017063, 0.20846340634149835], "prob": [[0.00012196186311174957, 10], [3.6696655580599985e-08, 9], [1.2117930094418534e-13, 13], [1.5211249397651713e-18, 15], [1.4736331114742541e-18, 6]], "generated": ["An espresso is a stimulant.", "In contrast, fear is an emotion.", "Hence, an espresso has no effect on fear.", "Thus, there is no chance that it would assuage fear.", "So the answer is no."], "conf_comments": ["(A)\nFear is a", "(A)\nThus, an", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 64.25473117828369}
{"index": 161, "question": "If you add water to rice pudding is it horchata?", "answer": "no", "explanation": "No. Horchata is a drink made from soaking dry rice in water for hours, then pulverizing and straining the mixture and adding spices and sweetener. Rice pudding includes ingredients like eggs, and whole grains of cooked rice. ", "generated": [{"finished": true, "length": 4, "score": [0.7285396529640088, 0.5682339835623756, 0.3665682480580383, 0.305398054174215], "conf": [0.9433281516909008, 0.7219637523036257, 0.5336567595892643, 0.37297522158388513], "prob": [[0.003180064568785468, 10], [0.00016079642616004255, 13], [9.267407178042571e-10, 21], [8.891755470742779e-10, 6]], "generated": ["Horchata is a drink made from rice.", "Rice pudding is a dessert made from rice.", "Thus, if you add water to rice pudding, it is certainly not horchata.", "So the answer is no."], "conf_comments": ["(A)\nRice pud", "(A)\nThus, if", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6607091921084475, 0.5279335301480358, 0.3130679944226444, 0.25352937984434476], "conf": [0.9455773355398714, 0.7490305372158134, 0.522727236376857, 0.3471796413529094], "prob": [[0.00043978097561252243, 10], [4.101332714791277e-05, 11], [2.302147508339424e-11, 21], [2.1337026591486615e-11, 6]], "generated": ["Horchata is a type of beverage.", "Rice pudding is a type of food.", "Thus, you couldn’t make rice pudding into horchata by simply adding water.", "So the answer is no."], "conf_comments": ["(A)\nRice pud", "(A)\nThus, rice", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6607091921084475, 0.5279335301480358, 0.29401238807370517, 0.2366352111645525], "conf": [0.9455773355398714, 0.7490305372158134, 0.49612701200105963, 0.3303063477600661], "prob": [[0.00043978097561252243, 10], [4.101332714791277e-05, 11], [2.3708848258727144e-13, 25], [2.0115509412269323e-13, 6]], "generated": ["Horchata is a type of beverage.", "Rice pudding is a type of food.", "Thus, you cannot add water to rice pudding and expect that it will magically turn into horchata.", "So the answer is no."], "conf_comments": ["(A)\nRice pud", "(A)\nThus, rice", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6928062568075917, 0.46494291868591925, 0.25858399017872014, 0.20054031104671236], "conf": [0.9519565517723838, 0.7365304253446483, 0.5121645654816318, 0.33936942714533314], "prob": [[1.8525432426606272e-08, 26], [3.257151231457047e-16, 33], [1.332680848114309e-23, 21], [7.453283681673163e-24, 6]], "generated": ["Horchata is a Mexican drink or dessert that is made with rice, milk, sugar, and cinnamon.", "Rice pudding is a dessert that is made with rice, cream, milk, sugar, and flavorings such as cinnamon.", "Rice pudding does not contain cinnamon, so it is not horchada.", "So the answer is no."], "conf_comments": ["(A)\nRice pud", "(A)\nThus, if", "(A)\nThus, if", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6928062568075917, 0.46494291868591925, 0.25858399017872014, 0.1405909913204468, 0.11582121876428751], "conf": [0.9519565517723838, 0.7365304253446483, 0.5121645654816318, 0.3658011840953665, 0.2495215406711683], "prob": [[1.8525432426606272e-08, 26], [3.257151231457047e-16, 33], [1.332680848114309e-23, 21], [1.1998449885957304e-31, 21], [1.1638986725595055e-31, 6]], "generated": ["Horchata is a Mexican drink or dessert that is made with rice, milk, sugar, and cinnamon.", "Rice pudding is a dessert that is made with rice, cream, milk, sugar, and flavorings such as cinnamon.", "Rice pudding does not contain cinnamon, so it is not horchada.", "Thus, you can't make rice pudding to horchata just by adding water.", "So the answer is no."], "conf_comments": ["(A)\nRice pud", "(A)\nThus, if", "(A)\nThus, if", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.34620547294617}
{"index": 162, "question": "Did Bill Gates achieve Latin honors?", "answer": "no", "explanation": "No. Bill Gates left Harvard after two years. Latin honors are Latin phrases used in some colleges and universities to indicate the level of distinction with which an academic degree has been earned. You cannot earn an academic degree if you drop out of the program.", "generated": [{"finished": true, "length": 5, "score": [0.8214915962046047, 0.5149422276529083, 0.3215271557342913, 0.29261084737864446, 0.2650862810981685], "conf": [0.9745398492416687, 0.8875092779773444, 0.7651303214248819, 0.6710577031827821, 0.5520901126003668], "prob": [[0.004037380824573987, 15], [2.422973950727499e-13, 28], [6.856599474171754e-20, 19], [3.2559875232447157e-20, 13], [3.208810569292432e-20, 6]], "generated": ["Bill Gates graduated from Harvard University in 1975.", "Harward University has a Latin honors (cum laude, magna cum laude, summa cum laude) system.", "According to the Harvard Crimson, Bill Gates did not get Latin honors.", "Thus, Bill Gates did not achieve Latin honors.", "So the answer is no."], "conf_comments": ["(A)\nLatin honors", "(A)\nThus, Bill", "(A)\nThus, Bill", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.776498089058671, 0.6060480572346154, 0.4298310640206416, 0.23106452451072035, 0.18424369301070082, 0.15747817736397365], "conf": [0.9689408982296478, 0.8835346904017694, 0.7549793428968851, 0.5953125845414076, 0.46786826619709077, 0.3423871927843358], "prob": [[0.013991086941657146, 9], [1.470858495176499e-05, 17], [4.326542567215282e-08, 11], [1.4914334435729965e-18, 24], [1.1718880662749338e-19, 12], [1.1599936087394164e-19, 6]], "generated": ["Bill Gates graduated from Harvard University.", "According to US news, Harvard University is ranked the number 2 national college.", "This indicates that Harvard University is academically rigorous.", "Bill Gates also achieved Latin honors, which are usually a GPA of 3.5 or above.", "Thus, Bill Gates did achieve Latin honors.", "So the answer is yes."], "conf_comments": ["(A)\nHarvard University awards", "(A)\nThus, Bill", "(A)\nThus, Bill", "(A)\nThus, Bill", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.776498089058671, 0.6060480572346154, 0.3670401260275926, 0.2872910095677778], "conf": [0.9689408982296478, 0.8835346904017694, 0.5986976544693023, 0.39669291930127215], "prob": [[0.013991086941657146, 9], [1.470858495176499e-05, 17], [2.3404163883645106e-10, 18], [1.4625598943568378e-10, 6]], "generated": ["Bill Gates graduated from Harvard University.", "According to US news, Harvard University is ranked the number 2 national college.", "Thus, there is a good chance that Bill Gates could achieve Latin honors.", "So the answer is yes."], "conf_comments": ["(A)\nHarvard University awards", "(A)\nThus, Bill", "(A)\nHowever, Bill G", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8214915962046047, 0.5149422276529083, 0.3215271557342913, 0.21826863513466918, 0.19618434813141927], "conf": [0.9745398492416687, 0.8875092779773444, 0.7651303214248819, 0.6637660754545832, 0.5380494796137957], "prob": [[0.004037380824573987, 15], [2.422973950727499e-13, 28], [6.856599474171754e-20, 19], [9.76914836222343e-24, 14], [9.573924553772264e-24, 6]], "generated": ["Bill Gates graduated from Harvard University in 1975.", "Harward University has a Latin honors (cum laude, magna cum laude, summa cum laude) system.", "According to the Harvard Crimson, Bill Gates did not get Latin honors.", "Thus, Bill Gates also did not achieve Latin honors.", "So the answer is no."], "conf_comments": ["(A)\nLatin honors", "(A)\nThus, Bill", "(A)\nThus, Bill", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.776498089058671, 0.6060480572346154, 0.3670401260275926, 0.24499214040964992], "conf": [0.9689408982296478, 0.8835346904017694, 0.5986976544693023, 0.361134836464298], "prob": [[0.013991086941657146, 9], [1.470858495176499e-05, 17], [2.3404163883645106e-10, 18], [3.800016177416959e-11, 6]], "generated": ["Bill Gates graduated from Harvard University.", "According to US news, Harvard University is ranked the number 2 national college.", "Thus, there is a good chance that Bill Gates could achieve Latin honors.", "So the answer is maybe."], "conf_comments": ["(A)\nHarvard University awards", "(A)\nThus, Bill", "(A)\nHowever, Bill G", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 64.68823575973511}
{"index": 163, "question": "Was Superhero fiction invented in the digital format?", "answer": "no", "explanation": "No. The Golden Age of comics occurred between the 1930's and the 1950's. Shatter was the first digitally drawn, commercially published comic.", "generated": [{"finished": true, "length": 3, "score": [0.5887207119730297, 0.35434978623062724, 0.28513909996303555], "conf": [0.9587468852877779, 0.7631269865526209, 0.5041244748368285], "prob": [[1.4530925774178636e-09, 20], [1.0828951199111379e-14, 15], [9.603637613536544e-15, 6]], "generated": ["The first comic book was a 1933 copy of Famous Funnies.", "Thus, the first superhero fiction was not in digital format.", "So the answer is no."], "conf_comments": ["(A)\nThe first comic", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.4926347068463947, 0.31189885243523957, 0.25744772497395857], "conf": [0.9746545998530434, 0.7827617877830053, 0.5614066460100238], "prob": [[8.769788236069205e-10, 15], [8.071651011053998e-16, 20], [5.931686039100614e-16, 6]], "generated": ["The term Superhero fiction first appears in 1953.", "Therefore, the term Superhero fiction could not have been invented in the digital format.", "So the answer is no."], "conf_comments": ["(A)\nThe first Superh", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5131797402508773, 0.2645410899905583, 0.1740081548862853, 0.1331827747470992], "conf": [0.9442131808752662, 0.8211640420607559, 0.563889394447544, 0.3320514899150586], "prob": [[2.9101138682598356e-11, 19], [5.889063998203211e-18, 13], [1.4422711104998562e-21, 18], [1.398039191007799e-21, 6]], "generated": ["Stories about superheroes have existed even before the 1980s.", "Examples include Tarzan and The Lone Ranger.", "Thus, Superhero fiction was invented in the non-digital format.", "So the answer is no."], "conf_comments": ["(A)\nThe first superh", "(A)\nThus, super", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5131797402508773, 0.2645410899905583, 0.15688160817283756, 0.10382133876012226, 0.07977086856679401], "conf": [0.9442131808752662, 0.8211640420607559, 0.6606776394844197, 0.48629685203659107, 0.2944840434664341], "prob": [[2.9101138682598356e-11, 19], [5.889063998203211e-18, 13], [8.737963685649656e-25, 19], [7.633474458580497e-29, 18], [6.553189897387335e-29, 6]], "generated": ["Stories about superheroes have existed even before the 1980s.", "Examples include Tarzan and The Lone Ranger.", "Science fiction was invented in the digital format in the 1980s.", "Thus, superhero fiction was invented in a different format than science fiction.", "So the answer is no."], "conf_comments": ["(A)\nThe first superh", "(A)\nThus, super", "(A)\nThus, super", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.4987032331379274, 0.2409055749408251, 0.16377046085865193, 0.13021063205955585], "conf": [0.9656431784008671, 0.866978503377387, 0.657045743123867, 0.41734735813756463], "prob": [[3.1937089903271054e-17, 28], [7.887242428242648e-25, 13], [7.75510172767606e-28, 14], [7.535388237349923e-28, 6]], "generated": ["Superhero fiction in English started with pulp fiction magazines such as The Thunderbolt in 1915.", "Digital technology was invented in 1990s.", "Thus, superhero fiction was not invented digitally.", "So the answer is no."], "conf_comments": ["(A)\nThese magaz", "(A)\nThus, super", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 50.94104266166687}
{"index": 164, "question": "Are any animals in Chinese calendar Chordata?", "answer": "yes", "explanation": "Yes. The chinese zodiac based on the Chinese calendar has a number of animals including dogs and pigs. Chordata is a scientific classification of an animals phylum. The phylum of pigs is Chordata.", "generated": [{"finished": true, "length": 5, "score": [0.5803040644366106, 0.48281151186866095, 0.3088239315479768, 0.19120298235069652, 0.164177497741725], "conf": [0.9611406721656248, 0.8068747614703611, 0.6249109382366448, 0.5065041316156266, 0.3747857398671645], "prob": [[0.0018498887678069127, 6], [8.244017887120192e-07, 40], [5.0000720027112465e-09, 8], [7.005229944690562e-15, 18], [6.8556658630638336e-15, 6]], "generated": ["Chordates are animals.", "The Chinese calendar has twelve animals: rat, ox, tiger, rabbit, dragon, snake, horse, goat, monkey, rooster, dog, and pig.", "None of them are chordates.", "Thus, there are no animals in the Chinese calendar system which are chordates.", "So the answer is no."], "conf_comments": ["(A)\nThe Chinese calendar is", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7490040560884641, 0.5407823953569972, 0.3224225955832901, 0.17714224237702908, 0.14607758425744843], "conf": [0.9716632649727713, 0.8898708165766471, 0.6898723997425532, 0.5216025882345603, 0.3633793657078853], "prob": [[0.00015248983065060007, 16], [1.2188280409240756e-11, 29], [1.9829667794654283e-19, 23], [5.181179032464829e-25, 14], [4.48169949596484e-25, 6]], "generated": ["Chordata is a phylum of animals that have a backbone.", "All vertebrate animals are chordates, including fish, amphibians, reptiles, birds, and mammals.", "Animals in the Chinese calendar are the ox, the rat, the tiger, the rabbit, etc.", "These animals are all vertebrates and thus Chordate.", "So the answer is yes."], "conf_comments": ["(A)\nThe Chinese calendar is", "(A)\nThe Chinese calendar includes", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7490040560884641, 0.5407823953569972, 0.3098590943683066, 0.16914753067942184, 0.13998180032207228], "conf": [0.9716632649727713, 0.8898708165766471, 0.734687554535505, 0.5826557181548736, 0.4164032034160193], "prob": [[0.00015248983065060007, 16], [1.2188280409240756e-11, 29], [4.738095725348912e-20, 21], [7.909596440592074e-30, 23], [6.126507702561997e-30, 6]], "generated": ["Chordata is a phylum of animals that have a backbone.", "All vertebrate animals are chordates, including fish, amphibians, reptiles, birds, and mammals.", "Some invertebrates, such as jellyfish, also have a chordate ancestor.", "Since there are no invertebrates in the Chinese calendar, chordate animals are the only possible answer.", "So the answer is yes."], "conf_comments": ["(A)\nThe Chinese calendar is", "(A)\nThe Chinese calendar includes", "(A)\nThe Chinese calendar is", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 8, "score": [0.7490040560884641, 0.5407823953569972, 0.3098590943683066, 0.19755715388277612, 0.10007708645510828, 0.06342073751553909, 0.04945807877206026, 0.042977663936549684], "conf": [0.9716632649727713, 0.8898708165766471, 0.734687554535505, 0.6084521632561276, 0.4260567241073429, 0.3105491171955953, 0.26228678045698145, 0.1992100811315449], "prob": [[0.00015248983065060007, 16], [1.2188280409240756e-11, 29], [4.738095725348912e-20, 21], [2.63918256857724e-25, 17], [3.1298524060137846e-46, 48], [7.436171224029784e-50, 14], [2.796198875345661e-52, 17], [2.7003829667145163e-52, 6]], "generated": ["Chordata is a phylum of animals that have a backbone.", "All vertebrate animals are chordates, including fish, amphibians, reptiles, birds, and mammals.", "Some invertebrates, such as jellyfish, also have a chordate ancestor.", "In contrast, invertebrates are animals that do not have a backbone.", "Chinese zodiac animals include the rat, rabbit, tiger, ox, snake, cow, horse, tiger, dog, pig, mouse, monkey, goat, dragon, and rooster.", "None of these animals is in the phylum Chordata.", "Thus, none of these animals is in the phylum Chordata.", "So the answer is no."], "conf_comments": ["(A)\nThe Chinese calendar is", "(A)\nThe Chinese calendar includes", "(A)\nThe Chinese calendar is", "(A)\nThus, the", "(A)\nThus, none", "(A)\nThus, none", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7490040560884641, 0.4859042550375562, 0.2540768625390541, 0.18552694570254422], "conf": [0.9716632649727713, 0.7390678624477112, 0.553545061786929, 0.39551657443681354], "prob": [[0.00015248983065060007, 16], [7.476188815228627e-16, 44], [1.147207864765806e-20, 11], [1.9809540341114725e-21, 6]], "generated": ["Chordata is a phylum of animals that have a backbone.", "To list the 12 Chinese zodiac animals:\nRat, Ox, Tiger, Rabbit, Dragon, Snake, Horse, Goat, Monkey, Rooster, Dog, Pig", "There is no animal that is a Chordata.", "So the answer is no."], "conf_comments": ["(A)\nThe Chinese calendar is", "(A)\nThus, the", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": []}, "run_time": 75.81434178352356}
{"index": 165, "question": "Was Daniel thrown into the lion's den in the New Testament?", "answer": "no", "explanation": "No. The Book of Daniel is a book in the Old Testament of the Bible. The Bible is divided into the Old Testament and the New Testament. The New Testament focuses on four Gospels regarding the life of Jesus.", "generated": [{"finished": true, "length": 4, "score": [0.7876324887495499, 0.4993852456849625, 0.409013567722228, 0.3454894876247821], "conf": [0.942717524016523, 0.7801698045299976, 0.5927572552472048, 0.42406812835688007], "prob": [[0.00015260857432737972, 21], [1.6807169524792402e-10, 19], [1.5772618254923125e-11, 19], [1.5520894102150758e-11, 6]], "generated": ["Daniel was thrown into the lion’s den in the Old Testament, in the book of Daniel.", "There is no Daniel that was thrown into a lion’s den in the New Testament.", "Thus, Daniel was not thrown into the lion’s den in the New Testament.", "So the answer is no."], "conf_comments": ["(A)\nThe New Testament is", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.7876324887495499, 0.4993852456849625, 0.4140617781476527], "conf": [0.942717524016523, 0.7801698045299976, 0.6100723215443243], "prob": [[0.00015260857432737972, 21], [1.6807169524792402e-10, 19], [5.998298564258774e-11, 8]], "generated": ["Daniel was thrown into the lion’s den in the Old Testament, in the book of Daniel.", "There is no Daniel that was thrown into a lion’s den in the New Testament.", "Thus, the answer is no."], "conf_comments": ["(A)\nThe New Testament is", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7876324887495499, 0.4170509565372158, 0.3349338303952092, 0.2789885129925532], "conf": [0.942717524016523, 0.6803578992170405, 0.5032210430170749, 0.35005830688062217], "prob": [[0.00015260857432737972, 21], [4.294957048720132e-18, 33], [3.18279699549247e-19, 19], [3.1336251503993285e-19, 6]], "generated": ["Daniel was thrown into the lion’s den in the Old Testament, in the book of Daniel.", "The New Testament starts at the birth of Jesus Christ, so it is impossible for there to be an incident where Daniel was thrown into a lion’s den.", "Thus, Daniel was not thrown into the lion’s den in the New Testament.", "So the answer is no."], "conf_comments": ["(A)\nThe New Testament is", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7738603993841369, 0.4518759033940337, 0.32486714378288234, 0.2757603873600774], "conf": [0.929079638350954, 0.7961233818660138, 0.6228877487519674, 0.45113929580542333], "prob": [[0.0021370380626870873, 14], [8.440791586921373e-10, 16], [9.228834967541075e-14, 22], [8.946532706271117e-14, 6]], "generated": ["Daniel was thrown into the lions’ den in the Old Testament.", "According to Wikipedia, the New Testament follows the Old Testament chronologically.", "Thus, Daniel couldn’t have been thrown into a lion’s den in the New Testament.", "So the answer is no."], "conf_comments": ["(A)\nThe New Testament is", "(A)\nThus, Daniel", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7738603993841369, 0.4518759033940337, 0.3095936896418145, 0.25657985943201456], "conf": [0.929079638350954, 0.7961233818660138, 0.6566878405964577, 0.4750522572224296], "prob": [[0.0021370380626870873, 14], [8.440791586921373e-10, 16], [2.1705388959615482e-17, 31], [1.5901520752050573e-17, 6]], "generated": ["Daniel was thrown into the lions’ den in the Old Testament.", "According to Wikipedia, the New Testament follows the Old Testament chronologically.", "Thus, if Daniel was thrown into the lions’ den in the Bible, it would have been in the Old Testament and not the New Testament.", "So the answer is no."], "conf_comments": ["(A)\nThe New Testament is", "(A)\nThus, Daniel", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 51.158631324768066}
{"index": 166, "question": "Will 2020 elephant pregnancy last past next year with 4 solar eclipses?", "answer": "no", "explanation": "No. The gestation period of elephants are around 95 weeks. The year 2029 is the next year with 4 solar eclipses.", "generated": [{"finished": true, "length": 2, "score": [0.7784428919303857, 0.5480004798058082], "conf": [0.9450049399683769, 0.791276479811639], "prob": [[7.239676036082566e-08, 37], [2.9541215823755e-19, 50]], "generated": ["The gestation period for an elephant is 22 months, so a 2020 elephant pregnancy would last until 2021.", "Next year, there will be four solar eclipses: two solar eclipses on Dec. 14, 2020, June 10, 2021, and Dec. 4,"], "conf_comments": ["(A)\nThere will be ", "(A)\n2021"], "info": null}, {"finished": true, "length": 4, "score": [0.7784428919303857, 0.46775872914416633, 0.31948801387195447, 0.2564328177199792], "conf": [0.9450049399683769, 0.6764404565426388, 0.4594790744283457, 0.31635102110578345], "prob": [[7.239676036082566e-08, 37], [1.6323759086939604e-13, 19], [9.802881075547724e-19, 32], [6.579018919073927e-19, 6]], "generated": ["The gestation period for an elephant is 22 months, so a 2020 elephant pregnancy would last until 2021.", "2021 would have 3 solar eclipses, but not 4.", "Thus, the 2020 elephant pregnancy would last past next year, which would have 4 solar eclipses.", "So the answer is yes."], "conf_comments": ["(A)\nThere will be ", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7784428919303857, 0.46775872914416633, 0.31021078668970914, 0.21206062486064173], "conf": [0.9450049399683769, 0.6764404565426388, 0.4278477267703618, 0.2650773933138419], "prob": [[7.239676036082566e-08, 37], [1.6323759086939604e-13, 19], [4.899839445267654e-19, 35], [9.022331408210785e-20, 6]], "generated": ["The gestation period for an elephant is 22 months, so a 2020 elephant pregnancy would last until 2021.", "2021 would have 3 solar eclipses, but not 4.", "Thus, a 2020 elephant pregnancy may last past next year with 4 solar eclipses, but it is not guaranteed.", "So the answer is no."], "conf_comments": ["(A)\nThere will be ", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6628578357075529, 0.43244995600305614, 0.25670996668177076, 0.2080253763082806], "conf": [0.9527148042319201, 0.848729232091072, 0.6165184495471412, 0.42811099968832783], "prob": [[8.738469590229207e-08, 21], [5.906853558278982e-12, 13], [1.70388430462644e-19, 24], [1.2186002980144385e-19, 6]], "generated": ["2020 is a Leap Year, which means that it has 366 days.", "A pregnancy typically lasts 280 days.", "Thus, an elephant pregnancy would last more than a year with four solar eclipses.", "So the answer is yes."], "conf_comments": ["(A)\nElephant", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6628578357075529, 0.43244995600305614, 0.25670996668177076, 0.18110326743976057], "conf": [0.9527148042319201, 0.848729232091072, 0.6165184495471412, 0.3986145532582385], "prob": [[8.738469590229207e-08, 21], [5.906853558278982e-12, 13], [1.70388430462644e-19, 24], [3.544870143018249e-20, 6]], "generated": ["2020 is a Leap Year, which means that it has 366 days.", "A pregnancy typically lasts 280 days.", "Thus, an elephant pregnancy would last more than a year with four solar eclipses.", "So the answer is no."], "conf_comments": ["(A)\nElephant", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7689469552317184, 0.42318965477032705, 0.25604426636680183], "conf": [0.9629316220868466, 0.6818674209415091, 0.4569772157027236], "prob": [[0.00040846705034487706, 16], [1.923075194332535e-14, 28], [1.4199308658589187e-27, 50]], "generated": ["The length of an elephant pregnancy is 22 months.", "However, an elephant pregnancy can last for more than 22 months if it is a twin pregnancy.", "2020 has 4 solar eclipses, which are: solar eclipse of January 21st 2020, solar eclipse of June 21st 2020, solar eclipse of December 1"], "conf_comments": ["(A)\nThe year 2", "(A)\nThus, the", "(A)\n4th 2"], "info": null}, {"finished": true, "length": 4, "score": [0.7784428919303857, 0.46775872914416633, 0.31021078668970914, 0.14219001099469508], "conf": [0.9450049399683769, 0.6764404565426388, 0.4278477267703618, 0.26514379257255827], "prob": [[7.239676036082566e-08, 37], [1.6323759086939604e-13, 19], [4.899839445267654e-19, 35], [9.82886458463767e-24, 10]], "generated": ["The gestation period for an elephant is 22 months, so a 2020 elephant pregnancy would last until 2021.", "2021 would have 3 solar eclipses, but not 4.", "Thus, a 2020 elephant pregnancy may last past next year with 4 solar eclipses, but it is not guaranteed.", "So the answer is maybe, but not likely."], "conf_comments": ["(A)\nThere will be ", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 46.295377254486084}
{"index": 167, "question": "Does the word swastika have meaning in sanskrit?", "answer": "yes", "explanation": "Yes. In Sanskrit, the word swastika is a combination of ‘su’ (meaning ‘good’) and ‘asti’ (meaning ‘to exist’). This meaning of swastika  from Sanskrit this gets translated as ‘all is well.’ .", "generated": [{"finished": true, "length": 3, "score": [0.6533445855337171, 0.5418579955053064, 0.45858931706322725], "conf": [0.9417133050533967, 0.8014140192042443, 0.5764102843635939], "prob": [[2.7545426227585586e-08, 22], [1.1305030964551641e-09, 15], [1.1027686515459492e-09, 6]], "generated": ["Swastika is a Sanskrit word with the meaning of “lucky or auspicious object”.", "Thus, the word swastika has meaning in Sanskrit.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.645148417051633, 0.5139285114566883], "conf": [0.9302034480359167, 0.7336312819829108], "prob": [[1.1550547211595295e-06, 17], [3.1341522689757795e-07, 6]], "generated": ["Yes. The word swastika means “good fortune” in sanskrit.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6533445855337171, 0.35066583332142165, 0.2758660567272383], "conf": [0.9417133050533967, 0.7220342268049286, 0.459726561478695], "prob": [[2.7545426227585586e-08, 22], [1.6320162838792518e-15, 17], [1.3763579224106472e-15, 6]], "generated": ["Swastika is a Sanskrit word with the meaning of “lucky or auspicious object”.", "It is a Sanskrit word and it does have meaning in sanskrit.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5448071442783587, 0.31708279194480143, 0.16434183588879508, 0.13660942875068982, 0.11531399563676144], "conf": [0.9671745887621077, 0.8765365261814332, 0.7346593775231022, 0.5928739677335146, 0.42397603277969337], "prob": [[1.4945751737546405e-13, 25], [2.972066786492215e-18, 11], [1.2801520850480377e-29, 23], [1.0682704156328257e-30, 16], [1.0452587098178142e-30, 6]], "generated": ["Sanskrit is an ancient language that was the foundation of Indian, Nepalese, and Sri Lankan languages.", "Swastika also originated from Sanskrit.", "The swastika symbol comes from the word svastika, where svastika means ‘good fortune’.", "Thus, the word swastika does have meaning in Sanskrit.", "So the answer is yes."], "conf_comments": ["(A)\nThe word swast", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 1, "score": [0.6168796673137408], "conf": [0.938712505798976], "prob": [[3.880068513647339e-10, 24]], "generated": ["Yes, the word swastika in sanskrit translates to well-being, luck, or success."], "conf_comments": ["(A)\nThus, the"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 35.98839044570923}
{"index": 168, "question": "Did Sony definitively win the video game war against Sega?", "answer": "yes", "explanation": "Yes. Sony is the maker of the Playstation which has sold over 108 million PS4 units by March 2020. Sega's last console, the Sega Dreamcast, was discontinued in 2001. Sony Playstation competed with Sega's Dreamcast and Saturn systems in the 1990s. Sega now makes games for its former competitor, Sony, including Team Sonic Racing in 2019. At the height of the console wars, Sega Saturn sold 9.5 million units while Sony Playstation sold 102 million units.", "generated": [{"finished": true, "length": 4, "score": [0.5807031538653316, 0.3540392915961248, 0.27812869968234255, 0.227944152606868], "conf": [0.9623869129999998, 0.8410205410832036, 0.6140164724395892, 0.4140117261801404], "prob": [[1.7686524663895958e-13, 28], [1.2875745075939271e-24, 30], [1.0351453868175555e-25, 15], [1.0115734879257644e-25, 6]], "generated": ["The last console console Sony released that was against Sega was the PlayStation 2, which was released in 2000.", "In comparison, Sega released the Dreamcast in 1998 and discontinued the manufacturing of consoles after that year.", "Thus, Sony definitively won the video game war against Sega.", "So the answer is yes."], "conf_comments": ["(A)\nSega stopped making", "(A)\nThus, Sony", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5162978330454169, 0.3643412442841668, 0.2982352334988766], "conf": [0.9246827527313649, 0.667933155905955, 0.4511952589742309], "prob": [[5.174294161426466e-18, 32], [9.288383474013027e-21, 17], [8.846232202172045e-21, 6]], "generated": ["Sony released its first video game console in Nov 1994, which is after Sega stopped producing its next generation of video game consoles.", "Thus, Sony could not definitively win the video game war against Sega.", "So the answer is no."], "conf_comments": ["(A)\nThus, Sony", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.5807031538653316, 0.45744133918338264, 0.19639802439327092, 0.10826535272792197, 0.08652900989080996, 0.07328885254088917], "conf": [0.9623869129999998, 0.7978149341394761, 0.5864094413378222, 0.40546917231508833, 0.3117556058745413, 0.22463405780705042], "prob": [[1.7686524663895958e-13, 28], [2.2618949101866297e-16, 23], [8.66529323119897e-31, 24], [1.4291016101661687e-42, 33], [8.858981980802616e-44, 15], [8.628396372292664e-44, 6]], "generated": ["The last console console Sony released that was against Sega was the PlayStation 2, which was released in 2000.", "The last console Sega released against Sony was the Dreamcast, which was released in 1998.", "According to one source, the Dreamcast sold 1.2 million units by late 2000.", "During the same period from late 2000 to mid-2001, the PlayStation 2 sold around 25 million units.", "Thus, Sony definitively won the video game war against Sega.", "So the answer is yes."], "conf_comments": ["(A)\nSega stopped making", "(A)\nThus, Sony", "(A)\nAccording to another", "(A)\nThus, Sony", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5162978330454169, 0.3210659620277799, 0.262195554136127], "conf": [0.9246827527313649, 0.6629411955327627, 0.4482134203234297], "prob": [[5.174294161426466e-18, 32], [1.0787179141757843e-20, 10], [9.936226579384281e-21, 6]], "generated": ["Sony released its first video game console in Nov 1994, which is after Sega stopped producing its next generation of video game consoles.", "Thus, Sony did not definitively win.", "So the answer is no."], "conf_comments": ["(A)\nThus, Sony", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5807031538653316, 0.3540392915961248, 0.19412196006277288, 0.09930213771601515, 0.07976228082771382], "conf": [0.9623869129999998, 0.8410205410832036, 0.7055724253300393, 0.49222649053739537, 0.3237301175592024], "prob": [[1.7686524663895958e-13, 28], [1.2875745075939271e-24, 30], [3.412450713120191e-32, 17], [2.647281421395979e-37, 12], [2.3591685819480832e-37, 6]], "generated": ["The last console console Sony released that was against Sega was the PlayStation 2, which was released in 2000.", "In comparison, Sega released the Dreamcast in 1998 and discontinued the manufacturing of consoles after that year.", "Sony released a series of PlayStation consoles after the PlayStation 2.", "Thus, it seems that Sony won against Sega.", "So the answer is yes."], "conf_comments": ["(A)\nSega stopped making", "(A)\nThus, Sony", "(A)\nThus, Sony", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 56.432103395462036}
{"index": 169, "question": "Would an ethics professor teach a class on Cezanne?", "answer": "no", "explanation": "No. Cezanne was an Impressionist painter. Aesthetics is the branch of philosophy that deals with the arts.", "generated": [{"finished": true, "length": 4, "score": [0.6420533725720602, 0.4514579309123894, 0.3291734516365219, 0.2650218059527276], "conf": [0.9092186873222398, 0.7367339905500307, 0.5196109128683161, 0.33976588070068936], "prob": [[7.545480802848238e-05, 12], [3.2933390212956403e-07, 11], [8.706294847298437e-10, 21], [8.26239693546892e-10, 6]], "generated": ["A class on Cezanne is a class on art.", "An ethics professor is a professor of ethics.", "Thus, it would be unlikely for an ethics professor to teach a class on Cezanne.", "So the answer is no."], "conf_comments": ["(A)\nAn ethics professor", "(A)\nThus, an", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5196099044631735, 0.3446236381410415, 0.2723670850200289, 0.2187023653525476], "conf": [0.9698392850430206, 0.8292117760851258, 0.5863870538872281, 0.3791443005362914], "prob": [[1.6794650561655868e-08, 14], [5.7757197893794834e-12, 12], [7.003411469305686e-13, 17], [6.88612623981733e-13, 6]], "generated": ["Ethics professors teach about ethical behaviors and values.", "In comparison, Cezanne is a famous French painter.", "Thus, an ethics professor would not teach a class on Cezanne.", "So the answer is no."], "conf_comments": ["(A)\nCezanne was", "(A)\nThus, an", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5633495338439758, 0.33103443286424383, 0.24287955019453095, 0.20008321546858565], "conf": [0.8954404611471125, 0.7661005100585979, 0.5455388361568313, 0.3727077192867704], "prob": [[6.870960191477687e-13, 27], [5.179119606007377e-18, 13], [1.9238389923404337e-20, 20], [1.8481998595217708e-20, 6]], "generated": ["Cezanne is a French painter from the 19th century, so he is not famous for his ethics teachings.", "An ethics professor’s field of study is ethics.", "Thus, it is unlikely that an ethics professor would teach a class about Cezanne.", "So the answer is no."], "conf_comments": ["(A)\nAn ethics professor", "(A)\nThus, an", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5196099044631735, 0.3446236381410415, 0.23073289970594613, 0.1843637025270735], "conf": [0.9698392850430206, 0.8292117760851258, 0.580232674869129, 0.37233845268780597], "prob": [[1.6794650561655868e-08, 14], [5.7757197893794834e-12, 12], [7.252523421777147e-15, 15], [7.035025248310007e-15, 6]], "generated": ["Ethics professors teach about ethical behaviors and values.", "In comparison, Cezanne is a famous French painter.", "Thus, an ethics professor does not teach about Cezanne.", "So the answer is no."], "conf_comments": ["(A)\nCezanne was", "(A)\nThus, an", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6420533725720602, 0.4514579309123894, 0.2301063518006038, 0.1819794137460521], "conf": [0.9092186873222398, 0.7367339905500307, 0.4686179317582887, 0.2952881245782404], "prob": [[7.545480802848238e-05, 12], [3.2933390212956403e-07, 11], [1.5289141927264132e-16, 24], [1.4619903153290085e-16, 6]], "generated": ["A class on Cezanne is a class on art.", "An ethics professor is a professor of ethics.", "Thus an ethics professor will not teach a class on Cezanne because Cezanne is not ethics.", "So the answer is no."], "conf_comments": ["(A)\nAn ethics professor", "(A)\nThus, an", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 53.66295623779297}
{"index": 170, "question": "Can a dolphin keep a diary?", "answer": "no", "explanation": "No. A diary is a written record. Dolphins cannot write.", "generated": [{"finished": true, "length": 4, "score": [0.544004602195599, 0.41022904953202927, 0.3148158063735011, 0.2552993467362185], "conf": [0.9587158087589879, 0.8326067630384868, 0.5980062548921355, 0.3950946731229007], "prob": [[5.879436362891694e-12, 22], [9.72465127334948e-17, 26], [3.329504455843992e-18, 17], [3.23835907836595e-18, 6]], "generated": ["The average intelligence of a chimpanzee is about the same as an 8-year-old.", "In comparison, the average intelligence of a dolphin is higher than the average intelligence of an 8-year-old.", "Thus, it is possible for a dolphin to keep a diary.", "So the answer is yes."], "conf_comments": ["(A)\nThe average intelligence of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.544004602195599, 0.41022904953202927, 0.26547166171774794, 0.21172775476533356], "conf": [0.9587158087589879, 0.8326067630384868, 0.5874067415130565, 0.375260833223907], "prob": [[5.879436362891694e-12, 22], [9.72465127334948e-17, 26], [3.891722606579944e-20, 15], [3.79220224491263e-20, 6]], "generated": ["The average intelligence of a chimpanzee is about the same as an 8-year-old.", "In comparison, the average intelligence of a dolphin is higher than the average intelligence of an 8-year-old.", "Thus, dolphins have the ability to keep a diary.", "So the answer is yes."], "conf_comments": ["(A)\nThe average intelligence of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.544004602195599, 0.3128594739344986, 0.2087049435458553, 0.1201727374937658, 0.06214965748440223, 0.05004840997459387], "conf": [0.9587158087589879, 0.866271284986016, 0.7749691151703776, 0.6505742640887583, 0.47483213714769434, 0.3110501971809656], "prob": [[5.879436362891694e-12, 22], [2.2353021721040884e-19, 17], [2.3674066369756342e-26, 23], [4.910070304805064e-36, 24], [1.9031261199769642e-43, 17], [1.7911990643707868e-43, 6]], "generated": ["The average intelligence of a chimpanzee is about the same as an 8-year-old.", "In comparison, the average intelligence of a human is at about 100.", "This means that the average intelligence of a human is around 12 times higher than a chimpanzee.", "Dolphins have about the same average intelligence as chimpanzees and around 12 times less than humans.", "Thus, dolphins do not seem as capable of keeping a diary.", "So the answer is no."], "conf_comments": ["(A)\nThe average intelligence of", "(A)\nThus, the", "(A)\nThe average intelligence of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5220398603227624, 0.24451175917551257, 0.13892718115573582, 0.11400151595148916], "conf": [0.9610894181849998, 0.8441976538484806, 0.6194506278991141, 0.4225942593424081], "prob": [[1.1852004453624318e-05, 9], [6.981258415637099e-13, 12], [2.6629115817854893e-19, 18], [2.462273000888749e-19, 6]], "generated": ["Dolphins are not hominids.", "Homosapiens are the only true hominids.", "Thus, a dolphin is most likely not able to keep a diary.", "So the answer is no."], "conf_comments": ["(A)\nHominids are", "(A)\nThus, dol", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5293180865387718, 0.34392070024076776, 0.13667998855155503], "conf": [0.9688086543285804, 0.8373165596133079, 0.6734271024391458], "prob": [[8.278122920885478e-09, 15], [6.058430765305312e-21, 39], [2.7351198686486327e-56, 50]], "generated": ["To keep a diary means to record one's thoughts and experiences.", "According to [this](https://www.scientificamerican.com/article/dolphins-are-more-intelli/), dolphins are extremely intelligent.", "In fact according to [Animal Intelligence](https://www.google.com/books/edition/The_Cambridge_Encyclopedia_of_Animals/bHFq-v4KKFkC"], "conf_comments": ["(A)\nDolphins are", "(A)\nThus, dol", "(A)\nSo the answer is"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 56.65099287033081}
{"index": 171, "question": "Would Columbus have discovered Durian trees during his 1492 expedition?", "answer": "no", "explanation": "No. Columbus ended up in the Americas. Durian only exists in Southeast Asia.", "generated": [{"finished": true, "length": 4, "score": [0.7610287257618653, 0.5454763678367393, 0.4209520208467174, 0.3572801864884504], "conf": [0.9713995199374137, 0.8256014263993119, 0.6456543323954499, 0.4664821651905459], "prob": [[0.0020176720805939537, 12], [3.874926727316273e-07, 17], [1.1224095897095222e-08, 13], [1.1027003802835002e-08, 6]], "generated": ["Durian trees only grow in Southeast Asia.", "Columbus only discovered the Americas during his 1492 expedition.", "Thus, Columbus would not have discovered Durian trees.", "So the answer is no."], "conf_comments": ["(A)\nColumbus'", "(A)\nThus, Columb", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7610287257618653, 0.5454763678367393, 0.403375791681528, 0.3467564899545229], "conf": [0.9713995199374137, 0.8256014263993119, 0.6157551876750348, 0.45660095692368663], "prob": [[0.0020176720805939537, 12], [3.874926727316273e-07, 17], [2.258326739479464e-10, 24], [2.2120327765228317e-10, 6]], "generated": ["Durian trees only grow in Southeast Asia.", "Columbus only discovered the Americas during his 1492 expedition.", "Thus, it would be impossible for Columbus to discover Durian trees during his 1492 expedition.", "So the answer is no."], "conf_comments": ["(A)\nColumbus'", "(A)\nThus, Columb", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7610287257618653, 0.4532957674351583, 0.2827455920414509, 0.2290328734496937, 0.19720256963267313], "conf": [0.9713995199374137, 0.7309366274112781, 0.5547577884379519, 0.43256924171718003, 0.32144546165518856], "prob": [[0.0020176720805939537, 12], [1.3862299286808405e-11, 25], [2.34043211005333e-15, 13], [4.42064709323568e-17, 23], [4.358658474738572e-17, 6]], "generated": ["Durian trees only grow in Southeast Asia.", "During his 1492 expedition, Columbus had discovered the Bahamas and other Caribbean islands.", "These islands are very far away from Southeast Asia.", "Thus, it was impossible for Columbus to discover Durian trees during his 1492 expedition.", "So the answer is no."], "conf_comments": ["(A)\nColumbus'", "(A)\nThus, Columb", "(A)\nThus, Columb", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7610287257618653, 0.4532957674351583, 0.29825982212811125, 0.25513336293491895], "conf": [0.9713995199374137, 0.7309366274112781, 0.5528535281458611, 0.4059307505171034], "prob": [[0.0020176720805939537, 12], [1.3862299286808405e-11, 25], [6.030593347553402e-16, 18], [5.907161373660822e-16, 6]], "generated": ["Durian trees only grow in Southeast Asia.", "During his 1492 expedition, Columbus had discovered the Bahamas and other Caribbean islands.", "Thus, it was impossible for Columbus to discover Durian trees during the expedition.", "So the answer is no."], "conf_comments": ["(A)\nColumbus'", "(A)\nThus, Columb", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7610287257618653, 0.4532957674351583, 0.2827455920414509, 0.20625807189358922, 0.1756288038682157], "conf": [0.9713995199374137, 0.7309366274112781, 0.5547577884379519, 0.4296240891386474, 0.31222798531459706], "prob": [[0.0020176720805939537, 12], [1.3862299286808405e-11, 25], [2.34043211005333e-15, 13], [2.5932005603547842e-17, 12], [2.557139210479688e-17, 6]], "generated": ["Durian trees only grow in Southeast Asia.", "During his 1492 expedition, Columbus had discovered the Bahamas and other Caribbean islands.", "These islands are very far away from Southeast Asia.", "Thus, he would not have discovered Durian trees.", "So the answer is no."], "conf_comments": ["(A)\nColumbus'", "(A)\nThus, Columb", "(A)\nThus, Columb", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 57.584264278411865}
{"index": 172, "question": "Can you carry a Chrysler in a laptop bag?", "answer": "no", "explanation": "No. Chrysler manufactures automobiles, which weigh several thousand pounds. Laptop bags are designed to hold laptop computers, which typically weigh under ten pounds.", "generated": [{"finished": true, "length": 3, "score": [0.5695766642387636, 0.4465126843337673, 0.3798703538395461], "conf": [0.8911787870562908, 0.6515907522453817, 0.473169119651587], "prob": [[6.826310795622883e-14, 30], [3.561145165348994e-15, 17], [3.4910815617051107e-15, 6]], "generated": ["The mass of a Chrysler (3000 kg) is greater than the typical weight of a laptop (less than 5 kg).", "Thus, it is impossible to carry a Chrysler in a laptop bag.", "So the answer is no."], "conf_comments": ["(A)\nThus, it", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5695766642387636, 0.2661941448399112, 0.2173678718315928, 0.18277744447066183], "conf": [0.8911787870562908, 0.6144451889210709, 0.45165806324614993, 0.3200575300780355], "prob": [[6.826310795622883e-14, 30], [2.2250992775150713e-22, 17], [5.156508514873688e-23, 15], [5.0883036475951195e-23, 6]], "generated": ["The mass of a Chrysler (3000 kg) is greater than the typical weight of a laptop (less than 5 kg).", "The weight of an object must be greater than the carrying capacity of a laptop bag.", "Thus, you cannot carry a Chrysler in a laptop bag.", "So the answer is no."], "conf_comments": ["(A)\nThus, it", "(A)\nThus, you", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5996663837523184, 0.3953057092098078, 0.19318841504746412, 0.14026253608763162, 0.1188033831768904], "conf": [0.9644548068249824, 0.8219692899947628, 0.6873876031574976, 0.4927903426943635, 0.35501035472279435], "prob": [[0.00037350825450138735, 8], [1.7064149663778035e-06, 8], [6.16003914019303e-12, 10], [6.116377879597335e-14, 15], [5.965773625028411e-14, 6]], "generated": ["Chrysler is a vehicle brand.", "Laptops are portable computers.", "A computer bag is designed to hold a laptop.", "Thus, a Chrysler cannot be carried in a laptop bag.", "So the answer is no."], "conf_comments": ["(A)\nA laptop bag is", "(A)\nLaptop b", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5996663837523184, 0.3054070700911555, 0.2063631317019569, 0.17491971760275507], "conf": [0.9644548068249824, 0.7731550201636244, 0.5427071420061069, 0.3924561308576124], "prob": [[0.00037350825450138735, 8], [1.5198297727558264e-09, 11], [5.668747546491315e-12, 13], [5.4527240701161546e-12, 6]], "generated": ["Chrysler is a vehicle brand.", "Laptops are bags for laptops.", "Thus, you cannot carry a vehicle in a laptop bag.", "So the answer is no."], "conf_comments": ["(A)\nA laptop bag is", "(A)\nThus, you", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5695766642387636, 0.2661941448399112, 0.20111044462989613, 0.16575610976704871], "conf": [0.8911787870562908, 0.6144451889210709, 0.4305490126085803, 0.2931046637119836], "prob": [[6.826310795622883e-14, 30], [2.2250992775150713e-22, 17], [6.810940546248185e-24, 17], [6.723963290582393e-24, 6]], "generated": ["The mass of a Chrysler (3000 kg) is greater than the typical weight of a laptop (less than 5 kg).", "The weight of an object must be greater than the carrying capacity of a laptop bag.", "Thus, you can’t carry a Chrysler in a laptop bag.", "So the answer is no."], "conf_comments": ["(A)\nThus, it", "(A)\nThus, you", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 51.14114809036255}
{"index": 173, "question": "Has every astronaut survived their space journey?", "answer": "no", "explanation": "No. In 1986, the space shuttle Challenger exploded during launch, killing all astronauts aboard. In 2003, the space shuttle Columbia also exploded, again killing its entire crew. Various other space flights have resulted in fatal disasters.", "generated": [{"finished": true, "length": 4, "score": [0.6509727505344453, 0.3112606356574415, 0.2246397257800667, 0.18338847343071088], "conf": [0.9261198067814442, 0.6739667004666487, 0.4779560723041834, 0.3194271112491103], "prob": [[3.537614339497517e-07, 19], [2.7936049685606803e-25, 36], [2.7276750970828623e-27, 15], [2.682350939247534e-27, 6]], "generated": ["Astronauts have survived their space journeys the vast majority of the time.", "Although, there were four astronauts during the Soyuz 11 space mission on 30 June 1971 that didn’t survive.", "Thus, not all astronauts have survived their space journey.", "So the answer is no."], "conf_comments": ["(A)\nThus, it", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6509727505344453, 0.3112606356574415, 0.21046009429958726, 0.170458748545211], "conf": [0.9261198067814442, 0.6739667004666487, 0.4717220442563908, 0.3103974730130185], "prob": [[3.537614339497517e-07, 19], [2.7936049685606803e-25, 36], [7.1907209473059115e-28, 14], [7.059514348539633e-28, 6]], "generated": ["Astronauts have survived their space journeys the vast majority of the time.", "Although, there were four astronauts during the Soyuz 11 space mission on 30 June 1971 that didn’t survive.", "Thus, every astronaut did not survive their space journey.", "So the answer is no."], "conf_comments": ["(A)\nThus, it", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6511975766292073, 0.35746589602216083, 0.21117582647686164, 0.16716636147519004], "conf": [0.9514636948474535, 0.8224610032545228, 0.5776717509547661, 0.3812516036493042], "prob": [[1.220300020619858e-05, 14], [1.1271793862603179e-10, 11], [1.556133377004201e-15, 16], [1.1400391663706683e-15, 6]], "generated": ["There are over 550 people who have gone to space.", "Five astronauts have died while in space.", "Thus, more than half of astronauts survived their space journey.", "So the answer is yes."], "conf_comments": ["(A)\nOf those, ", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5501345304253731, 0.32453250670340494, 0.25597054222477567], "conf": [0.9203758156281356, 0.8049957910739861, 0.66911280593519], "prob": [[2.9869856851235024e-14, 28], [2.9528193357219943e-22, 20], [2.9073650476185807e-23, 8]], "generated": ["According to the NASA website, out of 534 human space flights, there were 41 astronaut fatalities.", "This means that there were 473 occasions where astronauts survived their space journey.", "Thus, the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.613923438118016, 0.36510940840683626], "conf": [0.8910664221759932, 0.6224647420743618], "prob": [[1.4212986339008673e-08, 21], [3.4397845420411827e-14, 19]], "generated": ["According to Wikipedia, about 60% of all astronauts survived their space journey.", "Thus, not every astronaut has survived their journey, and the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 45.9008891582489}
{"index": 174, "question": "Do members of the Supreme Court of the United States have longer terms than most senators?", "answer": "yes", "explanation": "Yes. Senators, on average, serve for 10 years. Supreme Court Justices serve for their entire life. The average term for a Supreme court justice is 16 years.", "generated": [{"finished": true, "length": 4, "score": [0.736537733825039, 0.4433161718072017, 0.3760081412929803, 0.31849925016203307], "conf": [0.9664719182358598, 0.8218572505578917, 0.6493806755118037, 0.467097788305807], "prob": [[5.410233613860806e-06, 21], [8.237699620699347e-11, 13], [1.2620992826570357e-11, 20], [1.2433025048668783e-11, 6]], "generated": ["Members of the Supreme Court of the United States serve for life or until they resign or die.", "Most senators serve in the Senate for 6 years.", "Thus, members of the Supreme Court of the United States have longer terms than most senators.", "So the answer is yes."], "conf_comments": ["(A)\nIn comparison, most", "(A)\nThus, members", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.736537733825039, 0.5208178434833327, 0.36814802690997045, 0.31621479130187563], "conf": [0.9664719182358598, 0.8728979187595778, 0.7112135328082487, 0.5268504653279069], "prob": [[5.410233613860806e-06, 21], [2.6429163679398305e-08, 9], [1.724383199409373e-11, 15], [1.6827751376653734e-11, 6]], "generated": ["Members of the Supreme Court of the United States serve for life or until they resign or die.", "Senators serve for 6 years.", "Thus, Supreme Court Justices have longer terms than most Senators.", "So the answer is yes."], "conf_comments": ["(A)\nIn comparison, most", "(A)\nThus, members", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7038281418346503, 0.4113442678329116, 0.3520127119512174, 0.3092406814005063], "conf": [0.9686867225964322, 0.8206188102430216, 0.6564836539604535, 0.5076061434102848], "prob": [[4.2780772616079106e-05, 15], [6.263748746988222e-17, 30], [1.0697966645469644e-17, 20], [1.0576520533035956e-17, 6]], "generated": ["All members of the Supreme Court of the United States are appointed for life.", "This means that members of the Supreme Court of the United States have longer terms than most elected senators, who only serve 6-year terms.", "Thus, members of the Supreme Court of the United States have longer terms than most senators.", "So the answer is yes."], "conf_comments": ["(A)\nIn comparison, most", "(A)\nThus, members", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.736537733825039, 0.5208178434833327, 0.349748294166949, 0.29561247282084174], "conf": [0.9664719182358598, 0.8728979187595778, 0.6756716706968691, 0.4842936051895786], "prob": [[5.410233613860806e-06, 21], [2.6429163679398305e-08, 9], [2.7122307212196582e-12, 17], [2.6588527040894982e-12, 6]], "generated": ["Members of the Supreme Court of the United States serve for life or until they resign or die.", "Senators serve for 6 years.", "Thus, members of the Supreme Court of the United States have longer tenures.", "So the answer is yes."], "conf_comments": ["(A)\nIn comparison, most", "(A)\nThus, members", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.7038281418346503, 0.4113442678329116, 0.3064758013796654], "conf": [0.9686867225964322, 0.8206188102430216, 0.6103294814195475], "prob": [[4.2780772616079106e-05, 15], [6.263748746988222e-17, 30], [1.0828939271451355e-17, 6]], "generated": ["All members of the Supreme Court of the United States are appointed for life.", "This means that members of the Supreme Court of the United States have longer terms than most elected senators, who only serve 6-year terms.", "So the answer is yes."], "conf_comments": ["(A)\nIn comparison, most", "(A)\nThus, members", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 51.24147391319275}
{"index": 175, "question": "Would Dante have hypothetically placed Nostradamus in 3rd Circle of Hell?", "answer": "no", "explanation": "No. Nostradamus was a famous seer and court astrologer. Dante's 3rd Circle of Hell is reserved for gluttons. The 8th Circle of Hell is reserved for frauds. Astrology was seen as a valuable skill during Dante's lifetime. Dante places the mystic and prophet Joachim of Flora in the heaven of the sun.", "generated": [{"finished": true, "length": 2, "score": [0.725565244618071, 0.5277592861069054], "conf": [0.9662929096532437, 0.7462938152252037], "prob": [[3.282090469671271e-05, 17], [2.0046533758076074e-13, 50]], "generated": ["Dante’s 3rd Circle of Hell is for gluttonous people.", "Nostradamus was a 16th century French physician, astrologer, and reputed seer who is best known for his book Les Propheties, a collection of 942 poetic quatra"], "conf_comments": ["(A)\nNostradam", "(A)\nThus, D"], "info": null}, {"finished": true, "length": 4, "score": [0.791157995740595, 0.4626667368661078, 0.23543047052798355, 0.17678007739137938], "conf": [0.966590140554025, 0.7754909602991472, 0.40563249600862983, 0.23864600633410557], "prob": [[0.002280222104836276, 14], [4.155022668168229e-14, 29], [3.479007376935471e-24, 33], [2.69509794088263e-24, 6]], "generated": ["Nostradamus was born in 1503.", "Inferno was written in the early 1320s and Paradiso was written in the early 1320s.", "Thus, Dante never had the chance to hypothetically place Nostradamus in 3rd Circle of Hell, as Dante is dead.", "So the answer is no."], "conf_comments": ["(A)\nDante's", "(A)\nThus, D", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.6111543642863766, 0.34738688377524457, 0.21276463089214598, 0.13498711252394485, 0.08398390207366241, 0.06739088470309827], "conf": [0.9655766326873428, 0.9031814431994716, 0.8233610878197669, 0.7181213809203117, 0.4788256726781759, 0.32312905463297026], "prob": [[1.6796207381382208e-06, 14], [2.3819673694792412e-14, 17], [3.3151473299181346e-18, 10], [1.3565126180224792e-25, 22], [1.4887096637356839e-30, 21], [1.1232587682216076e-30, 6]], "generated": ["The most famous work of Dante is the Divine Comedy.", "In this work, Dante placed Brutus and Cassius in Limbo.", "The Limbo is the first circle of Hell.", "In comparison, Nostradamus was known to be an astrologer and a prophet.", "Thus, Dante should have placed Nostradamus in 3rd Circle of Hell.", "So the answer is yes."], "conf_comments": ["(A)\nThe Divine Com", "(A)\nNostradam", "(A)\nNostradam", "(A)\nThus, D", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.725565244618071, 0.3428188177764971, 0.19134264455755348, 0.12295830728608012, 0.09803515277312927], "conf": [0.9662929096532437, 0.788997328852251, 0.6097025452954972, 0.4030396404097882, 0.2599297036400017], "prob": [[3.282090469671271e-05, 17], [2.0943844326297655e-11, 11], [1.0192591349603377e-17, 16], [8.3470625584176395e-22, 20], [7.655476448167153e-22, 6]], "generated": ["Dante’s 3rd Circle of Hell is for gluttonous people.", "Nostradamus was a mystic. ", "Mystics are people who are supposedly in touch with the supernatural.", "Thus, Dante could place Nostradamus in 3rd Circle of Hell.", "So the answer is yes."], "conf_comments": ["(A)\nNostradam", "(A)\nThus, D", "(A)\nThus, N", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.791157995740595, 0.4626667368661078, 0.20387219569953283, 0.15209378711506308], "conf": [0.966590140554025, 0.7754909602991472, 0.4590749347456256, 0.2824708954282313], "prob": [[0.002280222104836276, 14], [4.155022668168229e-14, 29], [9.988859547168305e-26, 24], [5.470371704209286e-26, 6]], "generated": ["Nostradamus was born in 1503.", "Inferno was written in the early 1320s and Paradiso was written in the early 1320s.", "Thus, Nostradamus wasn’t alive during these periods and he was never mentioned by Dante.", "So the answer is no."], "conf_comments": ["(A)\nDante's", "(A)\nThus, D", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 55.60286998748779}
{"index": 176, "question": "Was Mercedes-Benz associated with the Nazis?", "answer": "yes", "explanation": "Yes. During the 1930s, Mercedes-Benz produced the 770 model. The 770 was popular with Nazis, and Adolf Hitler used them as his personal vehicle.", "generated": [{"finished": true, "length": 4, "score": [0.7741425258962552, 0.49206710374375184, 0.3913280529864892, 0.3334560367845246], "conf": [0.9710802336217115, 0.8855570192101988, 0.6931005445058449, 0.5048563468712667], "prob": [[0.001883768674612249, 13], [4.660038594718325e-14, 30], [1.9063092805427723e-15, 15], [1.870402629059563e-15, 6]], "generated": ["Mercedes-Benz is a German automobile manufacturer.", "The Nazi Party, which ruled Germany from 1933-1945, was a far-right movement based in Germany.", "Thus, Mercedes-Benz was associated with the Nazis.", "So the answer is yes."], "conf_comments": ["(A)\nThe Nazis were", "(A)\nThus, Mer", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8139960415577889, 0.5407801772075853, 0.35494791481683524, 0.2974331563998555, 0.25327187231806453], "conf": [0.9683648461058932, 0.8957184827809905, 0.6895005136259867, 0.5217611725787861, 0.37937024427647437], "prob": [[7.588208713436204e-05, 25], [3.0861778594771275e-12, 23], [1.540060448039276e-18, 25], [5.014444595733171e-19, 15], [4.932246447493509e-19, 6]], "generated": ["Gottlieb Daimler and Wilhelm Maybach founded Daimler Motoren Gesellschaft in 1890.", "In 1923, it merged with Benz & Cie to form Mercedes-Benz.", "Gottlieb Daimler, Wilhelm Maybach, and Karl Benz were all members of the Nazi party.", "Thus, Mercedes-Benz was associated with the Nazis.", "So the answer is yes."], "conf_comments": ["(A)\nIn 19", "(A)\nGottlieb", "(A)\nThus, Mer", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7741425258962552, 0.49206710374375184, 0.3965195024705571, 0.33169910004094283], "conf": [0.9710802336217115, 0.8855570192101988, 0.6722982094667905, 0.47263562391347214], "prob": [[0.001883768674612249, 13], [4.660038594718325e-14, 30], [3.824280805089628e-15, 16], [3.719800713385813e-15, 6]], "generated": ["Mercedes-Benz is a German automobile manufacturer.", "The Nazi Party, which ruled Germany from 1933-1945, was a far-right movement based in Germany.", "Thus, Mercedes-Benz was not associated with the Nazis.", "So the answer is no."], "conf_comments": ["(A)\nThe Nazis were", "(A)\nThus, Mer", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.790799195615276, 0.4013797238543011, 0.3439112085463529, 0.29022101865890865], "conf": [0.9760175734520636, 0.8793953515824978, 0.7250054446026942, 0.5180734056165655], "prob": [[0.001965383050180336, 14], [4.275817517534831e-24, 38], [6.68365985323455e-25, 16], [6.547937639056741e-25, 6]], "generated": ["Mercedes-Benz was established in 1926.", "In the 1920s, Adolph Hitler was still a national figure as a World War One veteran, but he had not yet been appointed Chancellor of Germany.", "Thus, Mercedes-Benz was not associated with the Nazis.", "So the answer is no."], "conf_comments": ["(A)\nIn 19", "(A)\nThus, Mer", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.790799195615276, 0.4013797238543011, 0.2998376955626795, 0.2527330351633861], "conf": [0.9760175734520636, 0.8793953515824978, 0.7628731860364898, 0.5479827643135313], "prob": [[0.001965383050180336, 14], [4.275817517534831e-24, 38], [1.0776089894021196e-28, 24], [1.0089817093595503e-28, 6]], "generated": ["Mercedes-Benz was established in 1926.", "In the 1920s, Adolph Hitler was still a national figure as a World War One veteran, but he had not yet been appointed Chancellor of Germany.", "Thus, Mercedes-Benz had not been associated with the Nazis at the time of its founding.", "So the answer is no."], "conf_comments": ["(A)\nIn 19", "(A)\nThus, Mer", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.94043755531311}
{"index": 177, "question": "Is Disneyland Paris the largest Disney resort?", "answer": "no", "explanation": "No. Disneyland Paris contains two parks, several hotels, and a shopping district. By comparison, Walt Disney World in Florida contains four parks, two waterparks, a shopping district, and many hotels. Disney World is bigger than Disneyland Paris Resort and Disneyland California Resort combined.", "generated": [{"finished": true, "length": 3, "score": [0.5946494769709156, 0.407554299855588, 0.3288359610394401], "conf": [0.8771165956589317, 0.5860259049464419, 0.3848727198114696], "prob": [[3.397458037065807e-10, 24], [1.7218485903382634e-12, 15], [1.6334994059070447e-12, 6]], "generated": ["Disneyland Paris has 2 parks, which is more than the 1 park in other Disney resorts.", "Thus, Disneyland Paris is larger than the other Disney resorts.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Disney", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6105941426132843, 0.31421734604480284, 0.25661549141249435, 0.20926789553694936], "conf": [0.9020958203632182, 0.7627263207193108, 0.5428192545359877, 0.36178766441185833], "prob": [[4.353315264279722e-11, 27], [1.2389361296753626e-16, 11], [5.329602877728075e-17, 13], [5.25945355089774e-17, 6]], "generated": ["Disneyland Paris is the second largest Disney park after Walt Disney World, both in terms of number of rides and area.", "Walt Disney World is the first largest Disney resort.", "Thus, Disneyland Paris is not the largest Disney resort.", "So the answer is no."], "conf_comments": ["(A)\nThus, Disney", "(A)\nThus, Disney", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.49946886482317165, 0.2859343829771567, 0.2510268172569644, 0.21463008354832358, 0.12328201586193632, 0.08129858125324518, 0.05903873249402889], "conf": [0.9179097523703262, 0.8158738746850458, 0.72804671728877, 0.6434163694437927, 0.5292380076178699, 0.4119992605167361, 0.27917372221561965], "prob": [[2.4081669077042903e-10, 17], [1.6411601978760873e-12, 5], [9.13347400054271e-13, 4], [3.537337162751744e-13, 5], [2.554728004015633e-20, 18], [1.3534252804515444e-22, 9], [3.007560556179819e-23, 6]], "generated": ["There are 4 Disneyland resorts at the moment:\nDisneyland in California", "Tokyo Disneyland", "Disneyland Paris", "Hong Kong Disneyland", "Tokyo Disneyland is the largest, but Disneyland Paris is not the smallest.", "Disneyland Paris is the second largest.", "So the answer is yes."], "conf_comments": ["(A)\nDisneyland Paris", "(A)\nDisneyland Paris", "(A)\nDisneyland Paris", "(A)\nDisneyland Paris", "(A)\nThus, Disney", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.49946886482317165, 0.2859343829771567, 0.2510268172569644, 0.21463008354832358, 0.12328201586193632, 0.08129858125324518, 0.05779513676364519], "conf": [0.9179097523703262, 0.8158738746850458, 0.72804671728877, 0.6434163694437927, 0.5292380076178699, 0.4119992605167361, 0.25060314616223156], "prob": [[2.4081669077042903e-10, 17], [1.6411601978760873e-12, 5], [9.13347400054271e-13, 4], [3.537337162751744e-13, 5], [2.554728004015633e-20, 18], [1.3534252804515444e-22, 9], [4.4523955263753006e-23, 6]], "generated": ["There are 4 Disneyland resorts at the moment:\nDisneyland in California", "Tokyo Disneyland", "Disneyland Paris", "Hong Kong Disneyland", "Tokyo Disneyland is the largest, but Disneyland Paris is not the smallest.", "Disneyland Paris is the second largest.", "So the answer is no."], "conf_comments": ["(A)\nDisneyland Paris", "(A)\nDisneyland Paris", "(A)\nDisneyland Paris", "(A)\nDisneyland Paris", "(A)\nThus, Disney", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.49946886482317165, 0.2859343829771567, 0.2510268172569644, 0.21463008354832358, 0.12328201586193632, 0.0724579663549509, 0.05346979405218933], "conf": [0.9179097523703262, 0.8158738746850458, 0.72804671728877, 0.6434163694437927, 0.5292380076178699, 0.33939628759799956, 0.20869180128689052], "prob": [[2.4081669077042903e-10, 17], [1.6411601978760873e-12, 5], [9.13347400054271e-13, 4], [3.537337162751744e-13, 5], [2.554728004015633e-20, 18], [2.382846946767275e-24, 15], [1.1496742262015472e-24, 6]], "generated": ["There are 4 Disneyland resorts at the moment:\nDisneyland in California", "Tokyo Disneyland", "Disneyland Paris", "Hong Kong Disneyland", "Tokyo Disneyland is the largest, but Disneyland Paris is not the smallest.", "Thus, Disneyland Paris is neither the largest nor smallest Disneyland.", "So the answer is no."], "conf_comments": ["(A)\nDisneyland Paris", "(A)\nDisneyland Paris", "(A)\nDisneyland Paris", "(A)\nDisneyland Paris", "(A)\nThus, Disney", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 7, "score": [0.49946886482317165, 0.34849828362439444, 0.28595877768759126, 0.2529334724009197, 0.12287636434398887, 0.07242986328702732, 0.04869034623368785], "conf": [0.9179097523703262, 0.7933693491529914, 0.7106243744437194, 0.6244227602416564, 0.5143785608138677, 0.43370437905190384, 0.31245650367537503], "prob": [[2.4081669077042903e-10, 17], [7.690253406796577e-12, 6], [1.8456320681380033e-12, 5], [1.1598814997667431e-12, 4], [2.7225262188131905e-33, 38], [7.758228652630067e-40, 17], [1.80581312351573e-42, 13]], "generated": ["There are 4 Disneyland resorts at the moment:\nDisneyland in California", "Disneyland in Hong Kong", "Disneyland in Tokyo", "Disneyland Paris", "According to Wikipedia, the areas of each park are 102 acres, 110 acres, 148 acres, and 990+ acres respectively.", "Disneyland in California is the smallest and Disneyland in France is the largest.", "Thus, Disneyland Paris is not the largest Disneyland.", "So the answer is no."], "conf_comments": ["(A)\nDisneyland Paris", "(A)\nDisneyland in", "(A)\nDisneyland Paris", "(A)\nDisneyland in", "(A)\nThus, Disney", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[0.0023276100826231166, 13], [0.9792339569008703, 6]], 0.7204365894538999, "(A)\nSo the answer is", 7]}, {"finished": true, "length": 7, "score": [0.49946886482317165, 0.34849828362439444, 0.28595877768759126, 0.2529334724009197, 0.12287636434398887, 0.07242986328702732, 0.04855205438060036], "conf": [0.9179097523703262, 0.7933693491529914, 0.7106243744437194, 0.6244227602416564, 0.5143785608138677, 0.43370437905190384, 0.2914229561808117], "prob": [[2.4081669077042903e-10, 17], [7.690253406796577e-12, 6], [1.8456320681380033e-12, 5], [1.1598814997667431e-12, 4], [2.7225262188131905e-33, 38], [7.758228652630067e-40, 17], [6.938445918706303e-41, 6]], "generated": ["There are 4 Disneyland resorts at the moment:\nDisneyland in California", "Disneyland in Hong Kong", "Disneyland in Tokyo", "Disneyland Paris", "According to Wikipedia, the areas of each park are 102 acres, 110 acres, 148 acres, and 990+ acres respectively.", "Disneyland in California is the smallest and Disneyland in France is the largest.", "So the answer is no."], "conf_comments": ["(A)\nDisneyland Paris", "(A)\nDisneyland in", "(A)\nDisneyland Paris", "(A)\nDisneyland in", "(A)\nThus, Disney", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 7, "score": [0.49946886482317165, 0.2859343829771567, 0.2510268172569644, 0.21463008354832358, 0.12328201586193632, 0.0724579663549509, 0.024035666143226213], "conf": [0.9179097523703262, 0.8158738746850458, 0.72804671728877, 0.6434163694437927, 0.5292380076178699, 0.33939628759799956, 0.20020223661224215], "prob": [[2.4081669077042903e-10, 17], [1.6411601978760873e-12, 5], [9.13347400054271e-13, 4], [3.537337162751744e-13, 5], [2.554728004015633e-20, 18], [2.382846946767275e-24, 15], [3.4939911866478194e-30, 8]], "generated": ["There are 4 Disneyland resorts at the moment:\nDisneyland in California", "Tokyo Disneyland", "Disneyland Paris", "Hong Kong Disneyland", "Tokyo Disneyland is the largest, but Disneyland Paris is not the smallest.", "Thus, Disneyland Paris is neither the largest nor smallest Disneyland.", "So the answer is unknown (maybe).", "Q: If a girl named Patsy were given a car for Christmas, would she have a Honda?", "A:\nI looked up what types of cars Patsy has been connected"], "conf_comments": ["(A)\nDisneyland Paris", "(A)\nDisneyland Paris", "(A)\nDisneyland Paris", "(A)\nDisneyland Paris", "(A)\nThus, Disney", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": [[[1.466309530030032e-06, 8], [2.9211901333867723e-25, 23], [4.7805387997074655e-15, 15]], 0.5898775087645425, "(A)\n\n\n\n\n", 7]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 70.35253834724426}
{"index": 178, "question": "Would a Rockette look odd with a moustache? ", "answer": "yes", "explanation": "Yes. The Rockettes are an American dance group made up of all women. In America, it is uncommon to see a woman with a moustache.", "generated": [{"finished": true, "length": 4, "score": [0.6495985448415605, 0.3709946027134862, 0.30739163267232367, 0.2627082140728733], "conf": [0.9544420817569937, 0.8277298917701703, 0.6516530633522856, 0.48127695584266306], "prob": [[3.6003991974736296e-08, 21], [7.668617148434145e-13, 11], [9.8295476203823e-14, 15], [9.196892224905471e-14, 6]], "generated": ["A Rockette is a female dancer at Radio City Music Hall, and all Rockettes are female.", "A moustache is exclusively worn by males.", "Thus, a Rockette would look odd with a moustache.", "So the answer is yes."], "conf_comments": ["(A)\nMoustaches are", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6495985448415605, 0.3709946027134862, 0.28554444418243163, 0.24257327588416686], "conf": [0.9544420817569937, 0.8277298917701703, 0.6625356010815284, 0.4892801664648492], "prob": [[3.6003991974736296e-08, 21], [7.668617148434145e-13, 11], [6.214178196261706e-15, 16], [5.411581678845473e-15, 6]], "generated": ["A Rockette is a female dancer at Radio City Music Hall, and all Rockettes are female.", "A moustache is exclusively worn by males.", "Thus, a Rockette would look very odd with a moustache.", "So the answer is yes."], "conf_comments": ["(A)\nMoustaches are", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6495985448415605, 0.38450319206839717, 0.3276203856061038], "conf": [0.9544420817569937, 0.7632511572502291, 0.5693123406950854], "prob": [[3.6003991974736296e-08, 21], [1.8007511441423589e-12, 12], [1.5311105590235096e-12, 6]], "generated": ["A Rockette is a female dancer at Radio City Music Hall, and all Rockettes are female.", "Thus, they could not have a moustache.", "So the answer is no."], "conf_comments": ["(A)\nMoustaches are", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.6870719126001511, 0.4726888336911287], "conf": [0.8127824359502055, 0.6112813806583192], "prob": [[0.0004970740827435888, 14], [1.2230849686792439e-05, 8]], "generated": ["Yes, a Rockette would look odd with a moustache.", "Thus, the answer is yes."], "conf_comments": ["(A)\nRockettes are", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6870719126001511, 0.4279667731749922, 0.16054712310774683], "conf": [0.8127824359502055, 0.7091490007490039, 0.5070394066644175], "prob": [[0.0004970740827435888, 14], [5.167900173033864e-10, 17], [2.614271855886441e-45, 50]], "generated": ["Yes, a Rockette would look odd with a moustache.", "A Rockette is a showgirl in the Radio City Christmas Spectacular.", "Most male showgirls don't grow moustaches since moustaches are associated with a masculine look, and a male Rockette is supposed to be a graceful dancer so it looks a bit odd to have a feminine"], "conf_comments": ["(A)\nRockettes are", "(A)\nThe Radio City Christmas", "(A)\nThus, a"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 43.08039426803589}
{"index": 179, "question": "Can a sea turtle play tennis using a tennis racket?", "answer": "no", "explanation": "No. to play tennis, a human-like hand is needed to properly hold the tennis racket. sea turtles have flippers and not human-like hands.", "generated": [{"finished": true, "length": 3, "score": [0.6463272681900268, 0.4374508627290276, 0.3779992080638785], "conf": [0.9478585430028634, 0.7247928404507754, 0.5461938830158136], "prob": [[1.734072115528273e-07, 19], [1.3218478353857514e-12, 23], [1.250613726777728e-12, 6]], "generated": ["Sea turtles have fins on their limbs, which are used for swimming.", "Thus, it would be impossible for a sea turtle to hold a tennis racket and play tennis.", "So the answer is no."], "conf_comments": ["(A)\nTennis rackets", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7023765890283653, 0.30961608288116066, 0.2613545925371604, 0.22167489862848777], "conf": [0.9702277970282706, 0.7705284634708226, 0.5798323876915131, 0.418050384532358], "prob": [[0.001155216604569579, 10], [1.6714574331428172e-16, 21], [6.60976799164239e-17, 17], [6.523277649188192e-17, 6]], "generated": ["Tennis rackets are used for playing tennis.", "You cannot play tennis while using any other equipment like a sea turtle’s flippers.", "Thus, a sea turtle cannot play tennis using a tennis racket.", "So the answer is no."], "conf_comments": ["(A)\nSea turt", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7023765890283653, 0.30961608288116066, 0.23681129267020573, 0.20040054282424852], "conf": [0.9702277970282706, 0.7705284634708226, 0.5709752919080576, 0.40987862647486834], "prob": [[0.001155216604569579, 10], [1.6714574331428172e-16, 21], [3.8050638594197494e-18, 16], [3.750524692867378e-18, 6]], "generated": ["Tennis rackets are used for playing tennis.", "You cannot play tennis while using any other equipment like a sea turtle’s flippers.", "Thus, sea turtles cannot play tennis using a tennis racket.", "So the answer is no."], "conf_comments": ["(A)\nSea turt", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.655662644598949, 0.2675167018991649, 0.2163421715417042, 0.18671677270213677], "conf": [0.8905445335538179, 0.7420288592691976, 0.5824621238865408, 0.43493102403049116], "prob": [[4.197484301049997e-06, 17], [2.1573899357801738e-19, 19], [9.691725161024771e-21, 17], [9.549720775697795e-21, 6]], "generated": ["Sea turtles are not capable of playing tennis with a tennis racket.", "They have no arms or hands, nor can you hold a tennis racket in their mouth.", "Thus, a sea turtle cannot play tennis with a tennis racket.", "So the answer is no."], "conf_comments": ["(A)\nSea turt", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5568034834405943, 0.2564903258068447, 0.20751069076694093, 0.17782267708953992], "conf": [0.8808282059441536, 0.6747414167837468, 0.5210045944040721, 0.3835897357005854], "prob": [[3.7135690444504476e-11, 23], [9.487204915235595e-22, 19], [5.716493320567573e-23, 17], [5.627780762414223e-23, 6]], "generated": ["In order to use a tennis racket, a sea turtle would have to be conscious and have hands.", "Although sea turtles can move around on land, they cannot use their limbs.", "Thus, a sea turtle cannot play tennis using a tennis racket.", "So the answer is no."], "conf_comments": ["(A)\nSea turt", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 50.901010274887085}
{"index": 180, "question": "Did Donald Trump come up with the idea for the New York Harbor?", "answer": "no", "explanation": "No. The New York Harbor is at the mouth of the Hudson River. A harbor is a sheltered body of water where boats and ships can be docked. The New York Harbor has been used since colonial era of the 1500s. Donald Trump is a failed business man and 2016 president elect. Donald Trump makes outrageous deceitful claims .", "generated": [{"finished": true, "length": 4, "score": [0.8849531349397884, 0.6746830833666407, 0.4746803366992086, 0.38978246254839594], "conf": [0.9743028985094992, 0.8402892603668847, 0.6693005451885855, 0.45426796782123424], "prob": [[0.09049238730908524, 11], [0.00036087098019721875, 14], [6.898275256638514e-08, 18], [6.63204626415225e-08, 6]], "generated": ["Donald Trump was born in 1946.", "The New York Harbor was completed in 1898.", "Thus, Donald Trump was not involved with the creation of the New York Harbor.", "So the answer is no."], "conf_comments": ["(A)\nThe New York Har", "(A)\nThus, Donald", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8849531349397884, 0.6746830833666407, 0.46793837650928216, 0.38323125724815077], "conf": [0.9743028985094992, 0.8402892603668847, 0.623924851097259, 0.42912332627740657], "prob": [[0.09049238730908524, 11], [0.00036087098019721875, 14], [3.965614270276058e-08, 21], [3.4109751651691747e-08, 6]], "generated": ["Donald Trump was born in 1946.", "The New York Harbor was completed in 1898.", "Therefore, Donald Trump could not have possibly come up with the idea for the New York Harbor.", "So the answer is no."], "conf_comments": ["(A)\nThe New York Har", "(A)\nThus, Donald", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8849531349397884, 0.5535411910994346, 0.42038544441768727, 0.34538223941424423], "conf": [0.9743028985094992, 0.8636375290316711, 0.6505872613391445, 0.44133052830295005], "prob": [[0.09049238730908524, 11], [1.8782844307299253e-07, 16], [8.998900177713522e-10, 20], [8.735080550514925e-10, 6]], "generated": ["Donald Trump was born in 1946.", "Development of the New York Harbor started in 1609.", "Thus, Donald Trump could not have come up with the idea of the New York Harbor.", "So the answer is no."], "conf_comments": ["(A)\nThe New York Har", "(A)\nThus, Donald", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8849531349397884, 0.5535411910994346, 0.34463679137916137, 0.2717256426904636, 0.22412423081193664], "conf": [0.9743028985094992, 0.8636375290316711, 0.712092145747757, 0.5586303664164801, 0.3809942988503519], "prob": [[0.09049238730908524, 11], [1.8782844307299253e-07, 16], [1.2958091349765191e-17, 31], [1.2346208562345964e-19, 20], [1.2163770984151939e-19, 6]], "generated": ["Donald Trump was born in 1946.", "Development of the New York Harbor started in 1609.", "There is at least a 277-year difference between the birth of Donald Trump and the start of the development of the New York Harbor.", "Thus, Donald Trump didn’t come up with the idea for the New York Harbor.", "So the answer is no."], "conf_comments": ["(A)\nThe New York Har", "(A)\nThus, Donald", "(A)\nThus, Donald", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6112226234640165, 0.3728283295135476, 0.3016250058538922, 0.24145633480960488], "conf": [0.9501372670272001, 0.6984955718435581, 0.5371344778288831, 0.34519853314719195], "prob": [[1.3656828718296078e-05, 12], [5.000055505175976e-10, 15], [2.747450630942366e-11, 18], [2.7006577791271265e-11, 6]], "generated": ["The New York Harbor was formed by a glacier.", "Donald Trump was not even born when the New York Harbor was formed.", "Thus, he did not come up with the idea for the New York Harbor.", "So the answer is no."], "conf_comments": ["(A)\nDonald Trump was born", "(A)\nThus, Donald", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.28382182121277}
{"index": 181, "question": "Would it be unusual to use paypal for drug deals?", "answer": "yes", "explanation": "Yes. Paypal prohibits the use of their platform for drugs or drug paraphernalia.  Using paypal leaves a digital footprint of any drug purchase.", "generated": [{"finished": true, "length": 4, "score": [0.4882548179973052, 0.27218969895441636, 0.23339979900222205, 0.19973152670319627], "conf": [0.9595858807690308, 0.7748625437029225, 0.6316483272037007, 0.46543258447946584], "prob": [[5.230664190425269e-11, 17], [1.0234245988294606e-19, 21], [1.9650169050373657e-20, 16], [1.8933522201069515e-20, 6]], "generated": ["The use of paypal is generally associated with online shopping or freelancing.", "Drug deals usually happen in person, so it may be unusual for paypal to be used.", "Thus, it would be unusual to use paypal for drug deals.", "So the answer is yes."], "conf_comments": ["(A)\nDrug deals", "(A)\nHowever, it is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5751931693101519, 0.24695955214507093, 0.17333749974283047, 0.14790092721333847], "conf": [0.9747048428646976, 0.7884166539502512, 0.6060291223853902, 0.4478375012122254], "prob": [[7.939736844099431e-07, 13], [1.8447154846210027e-16, 15], [5.251639879191158e-18, 8], [4.802568651760116e-18, 6]], "generated": ["Paypal is a financial service company that transfers money online.", "It is more common and safer to pay cash for drugs.", "Thus, it would be unusual.", "So the answer is yes."], "conf_comments": ["(A)\nDrug deals", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5751931693101519, 0.24695955214507093, 0.16871312339713693, 0.1401024064213038], "conf": [0.9747048428646976, 0.7884166539502512, 0.5803940234217886, 0.41055227709352743], "prob": [[7.939736844099431e-07, 13], [1.8447154846210027e-16, 15], [7.966567203063398e-20, 17], [6.838467281887889e-20, 6]], "generated": ["Paypal is a financial service company that transfers money online.", "It is more common and safer to pay cash for drugs.", "Thus, it would be uncommon to use paypal for drug deals.", "So the answer is yes."], "conf_comments": ["(A)\nDrug deals", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5751931693101519, 0.269236712232022, 0.15435233684864194, 0.13111628306585102], "conf": [0.9747048428646976, 0.8765111519584016, 0.6987683124648537, 0.5236581255123358], "prob": [[7.939736844099431e-07, 13], [8.465340743622617e-15, 13], [1.193528637315381e-23, 23], [9.511872514829719e-24, 6]], "generated": ["Paypal is a financial service company that transfers money online.", "This implies that it is not a cash-based service.", "As drugs are often obtained through cash, it would be unusual to use Paypal for drug deals.", "So the answer is yes."], "conf_comments": ["(A)\nDrug deals", "(A)\nThus, it", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5751931693101519, 0.269236712232022, 0.1549900092150343, 0.12726996517513098], "conf": [0.9747048428646976, 0.8765111519584016, 0.724323278574455, 0.5346857027957703], "prob": [[7.939736844099431e-07, 13], [8.465340743622617e-15, 13], [1.5748864104886808e-23, 22], [9.147851768130525e-24, 6]], "generated": ["Paypal is a financial service company that transfers money online.", "This implies that it is not a cash-based service.", "In drug deals, cash is the preferred method of payment, so using paypal would be unusual.", "So the answer is yes."], "conf_comments": ["(A)\nDrug deals", "(A)\nThus, it", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 4, "score": [0.5751931693101519, 0.269236712232022, 0.1549900092150343, 0.11908696704452787], "conf": [0.9747048428646976, 0.8765111519584016, 0.724323278574455, 0.59092374358324], "prob": [[7.939736844099431e-07, 13], [8.465340743622617e-15, 13], [1.5748864104886808e-23, 22], [8.904350602835654e-26, 16]], "generated": ["Paypal is a financial service company that transfers money online.", "This implies that it is not a cash-based service.", "In drug deals, cash is the preferred method of payment, so using paypal would be unusual.", "Thus, it would be unusual to use paypal for a drug deal.", "So the answer is yes."], "conf_comments": ["(A)\nDrug deals", "(A)\nThus, it", "(A)\n# Is the above", "(A)\nSo the answer is"], "info": [[[0.005653963704006227, 16], [0.9668611004531295, 6]], 0.815828734299746, "(A)\nSo the answer is", 4]}, {"finished": true, "length": 4, "score": [0.5751931693101519, 0.24695955214507093, 0.16871312339713693, 0.11847908011363448], "conf": [0.9747048428646976, 0.7884166539502512, 0.5803940234217886, 0.40448790645222166], "prob": [[7.939736844099431e-07, 13], [1.8447154846210027e-16, 15], [7.966567203063398e-20, 17], [1.0002055529674385e-20, 6]], "generated": ["Paypal is a financial service company that transfers money online.", "It is more common and safer to pay cash for drugs.", "Thus, it would be uncommon to use paypal for drug deals.", "So the answer is no."], "conf_comments": ["(A)\nDrug deals", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 53.57904291152954}
{"index": 182, "question": "Is it safe to eat kidney?", "answer": "yes", "explanation": "Yes. Traditional British cuisine includes \"Kidney Pie\", or \"Steak and Kidney Pie\". Kidney Pie contains kidney.", "generated": [{"finished": true, "length": 6, "score": [0.5259700851146608, 0.2981272452597587, 0.24735510005116668, 0.1490626355065612, 0.09972940752305619, 0.08136246047461267], "conf": [0.9045599899311827, 0.8273753519144582, 0.7271720146639544, 0.5877972328758516, 0.4479275661491518, 0.3017762970225149], "prob": [[5.1250048018405356e-11, 20], [7.83312219879024e-18, 15], [2.006745492975755e-19, 15], [4.545824656035661e-27, 22], [9.130000886459553e-31, 16], [8.488223891844428e-31, 6]], "generated": ["To be safe, you need to know exactly what part of the kidney you are eating.", "The inner part of a kidney is called a renal medulla.", "The outer part of a kidney is called a renal cortex.", "The renal medulla contains toxins and is not safe while the renal cortex is safe.", "Thus, you can eat the renal cortex and it is safe.", "So the answer is yes."], "conf_comments": ["(A)\nThe kidney is", "(A)\nThe outer part of", "(A)\nThe renal med", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6178308671276133, 0.28364338105954146, 0.2108539630366753, 0.17209934946332975], "conf": [0.9352973455603302, 0.7593137102246859, 0.5180204901671037, 0.34635449034839627], "prob": [[4.027175393551577e-08, 19], [5.602039626574842e-14, 10], [5.51777554041277e-15, 11], [5.39871315826578e-15, 6]], "generated": ["Kidneys are organs that are responsible for filtering blood and waste from the body.", "Human kidneys are also edible.", "Thus, it is safe to eat kidney.", "So the answer is yes."], "conf_comments": ["(A)\nKidneys", "(A)\nThus, it", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 8, "score": [0.5259700851146608, 0.2981272452597587, 0.24735510005116668, 0.12991676023206142, 0.0838829143665543, 0.04293688878141541, 0.027331787201859827, 0.022624587298414666], "conf": [0.9045599899311827, 0.8273753519144582, 0.7271720146639544, 0.5957910086844754, 0.44678400710016775, 0.33914221265856326, 0.26291620318838166, 0.1862415665279434], "prob": [[5.1250048018405356e-11, 20], [7.83312219879024e-18, 15], [2.006745492975755e-19, 15], [3.167328681166275e-40, 44], [7.430716852842699e-47, 26], [5.101387811439667e-57, 22], [1.1816167317020458e-62, 20], [9.679940082306277e-63, 6]], "generated": ["To be safe, you need to know exactly what part of the kidney you are eating.", "The inner part of a kidney is called a renal medulla.", "The outer part of a kidney is called a renal cortex.", "Kidney can be safely eaten if it is the renal cortex, because this part contains a lot of water and a little bit of protein, minerals, and carbohydrates.", "However, if it is the renal medulla, it can be unsafe to eat, because it contains a lot of protein.", "Eating too much protein can cause excess amino acids in your blood, which can be harmful.", "Thus, as long as you are eating the renal cortex, it is safe.", "So the answer is yes."], "conf_comments": ["(A)\nThe kidney is", "(A)\nThe outer part of", "(A)\nThe renal med", "(A)\nKidney can", "(A)\nThus, it", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 1, "score": [0.6019893204041983], "conf": [0.9022450486810389], "prob": [[1.5583080020936367e-20, 50]], "generated": ["The National Institute of Diabetes and Digestive and Kidney Diseases states that eating kidney can be healthful because it contains essential vitamins and minerals such as Vitamin B6, folate, and iron"], "conf_comments": ["(A)\nThus, it"], "info": null}, {"finished": true, "length": 7, "score": [0.5259700851146608, 0.2981272452597587, 0.24735510005116668, 0.12991676023206142, 0.07843328767997901, 0.04498217059133394, 0.027546561843845796], "conf": [0.9045599899311827, 0.8273753519144582, 0.7271720146639544, 0.5957910086844754, 0.4534959449342093, 0.30815490218233027, 0.1984240129602173], "prob": [[5.1250048018405356e-11, 20], [7.83312219879024e-18, 15], [2.006745492975755e-19, 15], [3.167328681166275e-40, 44], [5.547934875675646e-46, 18], [3.136680996310119e-53, 23], [1.224174649214864e-54, 6]], "generated": ["To be safe, you need to know exactly what part of the kidney you are eating.", "The inner part of a kidney is called a renal medulla.", "The outer part of a kidney is called a renal cortex.", "Kidney can be safely eaten if it is the renal cortex, because this part contains a lot of water and a little bit of protein, minerals, and carbohydrates.", "You should not eat the renal medulla, because this part contains too much protein.", "So, it can be safe to eat the renal cortex and unsafe to eat the renal medulla.", "So the answer is yes."], "conf_comments": ["(A)\nThe kidney is", "(A)\nThe outer part of", "(A)\nThe renal med", "(A)\nKidney can", "(A)\nThus, it", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 8, "score": [0.5259700851146608, 0.2981272452597587, 0.24735510005116668, 0.12991676023206142, 0.0838829143665543, 0.04293688878141541, 0.027331787201859827, 0.01434415958603301], "conf": [0.9045599899311827, 0.8273753519144582, 0.7271720146639544, 0.5957910086844754, 0.44678400710016775, 0.33914221265856326, 0.26291620318838166, 0.1824485450256781], "prob": [[5.1250048018405356e-11, 20], [7.83312219879024e-18, 15], [2.006745492975755e-19, 15], [3.167328681166275e-40, 44], [7.430716852842699e-47, 26], [5.101387811439667e-57, 22], [1.1816167317020458e-62, 20], [1.8336411685733624e-65, 7]], "generated": ["To be safe, you need to know exactly what part of the kidney you are eating.", "The inner part of a kidney is called a renal medulla.", "The outer part of a kidney is called a renal cortex.", "Kidney can be safely eaten if it is the renal cortex, because this part contains a lot of water and a little bit of protein, minerals, and carbohydrates.", "However, if it is the renal medulla, it can be unsafe to eat, because it contains a lot of protein.", "Eating too much protein can cause excess amino acids in your blood, which can be harmful.", "Thus, as long as you are eating the renal cortex, it is safe.", "So, the answer is yes."], "conf_comments": ["(A)\nThe kidney is", "(A)\nThe outer part of", "(A)\nThe renal med", "(A)\nKidney can", "(A)\nThus, it", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": []}, "run_time": 64.9213809967041}
{"index": 183, "question": "Do shrimp taste best when cooked for a long time?", "answer": "no", "explanation": "No. Shrimp becomes tough and rubbery if cooked for a long time. The ideal texture for shrimp is soft and easily chewed.", "generated": [{"finished": true, "length": 3, "score": [0.5798492110032095, 0.4679476687720387, 0.3876670501998353], "conf": [0.9382693814273044, 0.6912766284852927, 0.4759235180339456], "prob": [[1.5655146347459929e-10, 22], [1.700509355417741e-11, 18], [1.668808455393043e-11, 6]], "generated": ["According to cooking.com, shrimp taste best when cooked in a matter of minutes.", "Thus, shrimp do not taste best when cooked for a long time.", "So the answer is no."], "conf_comments": ["(A)\nThus, sh", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5669945456716923, 0.2876251238100572, 0.15662778555957543, 0.11042089970356148, 0.08199894501226695], "conf": [0.9537069531677984, 0.7049628476428491, 0.5456971804791918, 0.36115691545072304, 0.20253329473146284], "prob": [[2.0162370251508984e-14, 29], [2.02493853777972e-25, 24], [1.6691152180613193e-32, 17], [7.232359961507094e-35, 19], [6.539745972739142e-35, 6]], "generated": ["If a shrimp is cooked for a long time (over 3 minutes), it can lose moisture and become tough.", "If a shrimp is not cooked long enough, it can be chewy, and taste like rubber.", "The most ideal cook time for a shrimp is 1-2 minutes.", "Thus, a shrimp tastes best when cooked for a short time.", "So the answer is no."], "conf_comments": ["(A)\nThus, sh", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A), but the previous step is"], "info": null}, {"finished": true, "length": 4, "score": [0.4861776356433304, 0.25995708734577677, 0.16935989594368756, 0.129442601317805], "conf": [0.8661218148270194, 0.6935327934324746, 0.4489649769273086, 0.2638640301546384], "prob": [[1.821311396437313e-20, 35], [2.3409818474864123e-38, 40], [3.307636705711118e-42, 21], [3.1894168414976154e-42, 6]], "generated": ["While there are factors that go into whether a shrimp tastes good, such as ingredients, one factor could be how long it was cooked.", "According to one website, shrimp tastes better if it is cooked for 1-2 minutes on the stovetop or for 4 minutes in the oven.", "Thus, shrimp tastes best when cooked for a relatively short period of time.", "So the answer is no."], "conf_comments": ["(A)\nShrimp are", "(A)\nThus, sh", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5669945456716923, 0.2983093743826229, 0.1560520351230778, 0.1114729799997433], "conf": [0.9537069531677984, 0.7648198938084647, 0.41624707313681764, 0.22227445102053506], "prob": [[2.0162370251508984e-14, 29], [1.990467816045401e-20, 13], [4.224771150185225e-26, 19], [3.216437826602748e-26, 6]], "generated": ["If a shrimp is cooked for a long time (over 3 minutes), it can lose moisture and become tough.", "Overcooking a shrimp can make it tough.", "Thus, cooked shrimp might taste best when cooked for a short time.", "So the answer is no."], "conf_comments": ["(A)\nThus, sh", "(A)\nThus, sh", "(A)\nSo the answer is", "(A), but the previous step is"], "info": null}, {"finished": true, "length": 4, "score": [0.5669945456716923, 0.2983093743826229, 0.16260792809151256, 0.09377402907932435], "conf": [0.9537069531677984, 0.7648198938084647, 0.578888591514138, 0.41755995207515284], "prob": [[2.0162370251508984e-14, 29], [1.990467816045401e-20, 13], [1.4021695524381393e-30, 25], [2.8632779364969096e-33, 8]], "generated": ["If a shrimp is cooked for a long time (over 3 minutes), it can lose moisture and become tough.", "Overcooking a shrimp can make it tough.", "In contrast, undercooking a shrimp can give the shrimp a fishy or metallic taste.", "Thus, the answer is no."], "conf_comments": ["(A)\nThus, sh", "(A)\nThus, sh", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 54.01450753211975}
{"index": 184, "question": "Hypothetically, will an African elephant be crushed by Hulk on its back?", "answer": "no", "explanation": "No. The Hulk is a Marvel comics character. The Hulk is said to be around 8 feet tall and weigh around 1400 pounds. An African elephant can carry up to 9,000 kg, or 19,841 pounds.", "generated": [{"finished": true, "length": 4, "score": [0.6767273420774519, 0.49263529856349036, 0.34296124237809056, 0.29011267245269395], "conf": [0.9676701170770017, 0.8433704784626763, 0.655256877082041, 0.4720491391757056], "prob": [[6.332889161138478e-06, 16], [9.833880380169402e-09, 13], [3.2248106128971366e-12, 17], [3.096779988952071e-12, 6]], "generated": ["An African elephant weighs about 4 to 7 tons.", "Hulk weighs about 1000 tons.", "Thus, Hulk could crush the African elephant on its back.", "So the answer is yes."], "conf_comments": ["(A)\nHulk weigh", "(A)\nThus, H", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6767273420774519, 0.49263529856349036, 0.3450351717405672, 0.2823762421180941], "conf": [0.9676701170770017, 0.8433704784626763, 0.6418569228160742, 0.4324968211477831], "prob": [[6.332889161138478e-06, 16], [9.833880380169402e-09, 13], [2.3366088348087956e-12, 19], [2.2536958834351505e-12, 6]], "generated": ["An African elephant weighs about 4 to 7 tons.", "Hulk weighs about 1000 tons.", "Thus, an African elephant would be crushed under Hulk’s weight.", "So the answer is yes."], "conf_comments": ["(A)\nHulk weigh", "(A)\nThus, H", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6767273420774519, 0.4402321210994433, 0.31267002206886313, 0.26046455250626777], "conf": [0.9676701170770017, 0.8611290311648446, 0.6425274482165243, 0.4509683397395739], "prob": [[6.332889161138478e-06, 16], [1.9157258843473636e-10, 14], [2.2486154089683686e-15, 29], [2.1005719726570194e-15, 6]], "generated": ["An African elephant weighs about 4 to 7 tons.", "In comparison, Hulk only weighs about 1 ton.", "Thus, even if Hulk was on the back of an African elephant, the African elephant would not be crushed.", "So the answer is no."], "conf_comments": ["(A)\nHulk weigh", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6767273420774519, 0.4402321210994433, 0.22481005988755992, 0.14142372921676108, 0.11825943180465706], "conf": [0.9676701170770017, 0.8611290311648446, 0.6802891268616055, 0.5091129180881835, 0.36000676660143127], "prob": [[6.332889161138478e-06, 16], [1.9157258843473636e-10, 14], [2.7216288074797405e-27, 35], [2.46063045767634e-30, 11], [2.3005366173307845e-30, 6]], "generated": ["An African elephant weighs about 4 to 7 tons.", "In comparison, Hulk only weighs about 1 ton.", "If the Hulk were to stand on the back of an African elephant, it would exert only about 15% more weight than the elephant.", "Thus, the elephant would be fine.", "So the answer is no."], "conf_comments": ["(A)\nHulk weigh", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5360648880009332, 0.3470135578614959, 0.2068730380137019, 0.12990316004960006], "conf": [0.9629481376601138, 0.8548704958504926, 0.6223154871274807, 0.4432083619193322], "prob": [[2.6167880170244886e-23, 43], [3.915850355687317e-31, 24], [4.424332371498708e-46, 48], [1.2742469724423095e-47, 6]], "generated": ["According to wolfram alpha, the weight of an African elephant is about 6.19 tons, meaning the weight of a fully-grown one is 6,190 kg.", "The weight of Mark Ruffalo is about 190 pounds, which is 86.2 kg.", "Thus, the weight of a Hulk on the back of an African elephant is about 6.19-86.2 = 5.33 tons, which is more than Hulk’s weight.", "So the answer is no."], "conf_comments": ["(A)\nAccording to wol", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 5, "score": [0.6767273420774519, 0.4402321210994433, 0.22481005988755992, 0.14142372921676108, 0.05649780073641225], "conf": [0.9676701170770017, 0.8611290311648446, 0.6802891268616055, 0.5091129180881835, 0.3851768018366129], "prob": [[6.332889161138478e-06, 16], [1.9157258843473636e-10, 14], [2.7216288074797405e-27, 35], [2.46063045767634e-30, 11], [2.5876867667188064e-53, 34]], "generated": ["An African elephant weighs about 4 to 7 tons.", "In comparison, Hulk only weighs about 1 ton.", "If the Hulk were to stand on the back of an African elephant, it would exert only about 15% more weight than the elephant.", "Thus, the elephant would be fine.", "Though the elephant might not feel comfortable with Hulk being on top of it, it would not be in any danger of being collapsed under Hulk.", "So the answer is no."], "conf_comments": ["(A)\nHulk weigh", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[1.051635672738299e-23, 34], [0.8732088880261777, 6]], 0.7565645815529992, "(A)\nSo the answer is", 5]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 57.792354345321655}
{"index": 185, "question": "Does Rupert Murdoch's alma mater have more history than the USA?", "answer": "yes", "explanation": "Yes. Rupert Murdoch's alma mater is Worcester College. Worcester College was founded in 1714. The first documented use of the term the United States of America was in a January 2, 1776 letter.", "generated": [{"finished": true, "length": 5, "score": [0.7658730586817764, 0.6451364935559786, 0.516240526036267, 0.393809493308936, 0.2939056556198686], "conf": [0.9574858793751991, 0.8834888185178785, 0.7459299508858975, 0.5594545414556933, 0.37291042521053136], "prob": [[0.0017114164124284162, 13], [1.5133273093932243e-05, 18], [3.152017522386615e-07, 14], [9.032509603363922e-09, 14], [3.0749017953633966e-09, 6]], "generated": ["Rupert Murdoch graduated from Flinders University.", "According to Wikipedia, Flinders University was established in 1966.", "The United States of America was established in 1776.", "Thus, the USA has more history than Flinders University.", "So the answer is yes."], "conf_comments": ["(A)\nFlinders University", "(A)\nThe United States was", "(A)\nThus, Fl", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8056013943759538, 0.7183773973690538, 0.4343012728680241, 0.33717816884199253, 0.26935965562371905], "conf": [0.9585539800478179, 0.9142139275868841, 0.7385804552003181, 0.4965374985419341, 0.318709136481645], "prob": [[0.0008937770251794833, 18], [7.009711990791468e-05, 14], [3.0066864041186266e-16, 33], [5.845543313824712e-17, 15], [5.647383983182644e-17, 6]], "generated": ["Rupert Murdoch’s alma mater is Georgetown University.", "Georgetown University was founded in 1789.", "In comparison, according to the History Channel, the United States was founded in 1776 (13 years before Georgetown University was founded).", "Thus, Georgetown University has more history than the USA.", "So the answer is yes."], "conf_comments": ["(A)\nGeorgetown", "(A)\nThe United States was", "(A)\nThus, Ge", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8056013943759538, 0.7183773973690538, 0.4343012728680241, 0.3190875642080438, 0.2568021485474796], "conf": [0.9585539800478179, 0.9142139275868841, 0.7385804552003181, 0.5584857196159456, 0.36626544882803275], "prob": [[0.0008937770251794833, 18], [7.009711990791468e-05, 14], [3.0066864041186266e-16, 33], [1.9161852927180723e-18, 15], [1.778277433462975e-18, 6]], "generated": ["Rupert Murdoch’s alma mater is Georgetown University.", "Georgetown University was founded in 1789.", "In comparison, according to the History Channel, the United States was founded in 1776 (13 years before Georgetown University was founded).", "Thus, Georgetown University is older than the United States.", "So the answer is yes."], "conf_comments": ["(A)\nGeorgetown", "(A)\nThe United States was", "(A)\nThus, Ge", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7658730586817764, 0.6451364935559786, 0.3616221026154455, 0.2824372560423203, 0.21656436318699698], "conf": [0.9574858793751991, 0.8834888185178785, 0.7207309308118026, 0.5365801009695048, 0.33393627753203803], "prob": [[0.0017114164124284162, 13], [1.5133273093932243e-05, 18], [4.1871025916655485e-10, 11], [2.5735370985855744e-11, 14], [1.8295164139471153e-11, 6]], "generated": ["Rupert Murdoch graduated from Flinders University.", "According to Wikipedia, Flinders University was established in 1966.", "The USA was established on 1776.", "Thus, the USA has more history than Flinders University.", "So the answer is no."], "conf_comments": ["(A)\nFlinders University", "(A)\nThe United States was", "(A)\nThus, Fl", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.8056013943759538, 0.7183773973690538, 0.4607087185131849, 0.2954752991013077, 0.1837313219033076, 0.13814445752622317], "conf": [0.9585539800478179, 0.9142139275868841, 0.7279444705021009, 0.5735742221116344, 0.3765731402026205, 0.21943793025892305], "prob": [[0.0008937770251794833, 18], [7.009711990791468e-05, 14], [9.297181216860177e-10, 17], [3.8082795038186637e-13, 12], [7.97550099577708e-17, 16], [6.649469688856832e-17, 6]], "generated": ["Rupert Murdoch’s alma mater is Georgetown University.", "Georgetown University was founded in 1789.", "The United States got its independence on July 4, 1776.", "1789 is after 1776.", "Thus, the United States has more history than Georgetown University.", "So the answer is no."], "conf_comments": ["(A)\nGeorgetown", "(A)\nThe United States was", "(A)\nThus, Ge", "(A)\nThus, Ge", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 69.74162173271179}
{"index": 186, "question": "Is Saturn named after king of gods in Greek mythology?", "answer": "no", "explanation": "No. Saturn, the sixth planet from the sun is named after the Roman god Saturn. The Roman god Saturn is derived from its Greek equivalent, Kronos. The king of the gods in Greek mythology was Zeus. Kronos was Zeus's father, and was the leader of the Titans.", "generated": [{"finished": true, "length": 4, "score": [0.7118041061663747, 0.4502016804163927, 0.3819832719092835, 0.3324889340585017], "conf": [0.9511445546770722, 0.8123151549841735, 0.6759132112284008, 0.5133177153633435], "prob": [[7.890722688197926e-05, 15], [9.5507243095937e-12, 21], [9.413815540948525e-13, 16], [9.280855138884951e-13, 6]], "generated": ["According to NASA, Saturn is the sixth planet from the Sun.", "The sixth planet is named after Cronus, the Greek god of time, in Greek mythology.", "Thus, Saturn is named after king of gods in Greek mythology.", "So the answer is yes."], "conf_comments": ["(A)\nIn Greek mythology", "(A)\nThus, Sat", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7803362056386597, 0.5418424372066211, 0.2842738455816209, 0.17890646979258523, 0.14573844466425476], "conf": [0.9151626695888987, 0.688664535558147, 0.5080322878564136, 0.37462756324104113, 0.2516401914594197], "prob": [[0.0014758543435514579, 16], [8.23856528418298e-08, 22], [6.037971025228051e-16, 19], [1.5562480616564515e-20, 17], [1.4466855921287646e-20, 6]], "generated": ["Saturn is named after the Roman god of agriculture and harvest.", "The Roman god of agriculture and harvest is similar to the Greek god of agriculture and harvest.", "The Greek god is named Cronus and is the son of Gaia and Ouranos.", "Thus, Saturn is named after the Greek counterpart of the Roman god.", "So the answer is yes."], "conf_comments": ["(A)\nIn Greek mythology", "(A)\nThus, Sat", "(A)\nThus, Sat", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7432874134865026, 0.42509740215744135, 0.23788298022288348, 0.17381498466925377, 0.14195881539716743], "conf": [0.9581643176289456, 0.8101138880248947, 0.6598452815015171, 0.47738015933261146, 0.3197476204645432], "prob": [[0.0013504632379634327, 12], [1.0140406653039377e-07, 10], [4.0658822146856793e-13, 13], [1.436926680430276e-14, 11], [1.4017762526149095e-14, 6]], "generated": ["The king of gods in Greek mythology is Zeus.", "Saturn is the father of Jupiter.", "Jupiter in Roman mythology is identical to Zeus.", "Thus, Saturn is named after Zeus.", "So the answer is yes."], "conf_comments": ["(A)\nSaturn is", "(A)\nThus, Sat", "(A)\nThus, Sat", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7118041061663747, 0.4502016804163927, 0.23940059275303333, 0.2033608596815085], "conf": [0.9511445546770722, 0.8123151549841735, 0.6647566829283038, 0.4828511147538957], "prob": [[7.890722688197926e-05, 15], [9.5507243095937e-12, 21], [4.7097889137486804e-20, 18], [4.5269329102781307e-20, 6]], "generated": ["According to NASA, Saturn is the sixth planet from the Sun.", "The sixth planet is named after Cronus, the Greek god of time, in Greek mythology.", "Thus, the fact that Saturn is named after a king of god is true.", "So the answer is yes."], "conf_comments": ["(A)\nIn Greek mythology", "(A)\nThus, Sat", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7432874134865026, 0.4144690931313205, 0.2364839690189296, 0.19040033478921792], "conf": [0.9581643176289456, 0.6590221599023283, 0.4669521345856929, 0.3043448843337944], "prob": [[0.0013504632379634327, 12], [4.110117909188376e-09, 16], [7.679075927272122e-14, 14], [7.432572226874565e-14, 6]], "generated": ["The king of gods in Greek mythology is Zeus.", "The name of planet Saturn is named after the Roman god of agriculture.", "Thus, the name is not derived from the king of gods.", "So the answer is no."], "conf_comments": ["(A)\nSaturn is", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.50621724128723}
{"index": 187, "question": "Does American Independence Day occur during autumn?", "answer": "no", "explanation": "No. Autumn runs from about September 20 to about December 20. American Independence Day is July 4, over two months before autumn begins.", "generated": [{"finished": true, "length": 4, "score": [0.774892784854833, 0.5078811763100969, 0.33724959163810914, 0.2469267880629303], "conf": [0.9574667905575455, 0.808090919369038, 0.5575654869935227, 0.3234404251850381], "prob": [[0.0014555703693691995, 14], [1.2881447037854792e-05, 7], [9.352707943712531e-08, 11], [5.825700788618063e-08, 6]], "generated": ["The American Independence Day is on the 4th of July.", "July is in the summer.", "Thus, American Independence Day occurs during summer.", "So the answer is no."], "conf_comments": ["(A)\nThe 4th", "(A)\nThus, the", "(A)\nAutumn is the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7598792637802908, 0.38096806073076844, 0.2853550832852198, 0.23728005940844554], "conf": [0.9678892019971616, 0.8682606138372022, 0.6349313209952802, 0.4408128813482038], "prob": [[0.000431472505448211, 15], [1.7451488893942942e-13, 17], [4.2724469172718405e-15, 14], [4.168876882215363e-15, 6]], "generated": ["The 4th of July is the date of American Independence Day.", "Autumn, or fall, is August 23 to November 21.", "Thus, American Independence Day does not occur during autumn.", "So the answer is no."], "conf_comments": ["(A)\nThe 4th", "(A)\nThus, American", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.722711234531264, 0.44175271263091936, 0.2543871258388356, 0.19269937197147247, 0.16179482331349182], "conf": [0.9473623057746509, 0.8465040408113773, 0.7176605771949852, 0.5368290945707622, 0.3792912544290363], "prob": [[1.1285238775837005e-06, 23], [2.356401577948624e-12, 15], [6.477778708422682e-21, 21], [1.582608575343958e-22, 14], [1.5615816119848385e-22, 6]], "generated": ["American Independence Day, otherwise known as the Fourth of July, occurs on the 4th of July.", "On a calendar, July is the 7th month of the year.", "Autumn starts on September 22 or 23, which is after the month of July.", "Thus, American Independence Day does not happen during autumn.", "So the answer is no."], "conf_comments": ["(A)\nThe Fourth of", "(A)\nThus, American", "(A)\nThus, American", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.774892784854833, 0.5078811763100969, 0.29769617659858005], "conf": [0.9574667905575455, 0.808090919369038, 0.5700994195381389], "prob": [[0.0014555703693691995, 14], [1.2881447037854792e-05, 7], [4.075866174266437e-08, 8]], "generated": ["The American Independence Day is on the 4th of July.", "July is in the summer.", "Thus, the answer is no."], "conf_comments": ["(A)\nThe 4th", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7132831012948564, 0.4164846015497164, 0.2933845588513631], "conf": [0.9694511703421486, 0.8227891393101721, 0.6642766972706028], "prob": [[9.119326310735978e-06, 18], [1.1948276465138933e-17, 30], [3.2219015010495505e-28, 50]], "generated": ["The Independence Day for the United States of America is celebrated on July 4th.", "According to [this Wikipedia article](http://en.wikipedia.org/wiki/Autumn), autumn occurs at the end of summer.", "According to [this Wikipedia article](http://en.wikipedia.org/wiki/United_States_Declaration_of_Independence), the Declaration of Independence was signed by congress in July 4th"], "conf_comments": ["(A)\nIn the Northern Hem", "(A)\nThus, the", "(A)\nThus, the"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 51.16616225242615}
{"index": 188, "question": "Is Albany, Georgia the most populous US Albany?", "answer": "no", "explanation": "No. Albany, Georgia had a population of 75,249 in 2018. Albany, New York had a population of 97,279 in 2018.", "generated": [{"finished": true, "length": 4, "score": [0.7822009069803063, 0.40678691238432013, 0.3464730299638014, 0.2950603571574115], "conf": [0.9490119392002266, 0.823551575444779, 0.6557640485971904, 0.4779249783057625], "prob": [[7.126130374164153e-06, 27], [5.483541476493333e-15, 18], [1.235466877983136e-15, 16], [1.1996526254381107e-15, 6]], "generated": ["Albany, Georgia is the most populous US Albany with a population of about 75,000 people.", "There is another Albany in Oregon with approximately 54,000 people.", "Thus, Albany, Georgia is the most populous US Albany.", "So the answer is yes."], "conf_comments": ["(A)\nAlbany,", "(A)\nThus, Alb", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.648480274824225, 0.407596941312901, 0.29840760957504187, 0.24666862518026908, 0.18643744651937633, 0.15025333530018375], "conf": [0.9486692678100026, 0.8631441537216407, 0.7384179820021345, 0.6075843272506416, 0.39853153285422266, 0.25962672694326283], "prob": [[1.1311024085986427e-05, 14], [2.205967960073371e-10, 13], [8.069887645113914e-13, 12], [8.679853099962097e-14, 12], [8.28346676269935e-15, 17], [8.135487504122872e-15, 6]], "generated": ["The most populous US Albany is located in New York state.", "The population is approximately 100,000.", "The second most populous Albany is located in Georgia state", "The population is approximately 80,000.", "Thus, Albany, Georgia is not the most populous US Albany.", "So the answer is no."], "conf_comments": ["(A)\nThe population of the", "(A)\nThe second most popul", "(A)\nThe population is approximately", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.648480274824225, 0.407596941312901, 0.2919716612851277, 0.2281414470092406, 0.18627315082664025], "conf": [0.9486692678100026, 0.8631441537216407, 0.6998917542713506, 0.4841841017613754, 0.32362270367431617], "prob": [[1.1311024085986427e-05, 14], [2.205967960073371e-10, 13], [3.750833755991921e-15, 24], [5.082590838815776e-16, 16], [5.003423194443964e-16, 6]], "generated": ["The most populous US Albany is located in New York state.", "The population is approximately 100,000.", "The second most populous Albany is in Georgia, with a population of around 80,000.", "Thus, Albany, Georgia is not the most populous Albany.", "So the answer is no."], "conf_comments": ["(A)\nThe population of the", "(A)\nThe second most popul", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.648480274824225, 0.407596941312901, 0.2919716612851277, 0.2249833578815524, 0.18281187491724638], "conf": [0.9486692678100026, 0.8631441537216407, 0.6998917542713506, 0.47049028888934913, 0.31138974915195433], "prob": [[1.1311024085986427e-05, 14], [2.205967960073371e-10, 13], [3.750833755991921e-15, 24], [4.548005840066849e-16, 17], [4.482777963284777e-16, 6]], "generated": ["The most populous US Albany is located in New York state.", "The population is approximately 100,000.", "The second most populous Albany is in Georgia, with a population of around 80,000.", "Thus, Albany, Georgia is not the most populous US Albany.", "So the answer is no."], "conf_comments": ["(A)\nThe population of the", "(A)\nThe second most popul", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.648480274824225, 0.407596941312901, 0.29840760957504187, 0.21066460875029744, 0.16581397790394264, 0.11917129891231938], "conf": [0.9486692678100026, 0.8631441537216407, 0.7384179820021345, 0.604111058219155, 0.4602634648839467, 0.2745151416541169], "prob": [[1.1311024085986427e-05, 14], [2.205967960073371e-10, 13], [8.069887645113914e-13, 12], [1.2841110454301009e-15, 13], [3.814892767952813e-17, 17], [1.6096443969900957e-17, 6]], "generated": ["The most populous US Albany is located in New York state.", "The population is approximately 100,000.", "The second most populous Albany is located in Georgia state", "with a population of about 75,000.", "Thus, Albany, Georgia is the second most populous US Albany.", "So the answer is no."], "conf_comments": ["(A)\nThe population of the", "(A)\nThe second most popul", "(A)\nThe population is approximately", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 60.9074444770813}
{"index": 189, "question": "Are the founders of Skype from Asia?", "answer": "no", "explanation": "No. Skype was created by the Niklas Zennström and the Dane Janus Friis. Niklas Zennström and Dane Janus Friis are from Sweden. Sweden is located in Europe, not Asia. ", "generated": [{"finished": true, "length": 6, "score": [0.8525777161246564, 0.6646181036230037, 0.5751340340188306, 0.43823168583924743, 0.3564969996011486, 0.2896167015824459], "conf": [0.9510756195453193, 0.8851076228704846, 0.732568535484862, 0.6249598609474665, 0.4661016271886959, 0.30826064495183236], "prob": [[0.006050694257077911, 19], [2.373803249727759e-05, 13], [6.463768691391414e-06, 13], [6.380760178134707e-08, 12], [1.1965542749482359e-08, 14], [1.1817355746011293e-08, 6]], "generated": ["The founders of Skype are Niklas Zennstrom and Janus Friis.", "Niklas Zennstrom is from Stockholm, Sweden.", "Janus Friis is from Copenhagen, Denmark.", "Stockholm and Copenhagen are located in Europe.", "Thus, the founders of Skype are not from Asia.", "So the answer is no."], "conf_comments": ["(A)\nNiklas Z", "(A)\nJanus Friis", "(A)\nThus, the", "(A)\nAsia is a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.8525777161246564, 0.6646181036230037, 0.5616053092658099, 0.40826122826110095, 0.33449970568437776, 0.26881723505638955], "conf": [0.9510756195453193, 0.8851076228704846, 0.7435856404640392, 0.6248001418587076, 0.4658877770691408, 0.30152088400338334], "prob": [[0.006050694257077911, 19], [2.373803249727759e-05, 13], [5.494057275225349e-06, 9], [2.1044473915335366e-08, 12], [4.83490755996651e-09, 14], [4.774282640539982e-09, 6]], "generated": ["The founders of Skype are Niklas Zennstrom and Janus Friis.", "Niklas Zennstrom is from Stockholm, Sweden.", "Janus Friis is from Denmark.", "Stockholm and Denmark are both located in Europe.", "Thus, the founders of Skype are not from Asia.", "So the answer is no."], "conf_comments": ["(A)\nNiklas Z", "(A)\nJanus Friis", "(A)\nThus, the", "(A)\nAsia is a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8525777161246564, 0.6646181036230037, 0.5751340340188306, 0.40094755043513686, 0.3228854501551981], "conf": [0.9510756195453193, 0.8851076228704846, 0.732568535484862, 0.5450870078643857, 0.3548218051961541], "prob": [[0.006050694257077911, 19], [2.373803249727759e-05, 13], [6.463768691391414e-06, 13], [3.896896655708287e-08, 12], [3.81050903757299e-08, 6]], "generated": ["The founders of Skype are Niklas Zennstrom and Janus Friis.", "Niklas Zennstrom is from Stockholm, Sweden.", "Janus Friis is from Copenhagen, Denmark.", "Thus, neither of the founders are from Asia.", "So the answer is no."], "conf_comments": ["(A)\nNiklas Z", "(A)\nJanus Friis", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.7198689155710429, 0.5648805881716734, 0.5109097406166838, 0.3583897280568562, 0.2759700612174964, 0.21866357762232552], "conf": [0.911735551093657, 0.8471594867828907, 0.7604739987866004, 0.566112827281124, 0.40567078380604643, 0.2552949074834756], "prob": [[2.2733845314550037e-06, 23], [4.745975145701349e-09, 15], [1.178049533920825e-09, 15], [2.3671473696907616e-12, 15], [1.669608514148945e-13, 14], [1.645803121306018e-13, 6]], "generated": ["The three founders of Skype were Niklas Zennström, Janus Friis, and Michael Jackson.", "Niklas Zennström is from Sweden, which is in Europe.", "Janus Friis is from Denmark, which is also in Europe.", "Michael Jackson is from the United States, which is in the Americas.", "Thus, the founders of Skype are not from Asia.", "So the answer is no."], "conf_comments": ["(A)\nNiklas Z", "(A)\nJanus Friis", "(A)\nMichael Jackson is from", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.8525777161246564, 0.6646181036230037, 0.5616053092658099, 0.40826122826110095, 0.2669578120776997, 0.20968912193900635], "conf": [0.9510756195453193, 0.8851076228704846, 0.7435856404640392, 0.6248001418587076, 0.44289620093071813, 0.27391957091005004], "prob": [[0.006050694257077911, 19], [2.373803249727759e-05, 13], [5.494057275225349e-06, 9], [2.1044473915335366e-08, 12], [3.897236391344548e-12, 17], [3.840867321124205e-12, 6]], "generated": ["The founders of Skype are Niklas Zennstrom and Janus Friis.", "Niklas Zennstrom is from Stockholm, Sweden.", "Janus Friis is from Denmark.", "Stockholm and Denmark are both located in Europe.", "Thus, the founders of Skype are European and are not from Asia.", "So the answer is no."], "conf_comments": ["(A)\nNiklas Z", "(A)\nJanus Friis", "(A)\nThus, the", "(A)\nAsia is a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 74.41674852371216}
{"index": 190, "question": "Are all Wednesdays in a year enough to read Bible 15 times?", "answer": "yes", "explanation": "Yes. There are 52 Wednesdays in a year. There are 1,248 hours over all the Wednesdays in a year. The Old Testament of the Bible takes an average of 52 hours to read. The New Testament of the Bible takes an average of 18 hours to read.", "generated": [{"finished": true, "length": 4, "score": [0.742120571520884, 0.4798181075544259, 0.36614316501804245, 0.3086435229605244], "conf": [0.9559285493865585, 0.8337518083724317, 0.6327337812826914, 0.4514185006341376], "prob": [[4.8912870686312384e-05, 18], [2.4257182942226506e-13, 26], [5.483031076612415e-16, 23], [5.35236285993645e-16, 6]], "generated": ["In the year 2019, there are 53 Wednesdays.", "If you read the Bible every Wednesday for an hour, you would read the Bible 53 times in a year.", "Thus, all Wednesdays in a year would not be enough to read the Bible 15 times.", "So the answer is no."], "conf_comments": ["(A)\nThe Bible is ", "(A)\nThus, all", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8717075741722765, 0.4241131578007455, 0.3100914685352992, 0.2661514712548799], "conf": [0.9696629400453849, 0.8273394121828601, 0.6343161356775747, 0.47087960436984966], "prob": [[0.08733904047222013, 10], [1.0475671154459323e-15, 25], [3.757093432948844e-19, 22], [3.5883592193474594e-19, 6]], "generated": ["There are 52 weeks in a year.", "So you can find that if each week has 7 days, there are about 364 days in a year.", "Thus, there are not enough Wednesdays in a year to read the Bible 15 times.", "So the answer is no."], "conf_comments": ["(A)\nThere are 1", "(A)\nThe Bible is about", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.742120571520884, 0.4798181075544259, 0.25918150332071027, 0.20820325148235042], "conf": [0.9559285493865585, 0.8337518083724317, 0.5360351427161033, 0.34769469305723294], "prob": [[4.8912870686312384e-05, 18], [2.4257182942226506e-13, 26], [7.343573603027085e-20, 19], [7.120011498307621e-20, 6]], "generated": ["In the year 2019, there are 53 Wednesdays.", "If you read the Bible every Wednesday for an hour, you would read the Bible 53 times in a year.", "Thus, all Wednesdays in a year are not enough and you would need more.", "So the answer is no."], "conf_comments": ["(A)\nThe Bible is ", "(A)\nThus, all", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 8, "score": [0.8717075741722765, 0.6020450618754309, 0.27501171554593606, 0.1521867894967099, 0.10788335371826421, 0.06361313353696033, 0.04628993041538012, 0.03874156990191304], "conf": [0.9696629400453849, 0.8765228559245053, 0.7760759240967291, 0.6399955045972827, 0.5006579986095948, 0.37274634572180726, 0.2897895363523589, 0.20417410888778917], "prob": [[0.08733904047222013, 10], [3.156392351152606e-06, 16], [3.559455210582701e-23, 27], [8.391951883903871e-38, 34], [3.810281972974748e-44, 33], [4.0992716600403837e-48, 12], [4.0714661583143745e-52, 24], [3.931255462356302e-52, 6]], "generated": ["There are 52 weeks in a year.", "Thus, there are 260 Wednesdays in a year.", "A biblical year consists of 365 days, so there are 57 Sunday to Saturday cycles in a year.", "Because there are 57 days in a cycle and 260 Wednesdays, there will be 4.76 Wednesdays per cycle.", "Thus, it would take 15 / 4.76 = 3.16 Wednesdays to read the Bible 15 times.", "3.16 is greater than 260.", "Thus, all of the Wednesdays in a year is not enough to read the Bible 15 times.", "So the answer is no."], "conf_comments": ["(A)\nThere are 1", "(A)\nThe Bible is ", "(A)\nThus, there", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.8717075741722765, 0.6020450618754309, 0.27501171554593606, 0.1521867894967099, 0.09299804510354204, 0.06538599023193768, 0.05382680633403888], "conf": [0.9696629400453849, 0.8765228559245053, 0.7760759240967291, 0.6399955045972827, 0.4988027566104127, 0.3588617719455828, 0.24755534338908278], "prob": [[0.08733904047222013, 10], [3.156392351152606e-06, 16], [3.559455210582701e-23, 27], [8.391951883903871e-38, 34], [7.116350207148235e-44, 19], [1.2699772995903814e-47, 23], [1.141542813024409e-47, 6]], "generated": ["There are 52 weeks in a year.", "Thus, there are 260 Wednesdays in a year.", "A biblical year consists of 365 days, so there are 57 Sunday to Saturday cycles in a year.", "Because there are 57 days in a cycle and 260 Wednesdays, there will be 4.76 Wednesdays per cycle.", "Thus, there will be about 25.25 Wednesdays per year.", "Therefore, all Wednesdays in a year will not be enough to read the Bible 15 times.", "So the answer is no."], "conf_comments": ["(A)\nThere are 1", "(A)\nThe Bible is ", "(A)\nThus, there", "(A)\nThus, it", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": []}, "run_time": 73.63382244110107}
{"index": 191, "question": "Did the butler Eugene Allen retire the same year a centuries-old war ended?", "answer": "yes", "explanation": "Yes. Eugene Allen was a butler at the White House for 34 years until 1986. The United Kingdom and the Kingdom of the Netherlands ended the Three Hundred and Thirty Five Years' Warnin 1986.", "generated": [{"finished": true, "length": 5, "score": [0.9077662148726112, 0.6048791331767197, 0.49436052740192904, 0.3265022793608945, 0.2699981490711372], "conf": [0.982144992131524, 0.8595797221682697, 0.710053485250316, 0.5794217874228701, 0.4327282516830524], "prob": [[0.12169397746097319, 12], [5.214763063670233e-09, 25], [2.1548477021417396e-10, 15], [1.7911548302317492e-14, 15], [1.0556146056533673e-14, 6]], "generated": ["Eugene Allen retired in 1986.", "The last centuries-old war was the Franco-Prussian War (1870-1871).", "The Franco-Prussian War ended in 1871.", "Thus, the war ended in the year before Eugene Allen retired.", "So the answer is yes."], "conf_comments": ["(A)\nThe Falklands", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.9077662148726112, 0.6048791331767197, 0.49436052740192904, 0.3265022793608945, 0.2429089216678152], "conf": [0.982144992131524, 0.8595797221682697, 0.710053485250316, 0.5794217874228701, 0.38071082174419274], "prob": [[0.12169397746097319, 12], [5.214763063670233e-09, 25], [2.1548477021417396e-10, 15], [1.7911548302317492e-14, 15], [6.40055611276709e-15, 6]], "generated": ["Eugene Allen retired in 1986.", "The last centuries-old war was the Franco-Prussian War (1870-1871).", "The Franco-Prussian War ended in 1871.", "Thus, the war ended in the year before Eugene Allen retired.", "So the answer is no."], "conf_comments": ["(A)\nThe Falklands", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7793331470520799, 0.4427914808024178, 0.3672347120421331, 0.3222068463437564], "conf": [0.973563143361726, 0.8230179719876114, 0.6876473358564376, 0.5317070346443363], "prob": [[2.9323692289828566e-06, 27], [4.678971251162118e-20, 33], [1.7147575741311434e-21, 17], [1.6697649833526828e-21, 6]], "generated": ["Eugene Allen began working at the White House in 1952, and he retired in 1986.", "The First Indochina War ended (1954-1975) about 31 years prior to Eugene Allen’s retirement.", "Thus, Eugene Allen retired the same year a centuries-old war ended.", "So the answer is yes."], "conf_comments": ["(A)\nThe American Civil War", "(A)\nThe Second Indoch", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7407595567776504, 0.5341922692565495, 0.3778138800821621, 0.25669296427492444, 0.2030683874201392], "conf": [0.9588433742089548, 0.7207277432044593, 0.5382178625862994, 0.35075573307169355, 0.22075667936996887], "prob": [[8.713073340254312e-07, 25], [1.382561481114091e-11, 30], [1.1281568646435477e-13, 12], [2.0323828776175613e-17, 25], [1.964677803827503e-17, 6]], "generated": ["The War in Vietnam (1945-46) ended on September 2, 1945.", "The War of the Roses (1455-1485) ended on April 21, 1455.", "Eugene Allen retired in 1986.", "Thus, the War in Vietnam and the War of the Roses did not end the same year Eugene Allen retired.", "So the answer is no."], "conf_comments": ["(A)\nEugene Allen", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.9077662148726112, 0.6048791331767197, 0.49436052740192904, 0.30301121589895025, 0.2016902452740793, 0.14197753997255905, 0.07690897073493969], "conf": [0.982144992131524, 0.8595797221682697, 0.710053485250316, 0.5595342826189001, 0.3754198826514766, 0.22560865796014687, 0.14997878746495197], "prob": [[0.12169397746097319, 12], [5.214763063670233e-09, 25], [2.1548477021417396e-10, 15], [1.5350542706694985e-15, 16], [2.518382949907846e-19, 21], [7.916037614456769e-20, 6], [7.0934583819908926e-37, 48]], "generated": ["Eugene Allen retired in 1986.", "The last centuries-old war was the Franco-Prussian War (1870-1871).", "The Franco-Prussian War ended in 1871.", "The Franco-Prussian War ended two years before Eugene Allen retired.", "Thus, it is possible that Eugene Allen retired the same year a centuries-old war ended.", "So the answer is maybe.", "Note: Eugene Allen did not retire the same year the last centuries-old war ended. He retired in 1986, and the Franco-Prussian War ended in 1871. So this question"], "conf_comments": ["(A)\nThe Falklands", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 69.02759528160095}
{"index": 192, "question": "Did the writer of Christmas carol fast during Ramadan? ", "answer": "no", "explanation": "No. The writer of Christmas carol is Charles Dickens, who is a Christian.  Christians do not fast during Ramadan. ", "generated": [{"finished": true, "length": 5, "score": [0.6970424492939797, 0.37677399222753755, 0.23877827004979654, 0.18847372544960345, 0.1550213823675791], "conf": [0.9589040920844868, 0.8587502536862289, 0.6646380448639267, 0.49244000919447617, 0.3346857111451665], "prob": [[0.0005651660999299024, 11], [4.150221551443826e-17, 27], [8.212854568333065e-21, 13], [7.25953239185262e-22, 14], [7.061469485598841e-22, 6]], "generated": ["The writer of Christmas carol is Charles Wesley.", "According to a Christmas carol website, he was Anglican, which means he is a member of the Church of England.", "Ramadan is a religious observance of Muslims.", "Thus, Charles Wesley did not fast during Ramadan.", "So the answer is no."], "conf_comments": ["(A)\nCharles Wesley", "(A)\nThe Church of England", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7320599097527631, 0.42439235940164377, 0.25862830290341704, 0.2019151536983159], "conf": [0.9703929535592525, 0.8679328730505372, 0.7230215487307946, 0.493376973571823], "prob": [[4.1349992234838855e-05, 17], [4.625114558472576e-11, 14], [6.940251980735838e-21, 28], [3.524713256062378e-21, 6]], "generated": ["Ramadan is the holy month of fasting in the Islamic calendar.", "Christmas carol was published in June 1843.", "In 1843, Ramadan happened in June and July, meaning that the writer did not fast during Ramadan.", "So the answer is no."], "conf_comments": ["(A)\nChristmas carol", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6970424492939797, 0.37677399222753755, 0.23877827004979654, 0.16349115908188347, 0.13343535654453345], "conf": [0.9589040920844868, 0.8587502536862289, 0.6646380448639267, 0.48232002524635453, 0.32318220548683824], "prob": [[0.0005651660999299024, 11], [4.150221551443826e-17, 27], [8.212854568333065e-21, 13], [8.505892883693736e-25, 21], [8.210446961725966e-25, 6]], "generated": ["The writer of Christmas carol is Charles Wesley.", "According to a Christmas carol website, he was Anglican, which means he is a member of the Church of England.", "Ramadan is a religious observance of Muslims.", "Thus, Charles Wesley is not a Muslim and would not have fasted during Ramadan.", "So the answer is no."], "conf_comments": ["(A)\nCharles Wesley", "(A)\nThe Church of England", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7320599097527631, 0.5139751030452869, 0.2284888633458504, 0.1633396923813505, 0.12758434498155233], "conf": [0.9703929535592525, 0.8960733361588016, 0.7071981673156374, 0.47507705232013236, 0.2916247101305678], "prob": [[4.1349992234838855e-05, 17], [1.1819807001561633e-08, 13], [1.1280684489396692e-17, 15], [1.0795763665033084e-19, 17], [1.0407816494149005e-19, 6]], "generated": ["Ramadan is the holy month of fasting in the Islamic calendar.", "Christmas carol was written in 1818.", "This is 150 years before the 20th century.", "Thus, the writer of Christmas carol did not fast during Ramadan.", "So the answer is no."], "conf_comments": ["(A)\nChristmas carol", "(A)\nIn comparison, Ram", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7320599097527631, 0.42439235940164377, 0.25862830290341704, 0.16347385827704328, 0.10865983904981066], "conf": [0.9703929535592525, 0.8679328730505372, 0.7230215487307946, 0.39140466526436624, 0.17852681054049543], "prob": [[4.1349992234838855e-05, 17], [4.625114558472576e-11, 14], [6.940251980735838e-21, 28], [5.376399624065082e-23, 16], [4.4409490054937235e-23, 6]], "generated": ["Ramadan is the holy month of fasting in the Islamic calendar.", "Christmas carol was published in June 1843.", "In 1843, Ramadan happened in June and July, meaning that the writer did not fast during Ramadan.", "Thus, the writer of Christmas carol fasted during Ramadan.", "So the answer is no."], "conf_comments": ["(A)\nChristmas carol", "(A)\nThus, the", "(A)\n# Is the above", "(A)\nSo the answer is", "(B), because the writer of Christmas"], "info": null}, {"finished": true, "length": 5, "score": [0.7320599097527631, 0.42439235940164377, 0.25862830290341704, 0.16347385827704328, 0.10259780720418167], "conf": [0.9703929535592525, 0.8679328730505372, 0.7230215487307946, 0.39140466526436624, 0.21364915631065698], "prob": [[4.1349992234838855e-05, 17], [4.625114558472576e-11, 14], [6.940251980735838e-21, 28], [5.376399624065082e-23, 16], [7.591325743931483e-24, 6]], "generated": ["Ramadan is the holy month of fasting in the Islamic calendar.", "Christmas carol was published in June 1843.", "In 1843, Ramadan happened in June and July, meaning that the writer did not fast during Ramadan.", "Thus, the writer of Christmas carol fasted during Ramadan.", "So the answer is yes."], "conf_comments": ["(A)\nChristmas carol", "(A)\nThus, the", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 66.39926409721375}
{"index": 193, "question": "Is entire Common Era minuscule to lifespan of some trees?", "answer": "yes", "explanation": "Yes. The Common Era has lasted for over 2,000 years as of 2020. A tree named Methuselah, from California's White Mountains, is almost 5,000 years old.", "generated": [{"finished": true, "length": 4, "score": [0.8165953006788437, 0.5661863734356481, 0.46694219066973797, 0.39586745580351146], "conf": [0.9678551544434781, 0.8785101552254517, 0.729915432634812, 0.5266822020593864], "prob": [[0.011440384589906429, 12], [6.513959514702819e-08, 19], [1.1898110584064686e-09, 20], [1.162148491520254e-09, 6]], "generated": ["The Common Era is about 2000 years.", "Bristlecone pines can live to be 5000 years old.", "Thus, the Common Era is minuscule compared to the lifespan of some trees.", "So the answer is yes."], "conf_comments": ["(A)\nThe lifespan", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8165953006788437, 0.5661863734356481, 0.38789588049677926, 0.3022464853711705, 0.25936425004309266], "conf": [0.9678551544434781, 0.8785101552254517, 0.7747512988128069, 0.6605760679184937, 0.4887760088163895], "prob": [[0.011440384589906429, 12], [6.513959514702819e-08, 19], [1.7383956494898508e-14, 24], [2.5466650456958956e-18, 26], [2.4742057502041892e-18, 6]], "generated": ["The Common Era is about 2000 years.", "Bristlecone pines can live to be 5000 years old.", "In comparison, the Common Era is only a small fraction of the lifetime of a bristlecone pine.", "Thus, the whole Common Era is minuscule to the lifespan of a bristlecone pine.", "So the answer is yes."], "conf_comments": ["(A)\nThe lifespan", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8165953006788437, 0.5661863734356481, 0.38789588049677926, 0.281507422247829, 0.24162636629342127], "conf": [0.9678551544434781, 0.8785101552254517, 0.7747512988128069, 0.6626395268464491, 0.4899598377965699], "prob": [[0.011440384589906429, 12], [6.513959514702819e-08, 19], [1.7383956494898508e-14, 24], [3.588193090672773e-20, 27], [3.51099827120508e-20, 6]], "generated": ["The Common Era is about 2000 years.", "Bristlecone pines can live to be 5000 years old.", "In comparison, the Common Era is only a small fraction of the lifetime of a bristlecone pine.", "Thus, the entire Common Era is minuscule to the lifespan of a given bristlecone pine.", "So the answer is yes."], "conf_comments": ["(A)\nThe lifespan", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7986100453650599, 0.5531678251845645, 0.33307416090324127, 0.2815372101545117], "conf": [0.9697405597782116, 0.8894235353490484, 0.7218056205061387, 0.5224425847624903], "prob": [[0.004307067709356094, 13], [2.0775603765916125e-10, 26], [4.157142242242146e-18, 22], [3.846124585468216e-18, 6]], "generated": ["The Common Era is about 2,000 years.", "In comparison, some species of bristlecone pine tree can live for about 5,000 years.", "Therefore, the Common Era is a minuscule when compared to the lifespan of some trees.", "So the answer is yes."], "conf_comments": ["(A)\nThe lifespan", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7117687498861327, 0.4559909524141503, 0.3253480940715597, 0.2733327391842074], "conf": [0.9746782277935091, 0.8844788209579703, 0.7128633709491848, 0.5055296907141694], "prob": [[0.0001050586826538932, 14], [2.7568792684201404e-12, 22], [7.135820666414968e-18, 28], [6.936212046068411e-18, 6]], "generated": ["The Common Era lasts approximately 2,000 years.", "Bristlecone pine is known to live for more than 4,000 years.", "Thus, the entire Common Era is minuscule in comparison to the lifespan of Bristlecone pine trees.", "So the answer is yes."], "conf_comments": ["(A)\nThe lifespan", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 5, "score": [0.8165953006788437, 0.5661863734356481, 0.38789588049677926, 0.281507422247829, 0.12671497618315525], "conf": [0.9678551544434781, 0.8785101552254517, 0.7747512988128069, 0.6626395268464491, 0.4975712066592703], "prob": [[0.011440384589906429, 12], [6.513959514702819e-08, 19], [1.7383956494898508e-14, 24], [3.588193090672773e-20, 27], [5.800044804914465e-35, 26]], "generated": ["The Common Era is about 2000 years.", "Bristlecone pines can live to be 5000 years old.", "In comparison, the Common Era is only a small fraction of the lifetime of a bristlecone pine.", "Thus, the entire Common Era is minuscule to the lifespan of a given bristlecone pine.", "It is because the maximum lifespan of a bristlecone pine would be much larger than the Common Era.", "So the answer is yes."], "conf_comments": ["(A)\nThe lifespan", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[1.616424941007558e-15, 26], [0.930304154137499, 6]], 0.7508927350398923, "(A)\nSo the answer is", 5]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 59.516138792037964}
{"index": 194, "question": "Is Tange Sazen hypothetically an ideal choice for a secretary job?", "answer": "no", "explanation": "No. Secretaries are required to type and also read copious amounts of notes. Tange Sazen is a one-eyed, one-armed swordsman in Japanese literature.", "generated": [{"finished": true, "length": 5, "score": [0.6192470227252671, 0.3071415422405792, 0.16581769094834742, 0.12308042667994439, 0.10185335644088019], "conf": [0.9412113431082911, 0.8112112859720074, 0.5926654834967912, 0.4347129717403673, 0.299185192961131], "prob": [[8.521756596735997e-06, 13], [2.4920591584556164e-12, 12], [2.571536320588357e-18, 15], [4.680786094224722e-20, 14], [4.542867198492555e-20, 6]], "generated": ["There is a fictional character named Tange Sazen.", "His specialty is cutting off people’s heads.", "A secretary job does not involve the cutting off of people’s heads.", "Thus, Tange Sazen is not an ideal choice.", "So the answer is no."], "conf_comments": ["(A)\nTange Saz", "(A)\nThus, T", "(A)\nThus, T", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6192470227252671, 0.3071415422405792, 0.16581769094834742, 0.1230932286939054, 0.10085036698385336], "conf": [0.9412113431082911, 0.8112112859720074, 0.5926654834967912, 0.4075349627467794, 0.2751339306638829], "prob": [[8.521756596735997e-06, 13], [2.4920591584556164e-12, 12], [2.571536320588357e-18, 15], [4.780982741271527e-20, 18], [4.619142226149977e-20, 6]], "generated": ["There is a fictional character named Tange Sazen.", "His specialty is cutting off people’s heads.", "A secretary job does not involve the cutting off of people’s heads.", "Thus, Tange Sazen is not a good choice for a secretary job.", "So the answer is no."], "conf_comments": ["(A)\nTange Saz", "(A)\nThus, T", "(A)\nThus, T", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5733828097726605, 0.24262054163909663, 0.15634609018864445, 0.12263196063035706], "conf": [0.9368351086502341, 0.6570994999391914, 0.44407513588447245, 0.27572870102863073], "prob": [[5.261466813349928e-13, 27], [2.0093248616879485e-28, 26], [7.268011836567128e-33, 21], [6.87800173224251e-33, 6]], "generated": ["Tange Sazen is a samurai from a famous Japanese children’s folklore, Tange Sazen.", "Since this is a fairy tale, it is hypothetical, so it doesn’t mean anything to the real world.", "Thus, Tange Sazen hypothetically is not the ideal choice for a secretary job.", "So the answer is no."], "conf_comments": ["(A)\nTange Saz", "(A)\nThus, T", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.70900616057494, 0.3022703394659177, 0.1419763255147114, 0.11391031717420155], "conf": [0.9537271230424621, 0.8342148188757823, 0.5490467440062038, 0.35591303635927046], "prob": [[0.0004597382164050321, 12], [2.152254159669791e-18, 21], [4.86659074869433e-25, 14], [4.66641032331884e-25, 6]], "generated": ["Tange Sazen was a famous swordsman.", "A swordsman is often pictured as cool, calm, collected, and levelheaded.", "Thus, he would be an ideal choice for the secretary position.", "So the answer is yes."], "conf_comments": ["(A)\nHe was known for", "(A)\nThus, T", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.70900616057494, 0.28766604202309365, 0.14746637815586308, 0.11250608220746401], "conf": [0.9537271230424621, 0.8237462795255667, 0.5517709433642233, 0.3474272228636282], "prob": [[0.0004597382164050321, 12], [2.656709003167505e-16, 17], [8.368344128830983e-23, 16], [5.221620116558251e-23, 6]], "generated": ["Tange Sazen was a famous swordsman.", "To be a famous swordsman, you should have skills of using a sword.", "Thus, Tange Sazen had necessary skills to be a secretary.", "So the answer is yes."], "conf_comments": ["(A)\nHe was known for", "(A)\nThus, T", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 58.67969465255737}
{"index": 195, "question": "Are Brussels sprout particularly good for adrenal fatigue?", "answer": "yes", "explanation": "Yes. Adenal fatigue is a disorder in which the body does not produce enough hormones and people get tired. Brussels sprout are foods rich in vitamin C. When stress levels rise, the adrenal glands require more Vitamin C and it is used very quickly.", "generated": [{"finished": true, "length": 4, "score": [0.7183756104724767, 0.38460726933010925, 0.3120097037892383, 0.26021965309763784], "conf": [0.9639599366435119, 0.8163657718289292, 0.6106450102950776, 0.42595794889915606], "prob": [[0.0005542811218308487, 12], [5.563298319850519e-12, 17], [5.552415454807516e-13, 18], [5.458563623140939e-13, 6]], "generated": ["Brussels sprouts are rich in Vitamin C.", "Several studies have shown that vitamin C could improve adrenal function.", "Thus, Brussels sprouts are particularly good for adrenal fatigue.", "So the answer is yes."], "conf_comments": ["(A)\nVitamin C", "(A)\nThus, Br", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7183756104724767, 0.3803934933459558, 0.30185819634414834, 0.24748293002542887], "conf": [0.9639599366435119, 0.6916999936860229, 0.4781547781125023, 0.3222696638135085], "prob": [[0.0005542811218308487, 12], [5.3106032651264475e-15, 27], [9.90710803818559e-16, 18], [9.748743849267407e-16, 6]], "generated": ["Brussels sprouts are rich in Vitamin C.", "Vitamin C is important for a healthy adrenal gland, thus making it better for adrenal fatigue.", "Thus, Brussels sprouts are particularly good for adrenal fatigue.", "So the answer is yes."], "conf_comments": ["(A)\nVitamin C", "(A)\nThus, Br", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6973982895704771, 0.5314377185723392, 0.24496499247731265, 0.17541844299853668, 0.14026384155443383], "conf": [0.959452479659295, 0.8636245252972042, 0.6415835097864563, 0.45195958380272855, 0.29020188223802623], "prob": [[3.750371526888184e-05, 15], [8.110642324195385e-08, 14], [2.081766774418782e-21, 25], [4.991797745128351e-24, 19], [4.865180513983577e-24, 6]], "generated": ["The adrenal gland is part of the endocrine system.", "Endocrinology is the study of the endocrine system.", "The Brussels sprout is the vegetable of choice when dealing with the symptoms of adrenal fatigue.", "Thus, Brussels sprouts can be particularly good for adrenal fatigue.", "So the answer is yes."], "conf_comments": ["(A)\nBrussels spr", "(A)\nBrussels spr", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7183756104724767, 0.38460726933010925, 0.24793352221726497, 0.1955362728826586], "conf": [0.9639599366435119, 0.8163657718289292, 0.5969294738780032, 0.41582751455175126], "prob": [[0.0005542811218308487, 12], [5.563298319850519e-12, 17], [3.745296836533229e-16, 17], [1.8977982407864404e-16, 6]], "generated": ["Brussels sprouts are rich in Vitamin C.", "Several studies have shown that vitamin C could improve adrenal function.", "So Brussels sprouts can be particularly good for adrenal fatigue.", "So the answer is yes."], "conf_comments": ["(A)\nVitamin C", "(A)\nThus, Br", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7183756104724767, 0.3803934933459558, 0.2871190297632042], "conf": [0.9639599366435119, 0.6916999936860229, 0.4591240545778318], "prob": [[0.0005542811218308487, 12], [5.3106032651264475e-15, 27], [2.123327491558247e-15, 6]], "generated": ["Brussels sprouts are rich in Vitamin C.", "Vitamin C is important for a healthy adrenal gland, thus making it better for adrenal fatigue.", "So the answer is yes."], "conf_comments": ["(A)\nVitamin C", "(A)\nThus, Br", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 52.57568407058716}
{"index": 196, "question": "Does Adam Sandler skip celebrating Easter?", "answer": "yes", "explanation": "Yes. Adam Sandler is Jewish. Jewish religious people do not celebrate Easter.", "generated": [{"finished": true, "length": 4, "score": [0.782619581433409, 0.5689434284475257, 0.4183295400820435, 0.3430116928917914], "conf": [0.9720397243262067, 0.8671060283946874, 0.6569006251286377, 0.4751554463122923], "prob": [[0.03943863212311037, 7], [7.374024116266037e-05, 12], [4.6749388909842564e-07, 15], [3.014702225460018e-07, 6]], "generated": ["Adam Sandler is Jewish.", "Jews celebrate Passover instead of Easter.", "Thus, Adam Sandler celebrates Passover instead of Easter.", "So the answer is yes."], "conf_comments": ["(A)\nThus, he", "(A)\nThus, Adam", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.782619581433409, 0.5689434284475257, 0.40547782954843725, 0.33019087834405336], "conf": [0.9720397243262067, 0.8671060283946874, 0.5991064610482099, 0.408603414705587], "prob": [[0.03943863212311037, 7], [7.374024116266037e-05, 12], [9.926683155065725e-07, 14], [8.386712010933148e-07, 6]], "generated": ["Adam Sandler is Jewish.", "Jews celebrate Passover instead of Easter.", "Thus, Adam Sandler probably skips celebrating Easter.", "So the answer is yes."], "conf_comments": ["(A)\nThus, he", "(A)\nThus, Adam", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.782619581433409, 0.5689434284475257, 0.4183295400820435, 0.31347344389627996], "conf": [0.9720397243262067, 0.8671060283946874, 0.6569006251286377, 0.4560683880986107], "prob": [[0.03943863212311037, 7], [7.374024116266037e-05, 12], [4.6749388909842564e-07, 15], [1.3085214029860642e-07, 6]], "generated": ["Adam Sandler is Jewish.", "Jews celebrate Passover instead of Easter.", "Thus, Adam Sandler celebrates Passover instead of Easter.", "So the answer is no."], "conf_comments": ["(A)\nThus, he", "(A)\nThus, Adam", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6537694569140307, 0.38088264116993553, 0.3190077641487797], "conf": [0.9332211893639166, 0.7130251087505483, 0.5060810304608316], "prob": [[3.901580941082621e-05, 13], [1.1678023760204936e-08, 10], [1.0884257834054015e-08, 6]], "generated": ["Easter is Christian holiday and Adam Sandler is Jewish.", "Thus, he would skip celebrating it.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Adam", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5167906727117705, 0.35713827738296156, 0.27683811882663595], "conf": [0.8645177810183413, 0.5792878744595791, 0.36760048342656004], "prob": [[5.981620735151923e-12, 22], [1.4420204113628355e-13, 11], [1.039334807234728e-13, 6]], "generated": ["Adam Sandler is Jewish, so there may be less of a chance that he observes Easter.", "Thus, he may skip celebrating Easter.", "So the answer is yes."], "conf_comments": ["(A)\nThus, there", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 4, "score": [0.8587146734871196, 0.5574000031180164, 0.27817058853453613, 0.14632390837921325], "conf": [0.9648467078285391, 0.821437271118771, 0.6424321670623131, 0.4863007627495347], "prob": [[0.007912433604687349, 18], [1.7082484899666e-06, 12], [5.7526975999342824e-11, 9], [5.0163173808147766e-14, 7]], "generated": ["Adam Sandler was born on September 9th, 1966.", "September 9th is in the month of September.", "The month of September is not in Spring.", "Easter is always in Spring.", "Therefore, Adam Sandler cannot celebrate Easter.", "So the answer is yes."], "conf_comments": ["(A)\nEaster is celebrated", "(A)\nEaster is celebrated", "(A)\nThus, Adam", "(A)\nThus, Adam"], "info": [[[0.0008719939287043494, 7], [0.00018108165208185798, 12], [0.7275808366589771, 6]], 0.7569682647948195, "(A)\nThus, Adam", 4]}, {"finished": false, "length": 4, "score": [0.8587146734871196, 0.5574000031180164, 0.27817058853453613, 0.12272096404374992], "conf": [0.9648467078285391, 0.821437271118771, 0.6424321670623131, 0.46420806215651594], "prob": [[0.007912433604687349, 18], [1.7082484899666e-06, 12], [5.7526975999342824e-11, 9], [3.1152880895188e-17, 11]], "generated": ["Adam Sandler was born on September 9th, 1966.", "September 9th is in the month of September.", "The month of September is not in Spring.", "Thus, September does not occur in the Spring.", "In other words, September 9th is not in Spring, meaning it is outside of the window of Easter.", "Thus, Adam Sandler skips celebrating Easter."], "conf_comments": ["(A)\nEaster is celebrated", "(A)\nEaster is celebrated", "(A)\nThus, Adam", "(A)\nEaster is celebrated"], "info": [[[5.415351729168571e-07, 11], [4.841606484023221e-17, 24], [0.02394617221828527, 13]], 0.7225791078912301, "(A)\nEaster is celebrated", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 52.01907253265381}
{"index": 197, "question": "Does March begin on the same day of the week as February during leap years?", "answer": "no", "explanation": "No. During normal years, February has exactly 28 days, so March begins on the same day of the week as February. However, on leap years, February has an extra day, so March begins the next day of the week from whichever day started February.", "generated": [{"finished": true, "length": 3, "score": [0.7645866896302728, 0.4323011797189497, 0.3321309930683624], "conf": [0.9293380862946992, 0.6924298958572366, 0.46573891120404265], "prob": [[3.7235233059472795e-05, 22], [1.5100467188779178e-16, 31], [6.89709018878675e-17, 6]], "generated": ["February has 29 days in a leap year, and March has 31 days.", "Since there are more days in March than February in a leap year, we can see that March starts on a different day of the week than February.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.5260674307579088, 0.3436442972638182, 0.22999663504986673, 0.1236315427889597, 0.07371953108356259, 0.0602674348400732], "conf": [0.9599953353709223, 0.9010428532279616, 0.7902584036008135, 0.6211141991246607, 0.4623826863099427, 0.317715293047065], "prob": [[2.27519863792524e-09, 16], [3.0421405790679004e-17, 23], [1.1492791412193745e-30, 46], [2.1212118459446326e-42, 27], [5.933842891197179e-54, 36], [5.0247958533028264e-54, 6]], "generated": ["Leap years and common years are defined based on the Gregorian calendar.", "Under the Gregorian calendar, leap years are a year that is divisible by 400.", "Thus, 2000, 2400 and 2800 are leap years but 2200, 2100 and 1800 are common years.", "Furthermore, 2100 and 1800 are not in the year 2000.", "Thus, for 2000, 2400 or 2800, February and March won’t have the same day of the week.", "So the answer is no."], "conf_comments": ["(A)\nThe Gregorian", "(A)\nCommon years are a", "(A)\nFebruary has", "(A)\nThus, March", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5260674307579088, 0.3436442972638182, 0.2294084059872687, 0.15332797381577953], "conf": [0.9599953353709223, 0.9010428532279616, 0.8098308245259088, 0.7401270784917388], "prob": [[2.27519863792524e-09, 16], [3.0421405790679004e-17, 23], [7.361514269115066e-25, 25], [2.1017186161290302e-40, 50]], "generated": ["Leap years and common years are defined based on the Gregorian calendar.", "Under the Gregorian calendar, leap years are a year that is divisible by 400.", "Common years are years that are not divisible by 400 and are usually divisible by 100.", "For example, since the years 2000, 2400, 2800, and 3200 are all divisible by 400, they are leap years and should have an extra"], "conf_comments": ["(A)\nThe Gregorian", "(A)\nCommon years are a", "(A)\nThus, le", "(A)\nday in February."], "info": null}, {"finished": true, "length": 3, "score": [0.7645866896302728, 0.34850709025981813, 0.24095803574607236], "conf": [0.9293380862946992, 0.5538031823693358, 0.34956713192787764], "prob": [[3.7235233059472795e-05, 22], [1.996217094340362e-18, 29], [2.1601472383519565e-19, 8]], "generated": ["February has 29 days in a leap year, and March has 31 days.", "Since 29 x 7 ≠ 31 x 8, February and March may not have the same day of the week.", "Thus, the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, March", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 8, "score": [0.5260674307579088, 0.3436442972638182, 0.2294084059872687, 0.12590332740405644, 0.0930285836149664, 0.03564814024487621, 0.020385919951613103, 0.014435109936942192], "conf": [0.9599953353709223, 0.9010428532279616, 0.8098308245259088, 0.6626607324483046, 0.5622243648859383, 0.366568834799918, 0.20483921292776083, 0.12497426993769012], "prob": [[2.27519863792524e-09, 16], [3.0421405790679004e-17, 23], [7.361514269115066e-25, 25], [1.676609513804322e-30, 13], [1.028753639567436e-34, 22], [1.0149097600469869e-45, 17], [1.5476055741399437e-51, 25], [4.767520841104521e-52, 6]], "generated": ["Leap years and common years are defined based on the Gregorian calendar.", "Under the Gregorian calendar, leap years are a year that is divisible by 400.", "Common years are years that are not divisible by 400 and are usually divisible by 100.", "Thus, leap years and common years can be different.", "In addition, February has 28 days in common years and 29 days in leap years.", "Since March starts after February, the date that March starts on would be the same.", "Thus, there is a chance that March will start on the same day of the week as February during leap years.", "So the answer is yes."], "conf_comments": ["(A)\nThe Gregorian", "(A)\nCommon years are a", "(A)\nThus, le", "(A)\nFebruary has", "(A)\nMarch has ", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 8, "score": [0.5260674307579088, 0.3436442972638182, 0.2294084059872687, 0.12590332740405644, 0.0930285836149664, 0.03564814024487621, 0.020385919951613103, 0.013996774541519251], "conf": [0.9599953353709223, 0.9010428532279616, 0.8098308245259088, 0.6626607324483046, 0.5622243648859383, 0.366568834799918, 0.20483921292776083, 0.11708995028918613], "prob": [[2.27519863792524e-09, 16], [3.0421405790679004e-17, 23], [7.361514269115066e-25, 25], [1.676609513804322e-30, 13], [1.028753639567436e-34, 22], [1.0149097600469869e-45, 17], [1.5476055741399437e-51, 25], [4.868475363161242e-52, 6]], "generated": ["Leap years and common years are defined based on the Gregorian calendar.", "Under the Gregorian calendar, leap years are a year that is divisible by 400.", "Common years are years that are not divisible by 400 and are usually divisible by 100.", "Thus, leap years and common years can be different.", "In addition, February has 28 days in common years and 29 days in leap years.", "Since March starts after February, the date that March starts on would be the same.", "Thus, there is a chance that March will start on the same day of the week as February during leap years.", "So the answer is maybe."], "conf_comments": ["(A)\nThe Gregorian", "(A)\nCommon years are a", "(A)\nThus, le", "(A)\nFebruary has", "(A)\nMarch has ", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": []}, "run_time": 57.24513816833496}
{"index": 198, "question": "Is Shiva's divine dance an ancient physical fitness pose?", "answer": "no", "explanation": "No. In yoga as exercise, the pose called Natarajasara represents Shiva's divine dance. This pose is new, originating in the early 20th century. Exercise is an activity that maintains physical fitness.", "generated": [{"finished": true, "length": 3, "score": [0.6244602367722667, 0.3019198395059625, 0.23967383837711811], "conf": [0.8153191104779414, 0.5265874383528822, 0.33454806704793016], "prob": [[3.5858594689737073e-06, 17], [5.0064817822801727e-11, 11], [4.768175261070345e-11, 6]], "generated": ["Shiva's divine dance does not look like an ancient physical fitness pose.", "Thus, it does not seem to be one.", "So the answer is no."], "conf_comments": ["(A)\nThus, Sh", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5173888261107108, 0.24224592718105434, 0.1843635980598852, 0.1416703194501049], "conf": [0.9578784543245489, 0.7553436846720781, 0.4665377284461738, 0.2761359451145021], "prob": [[4.95285123618561e-09, 15], [1.0553376820344476e-15, 12], [3.3197494207978055e-16, 18], [3.2728992931802546e-16, 6]], "generated": ["Shiva's divine dance is an ancient Hindu sculpture movement.", "Physical fitness exercises are usually very modern.", "Thus, Shiva's divine dance is not an ancient physical fitness pose.", "So the answer is no."], "conf_comments": ["(A)\nIt is a pose", "(A)\nThus, Sh", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5173888261107108, 0.26641787924663207, 0.1765197551953423, 0.1398304397432888], "conf": [0.9578784543245489, 0.834370179397104, 0.5151067703979042, 0.32650588101329814], "prob": [[4.95285123618561e-09, 15], [7.590956369216359e-19, 19], [4.193412871694208e-22, 22], [3.947353376645507e-22, 6]], "generated": ["Shiva's divine dance is an ancient Hindu sculpture movement.", "Physical fitness poses are human movements designed to improve health and strengthen muscles.", "Thus, it is unlikely for Shiva's divine dance to be an ancient physical fitness pose.", "So the answer is no."], "conf_comments": ["(A)\nIt is a pose", "(A)\nThus, Sh", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5173888261107108, 0.24224592718105434, 0.10145865145873197, 0.06099499142972313, 0.04610694820751247], "conf": [0.9578784543245489, 0.7553436846720781, 0.49363023578044485, 0.29981697540993885, 0.17528625839480275], "prob": [[4.95285123618561e-09, 15], [1.0553376820344476e-15, 12], [2.0486698113160286e-21, 10], [1.066671049569115e-25, 19], [9.297089871796649e-26, 6]], "generated": ["Shiva's divine dance is an ancient Hindu sculpture movement.", "Physical fitness exercises are usually very modern.", "Thus, they could not coincide.", "Therefore, Shiva's divine dance is probably not an ancient physical fitness pose.", "So the answer is no."], "conf_comments": ["(A)\nIt is a pose", "(A)\nThus, Sh", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5173888261107108, 0.24224592718105434, 0.10145865145873197, 0.08228721420290577], "conf": [0.9578784543245489, 0.7553436846720781, 0.49363023578044485, 0.3274202280284763], "prob": [[4.95285123618561e-09, 15], [1.0553376820344476e-15, 12], [2.0486698113160286e-21, 10], [1.948806578558128e-21, 6]], "generated": ["Shiva's divine dance is an ancient Hindu sculpture movement.", "Physical fitness exercises are usually very modern.", "Thus, they could not coincide.", "So the answer is no."], "conf_comments": ["(A)\nIt is a pose", "(A)\nThus, Sh", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 53.03332591056824}
{"index": 199, "question": "Was Land of Israel in possession of an Islamic empire in 16th century?", "answer": "yes", "explanation": "Yes. Land of Israel was controlled by the Ottoman Empire in 16th century.  The religion of Ottoman Empire was Sunni Islam. ", "generated": [{"finished": true, "length": 4, "score": [0.759553266736713, 0.6067495972081771, 0.5166813276912808, 0.4306794442258981], "conf": [0.9733840708684656, 0.8872381383587702, 0.6998840526118908, 0.487235869316955], "prob": [[8.146672642809081e-05, 18], [1.6128440085343107e-06, 11], [2.530967345197517e-07, 22], [2.501405715129715e-07, 6]], "generated": ["Land of Israel was conquered by the Ottoman Empire in 1516.", "Ottoman Empire was an Islamic empire.", "Thus, Land of Israel was in possession of an Islamic empire in 16th century.", "So the answer is yes."], "conf_comments": ["(A)\nThe Ottoman Empire", "(A)\nThus, Land", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.759553266736713, 0.6067495972081771, 0.4791637038746351, 0.40119050898601066], "conf": [0.9733840708684656, 0.8872381383587702, 0.7046081035160568, 0.49495589609765456], "prob": [[8.146672642809081e-05, 18], [1.6128440085343107e-06, 11], [6.216480849144618e-09, 23], [6.140902939689944e-09, 6]], "generated": ["Land of Israel was conquered by the Ottoman Empire in 1516.", "Ottoman Empire was an Islamic empire.", "Thus, Land of Israel was under the possession of an Islamic empire in 16th century.", "So the answer is yes."], "conf_comments": ["(A)\nThe Ottoman Empire", "(A)\nThus, Land", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.759553266736713, 0.46906700766267123, 0.3891573280861522, 0.32384524307282886], "conf": [0.9733840708684656, 0.8373034328210696, 0.6693093137155509, 0.4644184876635647], "prob": [[8.146672642809081e-05, 18], [1.7942553467267455e-16, 33], [5.7507871913661336e-18, 23], [5.683020448182047e-18, 6]], "generated": ["Land of Israel was conquered by the Ottoman Empire in 1516.", "Between 1299 and 1923, the Ottoman Empire was the biggest and most influential Islamic empire in history.", "Thus, the Land of Israel was in possession of an Islamic empire in 16th century.", "So the answer is yes."], "conf_comments": ["(A)\nThe Ottoman Empire", "(A)\nThus, Land", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.759553266736713, 0.46906700766267123, 0.30688562800982844, 0.25656181412276663], "conf": [0.9733840708684656, 0.8373034328210696, 0.6310113037435378, 0.44244664593514765], "prob": [[8.146672642809081e-05, 18], [1.7942553467267455e-16, 33], [7.352579977181582e-23, 26], [7.212449089778471e-23, 6]], "generated": ["Land of Israel was conquered by the Ottoman Empire in 1516.", "Between 1299 and 1923, the Ottoman Empire was the biggest and most influential Islamic empire in history.", "Thus, it can be concluded that Land of Israel was possessed by an Islamic empire in 16th century.", "So the answer is yes."], "conf_comments": ["(A)\nThe Ottoman Empire", "(A)\nThus, Land", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5796416743209862, 0.38678146356578486, 0.3127725719283337], "conf": [0.9622059708393934, 0.7078072069331343, 0.47531816968510504], "prob": [[2.385816422323565e-15, 32], [2.8379846296621476e-19, 18], [2.419633179443465e-19, 6]], "generated": ["The first Islamic empire is thought to be the Rashidun Caliphate of Medina, which emerged in 622.", "Thus, there was no Islamic empire in the 16th century.", "So the answer is no."], "conf_comments": ["(A)\nThe Land of Israel", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.759553266736713, 0.46906700766267123, 0.30688562800982844, 0.12535554091348153], "conf": [0.9733840708684656, 0.8373034328210696, 0.6310113037435378, 0.3320226457395488], "prob": [[8.146672642809081e-05, 18], [1.7942553467267455e-16, 33], [7.352579977181582e-23, 26], [7.4758430885476e-26, 6]], "generated": ["Land of Israel was conquered by the Ottoman Empire in 1516.", "Between 1299 and 1923, the Ottoman Empire was the biggest and most influential Islamic empire in history.", "Thus, it can be concluded that Land of Israel was possessed by an Islamic empire in 16th century.", "So the answer is no."], "conf_comments": ["(A)\nThe Ottoman Empire", "(A)\nThus, Land", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 50.37852668762207}
{"index": 200, "question": "Did Medieval English lords engage in fair trade with peasants?", "answer": "no", "explanation": "No. Fair trade is a system in which fair prices are paid to the producers of a product. English lords had peasants working on their manors and the peasants were indentured servants. The peasants had few rights, were unpaid, and had to even ask their lord for permission to marry.", "generated": [{"finished": true, "length": 4, "score": [0.8092872972918382, 0.3633649503348011, 0.2891447962115776, 0.23712273302771322], "conf": [0.9148474806752749, 0.7923289898775511, 0.5635827751805826, 0.38027588712086385], "prob": [[0.003408511517116196, 17], [2.007122017332757e-11, 13], [1.9607596350906656e-12, 20], [1.922523388277655e-12, 6]], "generated": ["Medieval English lords did not engage in fair trade with peasants.", "They would unfairly take money from the peasants.", "Thus, Medieval English lords did not engage in fair trade with peasants.", "So the answer is no."], "conf_comments": ["(A)\nMedieval English l", "(A)\nThus, Med", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6448898354125918, 0.2738302176112524, 0.21288178061822624, 0.18325391325817325], "conf": [0.969722889116244, 0.8845856014530306, 0.6666088604491214, 0.4961863007052282], "prob": [[8.149203618503385e-09, 22], [7.950612687284071e-28, 27], [9.639648167252307e-30, 20], [9.384177567434087e-30, 6]], "generated": ["Medieval England is a time period from 1066 to 1485 in England.", "This timeframe was when lords ruled over peasants and there was significant class division between the nobles and the peasants.", "Thus, Medieval English lords did not engage in fair trade with peasants.", "So the answer is no."], "conf_comments": ["(A)\nMedieval English l", "(A)\nThus, it", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6448898354125918, 0.2738302176112524, 0.18476524920676202, 0.15821895901050423], "conf": [0.969722889116244, 0.8845856014530306, 0.6617687232939992, 0.4885002982985073], "prob": [[8.149203618503385e-09, 22], [7.950612687284071e-28, 27], [6.343347735773227e-32, 19], [6.09572987325522e-32, 6]], "generated": ["Medieval England is a time period from 1066 to 1485 in England.", "This timeframe was when lords ruled over peasants and there was significant class division between the nobles and the peasants.", "Thus, medieval lords did not engage in fair trade with the peasants.", "So the answer is no."], "conf_comments": ["(A)\nMedieval English l", "(A)\nThus, it", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6448898354125918, 0.362137600160341, 0.17149954083742047, 0.13217957487448734], "conf": [0.969722889116244, 0.8965145534214787, 0.5226240737470715, 0.3364889433002661], "prob": [[8.149203618503385e-09, 22], [2.3509633417252834e-15, 14], [4.563781391352054e-24, 21], [2.814836524947256e-24, 6]], "generated": ["Medieval England is a time period from 1066 to 1485 in England.", "Trade is a system of buying and selling of goods.", "Thus, it is hard to believe that lord would engage in fair trade with peasants.", "So the answer is no."], "conf_comments": ["(A)\nMedieval English l", "(A)\nThus, Med", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.8092872972918382, 0.3059386035161461, 0.20998776639961506], "conf": [0.9148474806752749, 0.7126884439851516, 0.5312835292844913], "prob": [[0.003408511517116196, 17], [2.4581530036687178e-36, 45], [6.253904630484205e-38, 8]], "generated": ["Medieval English lords did not engage in fair trade with peasants.", "Peasants often had to give up most of their income, meaning that the value of what they received in exchange for their products, the so-called “fair trade”, was not equivalent to their products’ values.", "Thus, the answer is no."], "conf_comments": ["(A)\nMedieval English l", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": false, "length": 4, "score": [0.6448898354125918, 0.362137600160341, 0.17149954083742047, 0.06474780440342796], "conf": [0.969722889116244, 0.8965145534214787, 0.5226240737470715, 0.3168284772650643], "prob": [[8.149203618503385e-09, 22], [2.3509633417252834e-15, 14], [4.563781391352054e-24, 21], [1.2165003206138146e-36, 20]], "generated": ["Medieval England is a time period from 1066 to 1485 in England.", "Trade is a system of buying and selling of goods.", "Thus, it is hard to believe that lord would engage in fair trade with peasants.", "Fair trade is buying and selling goods without any cheating or other unfair behavior.", "So the answer would be a no."], "conf_comments": ["(A)\nMedieval English l", "(A)\nThus, Med", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[2.6655534441657764e-13, 20], [3.688306515937019e-07, 8]], 0.6062263358698554, "(A)\nSo the answer is", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 50.52240180969238}
{"index": 201, "question": "Paleography hypothetically helps to understand Cthulhu?", "answer": "yes", "explanation": "Yes. Palaeography involves the study of ancient writings. Paleography has helped decode Anatolian hieroglyphics from all the way back as the first millenia BC. Author H.P. Lovecraft's Cthulhu is an ancient mystical being from eons ago.", "generated": [{"finished": true, "length": 4, "score": [0.8489036589849044, 0.6475014269945032, 0.4106752077530379, 0.33950739803467767], "conf": [0.9769646194475607, 0.8416291643544355, 0.61853365005238, 0.4279602156190766], "prob": [[0.03517367315436856, 11], [1.3698464667707625e-05, 20], [4.369220600904491e-11, 21], [4.0585664049575115e-11, 6]], "generated": ["Paleography is the study of ancient handwriting.", "Cthulhu is a fictional character in the works of H.P. Lovecraft.", "Thus, paleography would not help us to understand Cthulhu, which is fictional.", "So the answer is no."], "conf_comments": ["(A)\nCthulhu", "(A)\nThus, pale", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7826435966717713, 0.389131599401268, 0.308505643901141, 0.2531176191315424], "conf": [0.9537687638457389, 0.597694493947392, 0.4458228693327161, 0.301545714174022], "prob": [[0.00022182708609196284, 19], [1.187223141931934e-11, 18], [1.0805237877745837e-12, 14], [1.0500272873821784e-12, 6]], "generated": ["Cthulhu is a fictional character invented by H.P. Lovecraft.", "Cthulhu does not exist in real life and cannot be understood with paleography.", "Thus, paleography cannot help to understand Cthulhu.", "So the answer is no."], "conf_comments": ["(A)\nPaleography is", "(A)\nThus, pale", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8489036589849044, 0.5489334027627719, 0.306335646084314, 0.25174753727476834], "conf": [0.9769646194475607, 0.8433931188852543, 0.5804994808015794, 0.3974852414540245], "prob": [[0.03517367315436856, 11], [3.668362216618165e-08, 19], [1.0484981588347192e-14, 19], [9.653059454757999e-15, 6]], "generated": ["Paleography is the study of ancient handwriting.", "Cthulhu is a fictional character in the work H.P. Lovecraft.", "Thus, it will not be possible to use paleography to understand Cthulhu.", "So the answer is no."], "conf_comments": ["(A)\nCthulhu", "(A)\nThus, pale", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8489036589849044, 0.6475014269945032, 0.3043388552081988, 0.24817173919896193], "conf": [0.9769646194475607, 0.8416291643544355, 0.5426157428757052, 0.3700312579700281], "prob": [[0.03517367315436856, 11], [1.3698464667707625e-05, 20], [3.783891388222007e-18, 27], [3.2523822864305865e-18, 6]], "generated": ["Paleography is the study of ancient handwriting.", "Cthulhu is a fictional character in the works of H.P. Lovecraft.", "Thus, it is not possible to apply paleography to examine Cthulhu, for Cthulhu is fictional.", "So the answer is no."], "conf_comments": ["(A)\nCthulhu", "(A)\nThus, pale", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7288500491915811, 0.4824775861721838, 0.25812619207786414, 0.18922863989830394], "conf": [0.9769768655016213, 0.8053864885356797, 0.480064265254525, 0.3381812917852084], "prob": [[0.0012283243031306985, 11], [3.985383003288859e-09, 20], [3.1849839466027016e-14, 16], [3.65411611868136e-15, 8]], "generated": ["Paleography is the study of historic handwriting.", "Cthulhu is a fictional character invented in the 1920s.", "There is no evidence that paleography would help to understand Cthulhu.", "Thus, the answer is no."], "conf_comments": ["(A)\nCthulhu", "(A)\nThus, pale", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": false, "length": 4, "score": [0.7288500491915811, 0.4824775861721838, 0.25812619207786414, 0.1449025758494122], "conf": [0.9769768655016213, 0.8053864885356797, 0.480064265254525, 0.3528295260410503], "prob": [[0.0012283243031306985, 11], [3.985383003288859e-09, 20], [3.1849839466027016e-14, 16], [1.5599473120287468e-17, 9]], "generated": ["Paleography is the study of historic handwriting.", "Cthulhu is a fictional character invented in the 1920s.", "There is no evidence that paleography would help to understand Cthulhu.", "Thus, the premise is false.", "So the answer is no."], "conf_comments": ["(A)\nCthulhu", "(A)\nThus, pale", "(A)\nThus, the", "(A)\nSo the answer is"], "info": [[[0.0004897818444870599, 9], [0.8600150387122376, 6]], 0.7349631113534014, "(A)\nSo the answer is", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 53.3718535900116}
{"index": 202, "question": "Have jokes killed more people than rats in history?", "answer": "no", "explanation": "No. Greek philosopher Chrysippus was said to have died from laughter after seeing a donkey eating figs. There are only a handful of deaths attributed to jokes throughout history including King Martin of Aragon. There are an average of 30 deaths by rat attacks every century. The plague which is sometimes associated with diseased rats killed millions of people.", "generated": [{"finished": true, "length": 5, "score": [0.5202452650489137, 0.24388943716092717, 0.13817329964013575, 0.10169375790363756, 0.08049096256363633], "conf": [0.9667700904011799, 0.9284700251781743, 0.7238844519089674, 0.487961083900477, 0.31086910445378396], "prob": [[6.855631477164668e-13, 22], [3.2349892305734357e-21, 13], [1.8169156562853226e-30, 24], [6.834404927988055e-32, 15], [6.179884464982349e-32, 6]], "generated": ["The plague is a disease caused by bacteria and most commonly spread by fleas on rodents.", "It resulted in around 25 million deaths throughout history.", "The number of people killed by the plague is most likely much higher than the number of people killed by jokes.", "Thus, jokes have killed fewer people than rats in history.", "So the answer is no."], "conf_comments": ["(A)\nThe plague has", "(A)\nJokes are not", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5903540358682321, 0.2422731517175909, 0.12482074826286631, 0.08693491122867478, 0.06947977613972707], "conf": [0.9168367578585391, 0.4711371835306868, 0.29159920117718485, 0.1934276626062955, 0.1238590308698657], "prob": [[1.900767085026031e-07, 16], [1.1039293701261407e-15, 17], [7.861000121734352e-21, 14], [7.189803033094024e-23, 15], [7.083253362848008e-23, 6]], "generated": ["Rats have caused the death of more than 3 billion people in history.", "It is not possible to have a specific number of deaths caused by jokes.", "However, jokes cannot have killed more than 3 billion people.", "Thus, jokes have not killed more than rats in history.", "So the answer is no."], "conf_comments": ["(A)\nJokes have killed", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5055719812087491, 0.24353759703716701, 0.10285258953735359, 0.07330406356223805, 0.0584412411892923], "conf": [0.9133120843202465, 0.7396746088535038, 0.5765918474234818, 0.42732976954060203, 0.27293600366905485], "prob": [[3.763347532262877e-05, 8], [1.7091854572758363e-28, 43], [2.2069377654204267e-39, 17], [7.634048410838335e-42, 15], [7.414218861662005e-42, 6]], "generated": ["Jokes generally do not kill people.", "The black death infamously killed around 75-200 million people, and the plague (also known as the second outbreak of the black death) killed around 75 million people.", "In comparison, the deaths caused by rodents have generally been in the millions.", "Thus, jokes have not killed as many people as rats.", "So the answer is no."], "conf_comments": ["(A)\nRats have killed", "(A)\nThus, j", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5903540358682321, 0.2422731517175909, 0.12482074826286631, 0.08034251792767406], "conf": [0.9168367578585391, 0.4711371835306868, 0.29159920117718485, 0.1756530017143723], "prob": [[1.900767085026031e-07, 16], [1.1039293701261407e-15, 17], [7.861000121734352e-21, 14], [8.320821536175412e-22, 6]], "generated": ["Rats have caused the death of more than 3 billion people in history.", "It is not possible to have a specific number of deaths caused by jokes.", "However, jokes cannot have killed more than 3 billion people.", "So the answer is no."], "conf_comments": ["(A)\nJokes have killed", "(A)\nThus, the", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.5202452650489137, 0.2665084536598771], "conf": [0.9667700904011799, 0.8925771226388856], "prob": [[6.855631477164668e-13, 22], [3.31287850842359e-40, 50]], "generated": ["The plague is a disease caused by bacteria and most commonly spread by fleas on rodents.", "In the past, there has been a pandemic called “the Black Death” caused by the bacteria Yersinia pestis as a result of people transporting flea-infested black rats throughout Eurasia and Africa"], "conf_comments": ["(A)\nThe plague has", "(A)\nThe Black Death killed"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.20533061027527}
{"index": 203, "question": "Is slitting your wrists an unreliable suicide method?", "answer": "yes", "explanation": "Yes. Wrist slitting has only a 6% mortality rate. Many people cannot complete the action of slitting their wrists due to pain or shock.", "generated": [{"finished": true, "length": 5, "score": [0.7161061465463225, 0.3062334919283398, 0.1360209710406392, 0.0983815550345136, 0.07766372535484171], "conf": [0.9480160294309147, 0.7714388570899533, 0.6233698274620099, 0.402686873727184, 0.2567801468716358], "prob": [[0.0006275805371242091, 12], [1.18206286913837e-13, 15], [8.897368738178991e-20, 10], [1.3096044803747108e-21, 20], [1.1408558315160503e-21, 6]], "generated": ["Slitting your wrists is a suicide attempt.", "Suicide attempts only indicate that the patient wants to commit suicide.", "A successful suicide requires a patient to die.", "Thus, slitting your wrists is not an unreliable suicide method.", "So the answer is no."], "conf_comments": ["(A)\nSlitting your w", "(A)\nThus, sl", "(A)\nThus, sl", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.4842738678149109, 0.3402974111086197], "conf": [0.9130658416275697, 0.7286645376145097], "prob": [[3.3070374365003224e-20, 33], [7.1042228612873e-22, 8]], "generated": ["According to Medscape, which uses multiple sources, slitting your wrists with a knife or razor is a very reliable suicide method.", "Thus, the answer is no."], "conf_comments": ["(A)\nThus, sl", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5913235536990162, 0.3611764592090832, 0.14028321580987937, 0.10996201598827264], "conf": [0.7270689310509287, 0.6448658643159703, 0.4134767734135916, 0.25683395562954175], "prob": [[3.937958030652278e-06, 17], [8.629929010802327e-18, 31], [3.234964926492727e-27, 15], [3.0304380425873822e-27, 6]], "generated": ["Slitting our wrists is not an unreliable suicide method.", "According to the World Health Organization, an estimated 793,000 humans died by suicide in 2016.", "Thus, it is reliable enough and also a popular suicide method.", "So the answer is no."], "conf_comments": ["(A)\nSlitting our w", "(A)\nOf those, ", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5913235536990162, 0.21040215795837247, 0.13851634577959482, 0.10578810764833091], "conf": [0.7270689310509287, 0.46447041474968037, 0.2745634040480727, 0.16101663397795854], "prob": [[3.937958030652278e-06, 17], [1.736019532168225e-19, 19], [1.2106757640040856e-21, 16], [1.1718962445538948e-21, 6]], "generated": ["Slitting our wrists is not an unreliable suicide method.", "Suicide can be prevented, but many people still use this method for suicide.", "Thus, slitting our wrists is not unreliable.", "So the answer is no."], "conf_comments": ["(A)\nSlitting our w", "(A)\nThus, sl", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5913235536990162, 0.3611764592090832, 0.10961502158183015, 0.05329467935280366, 0.04129998780375899], "conf": [0.7270689310509287, 0.6448658643159703, 0.41214162700433576, 0.2561626460563721, 0.15570069574576328], "prob": [[3.937958030652278e-06, 17], [8.629929010802327e-18, 31], [1.8587246280539503e-35, 21], [3.5624345932327336e-42, 16], [3.313532400921588e-42, 6]], "generated": ["Slitting our wrists is not an unreliable suicide method.", "According to the World Health Organization, an estimated 793,000 humans died by suicide in 2016.", "However, this does not include unknown causes of death, as this method is often reported as an accident.", "Thus, slitting our wrists are reliable enough to kill ourselves.", "So the answer is no."], "conf_comments": ["(A)\nSlitting our w", "(A)\nOf those, ", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 53.37144136428833}
{"index": 204, "question": "Is the rise of agriculture attributed to rivers?", "answer": "yes", "explanation": "Yes. Some of the earliest civilizations on record are in river valleys. Early civilizations used water to irrigate crops, leading to the growth of agriculture.", "generated": [{"finished": true, "length": 6, "score": [0.7204657594120866, 0.5351886473006482, 0.21412952798761625, 0.12041628529233482, 0.08423282609242082, 0.06832639653697739], "conf": [0.9390832357185313, 0.8544163063447275, 0.5151029759498517, 0.3220574598831706, 0.2344574073596453, 0.1546693261100858], "prob": [[7.592095345237247e-05, 16], [6.91615628948373e-08, 14], [1.590694778945052e-16, 15], [7.467777179377695e-22, 18], [1.405427116376524e-23, 10], [1.3837341518130825e-23, 6]], "generated": ["The rise of agriculture is attributed to the invention of the plow.", "The plow was invented around 4000 BC.", "Rivers weren’t invented until ~600 BC.", "This indicates that rivers didn’t have anything to do with the rise of agriculture.", "Thus, it is not attributed to rivers.", "So the answer is no."], "conf_comments": ["(A)\nThe plow was", "(A)\nThe first plow", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7204657594120866, 0.30204210815896076, 0.2016622079882137, 0.1580989973667367], "conf": [0.9390832357185313, 0.5220534594451222, 0.32876702946202446, 0.2034501917869936], "prob": [[7.592095345237247e-05, 16], [2.3912089484189997e-13, 17], [1.895781649232539e-15, 14], [1.8198031517482265e-15, 6]], "generated": ["The rise of agriculture is attributed to the invention of the plow.", "However, rivers are used for irrigation, which is part of farming.", "Thus, rivers are not attributed to the rise of agriculture.", "So the answer is no."], "conf_comments": ["(A)\nThe plow was", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7196173194830271, 0.23708672620024523, 0.18533617792961488, 0.14978368843550416], "conf": [0.9655666964988042, 0.776042805764985, 0.5506503859950826, 0.36099452526907183], "prob": [[2.1951791352272433e-09, 32], [2.2036919345570226e-29, 23], [2.7212600755531564e-30, 14], [2.661177631694228e-30, 6]], "generated": ["The rise of agriculture happened around 10,000 to 8,000 years ago, during the Neolithic period.", "Rivers have existed since many years before the start of the era (the pre-Neolithic era).", "Thus, the rise of agriculture was not attributed to rivers.", "So the answer is no."], "conf_comments": ["(A)\nRivers are a", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7204657594120866, 0.5351886473006482, 0.19082045754874755, 0.11277834823175063, 0.08624419156984514], "conf": [0.9390832357185313, 0.8544163063447275, 0.41569921296313145, 0.23950643989145678, 0.14058169938603549], "prob": [[7.592095345237247e-05, 16], [6.91615628948373e-08, 14], [3.976668080889691e-20, 21], [3.604207187531277e-23, 14], [3.525248197725843e-23, 6]], "generated": ["The rise of agriculture is attributed to the invention of the plow.", "The plow was invented around 4000 BC.", "Rivers are geographical features while the plow is a tool used to prepare soil for farming.", "Thus, rivers is not attributed to the rise of agriculture.", "So the answer is no."], "conf_comments": ["(A)\nThe plow was", "(A)\nThe first plow", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7204657594120866, 0.5351886473006482, 0.21412952798761625, 0.12041628529233482, 0.07894648377464485], "conf": [0.9390832357185313, 0.8544163063447275, 0.5151029759498517, 0.3220574598831706, 0.1934839348515757], "prob": [[7.592095345237247e-05, 16], [6.91615628948373e-08, 14], [1.590694778945052e-16, 15], [7.467777179377695e-22, 18], [1.0015966350870328e-22, 6]], "generated": ["The rise of agriculture is attributed to the invention of the plow.", "The plow was invented around 4000 BC.", "Rivers weren’t invented until ~600 BC.", "This indicates that rivers didn’t have anything to do with the rise of agriculture.", "So the answer is no."], "conf_comments": ["(A)\nThe plow was", "(A)\nThe first plow", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 53.15852427482605}
{"index": 205, "question": "Do the Eskimos sunbathe frequently?", "answer": "no", "explanation": "No. Sunbathing requires a high amount of sunshine. The Eskimos live in regions that receive very little sunshine. The Eskimos live in very cold regions, which would make it dangerous to be exposed to outside temperatures without clothes.", "generated": [{"finished": true, "length": 3, "score": [0.538048109325219, 0.3920620884329826, 0.32648815936315845], "conf": [0.9274554197362552, 0.7080530461395076, 0.5000906696576596], "prob": [[7.911979720426785e-10, 18], [7.048823093115788e-12, 13], [6.314949923770029e-12, 6]], "generated": ["Eskimos live in a place where there is no sun and it is always cold.", "Thus, the Eskimos do not sunbathe.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6017326167119486, 0.3201019500669727, 0.23925216755583084, 0.2059028192789059], "conf": [0.9685934536683816, 0.8860315391848798, 0.7160334636601796, 0.5325494029734462], "prob": [[1.454283983998396e-07, 16], [3.1659293378074215e-16, 17], [4.113570389712839e-19, 18], [4.0117732848506286e-19, 6]], "generated": ["The Eskimos live in the Arctic, north of the treeline.", "This means there will be no sunlight for 6 months out of the year.", "Thus, the Eskimos do not have the opportunity to sunbathe frequently.", "So the answer is no."], "conf_comments": ["(A)\nThe treeline is", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6597292322604636, 0.3599046108062727, 0.18744129657563655, 0.1319828125039118, 0.1139426266264485], "conf": [0.9733943190876077, 0.8565681894785897, 0.7497342972861308, 0.5721582046594075, 0.42886510916178583], "prob": [[5.104670005504055e-07, 18], [1.4951727153193538e-14, 16], [3.6343377608450705e-21, 13], [2.3781972818071607e-24, 17], [2.298510767746079e-24, 6]], "generated": ["Eskimos live in the Arctic, which is very cold most of the time.", "Most of the time, the Arctic is covered in snow and ice.", "The amount of solar radiation reaching the Arctic is very low.", "Thus, it is not possible for Eskimos to sunbathe frequently.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5820510282324158, 0.4004464404340129, 0.20300417269993629, 0.16647174927129657], "conf": [0.9588069255982923, 0.7724785083354697, 0.5428738718233391, 0.368944078449035], "prob": [[3.2509489036652526e-10, 21], [1.897454149340729e-13, 14], [1.2689666616467169e-22, 21], [1.190988381545275e-22, 6]], "generated": ["Eskimos live in areas above the Arctic Circle where the sun only rises in the summer.", "Thus, Eskimos only sunbathe during the summer.", "Thus, for most of the year, no the Eskimos do not sunbathe frequently.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSince the summer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5820510282324158, 0.4004464404340129, 0.20300417269993629, 0.13424093673767842], "conf": [0.9588069255982923, 0.7724785083354697, 0.5428738718233391, 0.39001668474856116], "prob": [[3.2509489036652526e-10, 21], [1.897454149340729e-13, 14], [1.2689666616467169e-22, 21], [2.3903218833022334e-24, 8]], "generated": ["Eskimos live in areas above the Arctic Circle where the sun only rises in the summer.", "Thus, Eskimos only sunbathe during the summer.", "Thus, for most of the year, no the Eskimos do not sunbathe frequently.", "Thus, the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSince the summer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 52.6221923828125}
{"index": 206, "question": "Do the telescopes at Goldstone Deep Space Communications Complex work the night shift?", "answer": "yes", "explanation": "Yes. The night shift is considered to be the hours of 11pm - 7am. The telescopes at Goldstone Deep Space Communications Complex are running 24 hours a day.", "generated": [{"finished": true, "length": 5, "score": [0.6026036199670199, 0.4152353553521151, 0.2469520930412793, 0.20269819416181772, 0.17505370850978794], "conf": [0.9268271745355506, 0.8176948485265713, 0.6788468892728271, 0.5564903695901787, 0.4165255388967821], "prob": [[2.846813308329743e-09, 21], [1.4102034314378762e-13, 16], [6.522102125674755e-17, 9], [8.704321815191515e-19, 22], [8.520955188192433e-19, 6]], "generated": ["According to Wikipedia, the Goldstone Deep Space Communications Complex contains four large dish antennas.", "The Goldstone Deep Space Communications Complex is part of the Deep Space Network.", "The Deep Space Network works around the clock.", "Thus, the telescopes at the Goldstone Deep Space Communications Complex do work the night shift.", "So the answer is yes."], "conf_comments": ["(A)\nThe Goldstone Deep", "(A)\nThe Deep Space Network", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6327894457591944, 0.3304761319638354, 0.2750660742345141, 0.23397445260610386], "conf": [0.9376801980281805, 0.796010879579353, 0.6424257402002325, 0.4660886093012319], "prob": [[3.5024745564607418e-12, 31], [1.7555049063366828e-26, 29], [6.103522131165537e-28, 22], [6.00460048738791e-28, 6]], "generated": ["The telescopes at the Goldstone Deep Space Communications Complex use radio and microwave wavelengths to communicate with probes in space.", "Light does not travel through space at those wavelengths, so radio and microwave wavelengths will work during the night.", "Thus, the telescopes at the Goldstone Deep Space Communications Complex do work the night shift.", "So the answer is yes."], "conf_comments": ["(A)\nRadio waves are not", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6391148376865747, 0.35636906712253485, 0.23663832785547517, 0.16328290217567537, 0.1371798516034042], "conf": [0.9648542078038164, 0.863746977670265, 0.7214340461528606, 0.5474388379742547, 0.38787816520518026], "prob": [[2.594241617509679e-09, 23], [4.8712470316337345e-18, 19], [9.35753696995326e-23, 17], [2.1255067978568185e-26, 18], [2.0773015612647795e-26, 6]], "generated": ["The Goldstone Deep Space Communications Complex in California is currently used as part of NASA’s Deep Space Network.", "The primary mission of the Deep Space Network is to relay communications to and from spacecraft.", "NASA’s Deep Space Network is active 24 hours a day.", "Thus, the Goldstone Deep Space Communications Complex does not work the night shift.", "So the answer is no."], "conf_comments": ["(A)\nThe Deep Space Network", "(A)\nThe Deep Space Network", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6327894457591944, 0.3304761319638354, 0.21125659251901988, 0.18016286058287193], "conf": [0.9376801980281805, 0.796010879579353, 0.6662344327348406, 0.4911806352123143], "prob": [[3.5024745564607418e-12, 31], [1.7555049063366828e-26, 29], [1.5726120189452287e-30, 13], [1.4494278386221365e-30, 6]], "generated": ["The telescopes at the Goldstone Deep Space Communications Complex use radio and microwave wavelengths to communicate with probes in space.", "Light does not travel through space at those wavelengths, so radio and microwave wavelengths will work during the night.", "Thus, the telescopes can operate during the night.", "So the answer is yes."], "conf_comments": ["(A)\nRadio waves are not", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6026036199670199, 0.4152353553521151, 0.2469520930412793, 0.20269819416181772, 0.09608114374896842], "conf": [0.9268271745355506, 0.8176948485265713, 0.6788468892728271, 0.5564903695901787, 0.31116884011436585], "prob": [[2.846813308329743e-09, 21], [1.4102034314378762e-13, 16], [6.522102125674755e-17, 9], [8.704321815191515e-19, 22], [3.66406920121424e-21, 6]], "generated": ["According to Wikipedia, the Goldstone Deep Space Communications Complex contains four large dish antennas.", "The Goldstone Deep Space Communications Complex is part of the Deep Space Network.", "The Deep Space Network works around the clock.", "Thus, the telescopes at the Goldstone Deep Space Communications Complex do work the night shift.", "So the answer is no."], "conf_comments": ["(A)\nThe Goldstone Deep", "(A)\nThe Deep Space Network", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6425404712350024, 0.3278494152294979, 0.23736823473413915], "conf": [0.9693134993283247, 0.8253340368413725, 0.6358467193192178], "prob": [[1.2708114890715637e-09, 24], [2.425584945927159e-17, 15], [1.1143100379422405e-18, 8]], "generated": ["Goldstone Deep Space Communications Complex is a radio telescope complex that is used primarily to communicate with spacecraft.", "Therefore, the telescopes do not have a “night shift.”", "Thus, the answer is no."], "conf_comments": ["(A)\nIt is located in", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 53.572688579559326}
{"index": 207, "question": "Is a thousand dollars per Days of Our Lives episodes preferred to other soaps?", "answer": "no", "explanation": "No. Days of Our Lives has aired around 13,900 episodes as of 2020. General Hospital aired their 14,000th episode on February 23, 2018.", "generated": [{"finished": true, "length": 11, "score": [0.5908429425365038, 0.3570436310597402, 0.20979727288184943, 0.16641354501689667, 0.1143615505581289, 0.09838952970451587, 0.06537817791896325, 0.05579050266247828, 0.04154750252717737, 0.02802611617648179, 0.022267304420465678], "conf": [0.9684110185355744, 0.7652050520616509, 0.6240881090773083, 0.4832699997759396, 0.3615517415411101, 0.27216382928113564, 0.1945544825644133, 0.1425055020859275, 0.10251111373715747, 0.06907426354761158, 0.04428203366561302], "prob": [[1.0566201433702503e-08, 18], [2.033332967194131e-17, 26], [1.2074099142819403e-22, 14], [1.0186634868679338e-24, 23], [1.628811072952565e-28, 19], [1.0685982800436242e-28, 25], [1.8299568696176774e-32, 18], [1.5814452386179965e-32, 25], [1.7583421269933017e-36, 35], [2.6590746613211614e-43, 40], [2.423926836424248e-43, 6]], "generated": ["The average number of episodes for Days of Our Lives is about 223.", "So, 223 * 1000dollars is 223,000 dollars.", "The average number of episodes for General Hospital is about 56.", "So, 56*1000dollars is 56,000 dollars.", "The average number of episodes for The Bold and the Beautiful is about 217.", "So, 217*1000dollars is 217,000 dollars.", "The average number of episodes for Young and the Restless is about 252.", "So, 252*1000dollars is 252,000 dollars.", "223,000 dollars is greater than 217,000 dollars, which is greater than 252,000 dollars.", "Thus, a thousand dollars per Days of Our Lives episodes is greater than the other soaps, which are General Hospital, The Bold and the Beautiful, and Young and the Restless.", "So the answer is yes."], "conf_comments": ["(A)\nThe average cost of", "(A)\nThe average number of", "(A)\nSo, 5", "(A)\nThe average number of", "(A)\nSo, 2", "(A)\nThe average number of", "(A)\nSo, 2", "(A)\nThus, Days", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 11, "score": [0.5908429425365038, 0.3570436310597402, 0.20979727288184943, 0.16641354501689667, 0.1143615505581289, 0.09838952970451587, 0.06537817791896325, 0.05579050266247828, 0.04154750252717737, 0.026564800427792957, 0.020718072829818766], "conf": [0.9684110185355744, 0.7652050520616509, 0.6240881090773083, 0.4832699997759396, 0.3615517415411101, 0.27216382928113564, 0.1945544825644133, 0.1425055020859275, 0.10251111373715747, 0.0672673441696595, 0.041191243648291305], "prob": [[1.0566201433702503e-08, 18], [2.033332967194131e-17, 26], [1.2074099142819403e-22, 14], [1.0186634868679338e-24, 23], [1.628811072952565e-28, 19], [1.0685982800436242e-28, 25], [1.8299568696176774e-32, 18], [1.5814452386179965e-32, 25], [1.7583421269933017e-36, 35], [5.295196232952474e-41, 22], [5.086181980394927e-41, 6]], "generated": ["The average number of episodes for Days of Our Lives is about 223.", "So, 223 * 1000dollars is 223,000 dollars.", "The average number of episodes for General Hospital is about 56.", "So, 56*1000dollars is 56,000 dollars.", "The average number of episodes for The Bold and the Beautiful is about 217.", "So, 217*1000dollars is 217,000 dollars.", "The average number of episodes for Young and the Restless is about 252.", "So, 252*1000dollars is 252,000 dollars.", "223,000 dollars is greater than 217,000 dollars, which is greater than 252,000 dollars.", "Thus, spending a thousand dollars per Days of Our Lives episodes is preferred to other soaps.", "So the answer is yes."], "conf_comments": ["(A)\nThe average cost of", "(A)\nThe average number of", "(A)\nSo, 5", "(A)\nThe average number of", "(A)\nSo, 2", "(A)\nThe average number of", "(A)\nSo, 2", "(A)\nThus, Days", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 9, "score": [0.5908429425365038, 0.3570436310597402, 0.20979727288184943, 0.16641354501689667, 0.1143615505581289, 0.09563238612998572, 0.07446256525004528, 0.05187292718278048, 0.04111430727529387], "conf": [0.9684110185355744, 0.7652050520616509, 0.6240881090773083, 0.4832699997759396, 0.3615517415411101, 0.2725524585197188, 0.22663025609012122, 0.14957270025726876, 0.09523962767297862], "prob": [[1.0566201433702503e-08, 18], [2.033332967194131e-17, 26], [1.2074099142819403e-22, 14], [1.0186634868679338e-24, 23], [1.628811072952565e-28, 19], [2.3093921088097943e-29, 26], [1.4119845596703843e-34, 38], [3.014751667865839e-37, 20], [2.780264233959017e-37, 6]], "generated": ["The average number of episodes for Days of Our Lives is about 223.", "So, 223 * 1000dollars is 223,000 dollars.", "The average number of episodes for General Hospital is about 56.", "So, 56*1000dollars is 56,000 dollars.", "The average number of episodes for The Bold and the Beautiful is about 217.", "So, 217 * 1000dollars is 217,000 dollars.", "In comparison, 223,000 dollars is more than 217,000 dollars and is also more than 56,000 dollars.", "Therefore, a thousand dollars per Days of Our Lives episodes is preferred to other soaps.", "So the answer is yes."], "conf_comments": ["(A)\nThe average cost of", "(A)\nThe average number of", "(A)\nSo, 5", "(A)\nThe average number of", "(A)\nSo, 2", "(A)\nThe average number of", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 15, "score": [0.5908429425365038, 0.3570436310597402, 0.20979727288184943, 0.16641354501689667, 0.1143615505581289, 0.09563238612998572, 0.06521090378607132, 0.05436130747515187, 0.03151689619892105, 0.0259229326099997, 0.012652236881694846, 0.010299896100262714, 0.005231364430873839, 0.003404970931585576, 0.0026898470461933443], "conf": [0.9684110185355744, 0.7652050520616509, 0.6240881090773083, 0.4832699997759396, 0.3615517415411101, 0.2725524585197188, 0.1941004304741493, 0.14341337393620357, 0.09465392330440002, 0.06538593232762084, 0.034888244367979306, 0.023344298746439337, 0.01765987536204141, 0.011903992619376273, 0.007574503891988391], "prob": [[1.0566201433702503e-08, 18], [2.033332967194131e-17, 26], [1.2074099142819403e-22, 14], [1.0186634868679338e-24, 23], [1.628811072952565e-28, 19], [2.3093921088097943e-29, 26], [1.073669845472951e-32, 18], [2.318865763568075e-33, 25], [1.2317481748629356e-38, 18], [7.309941111038667e-39, 25], [1.8207865335635145e-44, 16], [1.4321634370655336e-44, 25], [2.2074926431805844e-54, 21], [1.2824137323901783e-58, 21], [1.1413767298502077e-58, 6]], "generated": ["The average number of episodes for Days of Our Lives is about 223.", "So, 223 * 1000dollars is 223,000 dollars.", "The average number of episodes for General Hospital is about 56.", "So, 56*1000dollars is 56,000 dollars.", "The average number of episodes for The Bold and the Beautiful is about 217.", "So, 217 * 1000dollars is 217,000 dollars.", "The average number of episodes for Young and the Restless is about 241.", "So, 241*1000dollars is 241,000 dollars.", "The average number of episodes for As the World Turns is about 254.", "So, 254*1000dollars is 254,000 dollars.", "The average number of episodes for All my Children is about 439.", "So, 439*1000dollars is 439,000 dollars.", "Days of Our Lives has the most average number of episodes and the highest amount of money in total.", "Thus, a thousand dollars per Days of Our Lives episodes is the most preferred for soaps.", "So the answer is yes."], "conf_comments": ["(A)\nThe average cost of", "(A)\nThe average number of", "(A)\nSo, 5", "(A)\nThe average number of", "(A)\nSo, 2", "(A)\nThe average number of", "(A)\nSo, 2", "(A)\nThus, the", "(A)\nSo, 2", "(A)\nThus, Days", "(A)\nSo, 4", "(A)\nThus, Days", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 13, "score": [0.5908429425365038, 0.3570436310597402, 0.20979727288184943, 0.16641354501689667, 0.1143615505581289, 0.09563238612998572, 0.06521090378607132, 0.05436130747515187, 0.03151689619892105, 0.0259229326099997, 0.013657301975719356, 0.007498392456504419, 0.005882030123069834], "conf": [0.9684110185355744, 0.7652050520616509, 0.6240881090773083, 0.4832699997759396, 0.3615517415411101, 0.2725524585197188, 0.1941004304741493, 0.14341337393620357, 0.09465392330440002, 0.06538593232762084, 0.04521309725158066, 0.02929948290213461, 0.01853179336632215], "prob": [[1.0566201433702503e-08, 18], [2.033332967194131e-17, 26], [1.2074099142819403e-22, 14], [1.0186634868679338e-24, 23], [1.628811072952565e-28, 19], [2.3093921088097943e-29, 26], [1.073669845472951e-32, 18], [2.318865763568075e-33, 25], [1.2317481748629356e-38, 18], [7.309941111038667e-39, 25], [5.575595305429212e-48, 23], [1.2710785186233092e-56, 26], [1.0777954476878588e-56, 6]], "generated": ["The average number of episodes for Days of Our Lives is about 223.", "So, 223 * 1000dollars is 223,000 dollars.", "The average number of episodes for General Hospital is about 56.", "So, 56*1000dollars is 56,000 dollars.", "The average number of episodes for The Bold and the Beautiful is about 217.", "So, 217 * 1000dollars is 217,000 dollars.", "The average number of episodes for Young and the Restless is about 241.", "So, 241*1000dollars is 241,000 dollars.", "The average number of episodes for As the World Turns is about 254.", "So, 254*1000dollars is 254,000 dollars.", "Thus, 224,000 is larger than 254,000.", "So 224,000 dollars is preferred for Days of Our Lives, compared to the other soaps.", "So the answer is yes."], "conf_comments": ["(A)\nThe average cost of", "(A)\nThe average number of", "(A)\nSo, 5", "(A)\nThe average number of", "(A)\nSo, 2", "(A)\nThe average number of", "(A)\nSo, 2", "(A)\nThus, the", "(A)\nSo, 2", "(A)\nThus, Days", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 15, "score": [0.5908429425365038, 0.3570436310597402, 0.20979727288184943, 0.16641354501689667, 0.1143615505581289, 0.09563238612998572, 0.06521090378607132, 0.05436130747515187, 0.03151689619892105, 0.0259229326099997, 0.012652236881694846, 0.010299896100262714, 0.005231364430873839, 0.003404970931585576, 0.0022863567082958155], "conf": [0.9684110185355744, 0.7652050520616509, 0.6240881090773083, 0.4832699997759396, 0.3615517415411101, 0.2725524585197188, 0.1941004304741493, 0.14341337393620357, 0.09465392330440002, 0.06538593232762084, 0.034888244367979306, 0.023344298746439337, 0.01765987536204141, 0.011903992619376273, 0.007702651320997744], "prob": [[1.0566201433702503e-08, 18], [2.033332967194131e-17, 26], [1.2074099142819403e-22, 14], [1.0186634868679338e-24, 23], [1.628811072952565e-28, 19], [2.3093921088097943e-29, 26], [1.073669845472951e-32, 18], [2.318865763568075e-33, 25], [1.2317481748629356e-38, 18], [7.309941111038667e-39, 25], [1.8207865335635145e-44, 16], [1.4321634370655336e-44, 25], [2.2074926431805844e-54, 21], [1.2824137323901783e-58, 21], [3.460699191752066e-60, 10]], "generated": ["The average number of episodes for Days of Our Lives is about 223.", "So, 223 * 1000dollars is 223,000 dollars.", "The average number of episodes for General Hospital is about 56.", "So, 56*1000dollars is 56,000 dollars.", "The average number of episodes for The Bold and the Beautiful is about 217.", "So, 217 * 1000dollars is 217,000 dollars.", "The average number of episodes for Young and the Restless is about 241.", "So, 241*1000dollars is 241,000 dollars.", "The average number of episodes for As the World Turns is about 254.", "So, 254*1000dollars is 254,000 dollars.", "The average number of episodes for All my Children is about 439.", "So, 439*1000dollars is 439,000 dollars.", "Days of Our Lives has the most average number of episodes and the highest amount of money in total.", "Thus, a thousand dollars per Days of Our Lives episodes is the most preferred for soaps.", "So the answer is Days of Our Lives."], "conf_comments": ["(A)\nThe average cost of", "(A)\nThe average number of", "(A)\nSo, 5", "(A)\nThe average number of", "(A)\nSo, 2", "(A)\nThe average number of", "(A)\nSo, 2", "(A)\nThus, the", "(A)\nSo, 2", "(A)\nThus, Days", "(A)\nSo, 4", "(A)\nThus, Days", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": [], "9": [], "10": [], "11": [], "12": [], "13": [], "14": [], "15": []}, "run_time": 207.32108068466187}
{"index": 208, "question": "Does Pikachu like Charles Darwin?", "answer": "yes", "explanation": "Yes. Charles Darwin developed the theory of evolution. Pikachu is a Pokemon. Pokemon rely on evolution in order to transform into stronger forms.", "generated": [{"finished": true, "length": 4, "score": [0.5894426865603856, 0.30899631923812204, 0.24232538040144105, 0.19503744166890216], "conf": [0.9210876763483666, 0.6071261229292177, 0.4160275772950539, 0.2705128139235672], "prob": [[6.337198687187844e-08, 17], [2.530825692441906e-19, 30], [5.571012736677076e-20, 14], [5.44714428419754e-20, 6]], "generated": ["Pikachu is a Pokemon character, which is an imaginary animal.", "Charles Darwin was a real person, so there is no possible way for Pikachu to like him, or vice-versa.", "Thus, Pikachu does not like Charles Darwin.", "So the answer is no."], "conf_comments": ["(A)\nCharles Darwin", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5894426865603856, 0.3453474675562006, 0.21366572974312295, 0.17007786712409165], "conf": [0.9210876763483666, 0.703638011941351, 0.48489312846131793, 0.30983651512370103], "prob": [[6.337198687187844e-08, 17], [3.895949039953353e-13, 15], [5.484813561100024e-18, 19], [5.2143192516176135e-18, 6]], "generated": ["Pikachu is a Pokemon character, which is an imaginary animal.", "Charles Darwin was a naturalist, a person who studies nature.", "Thus, Pikachu does not like Charles Darwin because they are very different.", "So the answer is no."], "conf_comments": ["(A)\nCharles Darwin", "(A)\nThus, P", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6910854300896367, 0.30891129717187205, 0.17938242864739623, 0.1365984976220656], "conf": [0.9488289222537141, 0.5938828398645716, 0.36384822913249526, 0.21251771012011736], "prob": [[3.3739092232934963e-05, 15], [3.9973835750993956e-14, 18], [9.358236481217038e-18, 14], [8.960600610314464e-18, 6]], "generated": ["Charles Darwin was a scientist known for his theory of evolution.", "Pikachu is a fictional Pokemon, but is not a real animal.", "Thus, Pikachu did not like Charles Darwin.", "So the answer is no."], "conf_comments": ["(A)\nPikachu", "(A)\nThus, P", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5894426865603856, 0.30899631923812204, 0.20829549981414575], "conf": [0.9210876763483666, 0.6071261229292177, 0.40240633251647223], "prob": [[6.337198687187844e-08, 17], [2.530825692441906e-19, 30], [2.62824524411585e-20, 6]], "generated": ["Pikachu is a Pokemon character, which is an imaginary animal.", "Charles Darwin was a real person, so there is no possible way for Pikachu to like him, or vice-versa.", "So the answer is no."], "conf_comments": ["(A)\nCharles Darwin", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.7285282130768697, 0.33573508540037605], "conf": [0.9206095501088465, 0.6710071415152482], "prob": [[9.487739607146985e-06, 21], [2.5606334475379148e-14, 16]], "generated": ["There is no evidence to indicate that Pikachu likes or dislikes Charles Darwin.", "Since there was not enough information, the answer is we don’t know."], "conf_comments": ["(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 45.04421830177307}
{"index": 209, "question": "Does a person using tonsure have hair at the top of their scalp?", "answer": "no", "explanation": "No. Tonsure involves shaving some or all of the hair from the head. Tonsure styles include a large bald spot at the top of the scalp.", "generated": [{"finished": true, "length": 3, "score": [0.736168269707068, 0.5317507987765393, 0.46607936045858483], "conf": [0.9540089275651885, 0.7777735533953064, 0.6170256678192776], "prob": [[0.0006415385439357268, 13], [3.4895796086172875e-08, 22], [2.8780173077884665e-08, 6]], "generated": ["Tonsure is a religious practice of shaving the head.", "Thus, by definition a person using tonsure does not have hair at the top of their scalp.", "So the answer is no."], "conf_comments": ["(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5147009884950893, 0.31211891662356406, 0.2549373154640513, 0.2187408379441496], "conf": [0.9508401348651471, 0.7646875062983034, 0.5762890037974434, 0.42574405886154487], "prob": [[2.2133622228760626e-12, 21], [3.377437344985876e-20, 23], [2.951163264448488e-21, 20], [2.890022434231814e-21, 6]], "generated": ["Tonsure is when Buddhist and Jain monks and nuns have their heads shaved.", "They do not have hair, so it is impossible for them to have hair at the top of their scalp.", "Thus, a person using tonsure does not have hair at the top of their scalp.", "So the answer is no."], "conf_comments": ["(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.736168269707068, 0.3840283244332398, 0.3058603963779205], "conf": [0.9540089275651885, 0.7189831794791618, 0.5586559427112201], "prob": [[0.0006415385439357268, 13], [1.187286441586071e-13, 22], [2.3426629022567135e-14, 8]], "generated": ["Tonsure is a religious practice of shaving the head.", "So, obviously a person would not have hair at the top of their scalp when practicing tonsure.", "Thus, the answer is no."], "conf_comments": ["(A)\nThus, a", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5147009884950893, 0.31211891662356406, 0.2590252106675827], "conf": [0.9508401348651471, 0.7646875062983034, 0.5696578589204367], "prob": [[2.2133622228760626e-12, 21], [3.377437344985876e-20, 23], [2.1089682764054226e-20, 6]], "generated": ["Tonsure is when Buddhist and Jain monks and nuns have their heads shaved.", "They do not have hair, so it is impossible for them to have hair at the top of their scalp.", "So the answer is no."], "conf_comments": ["(A)\nThus, a", "(A)\nThus, a", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5147009884950893, 0.27775404745897997, 0.23124767660518236], "conf": [0.9508401348651471, 0.6751328884265216, 0.47349363287873975], "prob": [[2.2133622228760626e-12, 21], [1.3182394594304986e-23, 29], [1.2287083693143188e-23, 6]], "generated": ["Tonsure is when Buddhist and Jain monks and nuns have their heads shaved.", "Thus, a person using tonsure has his/her hair shaved, which is not having hair at the top of their scalp.", "So the answer is no."], "conf_comments": ["(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 42.28379964828491}
{"index": 210, "question": "Can a grey seal swim in the same water as the subject of Moby Dick?", "answer": "yes", "explanation": "Yes. The range of gray seals is limited to parts of the northern hemisphere bordered by the Atlantic ocean. The subject of Moby Dick was a sperm whale. Sperm whales can be found in the north Atlantic, in addition to most other bodies of water on earth.", "generated": [{"finished": true, "length": 4, "score": [0.6452300214919472, 0.40726196148152816, 0.3363660374123858, 0.29371846744819713], "conf": [0.9607353496159888, 0.8398122545076716, 0.6647902273425852, 0.5082833840780446], "prob": [[3.609602737188514e-10, 26], [1.3215423018334702e-14, 13], [4.312079814079008e-16, 23], [4.2421737041766354e-16, 6]], "generated": ["Grey seals live and swim in the North Atlantic, the North Pacific, and the Baltic and North Seas.", "The subject of Moby Dick was found in the Pacific Ocean.", "Thus, a grey seal could not swim in the same water as the subject of Moby Dick.", "So the answer is no."], "conf_comments": ["(A)\nMoby Dick is", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6452300214919472, 0.40726196148152816, 0.27391535540037126, 0.23538146446631833], "conf": [0.9607353496159888, 0.8398122545076716, 0.6799028059586624, 0.5041477572623194], "prob": [[3.609602737188514e-10, 26], [1.3215423018334702e-14, 13], [1.9271456989325235e-22, 31], [1.8798322344515732e-22, 6]], "generated": ["Grey seals live and swim in the North Atlantic, the North Pacific, and the Baltic and North Seas.", "The subject of Moby Dick was found in the Pacific Ocean.", "Thus, a grey seal could not swim in the same water as the subject of Moby Dick, as they are from different seas.", "So the answer is no."], "conf_comments": ["(A)\nMoby Dick is", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7118825697710385, 0.3587380915847438, 0.19771340756706132, 0.13933493332189123, 0.11914603621146998], "conf": [0.9652318896213113, 0.8747621577651947, 0.6484352936834008, 0.5126137911906478, 0.37652650885025457], "prob": [[0.00023035929632564983, 13], [2.6124302809965928e-14, 18], [1.6507992806778905e-20, 16], [3.754478315074363e-25, 23], [3.653869116212688e-25, 6]], "generated": ["The sperm whale is the subject of Moby Dick.", "The sperm whale lives and swims in the deep sea and the deep ocean.", "According to Encyclopedia Britannica, the grey seal prefers shallow water.", "Thus, the grey seal could not swim in the same water as a subject of Moby Dick.", "So the answer is no."], "conf_comments": ["(A)\nThe grey seal", "(A)\nThe grey seal", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7118825697710385, 0.3587380915847438, 0.19771340756706132, 0.1339601251954535, 0.11476115286941789], "conf": [0.9652318896213113, 0.8747621577651947, 0.6484352936834008, 0.511037707004533, 0.3767857498541235], "prob": [[0.00023035929632564983, 13], [2.6124302809965928e-14, 18], [1.6507992806778905e-20, 16], [2.2387620153150394e-26, 25], [2.1776650502709483e-26, 6]], "generated": ["The sperm whale is the subject of Moby Dick.", "The sperm whale lives and swims in the deep sea and the deep ocean.", "According to Encyclopedia Britannica, the grey seal prefers shallow water.", "Thus, a grey seal and the subject of Moby Dick could not swim in the same body of water.", "So the answer is no."], "conf_comments": ["(A)\nThe grey seal", "(A)\nThe grey seal", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7118825697710385, 0.40671193751569223, 0.21726782455365498, 0.13887530130485923, 0.07157856725294966], "conf": [0.9652318896213113, 0.8341571391851577, 0.7137324596438926, 0.5498422947863417, 0.41792369073231006], "prob": [[0.00023035929632564983, 13], [2.1564865005607612e-23, 45], [6.959753804187112e-34, 22], [4.0660002937275724e-39, 19], [6.0537396703695805e-62, 50]], "generated": ["The sperm whale is the subject of Moby Dick.", "The sperm whale is classified under the phylum Cetacea (whales, dolphins and porpoises), and family Physeteridae (only contains species of sperm whale).", "The family of Sperm whales is suborder Odontoceti (all toothed whales).", "The grey seal is classified under the phylum Pinnipedia (seals).", "Pinnipeds (seals, sea lions, walrusses, and sea otters) are suborder Pinnipedia in the order Carnivora (e.g., wolves, foxes, dingoes)."], "conf_comments": ["(A)\nThe grey seal", "(A)\nThe grey seal", "(A)\nThe grey seal", "(A)\nThus, the", "(A)\nThus, the"], "info": null}, {"finished": false, "length": 5, "score": [0.7118825697710385, 0.40671193751569223, 0.21726782455365498, 0.13887530130485923, 0.06874589480790884], "conf": [0.9652318896213113, 0.8341571391851577, 0.7137324596438926, 0.5498422947863417, 0.4025814073267602], "prob": [[0.00023035929632564983, 13], [2.1564865005607612e-23, 45], [6.959753804187112e-34, 22], [4.0660002937275724e-39, 19], [2.228463867787556e-53, 30]], "generated": ["The sperm whale is the subject of Moby Dick.", "The sperm whale is classified under the phylum Cetacea (whales, dolphins and porpoises), and family Physeteridae (only contains species of sperm whale).", "The family of Sperm whales is suborder Odontoceti (all toothed whales).", "The grey seal is classified under the phylum Pinnipedia (seals).", "The Pinnipedia is order Carnivoran (mammals) and suborder Caniformia (bears, dogs, etc.).", "The two suborders, Odontoceti and Caniformia, are distinct, so it"], "conf_comments": ["(A)\nThe grey seal", "(A)\nThe grey seal", "(A)\nThe grey seal", "(A)\nThus, the", "(A)\nThus, the"], "info": [[[5.4807272671015364e-15, 30], [2.1349070679540833e-10, 19]], 0.7321761369470416, "(A)\nThus, the", 5]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 61.79739689826965}
{"index": 211, "question": "Will Donald Duck hypothetically purchase bell-bottoms for himself?", "answer": "no", "explanation": "No. Bell-bottoms were a style of pants that were popular in the 60s and 70s. Donald Duck is an animated Disney character that never wears pants.", "generated": [{"finished": true, "length": 3, "score": [0.7075533760690664, 0.49487497771140254, 0.42231360533483353], "conf": [0.931954790193399, 0.6849840742169032, 0.5040601455940015], "prob": [[0.0001666210940998022, 14], [8.37838434643071e-07, 13], [7.87089094387614e-07, 6]], "generated": ["Donald Duck is a fictional character and does not actually exist.", "Thus, he cannot purchase bell-bottoms for himself.", "So the answer is no."], "conf_comments": ["(A)\nThus, Donald", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7195910042054114, 0.44040852889318044, 0.28514865206393414, 0.24162945761205049], "conf": [0.9629540957012653, 0.6568334783428194, 0.48330613843522774, 0.34957953467256575], "prob": [[1.9969683702466293e-10, 36], [6.821303660310507e-16, 21], [9.827979433083279e-23, 28], [9.407295342369045e-23, 6]], "generated": ["Bell-bottoms were quite popular during the 1960s, 1970s, and the beginning of the 1980s.", "However, Donald Duck is a cartoon character that was first created in 1934.", "Thus, Donald Duck would not be able to purchase bell-bottoms for himself as bell-bottoms did not exist yet.", "So the answer is no."], "conf_comments": ["(A)\nDonald Duck was", "(A)\nThus, Donald", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7195910042054114, 0.44040852889318044, 0.28269980558730623, 0.23628448921289144], "conf": [0.9629540957012653, 0.6568334783428194, 0.5103956479474341, 0.3614585715883186], "prob": [[1.9969683702466293e-10, 36], [6.821303660310507e-16, 21], [5.923809562705805e-22, 22], [5.457680903562801e-22, 6]], "generated": ["Bell-bottoms were quite popular during the 1960s, 1970s, and the beginning of the 1980s.", "However, Donald Duck is a cartoon character that was first created in 1934.", "Thus, it wasn’t possible for Donald Duck to purchase bell-bottoms in the past.", "So the answer is no."], "conf_comments": ["(A)\nDonald Duck was", "(A)\nThus, Donald", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7075533760690664, 0.33486861863062073, 0.24629766504506795, 0.20053640078601018], "conf": [0.931954790193399, 0.7455856658061851, 0.5091689181033929, 0.33872738303068284], "prob": [[0.0001666210940998022, 14], [2.37560636944376e-13, 16], [5.7107366834059e-15, 16], [5.59186474349671e-15, 6]], "generated": ["Donald Duck is a fictional character and does not actually exist.", "It’s not possible for Donald Duck to do things in real life.", "Thus, Donald Duck will not purchase bell-bottoms for himself.", "So the answer is no."], "conf_comments": ["(A)\nThus, Donald", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.7075533760690664, 0.33486861863062073, 0.23812701851891035], "conf": [0.931954790193399, 0.7455856658061851, 0.5006673603920576], "prob": [[0.0001666210940998022, 14], [2.37560636944376e-13, 16], [4.331863742433668e-14, 6]], "generated": ["Donald Duck is a fictional character and does not actually exist.", "It’s not possible for Donald Duck to do things in real life.", "So the answer is no."], "conf_comments": ["(A)\nThus, Donald", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 46.853142738342285}
{"index": 212, "question": "Do people put creatures from the Black Sea on their pizza?", "answer": "yes", "explanation": "Yes. Pizza toppings include pepperoni, sausage, bacon, meatball, and anchovies. The Black Sea is home to many animals including dogfish, jellyfish, and anchovies.", "generated": [{"finished": true, "length": 4, "score": [0.5614335693305992, 0.34953604021666934, 0.24401161644567024, 0.20444345882752982], "conf": [0.9500353621389604, 0.7989114721601285, 0.5779730850753068, 0.40777060265562626], "prob": [[5.903924123442833e-07, 13], [4.991367980228663e-15, 24], [4.073106799356895e-18, 18], [3.952106355093186e-18, 6]], "generated": ["There are no land mammals living in the Black Sea.", "In addition, there are no sea mammals, such as dolphins, that live in the Black Sea.", "Thus, people cannot put any creatures from the Black Sea on their pizza.", "So the answer is no."], "conf_comments": ["(A)\nThus, people", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5978232345410717, 0.26946545074808054, 0.21035175171299914, 0.1705832051697903], "conf": [0.9453860487187941, 0.5619593895781728, 0.37106407582647877, 0.24479205193112963], "prob": [[1.2176135523701085e-06, 14], [2.649464177086123e-11, 10], [6.247639601866385e-12, 18], [6.130659622450691e-12, 6]], "generated": ["Most people put cheese and tomato on their pizza.", "However, the Black Sea contains no creatures.", "Thus, people do not put creatures from the Black Sea on their pizza.", "So the answer is no."], "conf_comments": ["(A)\nThe Black Sea is", "(A)\nThus, people", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.550591821692232, 0.4046210674152495], "conf": [0.9275617719855994, 0.7010984098423453], "prob": [[1.4852516755277006e-06, 12], [1.0088130463177871e-07, 8]], "generated": ["Black Sea creatures are not often put on pizza.", "Thus, the answer is no."], "conf_comments": ["(A)\nThe Black Sea is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5614335693305992, 0.34953604021666934, 0.14743186423176308, 0.11835765766969834, 0.09820889533182384], "conf": [0.9500353621389604, 0.7989114721601285, 0.594297771676797, 0.432371342799443, 0.29863369410961493], "prob": [[5.903924123442833e-07, 13], [4.991367980228663e-15, 24], [3.271145158351481e-26, 18], [3.690877151098647e-27, 18], [3.621527037528772e-27, 6]], "generated": ["There are no land mammals living in the Black Sea.", "In addition, there are no sea mammals, such as dolphins, that live in the Black Sea.", "There are also no fish or seashells that could potentially be on a pizza.", "Thus, people do not put creatures from the Black Sea on their pizza.", "So the answer is no."], "conf_comments": ["(A)\nThus, people", "(A)\nThus, there", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 7, "score": [0.5614335693305992, 0.34953604021666934, 0.14743186423176308, 0.08576936007196534, 0.05684692714973831, 0.03948945219357166, 0.0330016918422793], "conf": [0.9500353621389604, 0.7989114721601285, 0.594297771676797, 0.4398157315692675, 0.3046852393965128, 0.21391194911747488, 0.1503296930455847], "prob": [[5.903924123442833e-07, 13], [4.991367980228663e-15, 24], [3.271145158351481e-26, 18], [1.1971244558741153e-31, 16], [2.0347225504173892e-34, 14], [2.3848881163116088e-37, 18], [2.2975808433083735e-37, 6]], "generated": ["There are no land mammals living in the Black Sea.", "In addition, there are no sea mammals, such as dolphins, that live in the Black Sea.", "There are also no fish or seashells that could potentially be on a pizza.", "In addition, there are no birds that could potentially be on a pizza.", "There are also no plants that could potentially be on a pizza.", "Thus, it is impossible to put creatures from the Black Sea on pizza.", "So the answer is no."], "conf_comments": ["(A)\nThus, people", "(A)\nThus, there", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 58.9975483417511}
{"index": 213, "question": "Are there special traffic laws associated with funerals?", "answer": "yes", "explanation": "Yes. Many funerals have a religious ceremony held in a chapel separate from the cemetery. The corpse and all the attendees have to move from the memorial site to the cemetery. Everyone at the funeral lines up their cars into a procession and follow special rules as they drive to keep the line assembled in transport.", "generated": [{"finished": true, "length": 4, "score": [0.7744675788424832, 0.4208856949624377, 0.36921422179571245, 0.3148942148333334], "conf": [0.9176050783040253, 0.8541461477161062, 0.7193364297756353, 0.5241623357932319], "prob": [[0.009308022564146378, 11], [1.0149382356728504e-15, 26], [2.8710491292831915e-16, 14], [2.8410129675447757e-16, 6]], "generated": ["There are special traffic laws associated with funerals.", "For example, in New York state, funeral processions may travel 15 mph slower than the normal speed limit.", "Thus, there are special traffic laws associated with funerals.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5644484789012942, 0.47633082275179855, 0.4116524521712467], "conf": [0.9546237230234285, 0.8083897108703596, 0.6054230066386878], "prob": [[1.2173340911037232e-12, 25], [1.0772899437258247e-13, 14], [1.059663576835912e-13, 6]], "generated": ["During a funeral procession, vehicles need to yield right of way to the vehicles of the funeral procession.", "Thus, there are special traffic laws associated with funerals.", "So the answer is yes."], "conf_comments": ["(A)\nThus, there", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.534270751414311, 0.45640254877592784, 0.39199428175564294], "conf": [0.9188429569919887, 0.7837348282169029, 0.5797173232717041], "prob": [[1.6383843826490964e-19, 37], [1.8443882300082574e-20, 14], [1.814469202216047e-20, 6]], "generated": ["Yes, in some cities there are laws that require drivers to yield to funeral processions regardless of whether they are on a road, in a crosswalk, or at an intersection.", "Thus, there are special traffic laws associated with funerals.", "So the answer is yes."], "conf_comments": ["(A)\nThus, there", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5196161926233329, 0.4246697712324164, 0.3657053317582027], "conf": [0.9300719297616863, 0.8017382971215276, 0.5957456838926491], "prob": [[6.732815259764383e-25, 45], [2.443841127032564e-26, 13], [2.4146970094443584e-26, 6]], "generated": ["According to the state of California, one of the traffic laws that specifically applies to funerals is that you must yield the right of way to any emergency vehicle, road roller, or funeral procession.", "Thus, there are special traffic laws for funerals.", "So the answer is yes."], "conf_comments": ["(A)\nThus, there", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7744675788424832, 0.34062168468432713, 0.2908410167625762, 0.2447357305848297], "conf": [0.9176050783040253, 0.8275216528588386, 0.6650690565681769, 0.471726100125816], "prob": [[0.009308022564146378, 11], [1.7720038046673186e-21, 28], [4.528720477062579e-22, 14], [4.482674816485913e-22, 6]], "generated": ["There are special traffic laws associated with funerals.", "For example, in some places, you are not supposed to honk when driving on the right side of the street behind the funeral.", "Thus, there are special traffic laws associated with funerals.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 44.74405336380005}
{"index": 214, "question": "Can I find my home with latitude and longitude?", "answer": "yes", "explanation": "Yes. My home is a location on earth where I live.  Latitude and Longitude are geographic coordinate systems that identify east/west and north/south locations. Specific Latitude and Longitude coordinates can be used to pinpoint specific locations.  Every point on the earth has a corresponding latitude and longitude coordinate. ", "generated": [{"finished": true, "length": 10, "score": [0.6382664462100028, 0.3316632887751433, 0.12831706595531736, 0.09585852373230389, 0.04747425336447703, 0.0372847050856442, 0.026827567917753748, 0.022687969395380123, 0.014527967279102296, 0.011154172537693062], "conf": [0.9602541169755736, 0.8292563790587789, 0.7069294739920833, 0.5802268222497837, 0.48148181725587447, 0.37774448020076856, 0.3006174916235832, 0.22184307258975822, 0.16798099309163672, 0.12003728614564602], "prob": [[8.013022344402795e-05, 11], [5.97156066481831e-18, 26], [3.41590369415517e-51, 44], [4.790013779289693e-55, 23], [2.4381701027101837e-60, 10], [1.560015618294369e-62, 21], [1.2530294915343792e-69, 38], [5.908671906046443e-70, 24], [9.559336486796387e-79, 33], [3.0121851783375834e-79, 6]], "generated": ["Latitude and longitude is a geographic coordinate system.", "The latitude of my home is 22.65° and the longitude is 114.13°.", "In Google Maps, a location’s latitude can be converted to geographical longitude (West and East of Greenwich Mean Time, also known as GMT) by adding or subtracting 15 from the latitude.", "In my case, 22.65 + 15 = 37.65°.", "The longitude is subtracted from 15.", "So 114.13 - 15 = 99.13°.", "In Google Maps, the longitude of a location can be converted to geographical latitude (North and South of the equator) by adding or subtracting 90 from the longitude.", "In my case, 99.13 + 90 = 189.13°.", "Thus, the latitude and longitude of my home is 37°39’0”N, 189°7’0”E.", "So the answer is yes."], "conf_comments": ["(A)\nIt is used to", "(A)\nThus, I", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nIn my case,", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.47967881369391885, 0.3020541349811683, 0.24513077028219935], "conf": [0.9489342291756504, 0.6354093472013068, 0.42088845212623566], "prob": [[1.0014851249590793e-08, 13], [6.530683548146737e-12, 14], [6.310077719897786e-12, 6]], "generated": ["You can use latitude and longitude to know approximately where something is.", "Thus, you can use latitude and longitude to find your home.", "So the answer is yes."], "conf_comments": ["(A)\nHowever, latitude and", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 10, "score": [0.6382664462100028, 0.3316632887751433, 0.12831706595531736, 0.09585852373230389, 0.04747425336447703, 0.0372847050856442, 0.026827567917753748, 0.021112536244476246, 0.011234356292582856, 0.009204796006739903], "conf": [0.9602541169755736, 0.8292563790587789, 0.7069294739920833, 0.5802268222497837, 0.48148181725587447, 0.37774448020076856, 0.3006174916235832, 0.2253776955862823, 0.1591073463733639, 0.10789987108977664], "prob": [[8.013022344402795e-05, 11], [5.97156066481831e-18, 26], [3.41590369415517e-51, 44], [4.790013779289693e-55, 23], [2.4381701027101837e-60, 10], [1.560015618294369e-62, 21], [1.2530294915343792e-69, 38], [1.8725559216608576e-71, 22], [1.3511005972986632e-78, 18], [1.2714381164965022e-78, 6]], "generated": ["Latitude and longitude is a geographic coordinate system.", "The latitude of my home is 22.65° and the longitude is 114.13°.", "In Google Maps, a location’s latitude can be converted to geographical longitude (West and East of Greenwich Mean Time, also known as GMT) by adding or subtracting 15 from the latitude.", "In my case, 22.65 + 15 = 37.65°.", "The longitude is subtracted from 15.", "So 114.13 - 15 = 99.13°.", "In Google Maps, the longitude of a location can be converted to geographical latitude (North and South of the equator) by adding or subtracting 90 from the longitude.", "In my case, 99.13 - 90 = -9.13°.", "Thus, I can find my latitude and longitude and find my home on Google Maps.", "So the answer is yes."], "conf_comments": ["(A)\nIt is used to", "(A)\nThus, I", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nIn my case,", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 11, "score": [0.6382664462100028, 0.3316632887751433, 0.12831706595531736, 0.09585852373230389, 0.04747425336447703, 0.0372847050856442, 0.026827567917753748, 0.022687969395380123, 0.014527967279102296, 0.006731289030332251, 0.005591294134822662], "conf": [0.9602541169755736, 0.8292563790587789, 0.7069294739920833, 0.5802268222497837, 0.48148181725587447, 0.37774448020076856, 0.3006174916235832, 0.22184307258975822, 0.16798099309163672, 0.13377976118200674, 0.10180611284504969], "prob": [[8.013022344402795e-05, 11], [5.97156066481831e-18, 26], [3.41590369415517e-51, 44], [4.790013779289693e-55, 23], [2.4381701027101837e-60, 10], [1.560015618294369e-62, 21], [1.2530294915343792e-69, 38], [5.908671906046443e-70, 24], [9.559336486796387e-79, 33], [2.664892820684528e-83, 8], [1.4802951524332851e-83, 6]], "generated": ["Latitude and longitude is a geographic coordinate system.", "The latitude of my home is 22.65° and the longitude is 114.13°.", "In Google Maps, a location’s latitude can be converted to geographical longitude (West and East of Greenwich Mean Time, also known as GMT) by adding or subtracting 15 from the latitude.", "In my case, 22.65 + 15 = 37.65°.", "The longitude is subtracted from 15.", "So 114.13 - 15 = 99.13°.", "In Google Maps, the longitude of a location can be converted to geographical latitude (North and South of the equator) by adding or subtracting 90 from the longitude.", "In my case, 99.13 + 90 = 189.13°.", "Thus, the latitude and longitude of my home is 37°39’0”N, 189°7’0”E.", "This is the location in Google Maps.", "So the answer is yes."], "conf_comments": ["(A)\nIt is used to", "(A)\nThus, I", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nIn my case,", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 12, "score": [0.6382664462100028, 0.3316632887751433, 0.12831706595531736, 0.09585852373230389, 0.04747425336447703, 0.0372847050856442, 0.026827567917753748, 0.022687969395380123, 0.012370122529191755, 0.0071462321916915235, 0.004056339340208094, 0.003357549261322048], "conf": [0.9602541169755736, 0.8292563790587789, 0.7069294739920833, 0.5802268222497837, 0.48148181725587447, 0.37774448020076856, 0.3006174916235832, 0.22184307258975822, 0.16787812667846216, 0.11666574221560974, 0.08287699482995048, 0.057084187817894705], "prob": [[8.013022344402795e-05, 11], [5.97156066481831e-18, 26], [3.41590369415517e-51, 44], [4.790013779289693e-55, 23], [2.4381701027101837e-60, 10], [1.560015618294369e-62, 21], [1.2530294915343792e-69, 38], [5.908671906046443e-70, 24], [3.350872587697197e-73, 8], [1.1418531155443812e-84, 36], [1.7796785760139216e-89, 14], [1.72388364194637e-89, 6]], "generated": ["Latitude and longitude is a geographic coordinate system.", "The latitude of my home is 22.65° and the longitude is 114.13°.", "In Google Maps, a location’s latitude can be converted to geographical longitude (West and East of Greenwich Mean Time, also known as GMT) by adding or subtracting 15 from the latitude.", "In my case, 22.65 + 15 = 37.65°.", "The longitude is subtracted from 15.", "So 114.13 - 15 = 99.13°.", "In Google Maps, the longitude of a location can be converted to geographical latitude (North and South of the equator) by adding or subtracting 90 from the longitude.", "In my case, 99.13 + 90 = 189.13°.", "This is North of the equator.", "Since the latitude of my home is 22.65° and the longitude is 114.13°, in Google Maps the location can be found.", "Thus, using latitude and longitude, one can find their home.", "So the answer is yes."], "conf_comments": ["(A)\nIt is used to", "(A)\nThus, I", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nIn my case,", "(A)\nThus, the", "(A)\nThus, I", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": [], "9": [], "10": [], "11": [], "12": []}, "run_time": 134.33207201957703}
{"index": 215, "question": "Does Woody Allen await the Second Coming?", "answer": "no", "explanation": "No. The Second Coming refers to Jesus Christ returning to earth. Christians and Muslims believe in Jesus Christ. Woody Allen is Jewish.", "generated": [{"finished": true, "length": 4, "score": [0.8142925069297648, 0.558195384616925, 0.35814652468094094, 0.30027235564708016], "conf": [0.9755869462410546, 0.8742835029437772, 0.6388954697250283, 0.451798407469926], "prob": [[0.00660440430221137, 13], [2.85323328832899e-06, 12], [5.210860041051935e-10, 15], [5.026612454737525e-10, 6]], "generated": ["Woody Allen was born in 1935.", "The Second Coming is supposed to happen in the future.", "Thus, Woody Allen obviously does not await the Second Coming.", "So the answer is no."], "conf_comments": ["(A)\nThe Second Coming", "(A)\nThus, Wo", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6582009169813298, 0.389217185930078, 0.3309587282226502, 0.2813419455784763], "conf": [0.9659529313333277, 0.8476840301525399, 0.6601868727010641, 0.47849379848010076], "prob": [[0.001637148256593771, 8], [4.201693289890308e-13, 24], [1.4847565267946535e-13, 14], [1.458567183022041e-13, 6]], "generated": ["Woody Allen is still alive.", "According to the Second Coming of Christ, the Second Coming will come after Woody Allen’s death.", "Thus, Woody Allen does not await the Second Coming.", "So the answer is no."], "conf_comments": ["(A)\nThe Second Coming", "(A)\nThus, Wo", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6582009169813298, 0.389217185930078, 0.29559280659751846, 0.2523520160329075], "conf": [0.9659529313333277, 0.8476840301525399, 0.6776879754987717, 0.49548675166816597], "prob": [[0.001637148256593771, 8], [4.201693289890308e-13, 24], [3.137207555718121e-15, 15], [3.0781137254298156e-15, 6]], "generated": ["Woody Allen is still alive.", "According to the Second Coming of Christ, the Second Coming will come after Woody Allen’s death.", "Thus, Woody Allen does not yet await the Second Coming.", "So the answer is no."], "conf_comments": ["(A)\nThe Second Coming", "(A)\nThus, Wo", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6318914688526508, 0.4742555839607295, 0.3043077322032209, 0.24082114541946925], "conf": [0.9720910067693956, 0.7973112421563265, 0.534760502213362, 0.3428912910886265], "prob": [[5.6151148781800016e-05, 11], [4.246245711567819e-07, 13], [1.7263119212313562e-10, 16], [1.49870458922436e-10, 6]], "generated": ["The Second Coming refers to the coming of Jesus.", "Woody Allen is Jewish and does not believe in Jesus.", "Thus, Woody Allen probably doesn’t await the Second Coming.", "So the answer is no."], "conf_comments": ["(A)\nWoody Allen", "(A)\nThus, Wo", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6948116163113309, 0.5204413565412257, 0.24956898878476846, 0.16340888989758545, 0.1278654470475811], "conf": [0.9705925567665286, 0.878949627787985, 0.6668824023246458, 0.41866549920924967, 0.2580477236227051], "prob": [[0.00046096614763789756, 11], [8.339744684669389e-08, 18], [4.2265949882331667e-16, 16], [1.3844386032545406e-18, 15], [1.330475277834256e-18, 6]], "generated": ["Woody Allen is 82 years old.", "Jesus Christ was about 33 years old when he was crucified.", "Thus, Woody Allen is the older and Jesus Christ is the younger.", "Thus, Woody Allen is not awaiting the Second Coming.", "So the answer is no."], "conf_comments": ["(A)\nThe Second Coming", "(A)\nThus, Wo", "(A)\nJesus Christ", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 55.620673418045044}
{"index": 216, "question": "Can you get a fever from consuming meat?", "answer": "yes", "explanation": "Yes. A fever is a medical symptom that elevates the core body temperature.  Eating under cooked meat can cause food poisoning. One of the symptoms of food poisoning is elevated core body temperature. ", "generated": [{"finished": true, "length": 4, "score": [0.6862594751616604, 0.32100727950925373, 0.2385841972833416, 0.20408352921744569], "conf": [0.9664012246157913, 0.766607156638149, 0.6012098249449083, 0.4413957778205658], "prob": [[8.742790416880732e-05, 13], [9.069670069360349e-19, 25], [4.727127294742234e-21, 15], [4.632122547626302e-21, 6]], "generated": ["Fever is a symptom of infectious diseases.", "An infectious disease is caused by a pathogen and a pathogen cannot grow or produce toxins in meat.", "Thus, you can not get a fever from consuming meat.", "So the answer is no."], "conf_comments": ["(A)\nMeat is a", "(A)\nThus, you", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6862594751616604, 0.32100727950925373, 0.2381738597646458, 0.20326377440737017], "conf": [0.9664012246157913, 0.766607156638149, 0.5847283984596578, 0.42726690096461745], "prob": [[8.742790416880732e-05, 13], [9.069670069360349e-19, 25], [9.437805625085674e-21, 14], [9.25531781858053e-21, 6]], "generated": ["Fever is a symptom of infectious diseases.", "An infectious disease is caused by a pathogen and a pathogen cannot grow or produce toxins in meat.", "Thus, one cannot get a fever from consuming meat.", "So the answer is no."], "conf_comments": ["(A)\nMeat is a", "(A)\nThus, you", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5505614323834547, 0.34102949733195365, 0.23060128804540403, 0.1797263435505587], "conf": [0.9514031407801251, 0.6872311438128695, 0.4570748062318874, 0.2804664188693985], "prob": [[1.1440521250133468e-09, 18], [3.657018572645084e-15, 20], [1.926188546470887e-17, 14], [1.8127478761965084e-17, 6]], "generated": ["Fever is a symptom of infections, rather than a cause of it.", "You can only get a fever from consuming meat if you get an infection from it.", "Thus, you could get a fever from consuming meat.", "So the answer is yes."], "conf_comments": ["(A)\nConsuming meat can", "(A)\nThus, you", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.5152209844622779, 0.34804549483289776], "conf": [0.8579343975249212, 0.6166623136501177], "prob": [[2.38193043773701e-07, 13], [1.5598354648653085e-08, 6]], "generated": ["No, consumption of meat would not result in a fever.", "So the answer is no."], "conf_comments": ["(A)\nA fever is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.6112093069720578, 0.3197519266613962, 0.1770224513097594, 0.08020123155176745, 0.048897162202798154, 0.041658383765183944], "conf": [0.9689766432958064, 0.8906544237685238, 0.7677529322180654, 0.6732222912297366, 0.5070317632453113, 0.3758660077333818], "prob": [[2.5695971504568612e-12, 28], [8.69308804543899e-22, 18], [1.1464956594293544e-31, 22], [1.1950097438680552e-43, 19], [3.6081573119018575e-49, 18], [3.1792398543800646e-49, 6]], "generated": ["Fever is when the body temperature is exceeds the normal body temperature of 98.6°F (37°C).", "Common symptoms for fever are chills, headache, and general weakness.", "These symptoms are caused by infections, which are spread by viruses and bacteria.", "Fever happens because your immune system is trying to get rid of the foreign invaders.", "Thus, you can get a fever from consuming meat that is infected.", "So the answer is yes."], "conf_comments": ["(A)\nMeat is a", "(A)\nMeat is a", "(A)\nMeat is a", "(A)\nMeat is a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 50.31366491317749}
{"index": 217, "question": "Is eating a Dicopomorpha echmepterygis size Uranium pellet fatal?", "answer": "no", "explanation": "No. Dicopomorpha echmepterygis is a wingless insect that is .13mm large. Uranium is a radioactive element that is dangerous if ingested in large doses. 25mg of Uranium would cause kidney damage, while 50mg would cause complete kidney failure in humans.", "generated": [{"finished": true, "length": 5, "score": [0.8135749109369318, 0.4654548000467599, 0.28697646334738586, 0.23344153769395687, 0.20953638544756853], "conf": [0.9643271920576271, 0.8734366556363087, 0.7658444673087167, 0.6780027557343915, 0.5484251306846054], "prob": [[0.00036981482033773816, 21], [1.3119379134743461e-23, 44], [1.693418393329123e-34, 30], [3.6507022681332436e-38, 29], [3.564813695072806e-38, 6]], "generated": ["Dicopomorpha echmepterygis is a species of assassin bug.", "According to this source, Dicopomorpha echmepterygis are venomous with a potential lethal dose (LD) of 50.0 mg/kg.", "Uranium’s atomic weight is about 19x that of Dicopomorpha echmepterygis LD.", "Thus, eating a Dicopomorpha echmepterygis sized Uranium pellet would be fatal.", "So the answer is yes."], "conf_comments": ["(A)\nThe size of a", "(A)\nUranium is", "(A)\nThus, e", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8135749109369318, 0.4654548000467599, 0.28697646334738586, 0.156286135573768, 0.13396053301530625], "conf": [0.9643271920576271, 0.8734366556363087, 0.7658444673087167, 0.6270294693459532, 0.468060974204642], "prob": [[0.00036981482033773816, 21], [1.3119379134743461e-23, 44], [1.693418393329123e-34, 30], [2.9566592927317477e-51, 38], [2.6877654921442776e-51, 6]], "generated": ["Dicopomorpha echmepterygis is a species of assassin bug.", "According to this source, Dicopomorpha echmepterygis are venomous with a potential lethal dose (LD) of 50.0 mg/kg.", "Uranium’s atomic weight is about 19x that of Dicopomorpha echmepterygis LD.", "Thus, one size Uranium pellet is lethal to an adult human, and we can assume, a Dicopomorpha echmepterygis.", "So the answer is yes."], "conf_comments": ["(A)\nThe size of a", "(A)\nUranium is", "(A)\nThus, e", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.8135749109369318, 0.4654548000467599, 0.28697646334738586, 0.156286135573768, 0.10210662845860224, 0.08852779415060226], "conf": [0.9643271920576271, 0.8734366556363087, 0.7658444673087167, 0.6270294693459532, 0.5050582055014473, 0.3832179399388109], "prob": [[0.00036981482033773816, 21], [1.3119379134743461e-23, 44], [1.693418393329123e-34, 30], [2.9566592927317477e-51, 38], [4.071834459656183e-55, 14], [3.8501016244072286e-55, 6]], "generated": ["Dicopomorpha echmepterygis is a species of assassin bug.", "According to this source, Dicopomorpha echmepterygis are venomous with a potential lethal dose (LD) of 50.0 mg/kg.", "Uranium’s atomic weight is about 19x that of Dicopomorpha echmepterygis LD.", "Thus, one size Uranium pellet is lethal to an adult human, and we can assume, a Dicopomorpha echmepterygis.", "Therefore, eating a Uranium pellet is fatal.", "So the answer is yes."], "conf_comments": ["(A)\nThe size of a", "(A)\nUranium is", "(A)\nThus, e", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.8135749109369318, 0.4654548000467599, 0.2578006263774919, 0.15719000954271625, 0.10835696685695613, 0.08821954367558711], "conf": [0.9643271920576271, 0.8734366556363087, 0.7285462029266265, 0.5409459240265615, 0.4173622187664946, 0.2805474808504178], "prob": [[0.00036981482033773816, 21], [1.3119379134743461e-23, 44], [6.655506709473571e-35, 26], [5.0545800323460375e-49, 47], [5.061403246211035e-53, 19], [4.6537934281755595e-53, 6]], "generated": ["Dicopomorpha echmepterygis is a species of assassin bug.", "According to this source, Dicopomorpha echmepterygis are venomous with a potential lethal dose (LD) of 50.0 mg/kg.", "On the other hand, a Uranium pellet has a size of about 0.0015 grams.", "Dividing 50.0 mg/kg by 0.0015 gram means that to be fatal, a person would have to eat around 33,333 Uranium pellets.", "Thus, it is impossible to die from eating a single Uranium pellet.", "So the answer is no."], "conf_comments": ["(A)\nThe size of a", "(A)\nUranium is", "(A)\nThus, a", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8135749109369318, 0.4654548000467599, 0.2578006263774919, 0.15138972799126235, 0.12521110392840407], "conf": [0.9643271920576271, 0.8734366556363087, 0.7285462029266265, 0.6082576174320072, 0.468578540318259], "prob": [[0.00036981482033773816, 21], [1.3119379134743461e-23, 44], [6.655506709473571e-35, 26], [8.451538194521342e-52, 44], [4.143086081781732e-52, 6]], "generated": ["Dicopomorpha echmepterygis is a species of assassin bug.", "According to this source, Dicopomorpha echmepterygis are venomous with a potential lethal dose (LD) of 50.0 mg/kg.", "On the other hand, a Uranium pellet has a size of about 0.0015 grams.", "If you multiply the lethal dose by the weight of a Uranium pellet (0.0015 grams), you can obtain that a Uranium pellet would not be fatal.", "So the answer is no."], "conf_comments": ["(A)\nThe size of a", "(A)\nUranium is", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 75.00367164611816}
{"index": 218, "question": "Is menthol associated with Christmas?", "answer": "yes", "explanation": "Yes. Menthol is the chemical in mint products that give mint its characteristic cool and tangy taste. Peppermint is a popular candy flavor during Christmas season.", "generated": [{"finished": true, "length": 3, "score": [0.5773827164632326, 0.46784569156671907, 0.40666418117147596], "conf": [0.9116330986467079, 0.723813551666975, 0.5483273434906889], "prob": [[3.739616445467116e-08, 17], [5.5916233146323324e-09, 10], [5.50374359325534e-09, 6]], "generated": ["Menthol, peppermint, and wintergreen are associated with Christmas.", "Thus, menthol is associated with Christmas.", "So the answer is yes."], "conf_comments": ["(A)\nMenthol is", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6123548405167256, 0.3425903286962526, 0.2628996545003981, 0.21558826675572226], "conf": [0.9501613715809001, 0.7185666025085099, 0.48818824870193034, 0.32903247554681087], "prob": [[1.4272737547488378e-05, 12], [1.5604046592783591e-15, 26], [3.7353615324987885e-16, 10], [3.685065797596073e-16, 6]], "generated": ["Eucalyptus oil is associated with Christmas.", "Menthol is similar to eucalyptus oil because it is also an aromatic substance derived from mint.", "Thus, menthol is associated with Christmas.", "So the answer is yes."], "conf_comments": ["(A)\nMenthol is", "(A)\nThus, ment", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6123548405167256, 0.4213042720802721, 0.2201348542111579, 0.17858694211872786], "conf": [0.9501613715809001, 0.7699132296718803, 0.49426112443213177, 0.326425609929851], "prob": [[1.4272737547488378e-05, 12], [2.2542356982983706e-08, 12], [4.36226018999785e-12, 10], [4.272436589784193e-12, 6]], "generated": ["Eucalyptus oil is associated with Christmas.", "Menthol is in eucalyptus oil.", "Thus, menthol and Christmas are related.", "So the answer is yes."], "conf_comments": ["(A)\nMenthol is", "(A)\nThus, ment", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5285792005124803, 0.2952502962076997, 0.21089108158661177, 0.1754241601585333], "conf": [0.9319101647171618, 0.7503898885868323, 0.5510283693850456, 0.3820796144168313], "prob": [[6.056598866739574e-17, 31], [2.6881151615238095e-22, 13], [3.401206314986177e-24, 12], [3.358364504330096e-24, 6]], "generated": ["The most common associations with Christmas and holidays in general are things like candy canes, gingerbread houses, and pine trees.", "Menthol is not typically associated with these holiday items.", "Thus, menthol is not commonly associated with Christmas.", "So the answer is no."], "conf_comments": ["(A)\nMenthol is", "(A)\nThus, ment", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5285792005124803, 0.2952502962076997, 0.198085167305961, 0.16234537980182193], "conf": [0.9319101647171618, 0.7503898885868323, 0.5161093076876498, 0.3485759108194612], "prob": [[6.056598866739574e-17, 31], [2.6881151615238095e-22, 13], [2.5351373522782176e-24, 11], [2.4531445531805987e-24, 6]], "generated": ["The most common associations with Christmas and holidays in general are things like candy canes, gingerbread houses, and pine trees.", "Menthol is not typically associated with these holiday items.", "Therefore, menthol is not associated with Christmas.", "So the answer is no."], "conf_comments": ["(A)\nMenthol is", "(A)\nThus, ment", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 49.13479995727539}
{"index": 219, "question": "Did John Kerry run in the 2010 United Kingdom general election?", "answer": "no", "explanation": "No. John Kerry is an American citizen and politician. Only citizens of the UK, Ireland or a Commonwealth nation are eligible to run in the United Kingdom general elections.", "generated": [{"finished": true, "length": 4, "score": [0.746480374799646, 0.6336210366069688, 0.5465475008587896, 0.47767331701801113], "conf": [0.9589867300815589, 0.8594074161622158, 0.704034183004113, 0.5395988932608168], "prob": [[1.275277746915616e-06, 25], [2.5110233291248837e-08, 18], [3.3273062494982676e-09, 21], [3.2603616959966917e-09, 6]], "generated": ["John Kerry is the former US Secretary of State and ran in the 2004 United States presidential election.", "The 2010 United Kingdom general election happened in 2010.", "Thus, John Kerry did not run in the 2010 United Kingdom general election.", "So the answer is no."], "conf_comments": ["(A)\nThe 20", "(A)\nThus, John", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.74394607749207, 0.5978391413474177, 0.4358388497885382, 0.37708279416271195], "conf": [0.9684348385073, 0.7970317517012081, 0.5894807096043323, 0.44488643762406194], "prob": [[1.4733308810297864e-06, 24], [1.873053998941384e-08, 18], [4.839278802687253e-12, 25], [4.607152115669075e-12, 6]], "generated": ["The 2010 United Kingdom general election took place on May 6th , 2010.", "John Kerry was born on December 11th, 1943.", "Thus, it would be impossible for John Kerry to run in the 2010 United Kingdom general election.", "So the answer is no."], "conf_comments": ["(A)\nJohn Kerry was", "(A)\nThus, John", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.736157802883757, 0.5808290267464811, 0.3638672752187886, 0.3158958057230885, 0.2737680643112995], "conf": [0.9433346099535354, 0.802110802389536, 0.5918656328904957, 0.48598605116501925, 0.366148338446913], "prob": [[1.818569848627653e-07, 28], [4.0138122680147674e-11, 27], [1.093868350599674e-14, 13], [1.8104515827141325e-15, 21], [1.7768650375525383e-15, 6]], "generated": ["John Kerry ran in the 2004 United States Presidential Election, but he lost to President George W. Bush.", "He also ran in the 2012 United States Presidential Election, but he lost to President Barack Obama.", "There was no UK general election in 2010.", "Thus, John Kerry did not run in the 2010 United Kingdom general election.", "So the answer is no."], "conf_comments": ["(A)\nThe 20", "(A)\nThus, John", "(A)\nThus, John", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.7639272104799048, 0.5405218667930027, 0.45568191002780273], "conf": [0.9536826406741239, 0.7291458278695035, 0.5277102537122421], "prob": [[0.019660775875534754, 8], [6.305648819256719e-06, 19], [5.654903439263821e-06, 6]], "generated": ["John Kerry is an American politician.", "Thus, he could not run in the 2010 United Kingdom general election.", "So the answer is no."], "conf_comments": ["(A)\nThe 20", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.736157802883757, 0.5808290267464811, 0.366641177867522, 0.2874543084999802, 0.2484700119725876], "conf": [0.9433346099535354, 0.802110802389536, 0.6021163641345574, 0.4932739097338531, 0.36959800892356887], "prob": [[1.818569848627653e-07, 28], [4.0138122680147674e-11, 27], [5.658666666589601e-15, 14], [1.3580338196883723e-17, 21], [1.3351302061939626e-17, 6]], "generated": ["John Kerry ran in the 2004 United States Presidential Election, but he lost to President George W. Bush.", "He also ran in the 2012 United States Presidential Election, but he lost to President Barack Obama.", "However, he did not run in any elections in the United Kingdom.", "Thus, John Kerry did not run in the 2010 United Kingdom general elections.", "So the answer is no."], "conf_comments": ["(A)\nThe 20", "(A)\nThus, John", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 55.959465742111206}
{"index": 220, "question": "Would a Bengal cat be afraid of catching a fish?", "answer": "no", "explanation": "No. Fish live in water.  Many Bengal owners say that their Bengal naturally retrieves items. Bengal cats often enjoy playing in water.", "generated": [{"finished": true, "length": 5, "score": [0.5900939247355443, 0.28172909850256384, 0.1887082965333635, 0.13391014101926024, 0.11009283172363903], "conf": [0.9630788956409629, 0.7863213602587236, 0.615837902245695, 0.4212740615033718, 0.28614003201429616], "prob": [[5.269735721631048e-10, 21], [1.561580078729956e-20, 19], [2.100874170187884e-24, 16], [1.5615154847184237e-26, 16], [1.5163860699236648e-26, 6]], "generated": ["Bengal cats are a hybrid cat breed and they have a wildcat heritage.", "Wildcat’s ancestor include the lynx and the jungle cat.", "Both the lynx and the jungle cat can catch fish.", "Thus, a Bengal cat would not be afraid of catching fish.", "So the answer is no."], "conf_comments": ["(A)\nThus, Beng", "(A)\nThus, Beng", "(A)\nThus, Beng", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.632175263980202, 0.38700080243930873, 0.21677432684150572, 0.1684969385621987], "conf": [0.9656796184158128, 0.8201804890752671, 0.49478451101467846, 0.31514160468052715], "prob": [[0.00035610443841776926, 9], [1.517375537616418e-15, 32], [8.363201700743827e-20, 15], [6.093189887417596e-20, 6]], "generated": ["Bengal cats are domestic animals.", "According to the Florida Fish and Wildlife Conservation Commission (FWC), it is unlawful to catch a fish in the state of Florida.", "Thus, Bengal cats are not allowed to catch any fish.", "So the answer is no."], "conf_comments": ["(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.632175263980202, 0.38700080243930873, 0.18445139818006984, 0.14481553265809652], "conf": [0.9656796184158128, 0.8201804890752671, 0.5725545412434835, 0.38160313288447145], "prob": [[0.00035610443841776926, 9], [1.517375537616418e-15, 32], [8.26637653171227e-25, 19], [5.173007511070087e-25, 6]], "generated": ["Bengal cats are domestic animals.", "According to the Florida Fish and Wildlife Conservation Commission (FWC), it is unlawful to catch a fish in the state of Florida.", "Thus, a domestic Bengal cat likely wouldn’t be able to catch a fish.", "So the answer is no."], "conf_comments": ["(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.7043698882617548, 0.2820887503018101, 0.22280903654681702], "conf": [0.9587128208461241, 0.7071931826124545, 0.47958260920231305], "prob": [[5.082608023257646e-07, 22], [1.3775190792404118e-23, 25], [8.350449701459031e-24, 6]], "generated": ["Bengal cats are descended from Asian leopard cats, which are native to Asia.", "Those cats are known for hunting feral fish, so Bengal would not be afraid of catching a fish.", "So the answer is no."], "conf_comments": ["(A)\nAsian leop", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.632175263980202, 0.38700080243930873, 0.18445139818006984, 0.13143808900932633], "conf": [0.9656796184158128, 0.8201804890752671, 0.5725545412434835, 0.3773908746959162], "prob": [[0.00035610443841776926, 9], [1.517375537616418e-15, 32], [8.26637653171227e-25, 19], [1.727982787860701e-25, 6]], "generated": ["Bengal cats are domestic animals.", "According to the Florida Fish and Wildlife Conservation Commission (FWC), it is unlawful to catch a fish in the state of Florida.", "Thus, a domestic Bengal cat likely wouldn’t be able to catch a fish.", "So the answer is yes."], "conf_comments": ["(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 53.32951068878174}
{"index": 221, "question": "Are a dozen pickles good for easing hypertension?", "answer": "no", "explanation": "No. Hypertension is high blood pressure that can come from a number of factors including excess salt. Pickles are food that are high in sodium, or salt.", "generated": [{"finished": true, "length": 4, "score": [0.684742666100956, 0.36575278598514005, 0.30210208828570767, 0.2481552013351118], "conf": [0.9733392152168482, 0.7865276049957421, 0.5993758382486675, 0.40549779208028197], "prob": [[1.1742591251900701e-08, 25], [1.556265897203508e-14, 13], [2.3723634088216855e-15, 17], [2.3349710820903642e-15, 6]], "generated": ["According to Mayo Clinic, a low-sodium diet can help in lowering blood pressure.", "Pickles are high in sodium and are not recommended.", "Thus, a dozen pickles are not good for easing hypertension.", "So the answer is no."], "conf_comments": ["(A)\nPickles are", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.684742666100956, 0.36575278598514005, 0.29020496384378897, 0.23953966037323227], "conf": [0.9733392152168482, 0.7865276049957421, 0.6036470271127508, 0.41240527806534377], "prob": [[1.1742591251900701e-08, 25], [1.556265897203508e-14, 13], [5.36406151251391e-16, 17], [5.276160174134436e-16, 6]], "generated": ["According to Mayo Clinic, a low-sodium diet can help in lowering blood pressure.", "Pickles are high in sodium and are not recommended.", "Thus, a dozen pickles is not good for easing hypertension.", "So the answer is no."], "conf_comments": ["(A)\nPickles are", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7527858574675923, 0.37593630614483803, 0.27292482431264514, 0.22654966559339595], "conf": [0.9756419428312698, 0.8720448459137263, 0.6136794309225008, 0.42532554490770097], "prob": [[0.012954466257946258, 8], [2.8844756615760103e-09, 12], [5.0400397682141185e-11, 14], [4.866303544967224e-11, 6]], "generated": ["Hypertension is high blood pressure.", "Pickles and vinegar can reduce blood pressure.", "Thus, pickles are good for easing hypertension.", "So the answer is yes."], "conf_comments": ["(A)\nPickles are", "(A)\nThus, pick", "(A)\nThus, a", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7527858574675923, 0.37593630614483803, 0.2533226438682465, 0.2121166001550502], "conf": [0.9756419428312698, 0.8720448459137263, 0.629287059275025, 0.44446497469484136], "prob": [[0.012954466257946258, 8], [2.8844756615760103e-09, 12], [1.7418827825859393e-12, 16], [1.6668411817986276e-12, 6]], "generated": ["Hypertension is high blood pressure.", "Pickles and vinegar can reduce blood pressure.", "Thus, eating a dozen pickles could help ease hypertension.", "So the answer is yes."], "conf_comments": ["(A)\nPickles are", "(A)\nThus, pick", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6752374679478499, 0.31089679123408603, 0.20285233852554896, 0.17028536915187112], "conf": [0.9490716502665716, 0.8335262305673015, 0.6259166404564629, 0.44354689389199176], "prob": [[1.0973662863248939e-08, 25], [1.7784521804661552e-15, 11], [6.51010751820461e-20, 18], [6.295318270843543e-20, 6]], "generated": ["Pickles are often high in sodium, which is a known contributor to hypertension (high blood pressure).", "Saturated fats can also increase blood pressure.", "Thus, eating a dozen pickles would not be good for hypertension.", "So the answer is no."], "conf_comments": ["(A)\nThus, pick", "(A)\nPickles are", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 53.01352643966675}
{"index": 222, "question": "Does the book Revolutionary Road give a glimpse at life in a suburb?", "answer": "yes", "explanation": "Yes. The setting of the book is in Connecticut. The area of Connecticut where the book takes place is not in a major city. The book takes place in a suburb called Revolutionary Hill Estates.", "generated": [{"finished": true, "length": 3, "score": [0.6700461182136493, 0.5435415622513762, 0.47450751767026533], "conf": [0.9706829499882142, 0.8300887865195037, 0.6341485843451136], "prob": [[1.965313028546056e-09, 26], [1.758706499172662e-11, 18], [1.733478887343863e-11, 6]], "generated": ["Revolutionary Road is a book about the life of a suburban American family in the 1950’s.", "Thus, Revolutionary Road gives a glimpse at life in a suburb.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7746079969834347, 0.462883221632227, 0.3442497650417434, 0.3005943862247594], "conf": [0.9719511656902623, 0.8973750338469254, 0.7439200910617834, 0.5686103233423762], "prob": [[9.385732726265624e-06, 24], [1.3202194799993336e-20, 36], [2.6911673012136126e-24, 21], [2.6515299716491687e-24, 6]], "generated": ["The book Revolutionary Road was written in 1961 and set in the 1950s.", "According to an article in the New York Times, in the 1950s, suburbs in the United States had a reputation for conformity and safety.", "Thus, the book Revolutionary Road gives us a glimpse at life in a suburb.", "So the answer is yes."], "conf_comments": ["(A)\nIn comparison, the", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7068587310078532, 0.4548705750421049, 0.32977414058015664, 0.2863179880488762], "conf": [0.973915408676209, 0.7920949267194424, 0.6220412637377756, 0.47195088370250526], "prob": [[1.1815395911396193e-05, 17], [2.822693396191537e-13, 26], [6.145965521969286e-17, 21], [5.911652602230787e-17, 6]], "generated": ["The book Revolutionary Road is set in a fictional suburb of Long Island.", "Since the book talks about people living in a suburb, it gives a glimpse at life in a suburb.", "Thus, it is true that the book gives a glimpse at life in a suburb.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.7068587310078532, 0.4548705750421049, 0.3886011899575638], "conf": [0.973915408676209, 0.7920949267194424, 0.5989940452173176], "prob": [[1.1815395911396193e-05, 17], [2.822693396191537e-13, 26], [2.2813265535286757e-13, 6]], "generated": ["The book Revolutionary Road is set in a fictional suburb of Long Island.", "Since the book talks about people living in a suburb, it gives a glimpse at life in a suburb.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7746079969834347, 0.462883221632227, 0.30748237699039743], "conf": [0.9719511656902623, 0.8973750338469254, 0.7026619134788729], "prob": [[9.385732726265624e-06, 24], [1.3202194799993336e-20, 36], [7.830236715472292e-27, 25]], "generated": ["The book Revolutionary Road was written in 1961 and set in the 1950s.", "According to an article in the New York Times, in the 1950s, suburbs in the United States had a reputation for conformity and safety.", "Thus, the book Revolutionary Road does give a glimpse at life in a suburb, which is yes."], "conf_comments": ["(A)\nIn comparison, the", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 44.280569553375244}
{"index": 223, "question": "Is Hanuman associated with a Norse god?", "answer": "no", "explanation": "No. Hanuman is a companion of the god Rama. Rama is an avatar of the god Vishnu in Hindu mythology.", "generated": [{"finished": true, "length": 3, "score": [0.6953095024782946, 0.5692465066715423, 0.48491955060413344], "conf": [0.9294155218238849, 0.7277428252629091, 0.5298946376575291], "prob": [[5.691424537069268e-07, 22], [6.454917372144e-08, 14], [6.324889942300405e-08, 6]], "generated": ["Hanuman is a Hindu deity while Norse gods are found in Norse Mythology.", "Thus, Hanuman is not associated with a Norse god.", "So the answer is no."], "conf_comments": ["(A)\nThus, Han", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.856757863873791, 0.5123336243398577, 0.4053026867531031, 0.34117876100791766], "conf": [0.9764101267672224, 0.8146915059023584, 0.6230368873489683, 0.4429336733552616], "prob": [[0.0766929270265361, 9], [6.157219915653782e-10, 22], [3.719136433154291e-11, 14], [3.646888982834325e-11, 6]], "generated": ["Hanuman is a Hindu god.", "According to Norse mythology, the only deities were Odin, Thor, and Freya.", "Thus, Hanuman is not associated with any Norse god.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, Thor", "(A)\nThus, Han", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.856757863873791, 0.6055346002737757, 0.39262435282008956, 0.3198678941293934], "conf": [0.9764101267672224, 0.8668341310642592, 0.5414684654306634, 0.3612106647972111], "prob": [[0.0766929270265361, 9], [2.4454980727816994e-05, 14], [1.4219331636007235e-07, 13], [1.3793542613953556e-07, 6]], "generated": ["Hanuman is a Hindu god.", "Norse gods are from the Norse pantheon.", "Thus, Hanuman is associated with a Norse god.", "So the answer is yes."], "conf_comments": ["(A)\nIn comparison, Thor", "(A)\nThus, Han", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6936080444686764, 0.45926354999474495, 0.37738046447759627], "conf": [0.9048404032054718, 0.7351756070599195, 0.5105224966253551], "prob": [[6.13011903280809e-06, 19], [6.9232755281084785e-09, 11], [5.8502991852142745e-09, 6]], "generated": ["Hanuman is from Hinduism while Norse god is from Norse mythology.", "Thus, they are associated with different religions.", "So the answer is no."], "conf_comments": ["(A)\nThus, Han", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6759453618612589, 0.4592548298403697, 0.3124983973194485, 0.25916545081128645], "conf": [0.9756467342573663, 0.8913544917803021, 0.6431597428648909, 0.44479434856609457], "prob": [[2.7151593892391745e-09, 26], [4.900247470202465e-14, 16], [4.0488684723980394e-17, 16], [3.917806268891515e-17, 6]], "generated": ["Hanuman is a Hindu monkey god who is featured in the Ramayana, an ancient Hindu epic.", "Norse gods and goddesses are part of Norse mythology.", "Thus, there is no relationship between Hanuman and a Norse god.", "So the answer is no."], "conf_comments": ["(A)\nThe Norse gods", "(A)\nThus, Han", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 47.261014223098755}
{"index": 224, "question": "Do you need a large room if you want to get into 3D printing?", "answer": "no", "explanation": "No. Home 3D printers are sized to be able to sit on a desk or table. The accessories and materials needed for 3D Printers can be stored easily and efficiently in a box or tote.", "generated": [{"finished": true, "length": 3, "score": [0.4863554953749908, 0.29768859900523875, 0.2440306842416685], "conf": [0.9233939139599615, 0.6893693829368308, 0.4676006822356884], "prob": [[1.3421767269384947e-09, 15], [2.7087916427670895e-12, 9], [2.5610824275570782e-12, 6]], "generated": ["3D printing requires a large amount of space in order to print models.", "Thus, a large room is needed.", "So the answer is yes."], "conf_comments": ["(A)\nThus, you", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5570040240101362, 0.27847266112956603, 0.1827946084508322, 0.14129584423459518], "conf": [0.9419887904793276, 0.7108346889332358, 0.49814523196442184, 0.299067051108868], "prob": [[2.080064254708525e-09, 18], [4.3660240993992736e-17, 16], [6.886887365734992e-21, 18], [6.6917470444138784e-21, 6]], "generated": ["Some 3D printers are small and fit on the top of a desk.", "If you want more space, you can get a large 3D printer.", "Thus, you need not get a large room to get into 3D printing.", "So the answer is no."], "conf_comments": ["(A)\nThus, you", "(A)\nThus, you", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5570040240101362, 0.2102335976031531, 0.14348573894438724, 0.11116293494154732], "conf": [0.9419887904793276, 0.7370818123489365, 0.4953956705153548, 0.2994390925589125], "prob": [[2.080064254708525e-09, 18], [9.143730421206655e-20, 14], [8.627423480129075e-23, 19], [8.271010261229558e-23, 6]], "generated": ["Some 3D printers are small and fit on the top of a desk.", "Some, on the other hand, need to be free-standing.", "Thus, you don't need a large room to get into 3D printing.", "So the answer is no."], "conf_comments": ["(A)\nThus, you", "(A)\nThus, you", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.4863554953749908, 0.24192285328560842, 0.18985546661605937], "conf": [0.9233939139599615, 0.6156344414566502, 0.3911023717402718], "prob": [[1.3421767269384947e-09, 15], [1.737694764137756e-16, 16], [1.4425230816771897e-16, 6]], "generated": ["3D printing requires a large amount of space in order to print models.", "Thus having a large room could help you get into 3D printing.", "So the answer is yes."], "conf_comments": ["(A)\nThus, you", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5570040240101362, 0.27847266112956603, 0.1647837591284228], "conf": [0.9419887904793276, 0.7108346889332358, 0.44383018709827293], "prob": [[2.080064254708525e-09, 18], [4.3660240993992736e-17, 16], [1.3289313230969562e-20, 14]], "generated": ["Some 3D printers are small and fit on the top of a desk.", "If you want more space, you can get a large 3D printer.", "So the answer is no, you do not need a large room."], "conf_comments": ["(A)\nThus, you", "(A)\nThus, you", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 44.6360650062561}
{"index": 225, "question": "Would Robert Stack have been interested in Tower of London during 1400s for his 14 season show?", "answer": "yes", "explanation": "Yes. Robert Stack was an actor best known for Unsolved Mysteries which lasted for 14 seasons before being rebooted by Netflix. The Tower of London is a historic building in London. Unsolved Mysteries explored unexplained phenomenon and mysterious events. The heirs of Edward IV mysteriously vanished from the Tower of London in the 1400s and were presumed muredered.", "generated": [{"finished": true, "length": 4, "score": [0.8707352428680916, 0.5712072336006506, 0.3643922523436464, 0.29959574789750326], "conf": [0.9803984542737435, 0.8542402195173453, 0.6660401005869201, 0.46535143708513477], "prob": [[0.059165141220775895, 11], [3.0408462727003106e-06, 14], [6.848644911899428e-12, 20], [5.617184490147812e-12, 6]], "generated": ["Robert Stack died in 2003.", "The Tower of London was built around 1078 CE.", "Thus, Robert Stack couldn’t have visited it during the 1400s.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, the", "(A)\nThus, Robert", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7889237561096474, 0.5614490627152967, 0.3560706631790674, 0.2748722030036519], "conf": [0.9674500900290763, 0.7559519797323441, 0.46604893217742477, 0.28010238812283006], "prob": [[0.00323399598075394, 13], [1.3184684863367813e-06, 18], [3.593357565033565e-12, 30], [3.4144772673024917e-12, 6]], "generated": ["The Tower of London was built in 1078.", "The first season of Unsolved Mysteries premiered in 1987.", "Thus, Robert Stack would not have been interested in the Tower of London during the 1400s because there was no show yet.", "So the answer is no."], "conf_comments": ["(A)\nRobert Stack was", "(A)\nThus, Robert", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7889237561096474, 0.6164404498363292, 0.3192495475992199, 0.2509297694504903], "conf": [0.9674500900290763, 0.7604083136525496, 0.5079597255118885, 0.3211474546554204], "prob": [[0.00323399598075394, 13], [0.0001560509114642493, 12], [5.017038294064934e-16, 29], [4.367761888116049e-16, 6]], "generated": ["The Tower of London was built in 1078.", "Robert Stack was born in 1919.", "Thus, Robert Stack would not have been interested in the Tower of London as it did not exist during 1400’s!", "So the answer is no."], "conf_comments": ["(A)\nRobert Stack was", "(A)\nThus, Robert", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6497950325940222, 0.4146887095616952, 0.30569144665684145, 0.25021197443361226], "conf": [0.9679144288983726, 0.6888548902978323, 0.5213029863718921, 0.35295786609620367], "prob": [[2.0715982256148713e-05, 13], [1.8043766454532195e-11, 25], [8.871559066234929e-15, 23], [8.327251240416833e-15, 6]], "generated": ["Robert Stack was born in November 1919.", "The Tower of London was built in 1078, which was over 300 years before his birth.", "Thus, he would not have been interested in the Tower of London in the 1400s.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, the", "(A)\nThus, Robert", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6497950325940222, 0.4146887095616952, 0.30145104979449516, 0.24684524081981027], "conf": [0.9679144288983726, 0.6888548902978323, 0.47016709786100597, 0.31671792656377784], "prob": [[2.0715982256148713e-05, 13], [1.8043766454532195e-11, 25], [3.880270328773589e-15, 33], [3.774287749633067e-15, 6]], "generated": ["Robert Stack was born in November 1919.", "The Tower of London was built in 1078, which was over 300 years before his birth.", "Thus, it was impossible for Robert Stack to be interested in the Tower of London during the 1400s for his 14 season show.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, the", "(A)\nThus, Robert", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 53.59548735618591}
{"index": 226, "question": "Does a sea otter eat spiders?", "answer": "no", "explanation": "No. Sea otters prey mostly on marine invertebrates and other aquatic creatures. Spiders are not aquatic creatures and they reside on land.", "generated": [{"finished": true, "length": 4, "score": [0.7597950472105777, 0.4939188015774402, 0.38967093494662375, 0.3269799359024822], "conf": [0.9690851078340204, 0.8355557775659377, 0.5860403491187044, 0.41471867629939035], "prob": [[0.005627428266580909, 10], [7.80077961343935e-05, 6], [1.8609387093824494e-05, 12], [1.8057295618854676e-05, 6]], "generated": ["Sea otters are marine mammals.", "Spiders live on land.", "Thus, sea otters do not eat spiders.", "So the answer is no."], "conf_comments": ["(A)\nSpiders are ara", "(A)\nThus, sea", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.601521729342939, 0.48744541677280534, 0.4282353099542217], "conf": [0.8961919930256275, 0.7258799925957052, 0.56206599039188], "prob": [[8.716384334775771e-10, 23], [7.030128642019272e-11, 12], [6.89455396571949e-11, 6]], "generated": ["According to this website, sea otters do not eat spiders, but they eat invertebrates.", "Thus, sea otters do not eat spiders.", "So the answer is no."], "conf_comments": ["(A)\nInvertebrates", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6140224476729632, 0.4275975540941308, 0.34563587193831363, 0.3001963457989568], "conf": [0.9530885589357054, 0.822001214922555, 0.6353797972083357, 0.48032956381721653], "prob": [[2.970442017401483e-19, 46], [5.27910317617469e-21, 7], [7.02481960105288e-22, 12], [6.934877296976353e-22, 6]], "generated": ["Sea otters usually eat seafood like clams, snails, worms, crabs, sea urchins, mussels, scallops, abalone, octopus, and sea stars.", "They do not eat spiders.", "Thus, sea otters do not eat spiders.", "So the answer is no."], "conf_comments": ["(A)\nSpiders are not", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6459094453642154, 0.42704629494008695, 0.3263963276485845, 0.2787956978646386], "conf": [0.9593079761369881, 0.695264450890836, 0.4959625934122652, 0.3630145199918089], "prob": [[1.3468495750552053e-07, 19], [5.174644727223323e-10, 11], [4.707399908749979e-11, 12], [4.617642409245246e-11, 6]], "generated": ["Sea otters eat clams, urchins, mussels, and some fish.", "However, sea otters do not eat spiders.", "Thus, sea otters do not eat spiders.", "So the answer is no."], "conf_comments": ["(A)\nSpiders are not", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7597950472105777, 0.4939188015774402, 0.3038632017169374, 0.24857181145404994], "conf": [0.9690851078340204, 0.8355557775659377, 0.5776533994516039, 0.3901924850251096], "prob": [[0.005627428266580909, 10], [7.80077961343935e-05, 6], [2.497787754520918e-10, 21], [2.361409686320924e-10, 6]], "generated": ["Sea otters are marine mammals.", "Spiders live on land.", "Thus, sea otters do not eat spiders because they do not live where spiders live.", "So the answer is no."], "conf_comments": ["(A)\nSpiders are ara", "(A)\nThus, sea", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 50.15877962112427}
{"index": 227, "question": "Did The Who have to cancel tours due to World War II?", "answer": "no", "explanation": "No. The Who was formed in 1964. World War II ended in 1945.", "generated": [{"finished": true, "length": 4, "score": [0.7686383802781546, 0.6444826813854801, 0.4649927523253918, 0.3832912948782758], "conf": [0.9769580905035274, 0.8654229311030002, 0.6526271254770405, 0.4464973909790915], "prob": [[0.00031995667760993424, 16], [2.5175386180769734e-05, 11], [2.2009280681833122e-08, 19], [2.1119142728324014e-08, 6]], "generated": ["World War II occurred from 1939-1945.", "The Who was formed in 1964.", "Thus, The Who would not have had to cancel tours due to World War II.", "So the answer is no."], "conf_comments": ["(A)\nThe Who was formed", "(A)\nThus, The", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8569687275981711, 0.6361564477808528, 0.46748503967730337, 0.3744828457569858], "conf": [0.9806182311148547, 0.8676987550527848, 0.634227883956186, 0.4096854115066574], "prob": [[0.0555008773307812, 10], [1.7696658736788307e-05, 17], [7.61119870411196e-08, 18], [7.314633809031254e-08, 6]], "generated": ["The Who formed in 1964.", "World War II lasted between 1939-1945.", "Thus, the Who did not have to cancel tours due to World War II.", "So the answer is no."], "conf_comments": ["(A)\nWorld War II ended", "(A)\nThus, The", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7686383802781546, 0.6065380613940058, 0.3895311195161246, 0.3236899622563625], "conf": [0.9769580905035274, 0.8666848264557128, 0.6527353520162381, 0.4535817957575722], "prob": [[0.00031995667760993424, 16], [4.57648337086556e-06, 12], [2.6938726902787296e-11, 20], [2.593648114775238e-11, 6]], "generated": ["World War II occurred from 1939-1945.", "The Who did not form until 1964.", "Thus, The Who would not have canceled their tours due to World War II.", "So the answer is no."], "conf_comments": ["(A)\nThe Who was formed", "(A)\nThus, The", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.759379855313586, 0.6780105830393213, 0.4368970196123585, 0.28500640495474094, 0.22700484179911867], "conf": [0.9806351227150125, 0.8870191162793191, 0.7057230706609212, 0.5093585168084366, 0.3257952740724922], "prob": [[0.002907486464208293, 11], [0.00072433526112053, 11], [2.957646207683642e-07, 12], [3.719607727697339e-11, 17], [3.541081298780325e-11, 6]], "generated": ["The Who were formed in 1964.", "World War II ended in 1945.", "Thus, The Who was formed before World War II.", "This means they did not have to cancel any tours due to World War II.", "So the answer is no."], "conf_comments": ["(A)\nWorld War II ended", "(A)\nThus, The", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7686383802781546, 0.6444826813854801, 0.3950370037737686, 0.27684148124293656, 0.22546600926925514], "conf": [0.9769580905035274, 0.8654229311030002, 0.7219032521109198, 0.5453176428199221, 0.36270797382061976], "prob": [[0.00031995667760993424, 16], [2.5175386180769734e-05, 11], [8.649692875416554e-09, 10], [3.726427377212061e-12, 18], [3.664766414000645e-12, 6]], "generated": ["World War II occurred from 1939-1945.", "The Who was formed in 1964.", "World War II ended before The Who was formed.", "Thus, The Who had no tours canceled due to World War II.", "So the answer is no."], "conf_comments": ["(A)\nThe Who was formed", "(A)\nThus, The", "(A)\nThus, The", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.759379855313586, 0.6780105830393213, 0.4368970196123585, 0.28500640495474094, 0.18533730247400235], "conf": [0.9806351227150125, 0.8870191162793191, 0.7057230706609212, 0.5093585168084366, 0.3570931867589382], "prob": [[0.002907486464208293, 11], [0.00072433526112053, 11], [2.957646207683642e-07, 12], [3.719607727697339e-11, 17], [6.518755464256e-13, 8]], "generated": ["The Who were formed in 1964.", "World War II ended in 1945.", "Thus, The Who was formed before World War II.", "This means they did not have to cancel any tours due to World War II.", "Thus, the answer is no."], "conf_comments": ["(A)\nWorld War II ended", "(A)\nThus, The", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 57.72341561317444}
{"index": 228, "question": "Is nickel dominant material in US 2020 nickels?", "answer": "no", "explanation": "No. Nickels have been made of various materials including silver in the 1940s. Nickels in 2020 are made from a mix of copper and nickel. 2020 nickels are 25% nickel and 75% copper.", "generated": [{"finished": true, "length": 4, "score": [0.7539744564740509, 0.3885082476017707, 0.2867824050256368, 0.23790739998192426], "conf": [0.9478265408411846, 0.6373910887426752, 0.45660784780726327, 0.31603433999220404], "prob": [[6.0492788384250234e-05, 19], [4.3461781791841254e-21, 40], [2.4003971187247037e-23, 19], [2.319546340102265e-23, 6]], "generated": ["A nickel is about 75% nickel and 25% copper.", "US 2020 nickels are 19.13% zinc, 7.55% manganese, and 83.32% copper.", "Thus, US 2020 nickels are not dominated by nickel.", "So the answer is no."], "conf_comments": ["(A)\nThe US 2", "(A)\nThus, nick", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7539744564740509, 0.3885082476017707, 0.28654643725021334, 0.23268199158448602], "conf": [0.9478265408411846, 0.6373910887426752, 0.4507191017953713, 0.2986197526200953], "prob": [[6.0492788384250234e-05, 19], [4.3461781791841254e-21, 40], [1.7619288404834377e-23, 21], [1.7120879598120616e-23, 6]], "generated": ["A nickel is about 75% nickel and 25% copper.", "US 2020 nickels are 19.13% zinc, 7.55% manganese, and 83.32% copper.", "Thus, nickel is not the dominant material in the US 2020 nickels.", "So the answer is no."], "conf_comments": ["(A)\nThe US 2", "(A)\nThus, nick", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6986798772113675, 0.39917788872073184, 0.3233184545102892], "conf": [0.9227118735035388, 0.7043163422533899, 0.5227322156740386], "prob": [[3.968919453293044e-10, 34], [2.1229723703531327e-16, 17], [1.0126131092424247e-16, 6]], "generated": ["According to the website Metal Supermarkets, US 2020 nickels are about 75% copper and 25% nickel.", "Since there is more copper than nickel, nickel could not be dominant.", "So the answer is no."], "conf_comments": ["(A)\nThus, nick", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6471824496722081, 0.38328701312938707, 0.29940769287804386], "conf": [0.9513908946761389, 0.5478473076854571, 0.337217676284298], "prob": [[2.0456633877586577e-11, 30], [1.438057389981251e-13, 10], [1.3650156322517645e-13, 6]], "generated": ["The nickels in the US since 2020 are made from about 75% copper and 25% nickel.", "Thus, copper is the dominant material.", "So the answer is no."], "conf_comments": ["(A)\nThus, nick", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7539744564740509, 0.3885082476017707, 0.2867824050256368, 0.17248662752038957], "conf": [0.9478265408411846, 0.6373910887426752, 0.45660784780726327, 0.3211054903897443], "prob": [[6.0492788384250234e-05, 19], [4.3461781791841254e-21, 40], [2.4003971187247037e-23, 19], [1.1767794396154662e-25, 8]], "generated": ["A nickel is about 75% nickel and 25% copper.", "US 2020 nickels are 19.13% zinc, 7.55% manganese, and 83.32% copper.", "Thus, US 2020 nickels are not dominated by nickel.", "Thus, the answer is no."], "conf_comments": ["(A)\nThe US 2", "(A)\nThus, nick", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 4, "score": [0.7539744564740509, 0.4407291809677136, 0.2948491726992873, 0.1359969710098056], "conf": [0.9478265408411846, 0.8413172223420516, 0.6628101340225308, 0.48913526015012004], "prob": [[6.0492788384250234e-05, 19], [3.2695310650881376e-18, 32], [5.119965465527112e-29, 44], [4.8630821401655695e-36, 13]], "generated": ["A nickel is about 75% nickel and 25% copper.", "The US Mint changed the composition of its pennies, nickels, dimes and quarters in 1982 to save copper.", "Specifically, it became 75% zinc and 25% copper for pennies, and 75% copper and 25% nickel for the other coins.", "In comparison, nickel is not dominant in US coins.", "So the answer is no."], "conf_comments": ["(A)\nThe US 2", "(A)\nThus, the", "(A)\nThus, nick", "(A)\nThus, nick"], "info": [[[9.498271370986492e-08, 13], [0.4409345503922647, 6]], 0.7379719093635538, "(A)\nThus, nick", 4]}, {"finished": true, "length": 4, "score": [0.7539744564740509, 0.4407291809677136, 0.2948491726992873, 0.13018613710142207], "conf": [0.9478265408411846, 0.8413172223420516, 0.6628101340225308, 0.4891307120328502], "prob": [[6.0492788384250234e-05, 19], [3.2695310650881376e-18, 32], [5.119965465527112e-29, 44], [4.769879819788033e-44, 26]], "generated": ["A nickel is about 75% nickel and 25% copper.", "The US Mint changed the composition of its pennies, nickels, dimes and quarters in 1982 to save copper.", "Specifically, it became 75% zinc and 25% copper for pennies, and 75% copper and 25% nickel for the other coins.", "Thus, the US 2020 nickel is not dominant with its namesake material and the question is no."], "conf_comments": ["(A)\nThe US 2", "(A)\nThus, the", "(A)\nThus, nick", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 49.775678873062134}
{"index": 229, "question": "Is it more expensive to run for President of India than to buy a new iPhone 11?", "answer": "no", "explanation": "No. Candidates for the presidency of India must pay a deposit of Rs 15,000. A brand new iPhone 11 costs Rs 67,300.", "generated": [{"finished": true, "length": 5, "score": [0.5773194822350928, 0.3002497932497434, 0.23822294061230356, 0.18466689738623177, 0.15654729878370843], "conf": [0.8360800805679298, 0.7716082756116737, 0.6796397798375896, 0.5354108773108859, 0.39819117188512104], "prob": [[0.00025424999495765133, 9], [4.826910055229353e-24, 37], [5.557816525010859e-28, 27], [1.4380042095421057e-30, 22], [1.1706089545767106e-30, 6]], "generated": ["According to this article, it costs about", "\\$64,786,691.20 \\$64,786,691.20 to run for President of India. ", "In comparison, it costs \\$699.00 \\$699.00 to buy an iPhone 11.", "Thus, it is more expensive to run for President of India than buy a new iPhone 11.", "So the answer is yes."], "conf_comments": ["(A)\n$1.5", "(A)\nAccording to this", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.6727015674869763, 0.2988933707180544, 0.19716690590794175, 0.12529405338267924, 0.10449052030208314, 0.08616682312842795], "conf": [0.9616979826114672, 0.8848201387710151, 0.7821832323311336, 0.6452293813174881, 0.48609146124635144, 0.3463703781219083], "prob": [[2.7183474305292264e-06, 17], [1.1354320967014962e-23, 26], [3.269163924432138e-29, 18], [1.7410843922834157e-34, 17], [2.7699185784702123e-35, 23], [2.092612279842731e-35, 6]], "generated": ["The most recent election for President of India was in July 2017.", "According to CNN News 18, the presidential campaign costs are around 14.5 million rupees.", "In comparison, the price of a new iPhone 11 is $749.", "$749 is less than $14.5 million rupees.", "Thus, it is more expensive to run for President of India than to buy a new iPhone 11.", "So the answer is yes."], "conf_comments": ["(A)\nThe iPhone 1", "(A)\nThe iPhone 1", "(A)\nThus, it", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5773194822350928, 0.3002497932497434, 0.20708925350936375, 0.14000305899534696, 0.11702070735644705], "conf": [0.8360800805679298, 0.7716082756116737, 0.6767515676390177, 0.5128865803842909, 0.37489604307045854], "prob": [[0.00025424999495765133, 9], [4.826910055229353e-24, 37], [7.968573140086921e-29, 18], [1.5521900622130843e-34, 26], [1.1833415220671665e-34, 6]], "generated": ["According to this article, it costs about", "\\$64,786,691.20 \\$64,786,691.20 to run for President of India. ", "In comparison, the cost of an iPhone 11 is about \\$999.", "Thus, it is most likely much more expensive to run for President of India than to buy a new iPhone 11.", "So the answer is yes."], "conf_comments": ["(A)\n$1.5", "(A)\nAccording to this", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5773194822350928, 0.3002497932497434, 0.23822294061230356, 0.18365410176451785, 0.0925322517894265], "conf": [0.8360800805679298, 0.7716082756116737, 0.6796397798375896, 0.5312171409157562, 0.31764841075656175], "prob": [[0.00025424999495765133, 9], [4.826910055229353e-24, 37], [5.557816525010859e-28, 27], [1.765067318168683e-30, 21], [4.386660315328158e-33, 7]], "generated": ["According to this article, it costs about", "\\$64,786,691.20 \\$64,786,691.20 to run for President of India. ", "In comparison, it costs \\$699.00 \\$699.00 to buy an iPhone 11.", "Thus, running for President of India is much more expensive than buying an iPhone 11.", "So the answer is no."], "conf_comments": ["(A)\n$1.5", "(A)\nAccording to this", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5773194822350928, 0.3002497932497434, 0.20708925350936375, 0.10971983404594975, 0.09003618368349758], "conf": [0.8360800805679298, 0.7716082756116737, 0.6767515676390177, 0.45139246660541243, 0.30917894515671196], "prob": [[0.00025424999495765133, 9], [4.826910055229353e-24, 37], [7.968573140086921e-29, 18], [5.4824645629150043e-45, 43], [4.9502307588181653e-45, 6]], "generated": ["According to this article, it costs about", "\\$64,786,691.20 \\$64,786,691.20 to run for President of India. ", "In comparison, the cost of an iPhone 11 is about \\$999.", "Thus, to run for President of India is definitely \\$63,787,691.20 \\$999.00 more expensive than to buy an iPhone 11.", "So the answer is yes."], "conf_comments": ["(A)\n$1.5", "(A)\nAccording to this", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 57.90984749794006}
{"index": 230, "question": "Does New Year's Day always occur on a Wednesday?", "answer": "no", "explanation": "No. New Year's Day occurs on January 1st each year. The day of the week any given date falls on rotates by one each year. If Leap Year wasn't breaking up the cycle, New Year's Day would be on a Wednesday every seventh year.", "generated": [{"finished": true, "length": 4, "score": [0.6312330127158579, 0.3570946286917375, 0.2805251075716286, 0.2216875277565637], "conf": [0.959477922046722, 0.7850786219921391, 0.525650541308641, 0.3291580338117337], "prob": [[4.537555035610732e-06, 14], [4.4582722149532246e-17, 27], [1.0276336969537366e-17, 18], [1.0111801771705111e-17, 6]], "generated": ["New Year's Day occurs in the first day of a year.", "In 2009 through 2029, New Year's Day does not occur on a Wednesday.", "Thus, New Year's Day does not always occur on a Wednesday.", "So the answer is no."], "conf_comments": ["(A)\nIn 20", "(A)\nThus, New", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6312330127158579, 0.3570946286917375, 0.2460614468555105, 0.1987988456398046], "conf": [0.959477922046722, 0.7850786219921391, 0.529224541379307, 0.34643533268063303], "prob": [[4.537555035610732e-06, 14], [4.4582722149532246e-17, 27], [4.682572609674619e-19, 13], [4.602933672858439e-19, 6]], "generated": ["New Year's Day occurs in the first day of a year.", "In 2009 through 2029, New Year's Day does not occur on a Wednesday.", "Thus, it is not always on a Wednesday.", "So the answer is no."], "conf_comments": ["(A)\nIn 20", "(A)\nThus, New", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.5536913595930767, 0.2699234903889026, 0.1683337099857793, 0.09051603355732298, 0.06305846149223744, 0.049228602671969254], "conf": [0.9093190188648855, 0.7228535530486444, 0.5783294475647142, 0.4166366694200609, 0.29058718836152514, 0.18631480776449186], "prob": [[3.3830706519718704e-18, 37], [7.856307927244535e-32, 26], [3.713024414493904e-37, 17], [7.327359255217194e-48, 27], [1.5407495795867984e-50, 17], [1.136570177711583e-50, 6]], "generated": ["New Year usually occurs at the beginning of the year, so New Year's Day cannot happen during March, April, May, June, July, August, September, or October.", "New Year's Day can happen at the beginning of November or December, depending if there are 29 days in February.", "November has 30 days and December has up to 31 days.", "Thus, if a calendar year has 29 days in February, then New Year's Day can occur on a Friday.", "Otherwise, New Year's Day can occur on any day of the week.", "So the answer is no."], "conf_comments": ["(A)\nNew Year's", "(A)\nThus, New", "(A)\nThus, New", "(A)\nIf a calendar year", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6585583232446791, 0.2673999492094023, 0.16248035281352738, 0.11892020599027407], "conf": [0.9506308836243587, 0.5913440242480806, 0.29157764205845993, 0.15662500876285781], "prob": [[3.709329098514335e-05, 13], [5.353478325044537e-19, 24], [5.229702583343801e-21, 16], [5.143908692787012e-21, 6]], "generated": ["New Year’s Day is always on a January 1.", "January 1 only ever falls on a Wednesday 0 times out of every 10 years.", "Thus, New Year’s Day is never on a Wednesday.", "So the answer is no."], "conf_comments": ["(A)\nIn 20", "(A)\nThus, New", "(A)\nSo the answer is", "(A), but the previous step is"], "info": null}, {"finished": true, "length": 2, "score": [0.6312330127158579, 0.312356775620648], "conf": [0.959477922046722, 0.8504766753739803], "prob": [[4.537555035610732e-06, 14], [5.267124082031521e-34, 50]], "generated": ["New Year's Day occurs in the first day of a year.", "From 1972 to 2000, most years happened on the following days of the week: New Year's Day occurred on Wednesday 8 times, Saturday 4 times, and Thursday 4 times"], "conf_comments": ["(A)\nIn 20", "(A)\nThus, New"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 53.39059543609619}
{"index": 231, "question": "Was Donald Trump the target of Islamophobia?", "answer": "no", "explanation": "No. Islamophobia targets Muslims. Donald Trump is a Presbyterian, a denomination of Christianity.", "generated": [{"finished": true, "length": 3, "score": [0.54008068761412, 0.43833410747295354, 0.38035916342506826], "conf": [0.957535121856871, 0.8074988053807944, 0.6097431598485977], "prob": [[8.544573941218751e-19, 35], [3.4424621427296984e-20, 13], [3.3845606631986765e-20, 6]], "generated": ["Donald Trump was accused of Islamophobia when he signed an executive order in 2017 that prohibited entry of travellers from several muslim majority countries.", "Thus, he was the target of Islamophobia.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Donald", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6919880533494553, 0.3455923679002255, 0.24007440590736995], "conf": [0.966166023586858, 0.6562213621056538, 0.430403329143539], "prob": [[1.613478635670501e-06, 19], [6.480604779131454e-14, 17], [1.0281219422863935e-14, 6]], "generated": ["Islamophobia is the irrational fear of Arabs and Muslims.", "Donald Trump, who is American, cannot be a target of Islamophobia.", "So the answer is no."], "conf_comments": ["(A)\nDonald Trump is a", "(A)\nThus, Donald", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6919880533494553, 0.3455923679002255, 0.18516151710890238, 0.1446629894466475], "conf": [0.966166023586858, 0.6562213621056538, 0.4066671616509399, 0.2507873778706111], "prob": [[1.613478635670501e-06, 19], [6.480604779131454e-14, 17], [6.286111138229532e-19, 15], [5.911029063194001e-19, 6]], "generated": ["Islamophobia is the irrational fear of Arabs and Muslims.", "Donald Trump, who is American, cannot be a target of Islamophobia.", "Thus, Islamophobia is not a threat to Donald Trump.", "So the answer is no."], "conf_comments": ["(A)\nDonald Trump is a", "(A)\nThus, Donald", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 9, "score": [0.6919880533494553, 0.36761916091647057, 0.1575641632324496, 0.08370246909925194, 0.063431924057686, 0.03508337716130095, 0.01705857421669189, 0.012344237805995558, 0.010034349132797292], "conf": [0.966166023586858, 0.8247675687573642, 0.6815766734457936, 0.5206381044125344, 0.39447408644110227, 0.2850160956805672, 0.16732959957758253, 0.11369379087101616, 0.07562794998479756], "prob": [[1.613478635670501e-06, 19], [8.322910009699647e-12, 11], [1.6066457445365168e-25, 21], [5.23632157771709e-32, 15], [3.2778821332603545e-33, 10], [2.0127400822262878e-41, 22], [3.660489455949202e-46, 12], [9.546560103820588e-48, 14], [9.172179855845938e-48, 6]], "generated": ["Islamophobia is the irrational fear of Arabs and Muslims.", "Islam is a religion followed by Muslims.", "Islam’s basic doctrines and precepts are found in the Quran.", "Islam literally means “submission to the will of Allah.”", "Allah is the Arabic word for God.", "Thus, in order to be the target of Islamophobia, an individual has to be Muslim.", "Donald Trump is not Muslim, but he is a Christian.", "Therefore, he was not the target of Islamophobia.", "So the answer is no."], "conf_comments": ["(A)\nDonald Trump is a", "(A)\nDonald Trump is a", "(A)\nThe Quran", "(A)\nThus, Islam", "(A)\nThus, Islam", "(A)\nDonald Trump is not", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 8, "score": [0.6919880533494553, 0.36761916091647057, 0.1575641632324496, 0.08370246909925194, 0.063431924057686, 0.03508337716130095, 0.01705857421669189, 0.012500501763109921], "conf": [0.966166023586858, 0.8247675687573642, 0.6815766734457936, 0.5206381044125344, 0.39447408644110227, 0.2850160956805672, 0.16732959957758253, 0.1048628494613812], "prob": [[1.613478635670501e-06, 19], [8.322910009699647e-12, 11], [1.6066457445365168e-25, 21], [5.23632157771709e-32, 15], [3.2778821332603545e-33, 10], [2.0127400822262878e-41, 22], [3.660489455949202e-46, 12], [1.4489802274133941e-46, 6]], "generated": ["Islamophobia is the irrational fear of Arabs and Muslims.", "Islam is a religion followed by Muslims.", "Islam’s basic doctrines and precepts are found in the Quran.", "Islam literally means “submission to the will of Allah.”", "Allah is the Arabic word for God.", "Thus, in order to be the target of Islamophobia, an individual has to be Muslim.", "Donald Trump is not Muslim, but he is a Christian.", "So the answer is no."], "conf_comments": ["(A)\nDonald Trump is a", "(A)\nDonald Trump is a", "(A)\nThe Quran", "(A)\nThus, Islam", "(A)\nThus, Islam", "(A)\nDonald Trump is not", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": [], "9": []}, "run_time": 64.56366324424744}
{"index": 232, "question": "Does Disney's Alice in Wonderland involve the celebration of a holiday?", "answer": "yes", "explanation": "Yes. In the movie, Alice meets the Mad Hatter. The Mad Hatter is having a tea party to celebrate his Unbirthday. The Unbirthday is a holiday which happens every day of the year which is not the subject's actual birthday.", "generated": [{"finished": true, "length": 3, "score": [0.6453129500214769, 0.49838762233162726, 0.43321930665358654], "conf": [0.9215025269940197, 0.753761280473148, 0.5712576279320286], "prob": [[3.0248767869214663e-06, 16], [4.9870235192155454e-08, 13], [4.897099767397315e-08, 6]], "generated": ["Disney's Alice in Wonderland takes place on Halloween night.", "Thus, it involves the celebration of a holiday.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Disney", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 1, "score": [0.7084907955110519], "conf": [0.8788716972730355], "prob": [[6.873015053707926e-13, 50]], "generated": ["Disney's Alice in Wonderland, also known as Alice in Wonderland 1951, was based on Lewis Carroll's Alice's Adventures in Wonderland (1865) and Through the Looking-G"], "conf_comments": ["(A)\nThus, the"], "info": null}, {"finished": true, "length": 4, "score": [0.7527049260308598, 0.3421218194854371, 0.2775166694147996, 0.23386090960892408], "conf": [0.9436343520614731, 0.7280272030008536, 0.5656427315300224, 0.40264042978338693], "prob": [[4.81613414563037e-06, 24], [3.2368358678878094e-27, 37], [1.165629881279834e-28, 20], [1.1490326497252312e-28, 6]], "generated": ["Disney's Alice in Wonderland is based on the book \"Alice's Adventures in Wonderland\".", "According to the book, Alice falls down a rabbit hole, which is like the beginning of the story for Easter (celebrates the resurrection of Jesus).", "Thus, Disney's Alice in Wonderland does involve the celebration of a holiday.", "So the answer is yes."], "conf_comments": ["(A)\nThe book was written", "(A)\nThus, Disney", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7023242042408339, 0.3512357397330488, 0.2490211556473828, 0.18031424368724414, 0.1388622836935602], "conf": [0.9415063664783613, 0.8026504365053222, 0.6766988736126105, 0.45759127105949254, 0.30466782334153164], "prob": [[3.780596391018055e-09, 30], [6.410607174043317e-30, 39], [2.169531898960605e-32, 11], [6.237176130661317e-35, 23], [3.11563146629149e-35, 6]], "generated": ["Disney's Alice in Wonderland was released in 1951, which is 51 years after 1900.", "The earliest known version of Valentine’s Day is 270 CE, so in 1951, Valentine’s Day was 1312 years old.", "Valentine’s Day is a holiday.", "Thus, it is possible that Disney's Alice in Wonderland involves the celebration of a holiday.", "So the answer is yes."], "conf_comments": ["(A)\nIn comparison, ", "(A)\nThus, Disney", "(A)\nThus, Disney", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7527049260308598, 0.3977809448473558, 0.22813060731977755, 0.19240863917269033], "conf": [0.9436343520614731, 0.8127801224017542, 0.6050936667385614, 0.43279528111932913], "prob": [[4.81613414563037e-06, 24], [1.1857312197307423e-23, 36], [4.205411317441107e-31, 21], [4.069500990509712e-31, 6]], "generated": ["Disney's Alice in Wonderland is based on the book \"Alice's Adventures in Wonderland\".", "Alice's Adventures in Wonderland contains a chapter called \"The Queen of Hearts\" in which a character called \"The March Hare\" is holding a party.", "Thus, in Disney's Alice in Wonderland, a holiday celebration is occurring.", "So the answer is yes."], "conf_comments": ["(A)\nThe book was written", "(A)\nThus, Disney", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 45.714701414108276}
{"index": 233, "question": "Did François Mitterrand serve under Napoleon Bonapart in the French army?", "answer": "no", "explanation": "No. François Mitterrand was born in 1916. Napoleon Bonapart died in 1821.", "generated": [{"finished": true, "length": 4, "score": [0.9048273485802624, 0.7990864857812097, 0.6132892930722316, 0.5129187981789536], "conf": [0.9724787707499977, 0.8355620981090829, 0.6509816054791008, 0.4576646970442518], "prob": [[0.08985004788476775, 14], [0.02103203485694971, 15], [0.00023973565343459065, 16], [0.0002325194288423687, 6]], "generated": ["François Mitterrand was born in 1916.", "Napoleon Bonapart died in 1821.", "Thus, François Mitterrand did not serve under Napoleon Bonapart.", "So the answer is no."], "conf_comments": ["(A)\nNapoleon", "(A)\nThus, François", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.9048273485802624, 0.6492245084146001, 0.5044696174003259, 0.41658007043348794], "conf": [0.9724787707499977, 0.8729212426485059, 0.6240617625867202, 0.42751365359164234], "prob": [[0.08985004788476775, 14], [4.05191448448e-06, 18], [1.1663855056908777e-07, 21], [1.1346692993982277e-07, 6]], "generated": ["François Mitterrand was born in 1916.", "Napoleon Bonapart’s rule ended in 1815.", "Thus, François Mitterrand could not have served under Napoleon Bonapart in the French army.", "So the answer is no."], "conf_comments": ["(A)\nNapoleon", "(A)\nThus, François", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7349011409337007, 0.5627281789371887, 0.4289551040252472, 0.3607915596626737], "conf": [0.9613491893725726, 0.83125818838901, 0.6319624679166395, 0.4482836814577976], "prob": [[9.845252546203016e-05, 16], [9.303436471614607e-07, 12], [5.633683610248926e-09, 19], [5.543154009975448e-09, 6]], "generated": ["François Mitterrand was born in the late 19th century.", "Napoleon died in 1821.", "Thus, it was impossible for François Mitterrand to serve under Napoleon Bonapart.", "So the answer is no."], "conf_comments": ["(A)\nNapoleon", "(A)\nThus, François", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.9048273485802624, 0.6492245084146001, 0.37642460729312505, 0.31048714879187483, 0.2638191352512569], "conf": [0.9724787707499977, 0.8729212426485059, 0.7191504041501944, 0.566834101546378, 0.41040695453324244], "prob": [[0.08985004788476775, 14], [4.05191448448e-06, 18], [6.63882055599007e-14, 20], [3.020550070027685e-15, 21], [2.9695151816901205e-15, 6]], "generated": ["François Mitterrand was born in 1916.", "Napoleon Bonapart’s rule ended in 1815.", "In other words, François Mitterrand is 42 years younger than Napoleon Bonapart.", "Thus, François Mitterrand could not have served under Napoleon Bonapart in the French army.", "So the answer is no."], "conf_comments": ["(A)\nNapoleon", "(A)\nThus, François", "(A)\nThus, François", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8012992893594458, 0.5241033678257048, 0.3804768189786617, 0.314841531274236], "conf": [0.9688771107316053, 0.8229597408683854, 0.6122931143108913, 0.4221429968576785], "prob": [[0.002088393280269616, 15], [7.109346176764609e-08, 15], [2.855598496791792e-10, 16], [2.7407008341945516e-10, 6]], "generated": ["Francois Mitterrand was born in 1916.", "Napoleon Bonpaart died in 1821.", "Thus, Mitterrand could not have served under Napoleon Bonapart.", "So the answer is no."], "conf_comments": ["(A)\nNapoleon", "(A)\nThus, Mit", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.23252534866333}
{"index": 234, "question": "Can chemicals in onion help create a thermonuclear bomb?", "answer": "yes", "explanation": "Yes. A thermonuclear bomb, also called a hydrogen bomb, uses hydrogen under high temperatures to create an explosive reaction. While chopping onions, cells inside the onion are broken and the gas that comes out forms sulfenic acid. Sulfenic acid is composed of several elements including hydrogen.", "generated": [{"finished": true, "length": 4, "score": [0.5321973901029659, 0.29308267191656134, 0.22609405548462708, 0.17805759173898716], "conf": [0.8590553847559395, 0.6356606013155536, 0.43384040090480025, 0.2696447930227176], "prob": [[6.428555231781017e-09, 17], [2.0979225018852087e-12, 9], [1.7810205636147235e-13, 18], [1.7585425283187174e-13, 6]], "generated": ["The chemicals to help create a thermonuclear bomb is plutonium.", "An onion contains no plutonium.", "Thus, chemicals in onion cannot help create a thermonuclear bomb.", "So the answer is no."], "conf_comments": ["(A)\nOnion is a", "(A)\nThus, chemical", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5321973901029659, 0.29308267191656134, 0.20637426180524326, 0.162535675145902], "conf": [0.8590553847559395, 0.6356606013155536, 0.4277026225119423, 0.26591119656123113], "prob": [[6.428555231781017e-09, 17], [2.0979225018852087e-12, 9], [1.1690422396443627e-14, 17], [1.1528689377330938e-14, 6]], "generated": ["The chemicals to help create a thermonuclear bomb is plutonium.", "An onion contains no plutonium.", "Thus, onion chemicals cannot help create a thermonuclear bomb.", "So the answer is no."], "conf_comments": ["(A)\nOnion is a", "(A)\nThus, chemical", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5321973901029659, 0.2740884162658573, 0.2026583125014421, 0.1568442694008793], "conf": [0.8590553847559395, 0.48414736200325104, 0.29152844655034116, 0.1749848474213923], "prob": [[6.428555231781017e-09, 17], [3.7237884900952994e-14, 16], [6.543031814430004e-15, 18], [6.461194368189118e-15, 6]], "generated": ["The chemicals to help create a thermonuclear bomb is plutonium.", "Plutonium can be found in nuclear reactors, not onion.", "Thus, chemicals in onion cannot help create a thermonuclear bomb.", "So the answer is no."], "conf_comments": ["(A)\nOnion is a", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.5084286230176918, 0.2866531022221865, 0.17710715740405547, 0.08381161286016978, 0.0642649066667995, 0.05618248466787135], "conf": [0.946992150799967, 0.8667023568788245, 0.7611153270790517, 0.6007354007311151, 0.48931095476184755, 0.37485598436219597], "prob": [[3.9309661578319453e-13, 22], [6.1221557607543225e-21, 17], [2.7864142861035793e-25, 12], [9.03685629419151e-37, 21], [2.5579680417719144e-39, 18], [2.5219866676211712e-39, 6]], "generated": ["The basic requirements to manufacture a thermonuclear bomb are a fission device and a fusion device.", "In a fission device, high-enriched uranium is required.", "In a fusion device, a fission trigger is required.", "Onion only contains sulfur, and neither one is a requirement for a thermonuclear bomb.", "Thus, chemicals in onion can not create a thermonuclear bomb.", "So the answer is no."], "conf_comments": ["(A)\nThe fission device", "(A)\nIn a fusion device", "(A)\nOnions contain a", "(A)\nThus, chemical", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5321973901029659, 0.2740884162658573, 0.18706764372729404, 0.14433535359528674], "conf": [0.8590553847559395, 0.48414736200325104, 0.28626306699182574, 0.17077188397728163], "prob": [[6.428555231781017e-09, 17], [3.7237884900952994e-14, 16], [6.460484181365091e-16, 17], [6.380391143707314e-16, 6]], "generated": ["The chemicals to help create a thermonuclear bomb is plutonium.", "Plutonium can be found in nuclear reactors, not onion.", "Thus, chemicals in onion cannot create a thermonuclear bomb.", "So the answer is no."], "conf_comments": ["(A)\nOnion is a", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 58.3503954410553}
{"index": 235, "question": "Would downloading Mario 64 on an emulator be legal?", "answer": "no", "explanation": "No. Mario 64 is licenced to Nintendo Entertainment. Nintendo holds a copyright on all of their Mario games.", "generated": [{"finished": true, "length": 4, "score": [0.6133780094188026, 0.3994019128597103, 0.3399451914113402, 0.2970993946308897], "conf": [0.9674975285749646, 0.8863153894706852, 0.7321686610193653, 0.561220960811482], "prob": [[2.734668120495174e-07, 16], [2.6199101741658602e-12, 15], [3.6551615172709636e-13, 15], [3.578363573941568e-13, 6]], "generated": ["It is illegal to download any copyrighted material from the internet without permission.", "The Mario 64 video game is a copyright of Nintendo.", "Thus, downloading Mario 64 on an emulator would be illegal.", "So the answer is no."], "conf_comments": ["(A)\nMario 6", "(A)\nThus, downloading", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.8253943908492373, 0.5459096874298761, 0.3365627037235836, 0.24235626023214799, 0.1683143404395398, 0.1432528331553363], "conf": [0.9816047715104427, 0.8989855239192003, 0.7457145851593714, 0.5833754289792424, 0.44869056472188834, 0.32679605276743395], "prob": [[0.006017403282650709, 14], [6.785592923311878e-13, 31], [4.98056062929178e-21, 24], [1.5735929978237023e-23, 14], [1.4352140413264677e-26, 15], [1.389065379935342e-26, 6]], "generated": ["Mario 64 was released in 1996.", "According to the U.S. Copyright act, a work is protected for 70 years after the death of the last surviving author.", "As of 2020, the last known author, Shigeru Miyamoto, is alive.", "Therefore, Mario 64 is still protected by copyright.", "Thus, downloading Mario 64 using an emulator would be illegal.", "So the answer is no."], "conf_comments": ["(A)\nThus, it", "(A)\nThus, Mario", "(A)\nThus, Mario", "(A)\nThus, downloading", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.8253943908492373, 0.5459096874298761, 0.3365627037235836, 0.24235626023214799, 0.15493717265503743, 0.13269679368031875], "conf": [0.9816047715104427, 0.8989855239192003, 0.7457145851593714, 0.5833754289792424, 0.446961053139135, 0.3295494285565189], "prob": [[0.006017403282650709, 14], [6.785592923311878e-13, 31], [4.98056062929178e-21, 24], [1.5735929978237023e-23, 14], [2.36549979811402e-30, 25], [2.2933759825788246e-30, 6]], "generated": ["Mario 64 was released in 1996.", "According to the U.S. Copyright act, a work is protected for 70 years after the death of the last surviving author.", "As of 2020, the last known author, Shigeru Miyamoto, is alive.", "Therefore, Mario 64 is still protected by copyright.", "Thus, because it is still protected by copyright, downloading Mario 64 on an emulator would not be legal.", "So the answer is no."], "conf_comments": ["(A)\nThus, it", "(A)\nThus, Mario", "(A)\nThus, Mario", "(A)\nThus, downloading", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6133780094188026, 0.3994019128597103, 0.2580755872765341, 0.2271554266696972], "conf": [0.9674975285749646, 0.8863153894706852, 0.7416764957017752, 0.5766825672692043], "prob": [[2.734668120495174e-07, 16], [2.6199101741658602e-12, 15], [3.111013123591992e-16, 13], [3.0442639607198334e-16, 6]], "generated": ["It is illegal to download any copyrighted material from the internet without permission.", "The Mario 64 video game is a copyright of Nintendo.", "Thus, downloading it from an emulator would not be legal.", "So the answer is no."], "conf_comments": ["(A)\nMario 6", "(A)\nThus, downloading", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.7409113338382864, 0.4274877330590753], "conf": [0.9801548705584194, 0.8285299802116133], "prob": [[5.248584125658695e-05, 17], [3.0556791553303624e-25, 50]], "generated": ["Mario 64 was published in 1996 by Nintendo.", "According to the UCC, once a copyright is created, it expires 75 years after the author’s death or for any work for hire, 95 years from publication or 120 years from creation,"], "conf_comments": ["(A)\nEmulators are", "(A)\nThus, Mario"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 55.55959129333496}
{"index": 236, "question": "Is lunch on the beach a good activity to spot the full circle of a rainbow?", "answer": "no", "explanation": "No. The full circle of a rainbow cannot usually be seen from ground level. Sometimes the full circle of a rainbow can be seen from a high building or aircraft. You can see more of a rainbow the closer to the horizon the sun is. Lunch occurs at midday when the sun is likely high in the sky.", "generated": [{"finished": true, "length": 4, "score": [0.564178558127839, 0.3536065711964224, 0.2500253769024039, 0.2119072736250217], "conf": [0.9561715792104848, 0.8141442621071275, 0.6542951812467693, 0.47217616713626726], "prob": [[2.2738256303544418e-08, 16], [2.0028039081085464e-15, 21], [1.7385618580265326e-17, 10], [1.6910224881532453e-17, 6]], "generated": ["There needs to be rain in the air for a rainbow to be visible.", "If it is sunny on the beach, then there will be no rain and thus no rainbow.", "Thus, it is not a good activity.", "So the answer is no."], "conf_comments": ["(A)\nThe sun needs to", "(A)\nThus, l", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6936258722908504, 0.4233125204686525, 0.19771606035331216, 0.13842496336609483, 0.11448712250722928], "conf": [0.9548345252617068, 0.8301217820933054, 0.5270512963131179, 0.3704405100667788, 0.25457204441970105], "prob": [[5.607529064904731e-07, 21], [1.912074060261437e-15, 23], [2.4795038168338176e-23, 17], [1.2806520210231277e-26, 21], [1.2456166136149228e-26, 6]], "generated": ["To spot the full circle of a rainbow, you need to be underneath the rainbow.", "A rainbow is formed when sunlight refracts and reflects on raindrops in the atmosphere.", "A beach cannot have raindrops in the atmosphere to form a rainbow.", "Thus, you would not be able to spot the full circle of a rainbow at the beach.", "So the answer is no."], "conf_comments": ["(A)\nThe beach is not", "(A)\nThus, you", "(A)\nThus, l", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6936258722908504, 0.4233125204686525, 0.19771606035331216, 0.1337539475074221, 0.11099571091809494], "conf": [0.9548345252617068, 0.8301217820933054, 0.5270512963131179, 0.37497821036191153, 0.25908828119681154], "prob": [[5.607529064904731e-07, 21], [1.912074060261437e-15, 23], [2.4795038168338176e-23, 17], [9.705474731295196e-28, 23], [9.513962213833764e-28, 6]], "generated": ["To spot the full circle of a rainbow, you need to be underneath the rainbow.", "A rainbow is formed when sunlight refracts and reflects on raindrops in the atmosphere.", "A beach cannot have raindrops in the atmosphere to form a rainbow.", "Thus, lunch on the beach would not be good for spotting the full circle of a rainbow.", "So the answer is no."], "conf_comments": ["(A)\nThe beach is not", "(A)\nThus, you", "(A)\nThus, l", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.6936258722908504, 0.4233125204686525, 0.2643181412127249, 0.12925682329750862, 0.074896846970253, 0.06315196652059543], "conf": [0.9548345252617068, 0.8301217820933054, 0.69560814720186, 0.577062636916674, 0.426053020903656, 0.3064278693219018], "prob": [[5.607529064904731e-07, 21], [1.912074060261437e-15, 23], [2.040065270669285e-24, 27], [1.9363122302591176e-31, 13], [1.1848994091485692e-39, 24], [1.1055386589642417e-39, 6]], "generated": ["To spot the full circle of a rainbow, you need to be underneath the rainbow.", "A rainbow is formed when sunlight refracts and reflects on raindrops in the atmosphere.", "Therefore, to spot the full circle of a rainbow, you would need the sun to be between you and the rainbow.", "You would need to face the opposite direction of the rainbow.", "Thus, lunch on the the beach is not the best place to spot the full circle of a rainbow.", "So the answer is no."], "conf_comments": ["(A)\nThe beach is not", "(A)\nThus, you", "(A)\nThus, to", "(A)\nThus, you", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.564178558127839, 0.3536065711964224, 0.25041135615466903], "conf": [0.9561715792104848, 0.8141442621071275, 0.5592359523710267], "prob": [[2.2738256303544418e-08, 16], [2.0028039081085464e-15, 21], [3.0330561089157517e-16, 6]], "generated": ["There needs to be rain in the air for a rainbow to be visible.", "If it is sunny on the beach, then there will be no rain and thus no rainbow.", "So the answer is no."], "conf_comments": ["(A)\nThe sun needs to", "(A)\nThus, l", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 61.3151650428772}
{"index": 237, "question": "Does Düsseldorf have only a small number of smoggy days each year?", "answer": "yes", "explanation": "Yes. Mercer's 2012 Quality of Living survey ranked Düsseldorf the sixth most livable city in the world. Clean air is an important attribute for a livable city. Smog is a term for air pollution.", "generated": [{"finished": true, "length": 3, "score": [0.5590037518797483, 0.4879910150227248, 0.43742474501175366], "conf": [0.9238887813098313, 0.786410270887424, 0.6345320869985069], "prob": [[1.6991972612922419e-12, 25], [2.592054232131294e-13, 17], [2.527643809315757e-13, 6]], "generated": ["According to the Wikipedia page for pollution in Düsseldorf, Düsseldorf has only one or two smoggy days each year.", "Thus, Düsseldorf has only a small number of smoggy days each year.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Düsseldorf", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6556353366620115, 0.3215439406584903, 0.2686345514359708, 0.2315172037339602], "conf": [0.8949974596704441, 0.718125137741207, 0.5536478214112559, 0.41316140479917285], "prob": [[1.669335154194998e-05, 15], [3.7352280342594736e-20, 28], [6.88780070561577e-21, 17], [6.6960771497519684e-21, 6]], "generated": ["Düsseldorf has around 60 smoggy days per year.", "Compared to Los Angeles, which has around 470 smoggy days per year, Düsseldorf has very few polluted days.", "Thus, Düsseldorf has only a small number of smoggy days each year.", "So the answer is yes."], "conf_comments": ["(A)\nIn comparison, Berlin", "(A)\nThus, Düsseldorf", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7929725214195634, 0.40225520366525097, 0.24190685173447554, 0.12229005109598577, 0.10011823589796924], "conf": [0.9680576448160253, 0.7671962869615382, 0.5538679774728485, 0.3826791087523492, 0.2595143069396605], "prob": [[0.02058427217836989, 9], [3.8823843758195756e-14, 24], [9.650496302969214e-21, 22], [1.6202541852772224e-28, 18], [1.5103882179069175e-28, 6]], "generated": ["Düsseldorf is a city in Germany.", "The number of smoggy days in Düsseldorf varies between 80 and 150 days every year.", "Thus, Düsseldorf has between an average of 80 and 150 smoggy days.", "Both numbers are low, so Düsseldorf has a small number of smoggy days.", "So the answer is yes."], "conf_comments": ["(A)\nIt is located in", "(A)\nThus, Düsseldorf", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7929725214195634, 0.40225520366525097, 0.24190685173447554, 0.12195328093161023, 0.09463249355752015], "conf": [0.9680576448160253, 0.7671962869615382, 0.5538679774728485, 0.3766399057568106, 0.23697844071731544], "prob": [[0.02058427217836989, 9], [3.8823843758195756e-14, 24], [9.650496302969214e-21, 22], [3.742037101684406e-27, 15], [2.87458132998254e-27, 6]], "generated": ["Düsseldorf is a city in Germany.", "The number of smoggy days in Düsseldorf varies between 80 and 150 days every year.", "Thus, Düsseldorf has between an average of 80 and 150 smoggy days.", "This is more than one, which is the definition of a small number.", "So the answer is no."], "conf_comments": ["(A)\nIt is located in", "(A)\nThus, Düsseldorf", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7929725214195634, 0.40225520366525097, 0.222829188609379, 0.14414969847979986], "conf": [0.9680576448160253, 0.7671962869615382, 0.5145604467234243, 0.2910883416688845], "prob": [[0.02058427217836989, 9], [3.8823843758195756e-14, 24], [2.7476431753062184e-22, 24], [4.503288516663793e-23, 6]], "generated": ["Düsseldorf is a city in Germany.", "The number of smoggy days in Düsseldorf varies between 80 and 150 days every year.", "150 is greater than 50, meaning Düsseldorf has over 50 smoggy days each year.", "So the answer is no."], "conf_comments": ["(A)\nIt is located in", "(A)\nThus, Düsseldorf", "(A)\nThus, Düsseldorf", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.391700983047485}
{"index": 238, "question": "Has Elon Musk's hairline changed?", "answer": "yes", "explanation": "Yes. When Elon Musk was much younger, he was clearly balding. Elon Musk does not show any signs of balding as of 2020.", "generated": [{"finished": true, "length": 3, "score": [0.6755352203646355, 0.518700294183331, 0.4523168864122166], "conf": [0.9232648437839398, 0.7603148454572778, 0.5839486702358407], "prob": [[2.2348190713660447e-08, 25], [7.905704986946226e-10, 10], [7.446789386399786e-10, 6]], "generated": ["According to this website, Elon Musk's hairline has not changed in the past 10 years.", "Thus, his hairline has not changed.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.6755352203646355, 0.5012457464657156], "conf": [0.9232648437839398, 0.7363734945733983], "prob": [[2.2348190713660447e-08, 25], [2.4179299574713725e-09, 6]], "generated": ["According to this website, Elon Musk's hairline has not changed in the past 10 years.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6755352203646355, 0.518700294183331, 0.33918444191272656], "conf": [0.9232648437839398, 0.7603148454572778, 0.5738503361745265], "prob": [[2.2348190713660447e-08, 25], [7.905704986946226e-10, 10], [2.6142236069332332e-11, 6]], "generated": ["According to this website, Elon Musk's hairline has not changed in the past 10 years.", "Thus, his hairline has not changed.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.808616377693472, 0.4191033031749463, 0.21275745491104353, 0.15346867581616644, 0.1186168877035239, 0.09869684776766217], "conf": [0.9657783540913482, 0.8566134531778983, 0.7456532371729763, 0.5802768104726284, 0.4252238122701438, 0.29599555221916296], "prob": [[0.0006047061485491799, 19], [1.0006669316012235e-11, 15], [4.679245335704577e-27, 29], [1.1903299864104851e-32, 32], [5.555655622330572e-34, 15], [5.3778756497415235e-34, 6]], "generated": ["Elon Musk was born on June 28th, 1971.", "He would be 49 years old on 2019.", "We can see in this picture of Elon Musk in 1971 that he had a large forehead (hairline).", "We can also see in this picture of Elon Musk in 2018 that he still has a very large forehead (hairline).", "Thus, Elon Musk’s hairline has not changed.", "So the answer is no."], "conf_comments": ["(A)\nThus, El", "(A)\nThus, El", "(A)\nWe can see in", "(A)\nThus, El", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.808616377693472, 0.34960282099186946, 0.20224932320386133, 0.13208405637661239, 0.05743292123111655, 0.035532285379680034, 0.02441054332450347], "conf": [0.9657783540913482, 0.7562204147309319, 0.6339739588913125, 0.5092272423070542, 0.3605500868779586, 0.23058202549943155, 0.14903769654254836], "prob": [[0.0006047061485491799, 19], [3.6348206518844324e-10, 10], [9.756939137522447e-20, 24], [4.637687984161852e-26, 23], [8.278301493949198e-36, 17], [6.263916864602677e-39, 14], [2.699139155523293e-40, 10]], "generated": ["Elon Musk was born on June 28th, 1971.", "His hairline has clearly changed over time.", "For example, in April of 2007, Elon Musk had a receding hairline.", "However, in June of 2014, he didn't have a receding hairline.", "Elon Musk hasn't posted a recent picture of his hairline online.", "Thus, it is not known if his hairline has changed.", "So the answer is I don't know."], "conf_comments": ["(A)\nThus, El", "(A)\nThus, El", "(A)\nIn April of ", "(A)\nThus, El", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.808616377693472, 0.34960282099186946, 0.20224932320386133, 0.13208405637661239, 0.05743292123111655, 0.035532285379680034, 0.012115241970422191], "conf": [0.9657783540913482, 0.7562204147309319, 0.6339739588913125, 0.5092272423070542, 0.3605500868779586, 0.23058202549943155, 0.14841970160724727], "prob": [[0.0006047061485491799, 19], [3.6348206518844324e-10, 10], [9.756939137522447e-20, 24], [4.637687984161852e-26, 23], [8.278301493949198e-36, 17], [6.263916864602677e-39, 14], [1.3633876447680366e-48, 13]], "generated": ["Elon Musk was born on June 28th, 1971.", "His hairline has clearly changed over time.", "For example, in April of 2007, Elon Musk had a receding hairline.", "However, in June of 2014, he didn't have a receding hairline.", "Elon Musk hasn't posted a recent picture of his hairline online.", "Thus, it is not known if his hairline has changed.", "So the answer is invalid, or the answer is not known."], "conf_comments": ["(A)\nThus, El", "(A)\nThus, El", "(A)\nIn April of ", "(A)\nThus, El", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 56.895922899246216}
{"index": 239, "question": "Is there a Marvel villain with the same name as a kind of citrus fruit?", "answer": "yes", "explanation": "Yes. Mandarins are a type of orange popular in Asian cuisine. The Mandarin is also the name of a villain associated with Iron Man in the Marvel universe.", "generated": [{"finished": true, "length": 2, "score": [0.6789234883267641, 0.5537432977593376], "conf": [0.8885377904799481, 0.696678808760376], "prob": [[2.781074231160501e-07, 23], [7.467261934189378e-08, 8]], "generated": ["According to Marvel Database, there is no Marvel character with the same name as a kind of citrus fruit.", "Thus, the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6789234883267641, 0.4422754090354688, 0.3621940144812077], "conf": [0.8885377904799481, 0.6086054412917791, 0.40947839299760397], "prob": [[2.781074231160501e-07, 23], [2.8459571039585755e-12, 24], [2.7915094860948073e-12, 6]], "generated": ["According to Marvel Database, there is no Marvel character with the same name as a kind of citrus fruit.", "Thus, there isn’t any Marvel villain that has the same name as a kind of citrus fruit.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6760586420684098, 0.38014506337107273, 0.3103297108562792, 0.2552346563664489], "conf": [0.973795911136785, 0.7970169120442644, 0.5930680928506002, 0.40238689585109433], "prob": [[0.002355083592601546, 8], [8.644765345114014e-11, 18], [8.532647524114877e-12, 21], [8.37999251326417e-12, 6]], "generated": ["Thanos is a Marvel villain.", "There is also a citrus fruit called Tangerine which is a type of orange.", "Thus, there is a Marvel villain with the same name as a kind of citrus fruit.", "So the answer is yes."], "conf_comments": ["(A)\nA grapef", "(A)\nThus, there", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6760586420684098, 0.38014506337107273, 0.2912958168996495, 0.23887126226572855], "conf": [0.973795911136785, 0.7970169120442644, 0.6126522645096608, 0.4133329196832637], "prob": [[0.002355083592601546, 8], [8.644765345114014e-11, 18], [5.178399185931668e-13, 19], [5.077333454689803e-13, 6]], "generated": ["Thanos is a Marvel villain.", "There is also a citrus fruit called Tangerine which is a type of orange.", "Thus, there is a Marvel villain and a citrus fruit with the same name.", "So the answer is yes."], "conf_comments": ["(A)\nA grapef", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5216565418969561, 0.3487523444073945, 0.25905615322167913, 0.21359046487184502], "conf": [0.9262487584923664, 0.7122513467763135, 0.4835827855774827, 0.32967088292712315], "prob": [[1.7100163173781145e-13, 24], [1.686974861983503e-17, 17], [3.2772728843739136e-19, 19], [3.2218805624561994e-19, 6]], "generated": ["There are many villains in Marvel such as Dr. Doom, Taskmaster, and Cletus Kasady.", "However, none of them shares the same name as a kind of citrus fruit.", "Thus, there is no Marvel villain with the same name as a citrus fruit.", "So the answer is no."], "conf_comments": ["(A)\nThere are many kinds", "(A)\nThus, there", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 39.71401929855347}
{"index": 240, "question": "Could someone in the Canary Islands fish for largemouth bass?", "answer": "no", "explanation": "No. The Canary Islands are located in the Atlantic Ocean. The Atlantic Ocean is a body of salt water. Largemouth bass live in fresh water.", "generated": [{"finished": true, "length": 4, "score": [0.7838211562737285, 0.5725883231092336, 0.4596197751387021, 0.39058760867964004], "conf": [0.9755307132206237, 0.7958595464494291, 0.6187242182259237, 0.4484373809569058], "prob": [[0.006181917050236243, 11], [5.7987446682343795e-05, 11], [9.316923550777114e-07, 22], [9.11761909025236e-07, 6]], "generated": ["Largemouth bass are native to North America.", "The Canary Islands are off the coast of Africa.", "Thus, it would be impossible for someone in the Canary Islands to fish for largemouth bass.", "So the answer is no."], "conf_comments": ["(A)\nThe Canary Islands", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7838211562737285, 0.5569204465478012, 0.39784700323399486, 0.33429362030272236], "conf": [0.9755307132206237, 0.8358065042479484, 0.657926945974536, 0.4659792794708561], "prob": [[0.006181917050236243, 11], [1.573527856731517e-07, 20], [1.5319948673236845e-10, 16], [1.5033762621120083e-10, 6]], "generated": ["Largemouth bass are native to North America.", "In comparison, the Canary Islands are located in the Atlantic Ocean, west of Morocco.", "Thus, it is not possible to fish for largemouth bass there.", "So the answer is no."], "conf_comments": ["(A)\nThe Canary Islands", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7319582885821267, 0.5056236717341892, 0.3699362874098446, 0.30577252041525343], "conf": [0.9743940179833191, 0.8345767282511664, 0.6054706348264254, 0.41856853664320465], "prob": [[0.0013887325886006792, 11], [6.917907008632338e-07, 13], [1.1678635873538877e-09, 21], [1.0879511098155093e-09, 6]], "generated": ["Largemouth bass is native to North America.", "In comparison, the Canary Islands is in the Atlantic Ocean.", "Therefore, it is impossible for someone in the Canary Islands to fish for largemouth bass.", "So the answer is no."], "conf_comments": ["(A)\nThe Canary Islands", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7838211562737285, 0.5569204465478012, 0.35400575296847286, 0.28580874191625266], "conf": [0.9755307132206237, 0.8358065042479484, 0.6224189509672189, 0.40821917514117034], "prob": [[0.006181917050236243, 11], [1.573527856731517e-07, 20], [2.264300241634936e-13, 22], [2.181989782450329e-13, 6]], "generated": ["Largemouth bass are native to North America.", "In comparison, the Canary Islands are located in the Atlantic Ocean, west of Morocco.", "Thus, it’s very unlikely someone in the Canary Islands could fish for largemouth bass.", "So the answer is no."], "conf_comments": ["(A)\nThe Canary Islands", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7319582885821267, 0.4257311195287264, 0.3248735717958303, 0.2732840692127753], "conf": [0.9743940179833191, 0.7472059431798381, 0.5523644288831642, 0.39235616839115556], "prob": [[0.0013887325886006792, 11], [8.789221283767694e-07, 9], [1.5215435352892922e-08, 17], [1.4871503686889051e-08, 6]], "generated": ["Largemouth bass is native to North America.", "The Canary Islands is part of Europe.", "Thus, someone in the Canary Islands cannot fish for largemouth bass.", "So the answer is no."], "conf_comments": ["(A)\nThe Canary Islands", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 52.57114553451538}
{"index": 241, "question": "Are anchovies associated with Italian food?", "answer": "yes", "explanation": "Yes. Pizza is an Italian food. Anchovies are occasionally used as a pizza topping.", "generated": [{"finished": true, "length": 3, "score": [0.6666859671281974, 0.6031521728869805, 0.53444452606192], "conf": [0.941375894406224, 0.8211281887930625, 0.6459545876829101], "prob": [[1.6687321616990658e-10, 30], [7.7758158354272e-11, 12], [7.686146374910212e-11, 6]], "generated": ["According to Wikipedia, anchovies are used in Italian cuisine as a pizza topping and salad dressing ingredient.", "Thus, anchovies are associated with Italian food.", "So the answer is yes."], "conf_comments": ["(A)\nThus, anch", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6326202186397207, 0.5399152085590957, 0.46111876365763255], "conf": [0.9424063714056303, 0.7467049144276625, 0.5456944468703239], "prob": [[8.568612875177675e-08, 19], [3.121481782305956e-08, 12], [3.086060677142636e-08, 6]], "generated": ["Anchovies are used for Italian foods, such as pizza and pasta.", "Thus, anchovies are associated with Italian food.", "So the answer is yes."], "conf_comments": ["(A)\nThus, anch", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5476073875997779, 0.37850950202998823, 0.3346193063289171, 0.2881616117690291], "conf": [0.9348690476413755, 0.8335789734079344, 0.6753916289359423, 0.5016754633422006], "prob": [[4.028959192035536e-09, 17], [5.3399227319376565e-14, 18], [3.4642474134041416e-14, 12], [3.431052316478581e-14, 6]], "generated": ["If a food is associated with Italian food, it usually appears in Italian dishes.", "Anchovies can be found in pizza, which is an Italian dish.", "Thus, anchovies are associated with Italian food.", "So the answer is yes."], "conf_comments": ["(A)\nAnchovies", "(A)\nThus, anch", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6666859671281974, 0.38237289552166975, 0.32240011549049], "conf": [0.941375894406224, 0.7643175473046184, 0.5501659909965467], "prob": [[1.6687321616990658e-10, 30], [2.171195131633355e-16, 15], [2.0150065497676775e-16, 6]], "generated": ["According to Wikipedia, anchovies are used in Italian cuisine as a pizza topping and salad dressing ingredient.", "Thus, there is positive correlation between anchovies and Italian food.", "So the answer is yes."], "conf_comments": ["(A)\nThus, anch", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6326202186397207, 0.3692721066287408, 0.15295340470656135, 0.1010011658734415, 0.07554858477673908], "conf": [0.9424063714056303, 0.6809446672800115, 0.4302027537832601, 0.28749921873820744, 0.17097944391594136], "prob": [[8.568612875177675e-08, 19], [1.086199157544361e-12, 15], [4.743954410990614e-20, 13], [5.1200759692984905e-23, 16], [3.550032854650553e-23, 6]], "generated": ["Anchovies are used for Italian foods, such as pizza and pasta.", "They are also used for Japanese foods, such as sushi.", "This shows that not all Italian foods involve anchovies.", "Thus, there are Italian foods that do not involve anchovies.", "So the answer is no."], "conf_comments": ["(A)\nThus, anch", "(A)\nThus, anch", "(A)\nThus, anch", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6326202186397207, 0.3692721066287408, 0.15295340470656135, 0.1010011658734415, 0.07234346074210331], "conf": [0.9424063714056303, 0.6809446672800115, 0.4302027537832601, 0.28749921873820744, 0.18199942402175207], "prob": [[8.568612875177675e-08, 19], [1.086199157544361e-12, 15], [4.743954410990614e-20, 13], [5.1200759692984905e-23, 16], [1.4506113125230984e-23, 6]], "generated": ["Anchovies are used for Italian foods, such as pizza and pasta.", "They are also used for Japanese foods, such as sushi.", "This shows that not all Italian foods involve anchovies.", "Thus, there are Italian foods that do not involve anchovies.", "So the answer is yes."], "conf_comments": ["(A)\nThus, anch", "(A)\nThus, anch", "(A)\nThus, anch", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 45.693901777267456}
{"index": 242, "question": "Did Harry Houdini appear on Chris Angel Mindfreak?", "answer": "no", "explanation": "No. Chris Angel Mindfreak was released in 2005. Harry Houdini died in 1926.", "generated": [{"finished": true, "length": 4, "score": [0.9151476122137252, 0.6752757672036637, 0.5622542955344357, 0.4825338286414051], "conf": [0.9846510563477766, 0.8765889716320536, 0.7123193067395583, 0.5267855902124174], "prob": [[0.12192685123678555, 13], [2.4454548539287273e-06, 22], [1.6435257242799685e-07, 17], [1.60384088809843e-07, 6]], "generated": ["Harry Houdini died in 1926.", "The first episode of Chris Angel Mindfreak aired on January 3, 2005.", "Thus, Harry Houdini did not appear on Chris Angel Mindfreak.", "So the answer is no."], "conf_comments": ["(A)\nChris Angel Mind", "(A)\nThus, Harry", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.9151476122137252, 0.6752757672036637, 0.4582656636124732, 0.39407875224291405], "conf": [0.9846510563477766, 0.8765889716320536, 0.6995297829902405, 0.520909840331609], "prob": [[0.12192685123678555, 13], [2.4454548539287273e-06, 22], [3.702995945206656e-10, 16], [3.5514322937008097e-10, 6]], "generated": ["Harry Houdini died in 1926.", "The first episode of Chris Angel Mindfreak aired on January 3, 2005.", "Thus, Harry Houdini could not have appeared on Mindfreak.", "So the answer is no."], "conf_comments": ["(A)\nChris Angel Mind", "(A)\nThus, Harry", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7555789648871507, 0.6333856851086042, 0.42537424310118943, 0.3587098311764511], "conf": [0.977263390763652, 0.8644101873700192, 0.5668143542066035, 0.4055726605488497], "prob": [[4.271751830487833e-06, 23], [5.393962980997631e-08, 19], [3.031143615283923e-11, 20], [2.920818735501416e-11, 6]], "generated": ["Chris Angel Mindfreak aired from January 2005 to December 2010.", "Harry Houdini lived from 1874 to 1926.", "Thus, it is impossible for Harry Houdini to appear in Chris Angel Mindfreak.", "So the answer is no."], "conf_comments": ["(A)\nHarry Houd", "(A)\nThus, Harry", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8112801932651809, 0.5478514693876785, 0.3697673573428255, 0.32045125092702365, 0.27465376097666433], "conf": [0.9718376496567469, 0.7754249463669002, 0.5594255164800074, 0.4509689905226442, 0.3321280629157245], "prob": [[0.00012799841963021842, 23], [4.0135616242836045e-13, 35], [4.15273089577242e-22, 45], [1.2467934297244574e-22, 17], [1.227790022207401e-22, 6]], "generated": ["Harry Houdini was a magician that lived from 1874-1926.", "On the other hand, Chris Angel Mindfreak was a TV show on the A&E network that lasted from 2005-2010.", "Since Harry Houdini died in 1926 and Chris Angel Mindfreak aired from 2005-2010, the two could not have appeared on each other’s show.", "Thus, Harry Houdini did not appear on Chris Angel Mindfreak.", "So the answer is no."], "conf_comments": ["(A)\nChris Angel Mind", "(A)\nThus, Harry", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7555789648871507, 0.6333856851086042, 0.3689504704399536, 0.30989154478742714], "conf": [0.977263390763652, 0.8644101873700192, 0.5581424557554585, 0.3959787289835368], "prob": [[4.271751830487833e-06, 23], [5.393962980997631e-08, 19], [1.3912247622003724e-13, 20], [1.345043748576858e-13, 6]], "generated": ["Chris Angel Mindfreak aired from January 2005 to December 2010.", "Harry Houdini lived from 1874 to 1926.", "Thus, it is impossible for Harry Houdini to appears on Chris Angel Mindfreak.", "So the answer is no."], "conf_comments": ["(A)\nHarry Houd", "(A)\nThus, Harry", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.60896682739258}
{"index": 243, "question": "Can Ford F-350 tow entire Yale University student body?", "answer": "no", "explanation": "No. The Yale University student body consists of 12,385 people according to a 2015 poll. The average US male weighs 195 pounds. The average US female weighs 168 pounds. The maximum towing capacity of the Ford F-350 is 15,000 pounds.", "generated": [{"finished": true, "length": 4, "score": [0.7432246021288387, 0.5048084631867521, 0.3768640597986993, 0.3107603692084382], "conf": [0.9794161524650411, 0.894089673185325, 0.687587861667818, 0.474429418363575], "prob": [[5.98040695348264e-06, 21], [8.385354433736022e-10, 13], [5.098119085306567e-13, 23], [4.669177625828938e-13, 6]], "generated": ["Ford F-350 has a towing capacity of 6,800 pounds.", "Yale University has about 5,500 students.", "Thus, a Ford F-350 could theoretically tow the entire Yale University student body.", "So the answer is yes."], "conf_comments": ["(A)\nThe average weight of", "(A)\nThus, the", "(A)\nHowever, the Ford", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7911886199030341, 0.5034065114522394, 0.3765650884362776, 0.30647566720228436], "conf": [0.9762561522355562, 0.8775539973860665, 0.6826236508273738, 0.45825934728538864], "prob": [[9.591869979002731e-06, 26], [9.4331996316492e-15, 26], [1.2939072841179528e-18, 27], [1.193972971561431e-18, 6]], "generated": ["A 2017 Ford F-350 has a towing capacity of 21,000 pounds", "According to 2018 fall enrollment, Yale has about 13,000 students.", "Thus, a 2017 Ford F-350 can theoretically tow the entire Yale student body.", "So the answer is yes."], "conf_comments": ["(A)\nThe average weight of", "(A)\nThus, the", "(A)\nHowever, the average", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7911886199030341, 0.5034065114522394, 0.3341535106887922, 0.27562210405848064], "conf": [0.9762561522355562, 0.8775539973860665, 0.6243306002901433, 0.4364900146340885], "prob": [[9.591869979002731e-06, 26], [9.4331996316492e-15, 26], [2.774071858008914e-23, 41], [2.35600247907654e-23, 6]], "generated": ["A 2017 Ford F-350 has a towing capacity of 21,000 pounds", "According to 2018 fall enrollment, Yale has about 13,000 students.", "Since 13,000 pounds is less than 21,000 pounds, it would be possible for a Ford F-350 to tow Yale student body.", "So the answer is yes."], "conf_comments": ["(A)\nThe average weight of", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7432246021288387, 0.5048084631867521, 0.3207266204245144, 0.2567527090682784], "conf": [0.9794161524650411, 0.894089673185325, 0.6516051674883299, 0.4561940664950403], "prob": [[5.98040695348264e-06, 21], [8.385354433736022e-10, 13], [1.1178267859438062e-14, 19], [6.575732783681602e-15, 6]], "generated": ["Ford F-350 has a towing capacity of 6,800 pounds.", "Yale University has about 5,500 students.", "So the F-350 can tow the entire student body of Yale University.", "So the answer is yes."], "conf_comments": ["(A)\nThe average weight of", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6583606708380394, 0.4786354204338875, 0.3027238400807549, 0.2478833548300659], "conf": [0.9796926118034801, 0.8723513393096017, 0.6776346068041813, 0.4811759462217613], "prob": [[7.1494442070674446e-09, 23], [2.1080383819803435e-13, 20], [7.041865878623046e-19, 19], [4.99167907164958e-19, 6]], "generated": ["The Ford F-350 has a max towing capacity of 11,910 pounds.", "According to US news, Yale University has around 12,500 students.", "Thus, the entire Yale student body is heavier than the max towing capacity.", "So the answer is no."], "conf_comments": ["(A)\nThe average weight of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 53.70196270942688}
{"index": 244, "question": "Are queen bees unnecessary for growing apples?", "answer": "yes", "explanation": "Yes. Mason bees are solitary (they live alone). Mason bees are efficient pollinators for orchards. Apple trees are grown in orchards.", "generated": [{"finished": true, "length": 2, "score": [0.6299435989310647, 0.44305998617545084], "conf": [0.9102163976159879, 0.63534532553131], "prob": [[3.907330901842087e-06, 15], [2.4861797113707975e-07, 8]], "generated": ["It’s not necessary to have queen bees for growing apples.", "Thus, the answer is no."], "conf_comments": ["(A)\nQueen bees", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5736985848999554, 0.3229275457811768, 0.2561704762222066, 0.17307329792199655], "conf": [0.9694621874945314, 0.734271990377506, 0.5122512315713882, 0.3257960121291104], "prob": [[3.589717617565953e-09, 18], [7.547591375811803e-15, 15], [1.9755740486125036e-15, 13], [2.699782134065907e-16, 6]], "generated": ["Bees help with the pollination of fruits and flowers, including apples.", "Queen bees are necessary for the pollination of apples.", "Thus, queen bees are necessary for growing apples.", "So the answer is yes."], "conf_comments": ["(A)\nQueen bees", "(A)\nThus, queen", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5736985848999554, 0.3229275457811768, 0.2065784226778076, 0.1586010508431068], "conf": [0.9694621874945314, 0.734271990377506, 0.4825059814316654, 0.2853018653986299], "prob": [[3.589717617565953e-09, 18], [7.547591375811803e-15, 15], [1.5991538804442178e-17, 13], [1.5693941847010748e-17, 6]], "generated": ["Bees help with the pollination of fruits and flowers, including apples.", "Queen bees are necessary for the pollination of apples.", "Thus, queen bees are not unnecessary for apples.", "So the answer is no."], "conf_comments": ["(A)\nQueen bees", "(A)\nThus, queen", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5481154049831234, 0.32978612503900573, 0.15515083139429325, 0.12252341572031634, 0.09816010517198195], "conf": [0.9573815996277761, 0.8170810760759316, 0.5331255906484087, 0.3654976428091113, 0.23944917524703493], "prob": [[2.7746102294736395e-09, 17], [3.9923688370875306e-14, 13], [2.732124629721688e-19, 11], [7.977642061840018e-20, 13], [7.054934552885084e-20, 6]], "generated": ["Queen bees are female honeybees with a large abdomen.", "To grow apples, you need to plant apple seeds.", "There are no abdomens in apple seeds.", "Thus, queen bees are unnecessary for growing apples.", "So the answer is no."], "conf_comments": ["(A)\nHoneybees", "(A)\nThus, queen", "(A)\nThus, queen", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5481154049831234, 0.32231728277400395, 0.1645173674649213, 0.12682274755896036, 0.09794667978741525], "conf": [0.9573815996277761, 0.6418897748123051, 0.43422729187613146, 0.2844466860661816, 0.17528821837759812], "prob": [[2.7746102294736395e-09, 17], [3.6955566972473325e-12, 10], [1.205807048490473e-14, 6], [3.397891213215372e-15, 13], [2.793888769356601e-15, 6]], "generated": ["Queen bees are female honeybees with a large abdomen.", "Honeybees do not grow apples.", "Apples grow on trees.", "Thus, queen bees are unnecessary for growing apples.", "So the answer is no."], "conf_comments": ["(A)\nHoneybees", "(A)\nThus, queen", "(A)\nThus, queen", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 53.21959352493286}
{"index": 245, "question": "Would the top of Mount Fuji stick out of the Sea of Japan? ", "answer": "yes", "explanation": "Yes. The average depth of the Sea of Japan is  5,748 feet (1,752 metres) and its maximum depth is 12,276 feet (3,742 metres). Mount Fuji is 3,776.24 metres (12,389.2 ft) tall. ", "generated": [{"finished": true, "length": 3, "score": [0.6817447169380091, 0.5671272981216465, 0.480395179840664], "conf": [0.9233214097552761, 0.7438552242967976, 0.5362590637068474], "prob": [[1.4534217576367721e-10, 33], [9.420375309611007e-12, 18], [9.157309767126006e-12, 6]], "generated": ["The Sea of Japan has a depth of about 3800 m, and Mount Fuji has an elevation of about 3700 m.", "Thus, the top of Mount Fuji would stick out of the Sea of Japan.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7588524698665047, 0.5469980378610737, 0.37927913537740066, 0.2855506411051551, 0.23636917472358315], "conf": [0.9776018147411975, 0.8610079255864062, 0.7352593707021646, 0.5335230272589776, 0.3666951039919861], "prob": [[3.045544271472453e-06, 24], [5.674884019190023e-12, 25], [5.865695980275473e-19, 28], [5.374019747433991e-21, 19], [5.275698802503155e-21, 6]], "generated": ["Mount Fuji’s height is about 3,776 m (12,388 feet).", "The Sea of Japan’s depth is about 4,343 m (14,248 feet).", "In comparison, Mount Fuji is about 583 m (1,913 feet) shorter than the Sea of Japan.", "Thus, the top of Mount Fuji could not stick out of the Sea of Japan.", "So the answer is no."], "conf_comments": ["(A)\nThe Sea of Japan", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7588524698665047, 0.5469980378610737, 0.32510759828337715, 0.26835727280063665], "conf": [0.9776018147411975, 0.8610079255864062, 0.7243856610161953, 0.5043341308000586], "prob": [[3.045544271472453e-06, 24], [5.674884019190023e-12, 25], [9.017313425037089e-22, 26], [7.921748932742735e-22, 6]], "generated": ["Mount Fuji’s height is about 3,776 m (12,388 feet).", "The Sea of Japan’s depth is about 4,343 m (14,248 feet).", "As Mount Fuji is taller than the Sea of Japan, the top of the mountain will indeed stick out of the water.", "So the answer is yes."], "conf_comments": ["(A)\nThe Sea of Japan", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6817447169380091, 0.45093010456929017, 0.36974877884303975], "conf": [0.9233214097552761, 0.7536398558630725, 0.518784275167979], "prob": [[1.4534217576367721e-10, 33], [8.573584316209965e-17, 23], [7.443804612742651e-17, 6]], "generated": ["The Sea of Japan has a depth of about 3800 m, and Mount Fuji has an elevation of about 3700 m.", "Thus, the top of Mount Fuji could stick out of the Sea of Japan about 80 m.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7588524698665047, 0.5469980378610737, 0.37927913537740066, 0.22995779193026136, 0.18636033662580243], "conf": [0.9776018147411975, 0.8610079255864062, 0.7352593707021646, 0.5044803158194395, 0.332727161594304], "prob": [[3.045544271472453e-06, 24], [5.674884019190023e-12, 25], [5.865695980275473e-19, 28], [2.227478452995198e-24, 20], [2.1717688959704424e-24, 6]], "generated": ["Mount Fuji’s height is about 3,776 m (12,388 feet).", "The Sea of Japan’s depth is about 4,343 m (14,248 feet).", "In comparison, Mount Fuji is about 583 m (1,913 feet) shorter than the Sea of Japan.", "Thus, the top of Mount Fuji clearly does not stick out of the Sea of Japan.", "So the answer is no."], "conf_comments": ["(A)\nThe Sea of Japan", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 53.486809492111206}
{"index": 246, "question": "Does the Dalai Lama believe in the divine barzakh?", "answer": "no", "explanation": "No. The Dalai Lama is the spiritual leader of Tibetan Buddhism. The divine barzakh is a concept from Islam.", "generated": [{"finished": true, "length": 3, "score": [0.6769216631174454, 0.44431858598866375, 0.37619687540648883], "conf": [0.9375532110267666, 0.752136635772301, 0.5413817119706612], "prob": [[1.9690883374299435e-09, 28], [1.1334464551490347e-12, 12], [1.1061357448907412e-12, 6]], "generated": ["The Dalai Lama is an adherent of Tibetan Buddhism, which has no reference to a divine barzakh.", "Thus, the Dalai Lama does not believe.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7072319863843809, 0.4362828654881554, 0.369479074952221], "conf": [0.8799199793680469, 0.6868587726992085, 0.49468435669101835], "prob": [[2.2785262425084912e-06, 23], [3.544445651677263e-09, 9], [3.4565611592193203e-09, 6]], "generated": ["The Dalai Lama believes in reincarnation, which is part of the divine barzakh.", "Thus, he does believe in it.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7072319863843809, 0.4517672231190107, 0.361158809494356], "conf": [0.8799199793680469, 0.6433590754391597, 0.43594299865533026], "prob": [[2.2785262425084912e-06, 23], [1.1258376125095548e-10, 17], [7.925378842027458e-11, 6]], "generated": ["The Dalai Lama believes in reincarnation, which is part of the divine barzakh.", "Thus, it is possible for him to believe in the divine barzakh.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6769216631174454, 0.41781249659576675, 0.3354191766974197], "conf": [0.9375532110267666, 0.6796585197934137, 0.4457620114089013], "prob": [[1.9690883374299435e-09, 28], [1.840375904962528e-14, 18], [1.656953659676303e-14, 6]], "generated": ["The Dalai Lama is an adherent of Tibetan Buddhism, which has no reference to a divine barzakh.", "Thus, the Dalai Lamais unlikely to believe in the divine barzakh.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6332949945043027, 0.33895309182223, 0.2751044707953263, 0.22551875059730217], "conf": [0.9343928457501683, 0.6948933018832276, 0.47488408691010603, 0.3200455380767651], "prob": [[9.109796363116783e-05, 11], [6.5507850069518545e-09, 10], [3.508210491316756e-09, 17], [3.447948620937733e-09, 6]], "generated": ["The Dalai Lama believes in the soul.", "The soul refers to the divine barzakh.", "Thus, the Dalai Lama believes in the divine barzakh.", "So the answer is yes."], "conf_comments": ["(A)\nThe soul is a", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 41.201993465423584}
{"index": 247, "question": "Was there fear leading up to the year 2000?", "answer": "yes", "explanation": "Yes. Many computer programs were not designed with the year 2000 in mind. People were worried that computers would crash all over the world when the year 2000 arrived. Financial and electrical systems require computers to function. Without financial and electrical systems there could be global chaos.", "generated": [{"finished": true, "length": 5, "score": [0.5404267548880807, 0.3294817164066833, 0.2247601506598285, 0.17305413258405805, 0.15302788514750912], "conf": [0.9575001433424516, 0.828697502405515, 0.7241371048519898, 0.6081426009992513, 0.4774316554838259], "prob": [[9.61722031732462e-18, 33], [2.749639102273052e-27, 26], [2.1106069711117523e-34, 26], [1.4059243630680098e-37, 21], [1.3727469023915763e-37, 6]], "generated": ["According to Google NGrams, the term “Year 2000 problem” is most popular around the year 1999 in America.", "The term “Y2K problem” is popular around the year 1999 in America and all over the world.", "These terms indicate that there was fear of the year 2000 in the late 1990s.", "Thus, it was true that there was fear leading up to the year 2000.", "So the answer is yes."], "conf_comments": ["(A)\nThus, there", "(A)\nThus, there", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6089700911287302, 0.29715759918325385, 0.23654073520900704, 0.19820924366754647], "conf": [0.9665576324043995, 0.8034046617702137, 0.6266871390244845, 0.4410337428435304], "prob": [[2.794577017365709e-20, 47], [7.004358353237737e-28, 14], [1.660963862269657e-29, 18], [1.638511570398287e-29, 6]], "generated": ["Leading up to the year 2000, there was a concern of the Y2K problem, which was predicted to bring catastrophic failures of computers by the end of 1999.", "These concerns were unfounded and there were no major failures.", "Thus, there was no fear leading up to the year 2000.", "So the answer is no."], "conf_comments": ["(A)\nThus, there", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7411986448725655, 0.290147166920077, 0.22529575956123288, 0.19810638629195468], "conf": [0.9662520735980163, 0.892027511107302, 0.7835744041311468, 0.607085313447373], "prob": [[0.00020974031452025325, 15], [5.058142486348851e-27, 29], [1.8702769350833496e-30, 21], [1.8477129594488672e-30, 6]], "generated": ["The year 2000 is referred to as Y2K.", "There was much concern and fear because there were worries that electronic devices in computers would malfunction since they use 2 digits to represent years.", "Thus, there was widespread fear leading up to the year 2000.", "So the answer is yes."], "conf_comments": ["(A)\nY2K was", "(A)\nThus, there", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5404267548880807, 0.3294817164066833, 0.2247601506598285, 0.18852041557419988], "conf": [0.9575001433424516, 0.828697502405515, 0.7241371048519898, 0.5579242387430379], "prob": [[9.61722031732462e-18, 33], [2.749639102273052e-27, 26], [2.1106069711117523e-34, 26], [1.2233470696488264e-34, 6]], "generated": ["According to Google NGrams, the term “Year 2000 problem” is most popular around the year 1999 in America.", "The term “Y2K problem” is popular around the year 1999 in America and all over the world.", "These terms indicate that there was fear of the year 2000 in the late 1990s.", "So the answer is yes."], "conf_comments": ["(A)\nThus, there", "(A)\nThus, there", "(A)\nThus, there", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5652428033667579, 0.3078970450231446, 0.25177892799315604], "conf": [0.9535588647285067, 0.751610527713887, 0.5306180929864364], "prob": [[4.499735227674554e-13, 26], [2.9504050504746605e-23, 24], [2.130653139557647e-23, 6]], "generated": ["Some worried that something would go wrong when the calendar year rolled from 1999 to 2000.", "This turned out to be a non-issue, so there likely was fear leading up to 2000.", "So the answer is yes."], "conf_comments": ["(A)\nThus, there", "(A)\nThus, there", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 50.90956258773804}
{"index": 248, "question": "Was Walt Disney able to email his illustrations to people living far away?", "answer": "no", "explanation": "No. Walt Disney died in 1966. Modern email came into existence in 1971.", "generated": [{"finished": true, "length": 4, "score": [0.9038853162235426, 0.7649896560796591, 0.5766997495793397, 0.47121041076369297], "conf": [0.9770809180894247, 0.8834387582602066, 0.69525053374624, 0.46734571011566245], "prob": [[0.11682978259359875, 12], [0.007138650548775013, 12], [2.8197845425720615e-05, 17], [2.7065479056520018e-05, 6]], "generated": ["Walt Disney was born in 1901.", "The first email was sent in 1971.", "Thus, Walt Disney was not able to email his illustrations to people.", "So the answer is no."], "conf_comments": ["(A)\nIn 19", "(A)\nThus, W", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.9169316913366745, 0.6659534709503953, 0.49271192465107144, 0.40283511663473187], "conf": [0.9760191577980355, 0.9026987263043452, 0.7017979518131405, 0.47168918736821375], "prob": [[0.05061887438054005, 20], [1.112377715233759e-05, 15], [8.185774293092177e-08, 14], [7.92150700932408e-08, 6]], "generated": ["Walt Disney was born in 1901 and died in 1966.", "Electronic mail was invented in 1972.", "Thus, Walt Disney was unable to email his illustrations.", "So the answer is no."], "conf_comments": ["(A)\nEmail was invented", "(A)\nThus, W", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7886922947125079, 0.6379126383053356, 0.4799213517684501, 0.4028019127801915], "conf": [0.9800591793992247, 0.932042886905695, 0.77244791228185, 0.5461615999017786], "prob": [[0.0004401180877214169, 17], [1.106489150348628e-06, 16], [1.1557576946683875e-09, 18], [1.1303487749912562e-09, 6]], "generated": ["Walt Disney passed away on December 15, 1966.", "Email was invented in 1971 by Ray Tomlinson.", "Thus, Walt Disney was not able to send emails to people living far away.", "So the answer is no."], "conf_comments": ["(A)\nEmail was invented", "(A)\nThus, W", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7886922947125079, 0.6379126383053356, 0.4581574358414959, 0.3941411387528944], "conf": [0.9800591793992247, 0.932042886905695, 0.8002243555440411, 0.5949087071645155], "prob": [[0.0004401180877214169, 17], [1.106489150348628e-06, 16], [1.4704583299939092e-09, 13], [1.431085865061588e-09, 6]], "generated": ["Walt Disney passed away on December 15, 1966.", "Email was invented in 1971 by Ray Tomlinson.", "Thus, he was not able to email his illustrations.", "So the answer is no."], "conf_comments": ["(A)\nEmail was invented", "(A)\nThus, W", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.9169316913366745, 0.5860658483587808, 0.4128316266775756, 0.3530654557779705], "conf": [0.9760191577980355, 0.8956352083308801, 0.7200257068760713, 0.5282885042779106], "prob": [[0.05061887438054005, 20], [9.382650140502518e-10, 22], [3.728092946257939e-14, 21], [3.6587689692814116e-14, 6]], "generated": ["Walt Disney was born in 1901 and died in 1966.", "The first email was sent in 1971, so email was unavailable when he was alive.", "Thus, Walt Disney would not have been able to email illustrations to people living far away.", "So the answer is no."], "conf_comments": ["(A)\nEmail was invented", "(A)\nThus, W", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 49.05916118621826}
{"index": 249, "question": "Would a student in eleventh grade be unable to run for president of the United States?", "answer": "yes", "explanation": "Yes. Students in the eleventh grade are typically 16–17 years of age. To serve as president, one must be at least 35 years old.", "generated": [{"finished": true, "length": 4, "score": [0.7226932162760858, 0.5604435745938321, 0.488272589551833, 0.4300335505723687], "conf": [0.9647970577508302, 0.8842430323621956, 0.7346388237285075, 0.5779321237681477], "prob": [[4.671321065560935e-06, 20], [1.0229227771768822e-09, 20], [1.4013080350285916e-10, 22], [1.287643975759947e-10, 6]], "generated": ["A student in eleventh grade is typically between 15 and 17 years old.", "In the United States, one must be at least 35 years old to run for president.", "Thus, a student in eleventh grade would be unable to run for president of the United States.", "So the answer is yes."], "conf_comments": ["(A)\nThe minimum age to", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7226932162760858, 0.5604435745938321, 0.38694487677627293, 0.33884741834406623], "conf": [0.9647970577508302, 0.8842430323621956, 0.7335511722924818, 0.5707244858052061], "prob": [[4.671321065560935e-06, 20], [1.0229227771768822e-09, 20], [9.051725391160743e-15, 21], [8.298809447949097e-15, 6]], "generated": ["A student in eleventh grade is typically between 15 and 17 years old.", "In the United States, one must be at least 35 years old to run for president.", "Thus, a student in eleventh grade would be unable to run as a candidate for president.", "So the answer is yes."], "conf_comments": ["(A)\nThe minimum age to", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7729150361223797, 0.4674050051167586, 0.3949980202793063], "conf": [0.9755882213243723, 0.8277680265843873, 0.6120381206593182], "prob": [[3.36462169349714e-05, 21], [1.6464872172554278e-12, 20], [1.337082280621773e-12, 6]], "generated": ["In order to be president of the United States, you must be at least 35 years old.", "Thus, a person in eleventh grade (16 years old) cannot be president.", "So the answer is yes."], "conf_comments": ["(A)\nThe average age of", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.7226932162760858, 0.3746725299471037, 0.2857720785372952, 0.23572893337358125, 0.14276147594684657, 0.12256692220407137], "conf": [0.9647970577508302, 0.8837125621862554, 0.730054888356981, 0.6063016356506444, 0.4634649835037617, 0.3506481823763855], "prob": [[4.671321065560935e-06, 20], [7.761332819640801e-19, 24], [1.6388593099111116e-20, 11], [1.22865587508762e-21, 13], [1.182846040769552e-28, 22], [1.0114565873636562e-28, 6]], "generated": ["A student in eleventh grade is typically between 15 and 17 years old.", "In comparison, the age requirements in order to run for president of the United States are:\n1) age 35", "2) a natural born citizen of the United States", "3) a resident of the United States for 14 years", "Thus, a student in eleventh grade would be unable to run for president because of these requirements.", "So the answer is yes."], "conf_comments": ["(A)\nThe minimum age to", "(A)\n2) age ", "(A)\n3) a resident", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.7104215041080689, 0.3826996419795713, 0.3205887789685908], "conf": [0.9762778448010654, 0.7974927086228338, 0.5716093761260834], "prob": [[5.033631970330716e-05, 15], [1.4879074124650946e-22, 39], [1.3104586072774626e-22, 6]], "generated": ["The age requirement for a presidential candidate is 35 years old.", "Thus, no one in eleventh grade (the 11th grade is mostly composed of people who are 16 years old) can run for president of the United States.", "So the answer is yes."], "conf_comments": ["(A)\nThe average age of", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 53.59636092185974}
{"index": 250, "question": "Is the best tasting part of the papaya in the center?", "answer": "no", "explanation": "No. The center of a papaya contains a cluster of seeds. Papaya seeds, while edible, are often discarded for the soft flesh that surrounds them.", "generated": [{"finished": true, "length": 5, "score": [0.603752119015914, 0.2944322629454684, 0.1685039564989953, 0.14285418759289714, 0.12307385340691863], "conf": [0.9704107476441274, 0.8773774551786859, 0.7549552981453025, 0.6004687691286281, 0.4478844177119747], "prob": [[1.1134336306338183e-06, 14], [1.2216178018015118e-13, 12], [2.0491188922835594e-17, 9], [3.3075408859123944e-18, 18], [3.2116330671035415e-18, 6]], "generated": ["Papayas are sweetest when they are fully ripe.", "A ripe papaya will turn from yellow to orange.", "The center of the papaya is orange.", "Thus, the best tasting part of the papaya is in the center.", "So the answer is yes."], "conf_comments": ["(A)\nThe center of a", "(A)\nThe center of a", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5656343449494259, 0.31520764553236796, 0.2165066472847794, 0.17385056684890468], "conf": [0.9323812528153419, 0.5973994595999224, 0.40967399319675346, 0.26536907744598887], "prob": [[7.767086061609996e-06, 11], [1.6918810095414824e-11, 18], [4.261636848012467e-14, 16], [4.1453713045220994e-14, 6]], "generated": ["A papaya has soft seeds in the center.", "So the best tasting part of the papaya will not be in the center.", "Thus, the best part of the papaya is not in the center.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.5900278538893771, 0.39344160509292087], "conf": [0.8924971106723094, 0.6438834135790967], "prob": [[6.6491203110695e-09, 20], [3.6446422455137937e-10, 6]], "generated": ["The papaya is a melon and the best part of the melon is in the center.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5833165256449367, 0.2908365859210006, 0.1884803415963231, 0.15130520144544848], "conf": [0.9370503745739427, 0.7219102104584187, 0.49523407231938393, 0.32544152452390174], "prob": [[6.928491846130394e-07, 14], [3.9350252075748384e-21, 29], [5.744713049766808e-25, 18], [5.109133149992814e-25, 6]], "generated": ["The center of a papaya is usually soft and mushy.", "Papaya also has a lot of seeds, which may contribute to it not tasting as good as other parts of the fruit.", "Thus, the center of a papaya may not be the best part to eat.", "So the answer is no."], "conf_comments": ["(A)\nThe center of a", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5656343449494259, 0.31520764553236796, 0.23315761320256737], "conf": [0.9323812528153419, 0.5973994595999224, 0.40431389817840463], "prob": [[7.767086061609996e-06, 11], [1.6918810095414824e-11, 18], [4.723686215254784e-12, 6]], "generated": ["A papaya has soft seeds in the center.", "So the best tasting part of the papaya will not be in the center.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 47.755648612976074}
{"index": 251, "question": "Is radioactive waste a plot device for many shows?", "answer": "yes", "explanation": "Yes. Radioactive isotopes in an ooze-like waste cause turtles to become the Teenage Mutant Ninja Turtles. In the Fox animated hit, Family Guy, radioactive waste is used to turn give the main characters superpowers.  The superhero 'Daredevil' encounters radioactive waste that blinds him as a child and gives him super powers.", "generated": [{"finished": false, "length": 16, "score": [0.5372354463863154, 0.4453921600954112, 0.3582210953072282, 0.2814931740641323, 0.14698590389744204, 0.11257955315261722, 0.08934816108084799, 0.06877362432457773, 0.053467525960379236, 0.042325554119835844, 0.03442081869046104, 0.028790504362610148, 0.023910515681559247, 0.020007852564365245, 0.016609944036323706, 0.013786481691990341], "conf": [0.9509257887942024, 0.8349435363670005, 0.7175225716220252, 0.5968136607022668, 0.4457183794880485, 0.35741830185019247, 0.2791915314709707, 0.21365920180167816, 0.15984819041675616, 0.11396880197411383, 0.0796033853880276, 0.05626582223306971, 0.04013093514279925, 0.028242714625414108, 0.019766069792272967, 0.013657090660460638], "prob": [[4.4020387867814824e-11, 20], [4.196770802274757e-13, 19], [1.077279336743788e-15, 21], [8.46162809450058e-19, 24], [2.308669154678924e-25, 15], [2.3817325049350047e-28, 22], [2.5939079863086953e-30, 21], [9.305196436625181e-33, 22], [1.3053596213555273e-34, 20], [1.1238818842623913e-35, 19], [3.571435671756415e-36, 21], [2.791742649210205e-36, 24], [1.688675949256782e-36, 15], [1.5102763378242497e-36, 22], [1.0934723550779175e-36, 21], [1.0254795400702028e-36, 22]], "generated": ["For the show, The Simpsons, radioactive waste is one of the many plot devices.", "For the show, Futurama, radioactive waste is one of the many plot devices.", "For the movie, The Simpsons Movie, radioactive waste is one of the many plot devices.", "For the show, The Amazing World of Gumball, radioactive waste is one of the many plot devices.", "For the show, The Grim Adventures of Billy & Mandy.", "For the show, Rocko's Modern Life, radioactive waste is one of the many plot devices.", "For the movie, The Simpsons Movie, radioactive waste is one of the many plot devices.", "For the show, Ed, Edd n Eddy, radioactive waste is one of the many plot devices.", "For the show, The Simpsons, radioactive waste is one of the many plot devices.", "For the show, Futurama, radioactive waste is one of the many plot devices.", "For the movie, The Simpsons Movie, radioactive waste is one of the many plot devices.", "For the show, The Amazing World of Gumball, radioactive waste is one of the many plot devices.", "For the show, The Grim Adventures of Billy & Mandy.", "For the show, Rocko's Modern Life, radioactive waste is one of the many plot devices.", "For the movie, The Simpsons Movie, radioactive waste is one of the many plot devices.", "For the show, Ed, Edd n Eddy, radioactive waste is one of the many plot devices.", "Therefore, radioactive waste is a plot device for many shows.", "So the answer is yes."], "conf_comments": ["(A)\nFor the show,", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, the"], "info": [[[0.9378193562078031, 22], [0.00065780435152668, 14], [0.9152740576026944, 6]], 0.6909360739887463, "(A)\nThus, the", 16]}, {"finished": true, "length": 8, "score": [0.5372354463863154, 0.4453921600954112, 0.3582210953072282, 0.2814931740641323, 0.18021962029258795, 0.1414275125750471, 0.11867360381621042, 0.09959975583011574], "conf": [0.9509257887942024, 0.8349435363670005, 0.7175225716220252, 0.5968136607022668, 0.48869947665908725, 0.39439536839948053, 0.32686693416380835, 0.23074861625841925], "prob": [[4.4020387867814824e-11, 20], [4.196770802274757e-13, 19], [1.077279336743788e-15, 21], [8.46162809450058e-19, 24], [8.255809251238616e-25, 20], [3.700277679499309e-27, 20], [3.776175142082086e-28, 14], [3.726431545597619e-28, 6]], "generated": ["For the show, The Simpsons, radioactive waste is one of the many plot devices.", "For the show, Futurama, radioactive waste is one of the many plot devices.", "For the movie, The Simpsons Movie, radioactive waste is one of the many plot devices.", "For the show, The Amazing World of Gumball, radioactive waste is one of the many plot devices.", "For the movie, Radioactive Monster, radioactive waste is one of the many plot devices.", "For the show, The Big Bang Theory, radioactive waste is one of the many plot devices.", "Thus, radioactive waste is a plot device for many shows.", "So the answer is yes."], "conf_comments": ["(A)\nFor the show,", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.5372354463863154, 0.4453921600954112, 0.3582210953072282, 0.2814931740641323, 0.18021962029258795, 0.15366840442436858, 0.12989719499832006], "conf": [0.9509257887942024, 0.8349435363670005, 0.7175225716220252, 0.5968136607022668, 0.48869947665908725, 0.40788059619311834, 0.2920341048471478], "prob": [[4.4020387867814824e-11, 20], [4.196770802274757e-13, 19], [1.077279336743788e-15, 21], [8.46162809450058e-19, 24], [8.255809251238616e-25, 20], [1.196171838437833e-25, 14], [1.1818781443173549e-25, 6]], "generated": ["For the show, The Simpsons, radioactive waste is one of the many plot devices.", "For the show, Futurama, radioactive waste is one of the many plot devices.", "For the movie, The Simpsons Movie, radioactive waste is one of the many plot devices.", "For the show, The Amazing World of Gumball, radioactive waste is one of the many plot devices.", "For the movie, Radioactive Monster, radioactive waste is one of the many plot devices.", "Thus, radioactive waste is a plot device for many shows.", "So the answer is yes."], "conf_comments": ["(A)\nFor the show,", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 16, "score": [0.5372354463863154, 0.4453921600954112, 0.3582210953072282, 0.2814931740641323, 0.14698590389744204, 0.11257955315261722, 0.08934816108084799, 0.06651715255990107, 0.04896865444095824, 0.0394055723543116, 0.030434538703642666, 0.024707653842641716, 0.01975316806593323, 0.015935603885244665, 0.011508692373550466, 0.008535354192476977], "conf": [0.9509257887942024, 0.8349435363670005, 0.7175225716220252, 0.5968136607022668, 0.4457183794880485, 0.35741830185019247, 0.2791915314709707, 0.21523127015577492, 0.16220226148918326, 0.12091526886474264, 0.0876830589152147, 0.06295755798199029, 0.04416008189862426, 0.03038663546342017, 0.021823793126348877, 0.015246127420443433], "prob": [[4.4020387867814824e-11, 20], [4.196770802274757e-13, 19], [1.077279336743788e-15, 21], [8.46162809450058e-19, 24], [2.308669154678924e-25, 15], [2.3817325049350047e-28, 22], [2.5939079863086953e-30, 21], [4.9104391324304445e-33, 19], [6.686409534868021e-37, 27], [3.476495108080917e-38, 21], [4.733488039438085e-40, 22], [7.832969211241303e-41, 21], [1.1120518347662338e-41, 21], [3.452642293036727e-42, 21], [1.0898402029311078e-44, 18], [4.457432409165968e-47, 23]], "generated": ["For the show, The Simpsons, radioactive waste is one of the many plot devices.", "For the show, Futurama, radioactive waste is one of the many plot devices.", "For the movie, The Simpsons Movie, radioactive waste is one of the many plot devices.", "For the show, The Amazing World of Gumball, radioactive waste is one of the many plot devices.", "For the show, The Grim Adventures of Billy & Mandy.", "For the show, Rocko's Modern Life, radioactive waste is one of the many plot devices.", "For the movie, The Simpsons Movie, radioactive waste is one of the many plot devices.", "For the show, Duckman, radioactive waste is one of the many plot devices.", "For the show, The Wacky Adventures of Ronald McDonald, radioactive waste is one of the many plot devices.", "For the movie, The Simpsons Movie, radioactive waste is one of the many plot devices.", "For the show, Rocko's Modern Life, radioactive waste is one of the many plot devices.", "For the movie, The Simpsons Movie, radioactive waste is one of the many plot devices.", "For the movie, The Simpsons Movie, radioactive waste is one of the many plot devices.", "For the movie, The Simpsons Movie, radioactive waste is one of the many plot devices.", "For the show, South Park, radioactive waste is one of the many plot devices.", "For the show, Aqua Teen Hunger Force, radioactive waste is one of the many plot devices.", "For the show, South Park, radioactive waste is one of the many plot devices.", "For the show, The Simps"], "conf_comments": ["(A)\nFor the show,", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, the"], "info": [[[0.004089987134974261, 23], [0.050479594151833944, 18], [0.06732212482804768, 7]], 0.6986011703912313, "(A)\nThus, the", 16]}, {"finished": true, "length": 13, "score": [0.5372354463863154, 0.4453921600954112, 0.3582210953072282, 0.2814931740641323, 0.14698590389744204, 0.11257955315261722, 0.08934816108084799, 0.06651715255990107, 0.04896865444095824, 0.0394055723543116, 0.03138789046108307, 0.025211987174058564, 0.02073183130146057], "conf": [0.9509257887942024, 0.8349435363670005, 0.7175225716220252, 0.5968136607022668, 0.4457183794880485, 0.35741830185019247, 0.2791915314709707, 0.21523127015577492, 0.16220226148918326, 0.12091526886474264, 0.08576128944615, 0.06693805675188036, 0.0454388305763274], "prob": [[4.4020387867814824e-11, 20], [4.196770802274757e-13, 19], [1.077279336743788e-15, 21], [8.46162809450058e-19, 24], [2.308669154678924e-25, 15], [2.3817325049350047e-28, 22], [2.5939079863086953e-30, 21], [4.9104391324304445e-33, 19], [6.686409534868021e-37, 27], [3.476495108080917e-38, 21], [3.347471249102148e-39, 21], [2.3282534629342453e-40, 14], [2.274432488531208e-40, 6]], "generated": ["For the show, The Simpsons, radioactive waste is one of the many plot devices.", "For the show, Futurama, radioactive waste is one of the many plot devices.", "For the movie, The Simpsons Movie, radioactive waste is one of the many plot devices.", "For the show, The Amazing World of Gumball, radioactive waste is one of the many plot devices.", "For the show, The Grim Adventures of Billy & Mandy.", "For the show, Rocko's Modern Life, radioactive waste is one of the many plot devices.", "For the movie, The Simpsons Movie, radioactive waste is one of the many plot devices.", "For the show, Duckman, radioactive waste is one of the many plot devices.", "For the show, The Wacky Adventures of Ronald McDonald, radioactive waste is one of the many plot devices.", "For the movie, The Simpsons Movie, radioactive waste is one of the many plot devices.", "For the movie, The Simpsons Movie, radioactive waste is one of the many plot devices.", "Thus, radioactive waste is a plot device for many shows.", "So the answer is yes."], "conf_comments": ["(A)\nFor the show,", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nThus, radio", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": [], "9": [], "10": [], "11": [], "12": [], "13": [], "14": [], "15": [], "16": []}, "run_time": 165.23924136161804}
{"index": 252, "question": "Were any members of Canidae in Aesop's Fables?", "answer": "yes", "explanation": "Yes. Canidae is a family of mammals that includes dogs, foxes, and coyotes. Aesop's Fables was a collection of stories with animals as the main characters. One of the most famous stories involves a fox and a lion.", "generated": [{"finished": true, "length": 3, "score": [0.5625760893297089, 0.43041623670212453, 0.3497007670908557], "conf": [0.9388856412816539, 0.696751108046592, 0.461798771473115], "prob": [[2.7796971172296962e-08, 16], [2.4154225065841557e-10, 20], [2.357424243875342e-10, 6]], "generated": ["There are not any known canids in Aesop's fables.", "Thus, none of the members of Canidae were in Aesop's Fables.", "So the answer is no."], "conf_comments": ["(A)\nCanidae is the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.4746885563164236, 0.29757404035510227, 0.24054277545434039], "conf": [0.861987504929147, 0.6591076245140454, 0.4329467555178581], "prob": [[1.2652607917507243e-17, 29], [4.070115599324323e-23, 19], [3.9437114621132375e-23, 6]], "generated": ["According to wiki, there is a fable named “The Wolf and the Lamb,” which features a wolf and a goat.", "Thus, none of members of Canidae were in Aesop's Fables.", "So the answer is no."], "conf_comments": ["(A)\nCanidae is a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5625760893297089, 0.2671830994150503, 0.170523733841332, 0.13934896990924583], "conf": [0.9388856412816539, 0.756146961519281, 0.5545619693921777, 0.3717931445693508], "prob": [[2.7796971172296962e-08, 16], [3.1240064191637895e-18, 18], [1.3535562441604012e-23, 21], [1.3218911370145628e-23, 6]], "generated": ["There are not any known canids in Aesop's fables.", "If a species is not mentioned in a story, then it is not in that story.", "Thus, no members of the Canidae family can be in Aesop's Fables.", "So the answer is no."], "conf_comments": ["(A)\nCanidae is the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6266857552895897, 0.32510299128241865, 0.17573140837458856, 0.0855706389145312, 0.06623506918014391], "conf": [0.9761641763121802, 0.8835587155475156, 0.7007735756375187, 0.46385406177843064, 0.2826520679047357], "prob": [[1.1712556816942887e-06, 15], [1.8800906786585983e-12, 11], [7.971243596144873e-20, 17], [9.648694401907386e-29, 20], [8.717708978444663e-29, 6]], "generated": ["Aesop lived 620-560 BCE.", "Canidae was created in 1902.", "Thus, both Aesop and the Canidae were created before each other.", "We can assume that no one from the Canidae was in Aesop's Fables.", "So the answer is no."], "conf_comments": ["(A)\nCanidae was founded", "(A)\nThus, A", "(A)\nSo the answer is", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6266857552895897, 0.32510299128241865, 0.15375407172933953], "conf": [0.9761641763121802, 0.8835587155475156, 0.5730636656944986], "prob": [[1.1712556816942887e-06, 15], [1.8800906786585983e-12, 11], [4.365033912509449e-23, 23]], "generated": ["Aesop lived 620-560 BCE.", "Canidae was created in 1902.", "Thus, the answer is no, no one on Aesop’s Fables was a Canidae."], "conf_comments": ["(A)\nCanidae was founded", "(A)\nThus, A", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 45.828272581100464}
{"index": 253, "question": "Can vitamin C rich fruits be bad for health?", "answer": "yes", "explanation": "Yes. Oranges are fruits that are rich in vitamin C. Oranges are very acidic fruits that can wear down tooth enamel. Too much Vitamin C can cause nausea and diarrhea.", "generated": [{"finished": true, "length": 4, "score": [0.5402605683409472, 0.30045561016845856, 0.2171666884887566, 0.17355727197324108], "conf": [0.9266435021770458, 0.6955759610675677, 0.4220733659678992, 0.2709867691135436], "prob": [[0.0009767044019858896, 6], [2.337920316542541e-08, 12], [2.4749458109854515e-09, 15], [2.3988336541505462e-09, 6]], "generated": ["Fruits are healthy.", "Fruits contain vitamin C, which is healthy.", "Thus, vitamin C rich fruits cannot be bad for health.", "So the answer is no."], "conf_comments": ["(A)\nVitamin C", "(A)\nThus, vit", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5857439908169328, 0.30133213741688936, 0.20361357465746938, 0.1659940551302853], "conf": [0.9465736920538979, 0.7741962738396702, 0.5526856591534434, 0.37089715522055255], "prob": [[3.4737022880638627e-12, 26], [6.609866129980378e-26, 28], [8.102595672613109e-29, 15], [7.645431513875371e-29, 6]], "generated": ["Vitamin C promotes collagen production, strengthens the immune system, and improves the health of the skin.", "Also, most vitamin C rich fruits are nutrient dense and provide many other nutrients in addition to Vitamin C.", "Thus, vitamin C rich fruits are good for our health.", "So the answer is no."], "conf_comments": ["(A)\nThus, vit", "(A)\nThus, vit", "(A)\nHowever, some vit", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6615965634705754, 0.30297298577576154, 0.18714379684023957, 0.15005086137869897], "conf": [0.9481022410738571, 0.7448982768993483, 0.4497867155338356, 0.2910722367438698], "prob": [[9.375095470356143e-05, 12], [4.593572691487886e-11, 11], [1.8746425275432008e-14, 17], [1.8018149673264454e-14, 6]], "generated": ["Vitamin c is an antioxidant.", "Vitamin c has many benefits for the body.", "Thus, vitamin C rich fruits can't be bad for health.", "So the answer is no."], "conf_comments": ["(A)\nAntioxid", "(A)\nThus, vit", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5857439908169328, 0.2847186887049183, 0.2007529718426248], "conf": [0.9465736920538979, 0.592575345185704, 0.3730526558184976], "prob": [[3.4737022880638627e-12, 26], [1.30354124188439e-23, 27], [3.161677198623291e-24, 6]], "generated": ["Vitamin C promotes collagen production, strengthens the immune system, and improves the health of the skin.", "Based on this, vitamin C rich fruits could only be bad for the health if you are allergic to them.", "So the answer is no."], "conf_comments": ["(A)\nThus, vit", "(A)\nThus, vit", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6615965634705754, 0.3147176398871275, 0.1721125583451986, 0.10882648196503544], "conf": [0.9481022410738571, 0.8078261747874237, 0.5665777246326774, 0.3699460663714871], "prob": [[9.375095470356143e-05, 12], [7.586255916992566e-17, 21], [3.2778264378616375e-27, 28], [1.7273359187681485e-28, 6]], "generated": ["Vitamin c is an antioxidant.", "An antioxidant is what can fight free radicals, which are dangerous to the body.", "If antioxidants fight free radicals, then having more antioxidants in the body should be good for health.", "So the answer is no."], "conf_comments": ["(A)\nAntioxid", "(A)\nThus, vit", "(A)\nThus, vit", "(A)\n# Is the above"], "info": null}, {"finished": false, "length": 4, "score": [0.6615965634705754, 0.3147176398871275, 0.1721125583451986, 0.08932863869758857], "conf": [0.9481022410738571, 0.8078261747874237, 0.5665777246326774, 0.3332401501964017], "prob": [[9.375095470356143e-05, 12], [7.586255916992566e-17, 21], [3.2778264378616375e-27, 28], [1.0890506823427905e-35, 25]], "generated": ["Vitamin c is an antioxidant.", "An antioxidant is what can fight free radicals, which are dangerous to the body.", "If antioxidants fight free radicals, then having more antioxidants in the body should be good for health.", "Since vitamin C is an antioxidant, having more of it can’t be bad for your health.", "So the answer is no."], "conf_comments": ["(A)\nAntioxid", "(A)\nThus, vit", "(A)\nThus, vit", "(A)\nThus, vit"], "info": [[[3.3224781817711397e-09, 25], [0.6806429345137035, 6]], 0.5881631693382356, "(A)\nThus, vit", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 51.09465479850769}
{"index": 254, "question": "Were all the materials to make a cannon known during the bronze age?", "answer": "no", "explanation": "No. The Bronze Age happened from about 3300 BC to 300 BC. Cannons require a fuse, gunpowder, and iron or other material to house the chemicals. Gunpowder was discovered around the 9th century AD.", "generated": [{"finished": true, "length": 4, "score": [0.6316967122358331, 0.35888171736052427, 0.31564222349810284, 0.26974487649725926], "conf": [0.9713803714034278, 0.7221102751465786, 0.6115704390535641, 0.4480623505066222], "prob": [[6.577283382936898e-07, 16], [3.9832577171073436e-22, 42], [6.503237000879198e-23, 20], [6.380799535723907e-23, 6]], "generated": ["The Bronze Age started sometime around 3300 BCE.", "Sometime between 1327 BCE and 1046 BCE, cannons were invented in China, so cannons were not known during the Bronze Age.", "Thus, all the materials to make a cannon were not known during the Bronze Age.", "So the answer is no."], "conf_comments": ["(A)\nThe first cannon", "(A)\nThus, all", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6316967122358331, 0.36819128195402456, 0.27188537960296005, 0.23335691042466697], "conf": [0.9713803714034278, 0.8149110458378633, 0.6549312451306034, 0.48490366199258983], "prob": [[6.577283382936898e-07, 16], [1.5171973946771685e-15, 22], [6.483747795436672e-19, 20], [6.290502390261353e-19, 6]], "generated": ["The Bronze Age started sometime around 3300 BCE.", "The earliest cannons date from somewhere between 1280 and 1320 CE.", "Thus, the materials to make a cannon could not have been known during the bronze age.", "So the answer is no."], "conf_comments": ["(A)\nThe first cannon", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.527638707557693, 0.39403741599797387, 0.2668594584193416, 0.2252199160767521], "conf": [0.9760895906169058, 0.8154882236585365, 0.685937028481125, 0.4978838341189969], "prob": [[2.3581455213414025e-08, 14], [1.0903585073780031e-11, 19], [4.108447118278614e-15, 13], [3.668642051000049e-15, 6]], "generated": ["Canons were generally invented around the 13th century.", "The bronze age was around 3300 BC to 1200 BC.", "Thus, there was no cannon during the bronze age.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.527638707557693, 0.39403741599797387, 0.21399443434598067, 0.17406407833443097], "conf": [0.9760895906169058, 0.8154882236585365, 0.6790904636853677, 0.49611033767878104], "prob": [[2.3581455213414025e-08, 14], [1.0903585073780031e-11, 19], [2.604545429678144e-24, 28], [1.4371701473603663e-24, 6]], "generated": ["Canons were generally invented around the 13th century.", "The bronze age was around 3300 BC to 1200 BC.", "There was a 2100-year time difference, so it is impossible for all materials to make a cannon be known.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, the", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.527638707557693, 0.39403741599797387, 0.21399443434598067, 0.16685801002594886], "conf": [0.9760895906169058, 0.8154882236585365, 0.6790904636853677, 0.5384423003689098], "prob": [[2.3581455213414025e-08, 14], [1.0903585073780031e-11, 19], [2.604545429678144e-24, 28], [3.1128122683955007e-25, 8]], "generated": ["Canons were generally invented around the 13th century.", "The bronze age was around 3300 BC to 1200 BC.", "There was a 2100-year time difference, so it is impossible for all materials to make a cannon be known.", "Thus, the answer is no."], "conf_comments": ["(A)\nIn comparison, the", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.667310555224197, 0.32038189391127364, 0.25845015579409686], "conf": [0.9616790985837492, 0.8023264974782203, 0.5766418239631618], "prob": [[2.0388894053818757e-08, 23], [5.714234072428603e-28, 35], [3.1487197913694676e-28, 6]], "generated": ["During the Bronze Age, people discovered how to make bronze by smelting copper and tin together.", "Since materials needed to make cannons are metal (which includes steel, iron, and bronze), the materials to make a cannon were discovered during the Bronze Age.", "So the answer is yes."], "conf_comments": ["(A)\nBronze is", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": false, "length": 4, "score": [0.527638707557693, 0.39403741599797387, 0.2668594584193416, 0.15561820442661445], "conf": [0.9760895906169058, 0.8154882236585365, 0.685937028481125, 0.5632832683490201], "prob": [[2.3581455213414025e-08, 14], [1.0903585073780031e-11, 19], [4.108447118278614e-15, 13], [2.1818486543512014e-22, 19]], "generated": ["Canons were generally invented around the 13th century.", "The bronze age was around 3300 BC to 1200 BC.", "Thus, there was no cannon during the bronze age.", "In other words, the materials to make a cannon were not known in the bronze age.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[5.310640715427701e-08, 19], [0.9405961484676669, 6]], 0.8211880172095418, "(A)\nSo the answer is", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 51.43669605255127}
{"index": 255, "question": "Is the most recent Democrat President in the US known for his painting practice?", "answer": "no", "explanation": "No. George W. Bush is a former Republican President of the US. George W. Bush posts photos of his paintings online. Barack Obama succeeded George W. Bush as a Democrat President. Barack Obama doesn't post photos of paintings he has made.", "generated": [{"finished": true, "length": 5, "score": [0.8497986527904283, 0.5076645877933408, 0.38522119963717716, 0.33830020463378363, 0.2972417875430205], "conf": [0.9675343929910525, 0.8697321407886826, 0.7059807232565768, 0.5781613901621401, 0.4470775167436685], "prob": [[0.012430396157849853, 15], [1.1602427735067006e-12, 25], [6.721068153687804e-15, 15], [2.148170571636167e-15, 19], [2.126951295445085e-15, 6]], "generated": ["The most recent Democrat President in the US was Barack Obama.", "According to Business Insider, Barack Obama painted 65 paintings over the course of 8 years.", "This can indicate that Barack Obama is known for his painting practice.", "Thus, the most recent Democrat President in the US is known for his painting practice.", "So the answer is yes."], "conf_comments": ["(A)\nBarack Obama", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7157660542582357, 0.506531184558512, 0.4101775695380516, 0.3487951644396632], "conf": [0.9438131535456807, 0.7991195488455364, 0.6142943540036584, 0.44561647056447434], "prob": [[5.6819671616669586e-05, 16], [7.543178456688806e-09, 17], [5.929541428080759e-10, 16], [5.816951181217526e-10, 6]], "generated": ["Recent Democrat Presidents include Barack Obama and Bill Clinton.", "Both Bill Clinton and Barack Obama are known for their painting practices.", "Thus, the most recent Democrat President is known for his painting practice.", "So the answer is yes."], "conf_comments": ["(A)\nBoth Obama", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7157660542582357, 0.506531184558512, 0.36356771029769797, 0.30739177759023945], "conf": [0.9438131535456807, 0.7991195488455364, 0.600822094760217, 0.43075813752934045], "prob": [[5.6819671616669586e-05, 16], [7.543178456688806e-09, 17], [3.7939508679802225e-11, 14], [3.72777272045196e-11, 6]], "generated": ["Recent Democrat Presidents include Barack Obama and Bill Clinton.", "Both Bill Clinton and Barack Obama are known for their painting practices.", "Thus, a Democrat President is known for his painting practice.", "So the answer is yes."], "conf_comments": ["(A)\nBoth Obama", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8497986527904283, 0.5076645877933408, 0.38522119963717716, 0.28828926008474615], "conf": [0.9675343929910525, 0.8697321407886826, 0.7059807232565768, 0.5341766657012511], "prob": [[0.012430396157849853, 15], [1.1602427735067006e-12, 25], [6.721068153687804e-15, 15], [1.1053639293113256e-15, 6]], "generated": ["The most recent Democrat President in the US was Barack Obama.", "According to Business Insider, Barack Obama painted 65 paintings over the course of 8 years.", "This can indicate that Barack Obama is known for his painting practice.", "So the answer is yes."], "conf_comments": ["(A)\nBarack Obama", "(A)\nThus, the", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8976573967113878, 0.38276110504364025, 0.31768827076827005, 0.2791759021648492], "conf": [0.966012236223936, 0.8623379701547914, 0.7099708782594311, 0.5498492027623715], "prob": [[0.06585352119934912, 15], [5.775782677103852e-13, 16], [4.7619315597021886e-14, 14], [4.68042975843839e-14, 6]], "generated": ["The most recent Democrat President in the US is Barack Obama.", "While he was a Senator, he famously painted his own official portrait.", "Thus, Barack Obama is known for his painting practice.", "So the answer is yes."], "conf_comments": ["(A)\nBarack Obama", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 54.11836886405945}
{"index": 256, "question": "Would an environmentalist advocate for preventing domestic canine reproduction?", "answer": "yes", "explanation": "Yes. Domestic dogs are a large contributor to species depopulation and displacement. Domestic dogs have a diet that largely contributes to a harmful environmental impact. ", "generated": [{"finished": true, "length": 4, "score": [0.4365066573703811, 0.22608923958091853, 0.19189599725064077, 0.16896404002373036], "conf": [0.9373376094152129, 0.8115241978976635, 0.6579607827100847, 0.5116738442952334], "prob": [[4.2305771214316286e-20, 28], [1.594721965835154e-44, 48], [1.6884246540469384e-45, 19], [1.6575355974523928e-45, 6]], "generated": ["According to one survey, environmentalists are among the least likely of people to advocate that dogs should be prevented from breeding.", "(see https://www.statista.com/chart/3555/which-u-s-group-is-most-likely-to-advocate-for-pets-from-breeding/).", "Thus, an environmentalist would not advocate for preventing domestic canine reproduction.", "So the answer is no."], "conf_comments": ["(A)\nThus, an", "(A)\nThus, an", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.4365066573703811, 0.22608923958091853, 0.13518631478124377, 0.11489724576570996], "conf": [0.9373376094152129, 0.8115241978976635, 0.6390520091335766, 0.4767765099568734], "prob": [[4.2305771214316286e-20, 28], [1.594721965835154e-44, 48], [4.8644401746107026e-51, 19], [4.0075934735883145e-51, 6]], "generated": ["According to one survey, environmentalists are among the least likely of people to advocate that dogs should be prevented from breeding.", "(see https://www.statista.com/chart/3555/which-u-s-group-is-most-likely-to-advocate-for-pets-from-breeding/).", "This can suggest that an environmentalist would not advocate for preventing domestic dog reproduction.", "So the answer is no."], "conf_comments": ["(A)\nThus, an", "(A)\nThus, an", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.43766832854398363, 0.24095634014925515, 0.18827474667358], "conf": [0.8463948177149802, 0.5743985012018862, 0.3778092545207415], "prob": [[1.440444869710308e-15, 23], [2.2750423463740907e-25, 28], [1.455051239457431e-25, 6]], "generated": ["Most people consider canines as pets and domestic pets would not qualify as endangered species.", "Therefore, it is unlikely for an animal rights activist to advocate for the prevention of the reproduction of domestic canines.", "So the answer is no."], "conf_comments": ["(A)\nThus, an", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.4339857736703842, 0.23722552164137017, 0.1859153671203614], "conf": [0.9371421797260787, 0.7438679441469372, 0.5321137528452777], "prob": [[1.42388042576876e-12, 17], [4.646254888311505e-21, 20], [2.9887330794953937e-22, 18]], "generated": ["Environmentalists advocate for stopping domesticated animals from being over-populated.", "The population of domesticated animals such as dogs can be controlled through spaying and neutering.", "Thus, an environmentalist would advocate for preventing domestic canine reproduction."], "conf_comments": ["(A)\nDomestic canines", "(A)\nThus, an", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.43766832854398363, 0.24095634014925515, 0.16885524817706418], "conf": [0.8463948177149802, 0.5743985012018862, 0.4204311594533987], "prob": [[1.440444869710308e-15, 23], [2.2750423463740907e-25, 28], [9.340196012255095e-27, 8]], "generated": ["Most people consider canines as pets and domestic pets would not qualify as endangered species.", "Therefore, it is unlikely for an animal rights activist to advocate for the prevention of the reproduction of domestic canines.", "Thus, the answer is no."], "conf_comments": ["(A)\nThus, an", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 47.00335359573364}
{"index": 257, "question": "Does Iphone have more iterations than Samsung Galaxy?", "answer": "no", "explanation": "No. As of 2020 the latest Iphone is Iphone 11. As of 2020 the latest Samsung Galaxy phone is the Samsung Galaxy S20.", "generated": [{"finished": true, "length": 4, "score": [0.5293605345888556, 0.39861208866228826, 0.33501614748683284, 0.28454956531072895], "conf": [0.9466607524084211, 0.7742529082138662, 0.6289186744757902, 0.4555850426296154], "prob": [[1.3780419887907939e-16, 30], [8.165695284266524e-19, 14], [1.154669866643962e-19, 14], [1.1264560898596503e-19, 6]], "generated": ["As of 2021-05-13, the Samsung Galaxy lineup of phones has 12 models.", "The Iphone lineup of phones has 15 models.", "Thus, Iphone has more iterations than Samsung Galaxy.", "So the answer is yes."], "conf_comments": ["(A)\nAs of 2", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5991447460719983, 0.4999943963123931, 0.34668331338252867, 0.2819326569993086], "conf": [0.9647916707915634, 0.8112029130288333, 0.6131254730472252, 0.40901630906479913], "prob": [[9.74584527737774e-07, 14], [5.772735005838348e-08, 15], [1.0670401880789134e-11, 19], [1.0129372232331964e-11, 6]], "generated": ["The total number of iterations of the Iphone is 12.", "The total number of iterations of the Samsung Galaxy is 9.", "Thus, the total number of iterations of Iphone is greater than Samsung Galaxy.", "So the answer is yes."], "conf_comments": ["(A)\nThe total number of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6891838132987492, 0.4360405895702529, 0.35783166502637603], "conf": [0.9320733135253608, 0.7644262380960603, 0.563677323030128], "prob": [[1.2443476160732542e-08, 27], [1.290959796160052e-13, 16], [7.491435804928514e-14, 6]], "generated": ["The original iPhone was released in 2007, while the original Samsung Galaxy was in 2009.", "So there have been more iterations of the iPhone than the Samsung Galaxy.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5293605345888556, 0.39861208866228826, 0.2949626517660223, 0.2457666561106109], "conf": [0.9466607524084211, 0.7742529082138662, 0.612008159654155, 0.42918095378364385], "prob": [[1.3780419887907939e-16, 30], [8.165695284266524e-19, 14], [3.6620741375393344e-22, 21], [3.447457666010554e-22, 6]], "generated": ["As of 2021-05-13, the Samsung Galaxy lineup of phones has 12 models.", "The Iphone lineup of phones has 15 models.", "Thus, the number of Iphone models is greater than the number of Samsung Galaxy models.", "So the answer is yes."], "conf_comments": ["(A)\nAs of 2", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5991447460719983, 0.4999943963123931, 0.3084760753973341, 0.2183189204031481], "conf": [0.9647916707915634, 0.8112029130288333, 0.6307130986765739, 0.4713170155889065], "prob": [[9.74584527737774e-07, 14], [5.772735005838348e-08, 15], [7.375276181062345e-14, 19], [3.005012965104901e-15, 8]], "generated": ["The total number of iterations of the Iphone is 12.", "The total number of iterations of the Samsung Galaxy is 9.", "This means the number of iterations for Iphone is greater than that for Samsung Galaxy.", "Thus, the answer is yes."], "conf_comments": ["(A)\nThe total number of", "(A)\nThus, the", "(A)\nThus, I", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5293605345888556, 0.39861208866228826, 0.2949626517660223, 0.16710634381756936], "conf": [0.9466607524084211, 0.7742529082138662, 0.612008159654155, 0.36819231944862435], "prob": [[1.3780419887907939e-16, 30], [8.165695284266524e-19, 14], [3.6620741375393344e-22, 21], [8.443703151458748e-24, 6]], "generated": ["As of 2021-05-13, the Samsung Galaxy lineup of phones has 12 models.", "The Iphone lineup of phones has 15 models.", "Thus, the number of Iphone models is greater than the number of Samsung Galaxy models.", "So the answer is no."], "conf_comments": ["(A)\nAs of 2", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 4, "score": [0.5991447460719983, 0.4999943963123931, 0.3084760753973341, 0.11145373388596307], "conf": [0.9647916707915634, 0.8112029130288333, 0.6307130986765739, 0.4202386822820401], "prob": [[9.74584527737774e-07, 14], [5.772735005838348e-08, 15], [7.375276181062345e-14, 19], [5.1217715981228445e-28, 20]], "generated": ["The total number of iterations of the Iphone is 12.", "The total number of iterations of the Samsung Galaxy is 9.", "This means the number of iterations for Iphone is greater than that for Samsung Galaxy.", "Thus, the Iphone is not equivalent to Samsung Galaxy or they have more iterations.", "So the answer is yes."], "conf_comments": ["(A)\nThe total number of", "(A)\nThus, the", "(A)\nThus, I", "(A)\nSo the answer is"], "info": [[[6.944514988162922e-15, 20], [0.82456714579952, 6]], 0.666291350479049, "(A)\nSo the answer is", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 50.71016025543213}
{"index": 258, "question": "Does the central processing unit usually have a dedicated fan?", "answer": "yes", "explanation": "Yes. The CPU is the main chip on a computer's board, and generates a lot of heat. Computer manufacturers generally include a dedicated cooling system over the CPU in addition to the main board fans.", "generated": [{"finished": true, "length": 3, "score": [0.5057602247256339, 0.38965336887461466, 0.3159977219057652], "conf": [0.876066196583715, 0.6273162958069126, 0.4173313250345082], "prob": [[6.945379403639161e-11, 19], [4.163553995974015e-12, 15], [3.8865468520122e-12, 6]], "generated": ["A CPU is an integrated circuit, so the central processing unit usually cannot have a dedicated fan.", "Thus, the central processing unit usually does not have a dedicated fan.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5057602247256339, 0.34796808876228064, 0.2858294712025481], "conf": [0.876066196583715, 0.6440850135084256, 0.4377572403733286], "prob": [[6.945379403639161e-11, 19], [2.2692265503288544e-13, 13], [2.1724413005149448e-13, 6]], "generated": ["A CPU is an integrated circuit, so the central processing unit usually cannot have a dedicated fan.", "Thus, a CPU does not usually have a dedicated fan.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6624626934317265, 0.3115424767137831, 0.18299439538006257, 0.1574094897366654], "conf": [0.9535346952972864, 0.8642220471311082, 0.7102624711501363, 0.531901088972987], "prob": [[3.951306241398929e-07, 19], [6.24462229326018e-17, 16], [7.577116726635891e-25, 21], [7.049354939290708e-25, 6]], "generated": ["The central processing unit, also known as the CPU, is basically the brain of a computer.", "Computers need to keep its CPU cool to avoid overheating and damage.", "Thus, CPUs usually have a dedicated fan to dissipate heat and cool down the CPU.", "So the answer is yes."], "conf_comments": ["(A)\nIt is responsible for", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6624626934317265, 0.3115424767137831, 0.14419312833719522, 0.12377468276335833], "conf": [0.9535346952972864, 0.8642220471311082, 0.7115201691293841, 0.5328103210434922], "prob": [[3.951306241398929e-07, 19], [6.24462229326018e-17, 16], [1.8654134337410577e-27, 18], [1.6932056591842948e-27, 6]], "generated": ["The central processing unit, also known as the CPU, is basically the brain of a computer.", "Computers need to keep its CPU cool to avoid overheating and damage.", "Thus, there often is a dedicated fan to circulate cool air inside the CPU.", "So the answer is yes."], "conf_comments": ["(A)\nIt is responsible for", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.6236656057692366, 0.32120118121668284], "conf": [0.9211872105643641, 0.7555067278784418], "prob": [[5.725054641414967e-06, 14], [1.7611315095750824e-30, 50]], "generated": ["A central processing unit can be a microprocessor or a microcontroller.", "According to the article (https://www.allaboutcircuits.com/textbook/digital/chpt-14/detailed-operation-of-microprocessors/), most embedded microcontrollers do not have their"], "conf_comments": ["(A)\nA microprocessor is", "(A)\nown dedicated fan."], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 43.29148030281067}
{"index": 259, "question": "Would Christopher Hitchens be very unlikely to engage in tonsure?", "answer": "yes", "explanation": "Yes. Tonsure is the practice of cutting or shaving some or all of the hair on the scalp as a sign of religious devotion or humility. Christopher Hitchens was an anti-theist, and he regarded all religions as false, harmful, and authoritarian.", "generated": [{"finished": true, "length": 4, "score": [0.6944338537279848, 0.45488742239505686, 0.33518854153111993, 0.27517170141114994], "conf": [0.9698402048588542, 0.8559020651610741, 0.6300195011322861, 0.4439826163690613], "prob": [[3.4523000550387317e-06, 18], [5.242136403189986e-09, 9], [5.4590439959072665e-11, 15], [4.176572420861487e-11, 6]], "generated": ["Christopher Hitchens was an author, journalist, and a known atheist.", "Tonsure is a religious ritual.", "Thus, Christopher Hitchens would not engage in tonsure.", "So the answer is yes."], "conf_comments": ["(A)\nTonsure is", "(A)\nThus, Christopher", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6944338537279848, 0.45488742239505686, 0.31257467568204017, 0.25411994009107364], "conf": [0.9698402048588542, 0.8559020651610741, 0.5818471466328939, 0.3931011676025897], "prob": [[3.4523000550387317e-06, 18], [5.242136403189986e-09, 9], [5.153368004608222e-12, 19], [4.5179151251820745e-12, 6]], "generated": ["Christopher Hitchens was an author, journalist, and a known atheist.", "Tonsure is a religious ritual.", "Therefore, it is very unlikely that Christopher Hitchens would engage in tonsure.", "So the answer is yes."], "conf_comments": ["(A)\nTonsure is", "(A)\nThus, Christopher", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6944338537279848, 0.31966894025953135, 0.21320054160406615, 0.17334376634477702], "conf": [0.9698402048588542, 0.7618549757801949, 0.5490830497569373, 0.3763579792664263], "prob": [[3.4523000550387317e-06, 18], [1.6134484131469826e-29, 41], [1.680912099124982e-33, 19], [1.35271266691438e-33, 6]], "generated": ["Christopher Hitchens was an author, journalist, and a known atheist.", "Tonsure is a monastic rite of initiation in Roman Catholicism, Anglicanism, and other Christian faiths of having a portion of a monk’s head shaved.", "Therefore, it is highly unlikely that Christopher Hitchens would engage in tonsure.", "So the answer is yes."], "conf_comments": ["(A)\nTonsure is", "(A)\nThus, Christopher", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6081925217271363, 0.30298137333780484, 0.21561136600159633, 0.17305494949131736], "conf": [0.9605057822107573, 0.7113490577813844, 0.5225629481223246, 0.33991426003823394], "prob": [[1.578157528418742e-06, 14], [1.3365591372752198e-14, 17], [2.3973597376540378e-17, 17], [2.2620232607867814e-17, 6]], "generated": ["Tonsure means the cutting of a monk’s hair.", "Christopher Hitchens was an author, so he would not need tonsure.", "Thus, it would be very unlikely that he would engage in tonsure.", "So the answer is yes."], "conf_comments": ["(A)\nChristopher Hitch", "(A)\nThus, Christopher", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6081925217271363, 0.3398626754778145, 0.21173915497964516, 0.17090693299156642], "conf": [0.9605057822107573, 0.8632284701285852, 0.6073337530805853, 0.40354888680614764], "prob": [[1.578157528418742e-06, 14], [4.04706851834665e-11, 10], [2.5573514997845905e-17, 24], [2.27237536312072e-17, 6]], "generated": ["Tonsure means the cutting of a monk’s hair.", "Hitchens was an anti-theist.", "Thus, it is very unlikely that Hitchens would be a monk who would engage in tonsure.", "So the answer is yes."], "conf_comments": ["(A)\nChristopher Hitch", "(A)\nThus, H", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6944338537279848, 0.31966894025953135, 0.21320054160406615, 0.1308043346234484], "conf": [0.9698402048588542, 0.7618549757801949, 0.5490830497569373, 0.3937174048109209], "prob": [[3.4523000550387317e-06, 18], [1.6134484131469826e-29, 41], [1.680912099124982e-33, 19], [5.089027485048458e-36, 9]], "generated": ["Christopher Hitchens was an author, journalist, and a known atheist.", "Tonsure is a monastic rite of initiation in Roman Catholicism, Anglicanism, and other Christian faiths of having a portion of a monk’s head shaved.", "Therefore, it is highly unlikely that Christopher Hitchens would engage in tonsure.", "Thus, the answer is very unlikely."], "conf_comments": ["(A)\nTonsure is", "(A)\nThus, Christopher", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": false, "length": 4, "score": [0.6944338537279848, 0.45488742239505686, 0.31257467568204017, 0.0478052745584948], "conf": [0.9698402048588542, 0.8559020651610741, 0.5818471466328939, 0.4238678228984517], "prob": [[3.4523000550387317e-06, 18], [5.242136403189986e-09, 9], [5.153368004608222e-12, 19], [1.7587484628784596e-19, 5]], "generated": ["Christopher Hitchens was an author, journalist, and a known atheist.", "Tonsure is a religious ritual.", "Therefore, it is very unlikely that Christopher Hitchens would engage in tonsure.", "That's right.", "So the answer is yes."], "conf_comments": ["(A)\nTonsure is", "(A)\nThus, Christopher", "(A)\n# Is the above", "(A)\n# Is the above"], "info": [[[3.412813642079819e-08, 5], [0.62393188458647, 6]], 0.7284865541600456, "(A)\n# Is the above", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 53.53953552246094}
{"index": 260, "question": "Can a diamond float on water?", "answer": "no", "explanation": "No. Diamonds are formed by extreme heat and pressure being applied to carbon under the earth's crust. The density of a diamond is 3.51 g/cm³. The density of water is 997 kg/m³. A diamond is more dense than water. A diamond will sink in water.", "generated": [{"finished": true, "length": 5, "score": [0.9119370729596008, 0.8713012221434092, 0.7795734640926673, 0.6894499372897279, 0.6001087751384216], "conf": [0.9614760077135891, 0.8939634407805432, 0.7919628655698945, 0.656011191040117, 0.5083726231096342], "prob": [[0.15166549530212542, 13], [0.11728572066196286, 14], [0.006871191229093472, 28], [0.0041001199637828575, 9], [0.003580123547615987, 6]], "generated": ["Objects only float if they are less dense than the surrounding fluid.", "The density of water is about 1 g/cm^3.", "In comparison, the density of a diamond is about 3.52 g/cm^3, which is greater than water.", "Thus, a diamond would sink.", "So the answer is no."], "conf_comments": ["(A)\nDiamonds are", "(A)\nThe density of a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.849817558063988, 0.8109588373733513, 0.7252220203992482, 0.6404869418520134, 0.5440005188987506], "conf": [0.9558051891470186, 0.87946194335051, 0.7771750759001705, 0.6257979899578093, 0.45946546630623897], "prob": [[0.026161714535622146, 13], [0.022627989722438027, 14], [0.0013824098212067023, 28], [0.001037782799245666, 9], [0.0009338187934790815, 6]], "generated": ["Objects only sink if they are denser than the surrounding fluid.", "The density of water is about 1 g/cm^3.", "In comparison, the density of a diamond is about 3.52 g/cm^3, which is greater than water.", "Thus, a diamond would sink.", "So the answer is yes."], "conf_comments": ["(A)\nDiamonds are", "(A)\nThe density of a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.849817558063988, 0.8109588373733513, 0.7206350606024446, 0.6375168370752051, 0.5415637286642692], "conf": [0.9558051891470186, 0.87946194335051, 0.7828669850689335, 0.6329199641168914, 0.46496212496772804], "prob": [[0.026161714535622146, 13], [0.022627989722438027, 14], [0.0007899294646943233, 28], [0.000589676542284235, 9], [0.0005297806383352175, 6]], "generated": ["Objects only sink if they are denser than the surrounding fluid.", "The density of water is about 1 g/cm^3.", "In comparison, the density of a diamond is about 3.52 g/cm^3, which is more than water.", "Thus, a diamond would sink.", "So the answer is yes."], "conf_comments": ["(A)\nDiamonds are", "(A)\nThe density of a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.9119370729596008, 0.8713012221434092, 0.7795734640926673, 0.5932532860920645, 0.5249784400428826], "conf": [0.9614760077135891, 0.8939634407805432, 0.7919628655698945, 0.6643276646382559, 0.5238457576633384], "prob": [[0.15166549530212542, 13], [0.11728572066196286, 14], [0.006871191229093472, 28], [0.0002448106138698955, 9], [0.0002348115588016834, 6]], "generated": ["Objects only float if they are less dense than the surrounding fluid.", "The density of water is about 1 g/cm^3.", "In comparison, the density of a diamond is about 3.52 g/cm^3, which is greater than water.", "Thus, a diamond cannot float.", "So the answer is no."], "conf_comments": ["(A)\nDiamonds are", "(A)\nThe density of a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.849817558063988, 0.8109588373733513, 0.7206350606024446, 0.5541316854499895, 0.4729209216389119], "conf": [0.9558051891470186, 0.87946194335051, 0.7828669850689335, 0.6301770853932235, 0.46531477254049236], "prob": [[0.026161714535622146, 13], [0.022627989722438027, 14], [0.0007899294646943233, 28], [2.653472029241909e-05, 11], [2.444644055654615e-05, 6]], "generated": ["Objects only sink if they are denser than the surrounding fluid.", "The density of water is about 1 g/cm^3.", "In comparison, the density of a diamond is about 3.52 g/cm^3, which is more than water.", "Thus, a diamond would sink in water.", "So the answer is yes."], "conf_comments": ["(A)\nDiamonds are", "(A)\nThe density of a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 66.34228372573853}
{"index": 261, "question": "Did brother of Goofy creator's employer commit an excommunicable offense?", "answer": "yes", "explanation": "Yes. Goofy was created by Art Babbitt who worked for Walt Disney. Walt Disney's brother, Roy Disney, was a member of the Freemasons. The Catholic Church has a litany of offenses that can get someone excommunicated. Being initiated to Freemasonry, is listed as an excommunicable offense.", "generated": [{"finished": true, "length": 4, "score": [0.6570937662890493, 0.33334549174234085, 0.21049959957958086, 0.1690970015120809], "conf": [0.9681610800536189, 0.7894868392806147, 0.5983421430329928, 0.39954863036636795], "prob": [[1.5156470489367507e-10, 28], [1.4393465013613992e-23, 26], [5.5419800312636445e-30, 23], [4.5139861815762235e-30, 6]], "generated": ["Walt Disney’s first animation feature film, Snow White and the Seven Dwarfs, was released in 1937.", "According to Wikipedia, the Catholic Church excommunicated some priests in 1938 due to adultery.", "Thus, Walt Disney's employer could have a brother who committed an excommunicable offense.", "So the answer is yes."], "conf_comments": ["(A)\nWalt Disney’", "(A)\nThus, the", "(A)\nHowever, Walt", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6570937662890493, 0.33334549174234085, 0.20891685576058536, 0.1637563196094339], "conf": [0.9681610800536189, 0.7894868392806147, 0.5843177255181697, 0.3797293337907454], "prob": [[1.5156470489367507e-10, 28], [1.4393465013613992e-23, 26], [4.2523371711905845e-32, 31], [3.036492633132683e-32, 6]], "generated": ["Walt Disney’s first animation feature film, Snow White and the Seven Dwarfs, was released in 1937.", "According to Wikipedia, the Catholic Church excommunicated some priests in 1938 due to adultery.", "Thus, although unlikely, it was possible for a brother of Goofy creator’s employer to commit an excommunicable offense.", "So the answer is yes."], "conf_comments": ["(A)\nWalt Disney’", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.6624996130177851, 0.4778252602177301, 0.29491065044574627, 0.14447043043627777, 0.08049942955700916, 0.06323443676187938], "conf": [0.9564720545257626, 0.8726460042835724, 0.7809427050266814, 0.6311871141767926, 0.42474841768491295, 0.28734758073015276], "prob": [[1.8356319605759073e-05, 14], [2.1667021314440816e-08, 12], [8.902767281676265e-19, 28], [2.126599953503082e-40, 41], [1.840697570355348e-48, 24], [1.059883199128424e-48, 6]], "generated": ["The creator of Goofy was The Walt Disney Company.", "The Walt Disney Company was created by Walt Disney.", "In turn, Walt Disney’s brother, Roy Oliver Disney, was president and co-founder of The Walt Disney Company.", "According to the Holy See, excommunicable offenses include adultery, murder, prostitution, abortion, heresy, simony, and defrauding a vocation.", "Thus, brother of Goofy creator’s employer could probably commit an excommunicable offense.", "So the answer is yes."], "conf_comments": ["(A)\nThe creator of", "(A)\nWalt Disney'", "(A)\nThus, the", "(A)\nThus, Roy", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.6624996130177851, 0.4778252602177301, 0.29491065044574627, 0.13503861527862904, 0.07299311876256938, 0.05963436251148744], "conf": [0.9564720545257626, 0.8726460042835724, 0.7809427050266814, 0.664020417672943, 0.4865153310767763, 0.3493965410440798], "prob": [[1.8356319605759073e-05, 14], [2.1667021314440816e-08, 12], [8.902767281676265e-19, 28], [2.2744252250246105e-33, 24], [3.772741352647776e-44, 27], [2.4316470522877766e-44, 6]], "generated": ["The creator of Goofy was The Walt Disney Company.", "The Walt Disney Company was created by Walt Disney.", "In turn, Walt Disney’s brother, Roy Oliver Disney, was president and co-founder of The Walt Disney Company.", "The excommunicable offenses listed are any murder, apostasy, heresy, and idolatry.", "Thus, if Roy Oliver Disney committed any excommunicable offense, then he would have been excommunicated, so yes.", "So the answer is yes."], "conf_comments": ["(A)\nThe creator of", "(A)\nWalt Disney'", "(A)\nThus, the", "(A)\nThus, Roy", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.6624996130177851, 0.4778252602177301, 0.29491065044574627, 0.14447043043627777, 0.08049942955700916, 0.05338238066183682], "conf": [0.9564720545257626, 0.8726460042835724, 0.7809427050266814, 0.6311871141767926, 0.42474841768491295, 0.2587888567536622], "prob": [[1.8356319605759073e-05, 14], [2.1667021314440816e-08, 12], [8.902767281676265e-19, 28], [2.126599953503082e-40, 41], [1.840697570355348e-48, 24], [2.6023232695504896e-49, 6]], "generated": ["The creator of Goofy was The Walt Disney Company.", "The Walt Disney Company was created by Walt Disney.", "In turn, Walt Disney’s brother, Roy Oliver Disney, was president and co-founder of The Walt Disney Company.", "According to the Holy See, excommunicable offenses include adultery, murder, prostitution, abortion, heresy, simony, and defrauding a vocation.", "Thus, brother of Goofy creator’s employer could probably commit an excommunicable offense.", "So the answer is maybe."], "conf_comments": ["(A)\nThe creator of", "(A)\nWalt Disney'", "(A)\nThus, the", "(A)\nThus, Roy", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.6624996130177851, 0.4778252602177301, 0.29491065044574627, 0.13503861527862904, 0.07169705445303452, 0.04460697559211777], "conf": [0.9564720545257626, 0.8726460042835724, 0.7809427050266814, 0.664020417672943, 0.52902069029464, 0.3817040695781695], "prob": [[1.8356319605759073e-05, 14], [2.1667021314440816e-08, 12], [8.902767281676265e-19, 28], [2.2744252250246105e-33, 24], [6.995619461331128e-38, 10], [1.4633591106362833e-43, 21]], "generated": ["The creator of Goofy was The Walt Disney Company.", "The Walt Disney Company was created by Walt Disney.", "In turn, Walt Disney’s brother, Roy Oliver Disney, was president and co-founder of The Walt Disney Company.", "The excommunicable offenses listed are any murder, apostasy, heresy, and idolatry.", "Roy O. Disney died from natural causes.", "Thus, there was no excommunicable offense committed by his brother and the answer is no."], "conf_comments": ["(A)\nThe creator of", "(A)\nWalt Disney'", "(A)\nThus, the", "(A)\nThus, Roy", "(A)\nThus, Roy", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 6, "score": [0.6624996130177851, 0.4778252602177301, 0.29491065044574627, 0.13503861527862904, 0.07169705445303452, 0.039676048009925705], "conf": [0.9564720545257626, 0.8726460042835724, 0.7809427050266814, 0.664020417672943, 0.52902069029464, 0.3744043298505152], "prob": [[1.8356319605759073e-05, 14], [2.1667021314440816e-08, 12], [8.902767281676265e-19, 28], [2.2744252250246105e-33, 24], [6.995619461331128e-38, 10], [2.9998791604840156e-46, 23]], "generated": ["The creator of Goofy was The Walt Disney Company.", "The Walt Disney Company was created by Walt Disney.", "In turn, Walt Disney’s brother, Roy Oliver Disney, was president and co-founder of The Walt Disney Company.", "The excommunicable offenses listed are any murder, apostasy, heresy, and idolatry.", "Roy O. Disney died from natural causes.", "Thus, Roy did not commit any of these excommunicable offenses and it is unlikely that he would.", "So the answer is no."], "conf_comments": ["(A)\nThe creator of", "(A)\nWalt Disney'", "(A)\nThus, the", "(A)\nThus, Roy", "(A)\nThus, Roy", "(A)\nSo the answer is"], "info": [[[4.288225191587533e-09, 23], [0.8906420295850861, 6]], 0.7077309767260508, "(A)\nSo the answer is", 6]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 69.101243019104}
{"index": 262, "question": "Would a nickel fit inside a koala pouch?", "answer": "yes", "explanation": "Yes. Koala joeys (babies) enter their mother's pouch when they are about 2 to 3 centimeters long. An American nickel is 2.12 centimeters in diameter.", "generated": [{"finished": true, "length": 4, "score": [0.6389951966552302, 0.45008497844544193, 0.33567466021717174, 0.2824732960002627], "conf": [0.9647029444677923, 0.8379589504698224, 0.637506717320161, 0.4524783729642995], "prob": [[1.060780014469778e-06, 16], [1.4485034377111059e-11, 20], [9.653758982166887e-14, 16], [9.521911933132494e-14, 6]], "generated": ["A nickel has a diameter of around 0.2 centimeters.", "In comparison, a koala’s pouch is about 10 centimeters wide.", "Thus, a nickel would not fit in a koala pouch.", "So the answer is no."], "conf_comments": ["(A)\nA koala p", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7851135314966269, 0.5241606399238397, 0.33480676788476094, 0.2777988690316424], "conf": [0.9711021196211361, 0.799018194676417, 0.589755829071223, 0.416988642514689], "prob": [[0.0017234564826246405, 14], [7.029609542624085e-10, 24], [3.162307760910323e-13, 13], [2.6948039051720863e-13, 6]], "generated": ["A nickel is about 1.95 cm in diameter.", "In comparison, the pouch of a koala is about 30 cm long and 10 cm wide.", "So a nickel would fit inside a koala pouch.", "So the answer is yes."], "conf_comments": ["(A)\nA koala p", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6389951966552302, 0.45008497844544193, 0.3212978084737883, 0.27103242102329644], "conf": [0.9647029444677923, 0.8379589504698224, 0.640197973187994, 0.4566228648488139], "prob": [[1.060780014469778e-06, 16], [1.4485034377111059e-11, 20], [2.223712042437845e-14, 16], [2.1927047861781544e-14, 6]], "generated": ["A nickel has a diameter of around 0.2 centimeters.", "In comparison, a koala’s pouch is about 10 centimeters wide.", "Thus, the nickel would not fit inside a koala pouch.", "So the answer is no."], "conf_comments": ["(A)\nA koala p", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7851135314966269, 0.5241606399238397, 0.28432628834426704, 0.1921686237232713, 0.15135893903599837], "conf": [0.9711021196211361, 0.799018194676417, 0.6263987144518363, 0.36256290224931137, 0.22552210217193944], "prob": [[0.0017234564826246405, 14], [7.029609542624085e-10, 24], [4.091840753404577e-17, 17], [9.270293083084363e-19, 16], [9.123597685066426e-19, 6]], "generated": ["A nickel is about 1.95 cm in diameter.", "In comparison, the pouch of a koala is about 30 cm long and 10 cm wide.", "This means that 15 nickels can fit inside a koala pouch.", "Thus, a nickel does not fit inside a koala pouch.", "So the answer is no."], "conf_comments": ["(A)\nA koala p", "(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7851135314966269, 0.5241606399238397, 0.28432628834426704, 0.1532895543744187, 0.12570366202630726], "conf": [0.9711021196211361, 0.799018194676417, 0.6263987144518363, 0.43539377449423666, 0.29605143027197867], "prob": [[0.0017234564826246405, 14], [7.029609542624085e-10, 24], [4.091840753404577e-17, 17], [1.496353536589673e-23, 17], [1.4000673907028864e-23, 6]], "generated": ["A nickel is about 1.95 cm in diameter.", "In comparison, the pouch of a koala is about 30 cm long and 10 cm wide.", "This means that 15 nickels can fit inside a koala pouch.", "Therefore, 15 nickels do fit inside a koala pouch.", "So the answer is yes."], "conf_comments": ["(A)\nA koala p", "(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 58.82189726829529}
{"index": 263, "question": "Would Dave Chappelle pray over a Quran?", "answer": "yes", "explanation": "Yes. Dave Chappelle converted to Islam in 1998. Dave Chappelle has not deviated from his religious beliefs since 1998 and is a practicing Muslim. Practicing Muslims pray often.", "generated": [{"finished": true, "length": 4, "score": [0.8647459881760765, 0.46331177117706185, 0.37918107194782824, 0.33003426490357607], "conf": [0.974491270314041, 0.799040086713079, 0.6185151747473396, 0.4699370001605041], "prob": [[0.05432387563452152, 11], [2.2568148696515475e-08, 14], [3.440637000301e-09, 13], [3.3810512104238277e-09, 6]], "generated": ["Dave Chappelle is a comedian.", "He is not a Muslim and does not pray over Quran.", "Thus, he would not pray over a Quran.", "So the answer is no."], "conf_comments": ["(A)\nHe is known for", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8412806738836671, 0.3876839654935195, 0.29654653296770406, 0.2474016033831038], "conf": [0.9670790512291187, 0.8333206384923596, 0.6045162861884342, 0.4279466486628622], "prob": [[0.03225658133891587, 11], [6.570205330612703e-09, 11], [1.1056760589787695e-10, 19], [9.98743738998876e-11, 6]], "generated": ["Dave Chappelle is an atheist.", "A Quran is sacred religious text of Islam.", "Thus, it is unlikely that Dave Chappelle would pray over a Quran.", "So the answer is no."], "conf_comments": ["(A)\nThe Quran", "(A)\nThus, Dave", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.6450787949644092, 0.485171033442136], "conf": [0.8453100892054815, 0.60538860861425], "prob": [[5.855461783653971e-06, 17], [1.4217956229817164e-06, 6]], "generated": ["Dave Chappelle is a Muslim and would pray over a Quran.", "So the answer is yes."], "conf_comments": ["(A)\nQuran is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8412806738836671, 0.33505731495585217, 0.2718261255647073, 0.2348434359041089], "conf": [0.9670790512291187, 0.792283151340484, 0.6033864627563109, 0.4522888009093884], "prob": [[0.03225658133891587, 11], [3.490443534509604e-22, 28], [3.380222287250693e-23, 16], [3.295113646985765e-23, 6]], "generated": ["Dave Chappelle is an atheist.", "According to Muslims, a Quran needs to be treated with reverence while reading it aloud or in front of others.", "Thus, Dave Chappelle would not pray over a Quran.", "So the answer is no."], "conf_comments": ["(A)\nThe Quran", "(A)\nThus, Dave", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8412806738836671, 0.33505731495585217, 0.18965824181954172, 0.1315142265958961, 0.1128570006414261], "conf": [0.9670790512291187, 0.792283151340484, 0.5772406626326655, 0.4654379957860302, 0.3453052228883124], "prob": [[0.03225658133891587, 11], [3.490443534509604e-22, 28], [2.5553791460430766e-29, 20], [1.0962561444922682e-32, 15], [1.0484153813671399e-32, 6]], "generated": ["Dave Chappelle is an atheist.", "According to Muslims, a Quran needs to be treated with reverence while reading it aloud or in front of others.", "Dave Chappelle is unlikely to treat the Quran with this type of reverence.", "Thus, it is unlikely he would pray over the Quran.", "So the answer is no."], "conf_comments": ["(A)\nThe Quran", "(A)\nThus, Dave", "(A)\nThus, Dave", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 50.7703320980072}
{"index": 264, "question": "Is the span in C-SPAN named after Alan Greenspan?", "answer": "no", "explanation": "No. Alan Greenspan was chairman of the Federal Reserve from 1987 to 2006. CSPAN is a cable news network that was created in 1979. Alan Greenspan was relatively unknown to the world at large in 1979.", "generated": [{"finished": true, "length": 4, "score": [0.7273139327997277, 0.6026081101113652, 0.4882916794539214, 0.3948831933050729], "conf": [0.9610611727320174, 0.8423883425274239, 0.675017845846366, 0.4437608962126211], "prob": [[4.2433738959832054e-11, 40], [5.215283834333847e-13, 18], [4.3745871503359385e-15, 24], [4.2403952050277436e-15, 6]], "generated": ["The span in C-SPAN stands for “Cable-Satellite Public Affairs Network”, which was created in 1979 and started broadcasting in 1980.", "Alan Greenspan became chairman of the Federal Reserve in 1987.", "Thus, the span in C-SPAN was named before Alan Greenspan became chairman of the Federal Reserve.", "So the answer is no."], "conf_comments": ["(A)\nAlan Greens", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7273139327997277, 0.6026081101113652, 0.4312381838158295, 0.3555865491043023], "conf": [0.9610611727320174, 0.8423883425274239, 0.6675930949089304, 0.4646300146859704], "prob": [[4.2433738959832054e-11, 40], [5.215283834333847e-13, 18], [1.3008063572616867e-16, 19], [1.1307783669246976e-16, 6]], "generated": ["The span in C-SPAN stands for “Cable-Satellite Public Affairs Network”, which was created in 1979 and started broadcasting in 1980.", "Alan Greenspan became chairman of the Federal Reserve in 1987.", "This means that the span in C-SPAN is not named after Alan Greenspan.", "So the answer is no."], "conf_comments": ["(A)\nAlan Greens", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8353845122259913, 0.41826194451329957, 0.3112585784730076, 0.24456416064987566], "conf": [0.9683985371365614, 0.7678638422564824, 0.5453819403799325, 0.3382682338144003], "prob": [[0.0003848124670968226, 24], [3.8181225325697434e-20, 32], [3.3768272301551132e-22, 19], [3.2840055087264733e-22, 6]], "generated": ["Alan Greenspan was Chairman of the Federal Reserve from 1987-2006.", "C-SPAN was founded in 1979 and in 1987 the first Senate floor debates were taped and aired.", "Thus, the span in C-SPAN was not named after Alan Greenspan.", "So the answer is no."], "conf_comments": ["(A)\nC-SPAN", "(A)\nThus, Alan", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7083819396726004, 0.37216391586399183, 0.25336756082236267, 0.20041948836843926], "conf": [0.9373362845691685, 0.8257756650211544, 0.6182149975177168, 0.3881948434337348], "prob": [[8.802941545653412e-08, 26], [6.662469319892278e-23, 30], [1.189169966577651e-26, 18], [1.164259639852878e-26, 6]], "generated": ["The span in C-SPAN is named after Sinclair Broadcast Group’s Cable Satellite Public Affairs Network.", "Sinclair Broadcast Group’s name was shortened to the first letter of each word to make the abbreviation, C-SPAN.", "Thus the span in C-SPAN was not named after Alan Greenspan.", "So the answer is no."], "conf_comments": ["(A)\nAlan Greens", "(A)\nAlan Greens", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.7527679931060278, 0.4951807041512492, 0.3331899254251681, 0.15325162958504732, 0.109851951593726, 0.08801808540820313], "conf": [0.9664374600491001, 0.8887186731677308, 0.7804863886509928, 0.4700481780086531, 0.346267719192255, 0.22306903298397723], "prob": [[0.0009681239259444675, 13], [5.153197793510814e-07, 10], [2.330663591110454e-15, 29], [6.767145354256437e-35, 43], [3.5047382946722905e-38, 21], [3.432903626368953e-38, 6]], "generated": ["Alan Greenspan was the chairman of the Federal Reserve.", "He was born in 1926.", "He also served as the chairman of the Federal Reserve for over 18 years from 1987 to 2006.", "The C-SPAN channel was launched in 1979, but Alan Greenspan was serving at the Federal Reserve at this time, so it was most unlikely that the channel was named after him.", "Thus, the name of the C-SPAN channel was not named after Alan Greenspan.", "So the answer is no."], "conf_comments": ["(A)\nC-SPAN", "(A)\nC-SPAN", "(A)\nThe span in C", "(A)\nThe C-SP", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 59.517391204833984}
{"index": 265, "question": "Does Final Fantasy VI require electricity to play?", "answer": "yes", "explanation": "Yes. Final Fantasy VI is a video game. Video games are played using a video game console and television. Video game consoles and televisions require electricity in order to function.", "generated": [{"finished": true, "length": 3, "score": [0.6151426132406151, 0.4129200626801158, 0.34914475556272073], "conf": [0.9044762083833331, 0.6980451507180598, 0.500899559647502], "prob": [[3.684903706112336e-07, 17], [2.9045036399703048e-09, 9], [2.8414732557572374e-09, 6]], "generated": ["Final Fantasy VI is a NES game, which requires electricity to play.", "Thus, it does require electricity.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Final", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5975084391363469, 0.33885909212307563, 0.2739625975475072, 0.23002858050805702], "conf": [0.9548475160138202, 0.8702079186635846, 0.6805432742951213, 0.4811380072888816], "prob": [[1.5892010355914246e-19, 44], [7.386445623109123e-26, 14], [1.2289974970343827e-26, 10], [1.2082415735988137e-26, 6]], "generated": ["Final Fantasy VI (VI hereafter) was released on Super Nintendo Entertainment Systems (SNES hereafter) in 1994 and PlayStation (PS hereafter) in 2009.", "SNES and PS are devices and devices require electricity to work.", "Thus, VI requires electricity to play.", "So the answer is yes."], "conf_comments": ["(A)\nSNES and PS", "(A)\nThus, VI", "(A)\nHowever, VI can", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.6151426132406151, 0.46752402878361854], "conf": [0.9044762083833331, 0.7025206775040728], "prob": [[3.684903706112336e-07, 17], [3.4481043991064216e-08, 8]], "generated": ["Final Fantasy VI is a NES game, which requires electricity to play.", "Thus, the answer is yes."], "conf_comments": ["(A)\nThus, Final", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5964154657548695, 0.30794281717376415, 0.21932407832950446, 0.17714093054661448, 0.14818176038657632], "conf": [0.9810055774215586, 0.8449742878281261, 0.7238797414800557, 0.5462020423091309, 0.38297028381697595], "prob": [[3.237780376564689e-08, 17], [6.246911523294481e-20, 23], [2.4088427813157116e-23, 15], [3.630167145407345e-24, 13], [3.5872861214247875e-24, 6]], "generated": ["The game Final Fantasy VI was commercially available in 1994.", "In 1994, many people played the game on Super Nintendo Entertainment System, an electronic device.", "Electricity is needed to power the Super Nintendo Entertainment System.", "Thus, Final Fantasy VI requires electricity to play.", "So the answer is yes."], "conf_comments": ["(A)\nIn 19", "(A)\nThus, the", "(A)\nThus, electric", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5975084391363469, 0.33885909212307563, 0.2435892596794109, 0.2063382561395973], "conf": [0.9548475160138202, 0.8702079186635846, 0.6870997681097569, 0.49447811990777374], "prob": [[1.5892010355914246e-19, 44], [7.386445623109123e-26, 14], [6.968332417675929e-28, 11], [6.845806816779852e-28, 6]], "generated": ["Final Fantasy VI (VI hereafter) was released on Super Nintendo Entertainment Systems (SNES hereafter) in 1994 and PlayStation (PS hereafter) in 2009.", "SNES and PS are devices and devices require electricity to work.", "Thus, VI does require electricity to play.", "So the answer is yes."], "conf_comments": ["(A)\nSNES and PS", "(A)\nThus, VI", "(A)\nHowever, VI can", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 48.30994987487793}
{"index": 266, "question": "Did Sartre write a play about Hell?", "answer": "yes", "explanation": "Yes. In 1944, Sartre released No Exit. No Exit is a play about three people mysteriously locked in a room together. Late in the play, it is revealed the room is a version of Hell.", "generated": [{"finished": true, "length": 3, "score": [0.5969253560347312, 0.5064133444134048, 0.4415332806683422], "conf": [0.9481589456224773, 0.7844025578603011, 0.5977450381965546], "prob": [[1.6751545073886621e-10, 23], [2.739780241018783e-11, 13], [2.6999361502562825e-11, 6]], "generated": ["Jean-Paul Sartre wrote a play called No Exit which is a philosophical metaphor of Hell.", "Thus, Sartre did write a play about Hell.", "So the answer is yes."], "conf_comments": ["(A)\nThus, S", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6794694669641919, 0.35956396444278726, 0.3053910447294777, 0.2632416675094887], "conf": [0.9677087337307574, 0.8654413213540797, 0.6908280725030878, 0.514469958264939], "prob": [[1.933238977669636e-08, 24], [5.069385580875392e-18, 19], [1.3593094908489188e-18, 13], [1.3407818420465105e-18, 6]], "generated": ["Jean Paul Sartre was a famous philosopher who wrote “No Exit” in 1944.", "“No Exit” was a play about Hell and is one of his most famed works.", "Thus, Sartre did write a play about Hell.", "So the answer is yes."], "conf_comments": ["(A)\n“No Exit”", "(A)\nThus, S", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5969253560347312, 0.35668057280161114, 0.30671428352881636], "conf": [0.9481589456224773, 0.7258515982828871, 0.539699133352501], "prob": [[1.6751545073886621e-10, 23], [1.8263914176150333e-16, 18], [1.7669534543591072e-16, 6]], "generated": ["Jean-Paul Sartre wrote a play called No Exit which is a philosophical metaphor of Hell.", "Thus, it can be inferred Sartre did write a play about Hell.", "So the answer is yes."], "conf_comments": ["(A)\nThus, S", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6794694669641919, 0.35956396444278726, 0.2746676425732685], "conf": [0.9677087337307574, 0.8654413213540797, 0.710124449052937], "prob": [[1.933238977669636e-08, 24], [5.069385580875392e-18, 19], [3.3165157019895396e-19, 8]], "generated": ["Jean Paul Sartre was a famous philosopher who wrote “No Exit” in 1944.", "“No Exit” was a play about Hell and is one of his most famed works.", "Thus, the answer is yes."], "conf_comments": ["(A)\n“No Exit”", "(A)\nThus, S", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.5442180366444568, 0.35859539818859965], "conf": [0.9500294315638006, 0.785635506112925], "prob": [[2.5534775237581463e-08, 15], [1.4742399924841238e-10, 8]], "generated": ["Jean Paul Sartre did write a play named **No Exit**.", "Thus, the answer is yes."], "conf_comments": ["(A)\nThe play is about", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 40.414297580718994}
{"index": 267, "question": "Do members of NFL teams receive infantry training?", "answer": "no", "explanation": "No. Members of NFL teams play football. Infantry training is provided to members of the US armed forces.", "generated": [{"finished": true, "length": 6, "score": [0.696580880218019, 0.39566416662068, 0.19775855174037707, 0.11155297840788299, 0.08828113977277853, 0.07373349641840117], "conf": [0.9394686903857484, 0.8344727436345133, 0.6372174339773953, 0.519999260571958, 0.40285934681151603, 0.2820897105416872], "prob": [[0.00018611880306534335, 13], [2.4812106534375135e-16, 27], [1.305532895312126e-23, 15], [3.3596875900257357e-32, 21], [1.3824230408745337e-33, 15], [1.3514529197546891e-33, 6]], "generated": ["Members of the NFL teams do not receive infantry training.", "For a player to be eligible for the NFL, he must be at least three years removed from his college eligibility.", "Thus, players in the NFL have completed their college eligibility.", "In comparison, infantry training is a required part of basic training for soldiers in the United States Army.", "Thus, members of the NFL teams do not receive infantry training.", "So the answer is no."], "conf_comments": ["(A)\nInfantry training is", "(A)\nThus, the", "(A)\nInfantry training is", "(A)\nThus, members", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.696580880218019, 0.39566416662068, 0.19775855174037707, 0.11155297840788299, 0.08210869081605242, 0.0690687211240347], "conf": [0.9394686903857484, 0.8344727436345133, 0.6372174339773953, 0.519999260571958, 0.4128949888368603, 0.29328859169609667], "prob": [[0.00018611880306534335, 13], [2.4812106534375135e-16, 27], [1.305532895312126e-23, 15], [3.3596875900257357e-32, 21], [1.0863864155670727e-34, 15], [1.061593921075493e-34, 6]], "generated": ["Members of the NFL teams do not receive infantry training.", "For a player to be eligible for the NFL, he must be at least three years removed from his college eligibility.", "Thus, players in the NFL have completed their college eligibility.", "In comparison, infantry training is a required part of basic training for soldiers in the United States Army.", "Thus, the members of NFL teams do not receive infantry training.", "So the answer is no."], "conf_comments": ["(A)\nInfantry training is", "(A)\nThus, the", "(A)\nInfantry training is", "(A)\nThus, members", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.696580880218019, 0.39566416662068, 0.13359317602739118, 0.11260869327499773, 0.09500971500619858], "conf": [0.9394686903857484, 0.8344727436345133, 0.6247890523013141, 0.4916607960172326, 0.35089165607187406], "prob": [[0.00018611880306534335, 13], [2.4812106534375135e-16, 27], [3.929364203158186e-35, 23], [9.40428873215854e-36, 14], [9.260498056954549e-36, 6]], "generated": ["Members of the NFL teams do not receive infantry training.", "For a player to be eligible for the NFL, he must be at least three years removed from his college eligibility.", "In other words, NFL players only play the game of football and they don't even receive training in combat.", "Thus, members of NFL teams do not receive infantry training.", "So the answer is no."], "conf_comments": ["(A)\nInfantry training is", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7181496729670781, 0.3279633940731164, 0.16723105473735891, 0.1394167245332256], "conf": [0.9612576408108067, 0.8198461815779763, 0.634613343656026, 0.4503554098479967], "prob": [[0.001976520356374406, 10], [1.5107188986082545e-09, 10], [1.1810765396344285e-16, 15], [1.0422605294079376e-16, 6]], "generated": ["Members of NFL teams are professional athletes.", "Infantry is a special force in the military.", "Thus, NFL team members and infantry are not the same occupation.", "So the answer is no."], "conf_comments": ["(A)\nInfantry training is", "(A)\nThus, members", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5376467279811232, 0.24142745608678368, 0.1552122280845661, 0.1328198470807538], "conf": [0.9464043196973455, 0.8195907343694991, 0.6181781622730916, 0.45531846549105714], "prob": [[7.5742284978728e-05, 8], [8.260134917689208e-12, 11], [8.983877637878413e-17, 19], [8.675583876188001e-17, 6]], "generated": ["NFL stand for National Football League.", "Infantry usually refers to soldiers who serve on foot.", "Thus, it is clear that members of NFL teams don’t receive infantry training.", "So the answer is no."], "conf_comments": ["(A)\nInfantry is a", "(A)\nThus, NFL", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 63.27659320831299}
{"index": 268, "question": "Can voice actors for Goofy and Bugs Bunny each get one stripe from American flag?", "answer": "yes", "explanation": "Yes. The American flag has 13 stripes on it. Since the role originated in 1932, six people have voiced the character of Goofy. Since 1940, seven people have voiced the character of Bugs Bunny.", "generated": [{"finished": true, "length": 4, "score": [0.8303214871073333, 0.454383211372152, 0.3496659442400082, 0.2998464418918252], "conf": [0.9643799326524672, 0.8000377747301115, 0.6247889802051889, 0.4607099737126864], "prob": [[0.03486935288655141, 10], [3.7779653483609856e-10, 18], [3.7431674394974145e-13, 25], [3.6814634826107144e-13, 6]], "generated": ["The American flag has 13 stripes.", "One stripe is not enough to cut for Goofy or Bugs Bunny.", "Thus, voice actors for Goofy and Bugs Bunny cannot get one stripe each from the American flag.", "So the answer is no."], "conf_comments": ["(A)\nThe voice actors for", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8303214871073333, 0.454383211372152, 0.3032463090002586, 0.2633264848738256], "conf": [0.9643799326524672, 0.8000377747301115, 0.6541609398298495, 0.49440907913769844], "prob": [[0.03486935288655141, 10], [3.7779653483609856e-10, 18], [1.2364837156806363e-14, 17], [1.2194546351097081e-14, 6]], "generated": ["The American flag has 13 stripes.", "One stripe is not enough to cut for Goofy or Bugs Bunny.", "Thus, these voice actors could not get one stripe from the American flag.", "So the answer is no."], "conf_comments": ["(A)\nThe voice actors for", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7435552281345742, 0.3595359563232232, 0.23741753209456556, 0.20287120231130418], "conf": [0.9692873231318004, 0.8336769567147232, 0.6588470985849862, 0.48399911483803654], "prob": [[0.0003858646782751839, 14], [6.283334806802862e-11, 12], [2.3715142358592567e-16, 21], [2.2864281552901204e-16, 6]], "generated": ["The colors of the American flag are red, white, and blue.", "There are seven red stripes and six blue stripes.", "Thus, voice actors for Goofy and Bugs Bunny can each have one stripe.", "So the answer is yes."], "conf_comments": ["(A)\nThe voice actor for", "(A)\nThus, there", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8303214871073333, 0.606265116916799, 0.2223669065518578, 0.13511468057027395, 0.11450268485683468], "conf": [0.9643799326524672, 0.8530913169980839, 0.6189449731599461, 0.4842065880123583, 0.34894962211284614], "prob": [[0.03486935288655141, 10], [8.399127181370153e-07, 21], [6.66383134261677e-23, 22], [4.2418975843593663e-29, 19], [4.1545646784235145e-29, 6]], "generated": ["The American flag has 13 stripes.", "Pete Burness and Mel Blanc voiced Goofy and Bugs Bunny, respectively.", "According to Wikipedia, each actor died before the US flag could get 13 or more stripes.", "Thus, there is no way that each can get one stripe from the American flag.", "So the answer is no."], "conf_comments": ["(A)\nThe voice actors for", "(A)\nThus, each", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 8, "score": [0.5915143391952914, 0.43201184121679204, 0.21400459765567206, 0.14044698224200305, 0.0914714076695138, 0.05740054691090404, 0.03724835385539873, 0.030259063182308893], "conf": [0.9654905309518942, 0.8187935325052482, 0.668889501125957, 0.5220762284311673, 0.4245609670098422, 0.3262832686053314, 0.24789297406517058, 0.1659494700771983], "prob": [[6.738379928468458e-07, 14], [1.624808597200068e-09, 13], [1.9347545853208635e-19, 19], [6.2921506335175105e-27, 29], [1.4688333726511574e-34, 27], [1.1714156680465838e-40, 21], [7.810277659987866e-48, 28], [7.167658192542792e-48, 6]], "generated": ["Jim Cummings has provided the voice for Goofy.", "Jeff Bergman provided the voice for Bugs Bunny.", "Both voice actors have a career that began in the 1980’s.", "Jim Cummings has been active since 1981, while Jeff Bergman has been active since 1989.", "The American flag with 13 stripes was replaced with a flag with 15 stripes in 1818.", "The American flag had 13 stripes from 1777-1818.", "Thus, Jim Cummings was able to get one stripe, and Jeff Bergman was also able to get one stripe.", "So the answer is yes."], "conf_comments": ["(A)\nJeff Bergman", "(A)\nThe American flag has", "(A)\nThe American flag has", "(A)\nThus, both", "(A)\nThus, both", "(A)\nThus, both", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": []}, "run_time": 67.51590299606323}
{"index": 269, "question": "Could the surface of Europa fry an egg?", "answer": "no", "explanation": "No. Europa is known for having an icy surface. For an egg to become firm, the ground must be at least 158 degrees Fahrenheit.  Ice forms at 32 degrees Fahrenheit. Europa's temperatures are all in the negatives on the Fahrenheit scale.", "generated": [{"finished": true, "length": 3, "score": [0.6539205058356573, 0.5083491472361534, 0.4439142317727548], "conf": [0.9479517280789662, 0.7681367828065434, 0.5877079630666677], "prob": [[4.245322627786732e-11, 30], [2.9005729479906553e-13, 17], [2.8430817336276736e-13, 6]], "generated": ["The surface temperature of Europa is -220ºC (-370ºF), which is too cold for an egg to fry.", "Thus, it is impossible to fry an egg on the surface of Europa.", "So the answer is no."], "conf_comments": ["(A)\nThe surface temperature of", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6997259225303111, 0.41075769792661543, 0.3290067623887993, 0.27880252333800476], "conf": [0.9690524028249822, 0.8674141253711506, 0.6891376528169724, 0.49631664166702033], "prob": [[4.604218783984927e-06, 18], [4.1264112898424683e-13, 17], [2.562336120158598e-14, 13], [2.5178013197171742e-14, 6]], "generated": ["The average temperature of Europa’s surface is -200 degrees Celsius.", "Boiling temperatures for eggs is around 100 degrees Celsius.", "Thus, the surface of Europa cannot fry an egg.", "So the answer is no."], "conf_comments": ["(A)\nThe boiling point", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6539205058356573, 0.4355235098411504, 0.37729312684590766], "conf": [0.9479517280789662, 0.7566028824482052, 0.5696105400016374], "prob": [[4.245322627786732e-11, 30], [3.686455583545884e-14, 12], [3.6170651949262765e-14, 6]], "generated": ["The surface temperature of Europa is -220ºC (-370ºF), which is too cold for an egg to fry.", "Thus, an egg cannot be fried on Europa.", "So the answer is no."], "conf_comments": ["(A)\nThe surface temperature of", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7013795716745916, 0.37456558781536536, 0.2916563961773945, 0.2565164909605771], "conf": [0.9708146283401851, 0.8118003844786021, 0.6606158404749796, 0.5121476257784437], "prob": [[2.3535145026909293e-11, 36], [2.2354408533915628e-32, 45], [1.1187645897772327e-34, 18], [1.1040456985675523e-34, 6]], "generated": ["The surface temperature of Europa, one of the moons of Jupiter, is about -156 degree Celsius (about -249 degree Fahrenheit).", "On the other hand, an egg could be cooked with heat up to about 250 degree Celsius (about 482 degree Fahrenheit), which is over double of the temperature on Europa.", "Thus, the surface of Europa would not be hot enough to fry an egg.", "So the answer is no."], "conf_comments": ["(A)\nThe boiling point", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6997259225303111, 0.41075769792661543, 0.2824940202176029, 0.24067603331127033], "conf": [0.9690524028249822, 0.8674141253711506, 0.7153932468626555, 0.5209714931244476], "prob": [[4.604218783984927e-06, 18], [4.1264112898424683e-13, 17], [5.649792673033509e-17, 16], [5.539885636804086e-17, 6]], "generated": ["The average temperature of Europa’s surface is -200 degrees Celsius.", "Boiling temperatures for eggs is around 100 degrees Celsius.", "Thus, an egg could not be fried at the surface of Europa.", "So the answer is no."], "conf_comments": ["(A)\nThe boiling point", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 47.397178649902344}
{"index": 270, "question": "Is The Joy of Painting TV show still producing new episodes?", "answer": "no", "explanation": "No. The Joy of Painting is hosted by painter Bob Ross. Bob Ross died in 1995. The episodes currently airing are reruns.", "generated": [{"finished": true, "length": 3, "score": [0.7774361119877671, 0.601634117422667, 0.5127340012100204], "conf": [0.9780616536102805, 0.7675336783169033, 0.5594838080097784], "prob": [[0.00010673345370596053, 19], [8.225791115496469e-07, 18], [8.04920763032216e-07, 6]], "generated": ["The Joy of Painting TV show stopped producing new episodes in 2006.", "Thus, The Joy of Painting TV show is not currently producing new episodes.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7774361119877671, 0.5785010589191427, 0.49843710116485546], "conf": [0.9780616536102805, 0.7923816740736969, 0.5897480298621097], "prob": [[0.00010673345370596053, 19], [7.577740754811141e-07, 13], [7.461424131660612e-07, 6]], "generated": ["The Joy of Painting TV show stopped producing new episodes in 2006.", "Thus, the TV show is not still producing new episodes.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7957372263265309, 0.535334016147483, 0.46150192174748783], "conf": [0.9591865299915066, 0.7987148459283764, 0.5967029818998492], "prob": [[8.908731055394023e-06, 28], [1.0895669548352486e-08, 11], [1.0559401401213925e-08, 6]], "generated": ["The Joy of Painting aired its first episode in 1983 and its last episode in 1994.", "Thus, the show no longer produces new episodes.", "So the answer is no."], "conf_comments": ["(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7976112855322941, 0.48123528392492376, 0.18467951871197785, 0.15172718953091296], "conf": [0.9811762898993631, 0.8573544527085131, 0.6689423963267439, 0.4838626989259214], "prob": [[0.0006327091344624858, 17], [5.208441596716954e-10, 16], [9.092811139198807e-24, 19], [6.003904709294493e-24, 6]], "generated": ["The Joy of Painting TV show premiered in 1983.", "It lasted 39 seasons and ended in 2011.", "Since the ending year is later than 2015, the answer is not currently.", "So the answer is no."], "conf_comments": ["(A)\nIt stopped producing new", "(A)\nThus, it", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 11, "score": [0.7976112855322941, 0.5213037624331281, 0.25061132077877646, 0.10128707652047367, 0.0640148971604648, 0.04270201388534136, 0.024631959679562342, 0.016298295724667893, 0.010336064319364924, 0.006739918667557724, 0.005572581140262786], "conf": [0.9811762898993631, 0.7633336029482646, 0.6657171848562785, 0.5039732657016651, 0.4146476493686527, 0.32912501684440304, 0.2501421505186757, 0.18537060334722705, 0.1369112179177945, 0.10231598959751449, 0.07060661563668492], "prob": [[0.0006327091344624858, 17], [1.4323506038746752e-07, 14], [6.48240632174059e-14, 11], [3.129316462606374e-31, 26], [1.4472680700285415e-36, 17], [7.531336976192368e-43, 25], [4.7888353746406186e-55, 34], [8.108378037031998e-63, 34], [1.4369288612331852e-67, 18], [2.9071938924098327e-70, 11], [2.7471578933587067e-70, 6]], "generated": ["The Joy of Painting TV show premiered in 1983.", "The last episode that aired was in 1994.", "This means it was produced for 11 years.", "To determine whether or not the TV show is still in production, we need to find out its 15th anniversary.", "Multiplying 15 by 11 gives us 165.", "The 15th anniversary of The Joy of Painting TV show would be in 2019.", "If the show were still producing new episodes in the year 2019, its 165th anniversary would be in 2033.", "If the show was not producing new episodes in the year 2019, its 165th anniversary would be in 2058.", "2019 is between 2033 and 2058.", "Thus, the show is not producing new episodes.", "So the answer is no."], "conf_comments": ["(A)\nIt stopped producing new", "(A)\nThus, the", "(A)\nThus, it", "(A)\nThe 15", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSince the show is", "(A)\nSince the show was", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": [], "9": [], "10": [], "11": []}, "run_time": 65.78630685806274}
{"index": 271, "question": "Does Disney own a major comic book publisher?", "answer": "yes", "explanation": "Yes. The three biggest comic book publishers are DC, Marvel, and IDW. Disney has owned Marvel since 2007.", "generated": [{"finished": true, "length": 3, "score": [0.6319043776098316, 0.5708473468702435, 0.5134255320965981], "conf": [0.9542926080551124, 0.8387787153852899, 0.6796378387623079], "prob": [[6.468479354325462e-08, 19], [2.2887684352352942e-08, 14], [2.266264846052143e-08, 6]], "generated": ["Disney has a subsidiary, Marvel Entertainment, a major comic book publisher.", "Thus, Disney owns a major comic book publisher.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Disney", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6692451272114194, 0.5507639444131557, 0.49003497894697623], "conf": [0.9704975499944316, 0.8428410125698074, 0.6689358724494753], "prob": [[9.17571429708977e-06, 15], [2.823389488158777e-07, 14], [2.780204837284677e-07, 6]], "generated": ["The Walt Disney Company owns Marvel since 2009.", "Thus, Disney owns a major comic book publisher.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Disney", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6583549001433486, 0.4423547324254983, 0.37990312276337435, 0.3300869939624859], "conf": [0.9242370078614842, 0.8203809298215355, 0.6850089495738573, 0.5184526974324286], "prob": [[1.203544706739372e-06, 18], [8.215451059635593e-13, 21], [1.446648302422168e-13, 14], [1.4247943869540987e-13, 6]], "generated": ["It is true that Disney owns a major comic book publisher, Marvel Entertainment.", "Marvel Entertainment became a subsidiary of The Walt Disney Company in 2009.", "Thus, Disney owns a major comic book publisher.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, Disney", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6692451272114194, 0.4671390722854112, 0.365281614676446, 0.32793740228608154], "conf": [0.9704975499944316, 0.8873967381458604, 0.7687643295841924, 0.6212762140387128], "prob": [[9.17571429708977e-06, 15], [9.020365319906447e-09, 11], [1.704532624240907e-11, 18], [1.6773124122671157e-11, 6]], "generated": ["The Walt Disney Company owns Marvel since 2009.", "Marvel is an American comic book publisher.", "Thus, it is true that Disney owns a major comic book publisher.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Disney", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6583549001433486, 0.4459223249369958, 0.386764300968711], "conf": [0.9242370078614842, 0.7309186033727282, 0.5569671518134554], "prob": [[1.203544706739372e-06, 18], [5.883543664952209e-10, 14], [5.446550875584992e-10, 6]], "generated": ["It is true that Disney owns a major comic book publisher, Marvel Entertainment.", "Therefore, Disney owns a major comic book publisher.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6583549001433486, 0.4423547324254983, 0.37990312276337435, 0.09041358786614716], "conf": [0.9242370078614842, 0.8203809298215355, 0.6850089495738573, 0.4337021749624616], "prob": [[1.203544706739372e-06, 18], [8.215451059635593e-13, 21], [1.446648302422168e-13, 14], [5.934355624482158e-22, 8]], "generated": ["It is true that Disney owns a major comic book publisher, Marvel Entertainment.", "Marvel Entertainment became a subsidiary of The Walt Disney Company in 2009.", "Thus, Disney owns a major comic book publisher.", "So the answers are yes and false."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, Disney", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 47.441004037857056}
{"index": 272, "question": "Is someone more likely to survive having breast cancer in Japan than in Sweden?", "answer": "no", "explanation": "No. 84.70% of people in Japan with breast cancer survive. 86.20% of people in Sweden with breast cancer survive.", "generated": [{"finished": true, "length": 5, "score": [0.5749597764260792, 0.39606531273663964, 0.1992280946430403, 0.15656637988375885, 0.13493088556804442], "conf": [0.9541797146384058, 0.8253099330666877, 0.6177639006271484, 0.4722036556357533, 0.3521883415979382], "prob": [[1.1121032589298916e-18, 39], [3.055113003374831e-26, 29], [1.3249310760451943e-36, 22], [1.8619759005262907e-38, 20], [1.815743928820982e-38, 6]], "generated": ["The global breast cancer survival rate as of 2020 is about 86%, which means that around 14% of those who get diagnosed with breast cancer die.", "Japan and Sweden’s breast cancer survival rates in 2020 are 91% and 84%, respectively.", "This implies that a woman in Japan has a larger chance to survive breast cancer than a woman in Sweden.", "Thus, a woman is more likely to survive having breast cancer in Japan than in Sweden.", "So the answer is yes."], "conf_comments": ["(A)\nThe survival rate", "(A)\nThus, someone", "(A)\nThus, someone", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5749597764260792, 0.39606531273663964, 0.1992280946430403, 0.15092671423321863, 0.13009864057356774], "conf": [0.9541797146384058, 0.8253099330666877, 0.6177639006271484, 0.46614223886089556, 0.34778185066805045], "prob": [[1.1121032589298916e-18, 39], [3.055113003374831e-26, 29], [1.3249310760451943e-36, 22], [7.307367544060521e-39, 19], [7.130334571537409e-39, 6]], "generated": ["The global breast cancer survival rate as of 2020 is about 86%, which means that around 14% of those who get diagnosed with breast cancer die.", "Japan and Sweden’s breast cancer survival rates in 2020 are 91% and 84%, respectively.", "This implies that a woman in Japan has a larger chance to survive breast cancer than a woman in Sweden.", "Thus, a woman is more likely to survive breast cancer in Japan than in Sweden.", "So the answer is yes."], "conf_comments": ["(A)\nThe survival rate", "(A)\nThus, someone", "(A)\nThus, someone", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 1, "score": [0.6477179381858043], "conf": [0.7499180526586747], "prob": [[2.4428795554112474e-13, 50]], "generated": ["According to the World Health Organization in 2012, the mortality rate for breast cancer in Japan was 23 deaths per 100,000 people, whereas the mortality rate in Sweden was"], "conf_comments": ["(A)\nThus, someone"], "info": null}, {"finished": true, "length": 3, "score": [0.6273211129412736, 0.34847825560796986, 0.2670269604772526], "conf": [0.830957316583, 0.4896145748626381, 0.29427233989693236], "prob": [[3.421567604310785e-08, 23], [9.626912644806087e-18, 34], [8.368922349367445e-18, 6]], "generated": ["The survival rate for breast cancer in Japan is 87%, and in Sweden it is 89%.", "Breast cancer has the highest survival rate in Japan than in Sweden, which indicates that someone is more likely to survive having breast cancer in Japan than in Sweden.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.5749597764260792, 0.3464424914386978], "conf": [0.9541797146384058, 0.8408501854604219], "prob": [[1.1121032589298916e-18, 39], [6.181471762474099e-38, 50]], "generated": ["The global breast cancer survival rate as of 2020 is about 86%, which means that around 14% of those who get diagnosed with breast cancer die.", "The 5-year survival rate (% of people who survive 5 years after being diagnosed with breast cancer), according to [this link](https://www.cancercenter.com/breast-cancer/statistics"], "conf_comments": ["(A)\nThe survival rate", "(A)\nThe 5-"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 43.33517837524414}
{"index": 273, "question": "Did Irish mythology inspire Washington Irving?", "answer": "yes", "explanation": "Yes. Washington Irving's most famous novel was The Legend of Sleepy Hollow. The antagonist of the Legend of Sleepy Hollow, The Headless Horseman, was a man on horseback that was missing his head. The Dullahan is a mythological creature in Irish mythology. The Dullahan comes from the Irish word Gan Ceann, meaning \"without a head\". The Dullahan is depicted as a headless rider, usually on a black horse, who carries their own head in their arm.", "generated": [{"finished": true, "length": 4, "score": [0.8176407048600932, 0.5197739001058653, 0.2919968871046156, 0.2303877447672677], "conf": [0.9600681841978785, 0.87557398164489, 0.5169190347898628, 0.32964577869465517], "prob": [[0.03849518142980884, 9], [7.3987163850564855e-09, 19], [1.1511343131307311e-12, 14], [9.96207619142951e-13, 6]], "generated": ["Washington Irving was an American author.", "He wrote fictional short stories, such as The Legend of Sleepy Hollow.", "Thus, it is unlikely he was inspired by Irish mythology.", "So the answer is no."], "conf_comments": ["(A)\nIrish myth", "(A)\nIrish myth", "(A)\nIrish myth", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8176407048600932, 0.5795825524661138, 0.27703372610979693, 0.21530953471447528], "conf": [0.9600681841978785, 0.8114913914004258, 0.5055513799786674, 0.3230730002913595], "prob": [[0.03849518142980884, 9], [4.456718877528142e-05, 13], [1.3011120469518483e-11, 15], [9.27833265665505e-12, 6]], "generated": ["Washington Irving was an American author.", "Irish mythology is the folklore of Ireland.", "Therefore, Irving did not find inspiration in Irish mythology.", "So the answer is no."], "conf_comments": ["(A)\nIrish myth", "(A)\nThus, Washington", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8176407048600932, 0.5795825524661138, 0.27703372610979693, 0.20887640442865438], "conf": [0.9600681841978785, 0.8114913914004258, 0.5055513799786674, 0.35452781531495864], "prob": [[0.03849518142980884, 9], [4.456718877528142e-05, 13], [1.3011120469518483e-11, 15], [2.4262883812914602e-12, 8]], "generated": ["Washington Irving was an American author.", "Irish mythology is the folklore of Ireland.", "Therefore, Irving did not find inspiration in Irish mythology.", "Thus, the answer is no."], "conf_comments": ["(A)\nIrish myth", "(A)\nThus, Washington", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7501452074610241, 0.389790608549571, 0.2526070396647951, 0.20515371375423055], "conf": [0.9561991712776499, 0.8366552018396776, 0.5542688360299011, 0.36727843248008096], "prob": [[1.2367144926947415e-07, 30], [2.456822463887578e-17, 19], [6.562639022373587e-20, 13], [6.383162931465722e-20, 6]], "generated": ["Washington Irving (1783-1859) was an American writer famous for his work Rip Van Winkle.", "Meanwhile, Irish mythology is myths and legends from the British Isles.", "Thus, Washington Irving was inspired by Irish mythology.", "So the answer is yes."], "conf_comments": ["(A)\nIrish myth", "(A)\nThus, Washington", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8176407048600932, 0.5795825524661138, 0.2664583804802579, 0.19578054743278828], "conf": [0.9600681841978785, 0.8114913914004258, 0.4423136916024696, 0.2577841823817306], "prob": [[0.03849518142980884, 9], [4.456718877528142e-05, 13], [3.473129870051889e-16, 27], [2.1940553651046463e-16, 6]], "generated": ["Washington Irving was an American author.", "Irish mythology is the folklore of Ireland.", "Clearly, Irish mythology is not part of Washington Irving’s life, so he could not have been inspired by it.", "So the answer is no."], "conf_comments": ["(A)\nIrish myth", "(A)\nThus, Washington", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 4, "score": [0.7144899089686976, 0.49572121251651347, 0.23529255626923704, 0.10508666443522122], "conf": [0.9576441860594385, 0.8474665073140911, 0.6636390705703034, 0.38367319889975143], "prob": [[2.77276166383708e-07, 24], [2.9948780955920564e-11, 15], [6.1010723980404654e-27, 29], [2.0292608138370166e-39, 27]], "generated": ["Washington Irving (1783-1859) is a famous American writer of short stories.", "Irish mythology is a branch of Celtic mythology.", "The earliest known writing of Celtic mythology is Ovide moralisé, which dates back to around 1200.", "Thus, there is a gap of about three and a half centuries between the Celtic Mythology and Washington Irving.", "Thus, there is no evidence of Irish mythology inspiring Washington Irving at all.", "So the"], "conf_comments": ["(A)\nIrish myth", "(A)\nThus, Washington", "(A)\nThus, Washington", "(A)\nSo the answer is"], "info": [[[3.3260723385101476e-13, 27], [2.9622867876791753e-08, 19], [0.9599856832399083, 2]], 0.5781353387919715, "(A)\nSo the answer is", 4]}, {"finished": false, "length": 4, "score": [0.7144899089686976, 0.49572121251651347, 0.23529255626923704, 0.1027196140099184], "conf": [0.9576441860594385, 0.8474665073140911, 0.6636390705703034, 0.3420065262864849], "prob": [[2.77276166383708e-07, 24], [2.9948780955920564e-11, 15], [6.1010723980404654e-27, 29], [6.68411959002499e-40, 30]], "generated": ["Washington Irving (1783-1859) is a famous American writer of short stories.", "Irish mythology is a branch of Celtic mythology.", "The earliest known writing of Celtic mythology is Ovide moralisé, which dates back to around 1200.", "Thus, Washington Irving never even lived at the same time as Irish mythology and it is impossible for Irish mythology to have inspired him.", "So the answer is no."], "conf_comments": ["(A)\nIrish myth", "(A)\nThus, Washington", "(A)\nThus, Washington", "(A)\nSo the answer is"], "info": [[[1.0955647063247089e-13, 30], [0.9590726496150601, 6]], 0.5153501978004082, "(A)\nSo the answer is", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 52.86614799499512}
{"index": 274, "question": "Is the tibia required for floor exercises?", "answer": "yes", "explanation": "Yes. The tibia is a bone in the lower leg. Floor exercises are a program in gymnastics competitions. Gymnastics requires use of arms and legs, as well as other parts of the body.", "generated": [{"finished": true, "length": 4, "score": [0.7416916257216651, 0.36960541819084564, 0.3053144819688156, 0.264480156179917], "conf": [0.9601729242644862, 0.7905719164077418, 0.629871432519613, 0.47397862633090754], "prob": [[0.0012507414701692407, 12], [6.446133030556718e-11, 14], [6.308318302933034e-12, 15], [6.203271319542984e-12, 6]], "generated": ["A tibia is a bone in the lower leg.", "Floor exercises are often done without involving the lower leg.", "Thus, a tibia is not required for floor exercises.", "So the answer is no."], "conf_comments": ["(A)\nFloor exercis", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7416916257216651, 0.36960541819084564, 0.242345241081368, 0.20463918697006384], "conf": [0.9601729242644862, 0.7905719164077418, 0.6140715020543082, 0.43952712158177526], "prob": [[0.0012507414701692407, 12], [6.446133030556718e-11, 14], [9.628569547121637e-14, 11], [9.410507071365028e-14, 6]], "generated": ["A tibia is a bone in the lower leg.", "Floor exercises are often done without involving the lower leg.", "Thus, the tibia isn’t required.", "So the answer is no."], "conf_comments": ["(A)\nFloor exercis", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6439165143015428, 0.4063803264487278, 0.17647198043750742, 0.15020742669941098, 0.12914354990269233], "conf": [0.9497298964873471, 0.8357249649860019, 0.6226201596496328, 0.47745656653651913, 0.3540292904756517], "prob": [[2.2983513513281787e-09, 24], [7.133517390407173e-15, 16], [2.0254689494253086e-24, 16], [8.636436839269837e-25, 15], [8.477707464039275e-25, 6]], "generated": ["The word “tibia” refers to the smaller of the two bones in the lower part of the leg.", "In gymnastics, floor exercises are done on the floor.", "There is nothing on the ground, so one does not need a tibia.", "Thus, the tibia is not required for floor exercises.", "So the answer is no."], "conf_comments": ["(A)\nThe word “ex", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6439165143015428, 0.3554289636286796, 0.1832752580164763, 0.15113431311266864, 0.12434311143769931], "conf": [0.9497298964873471, 0.8134149435094883, 0.487669600680764, 0.3544742405846928, 0.24077262014989673], "prob": [[2.2983513513281787e-09, 24], [8.614701056666807e-19, 21], [8.557460441981972e-25, 17], [3.1494669989952206e-25, 15], [3.0846536018995116e-25, 6]], "generated": ["The word “tibia” refers to the smaller of the two bones in the lower part of the leg.", "“Floor exercises” refers to gymnasting activities that take place on the ground.", "There is no relationship between “floor exercises” and “tibia”.", "Thus, the tibia is not required for floor exercises.", "So the answer is no."], "conf_comments": ["(A)\nThe word “ex", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7416916257216651, 0.2775471089079957, 0.21500081084586808, 0.17236913615457025], "conf": [0.9601729242644862, 0.6216766059426856, 0.4281609943265815, 0.27634672151817236], "prob": [[0.0012507414701692407, 12], [2.9337468620891917e-18, 22], [3.7143746458225338e-19, 15], [3.6227244001647193e-19, 6]], "generated": ["A tibia is a bone in the lower leg.", "I can’t think of any exercise where a person with an intact tibia would have difficulty performing.", "Thus, the tibia is not required for floor exercises.", "So the answer is no."], "conf_comments": ["(A)\nFloor exercis", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 59.04214072227478}
{"index": 275, "question": "Would Emma Roberts's Nancy Drew be considered a private investigator?", "answer": "no", "explanation": "No. Emma Roberts starred as Nancy Drew in the 2007 film titled Nancy Drew. A private investigator is hired by an individual to solve a crime. Nancy Drew from the 2007 film was described as an amateur sleuth. Nancy Drew in the 2007 film was interested in a case and decided to pursue it on her own, without being paid for it.", "generated": [{"finished": true, "length": 4, "score": [0.60130040409613, 0.3684048604412736, 0.23826171153840806, 0.20677536251485376], "conf": [0.9659430600150674, 0.9065006352718695, 0.7749395248108017, 0.5899498520600848], "prob": [[2.080493518766223e-08, 18], [2.2332289122478597e-14, 15], [1.0063448216481763e-18, 14], [9.436208186195406e-19, 6]], "generated": ["Nancy Drew is a fictional character who is an aspiring amateur detective.", "She is also the titular protagonist of the Nancy Drew series.", "Thus, Nancy Drew is a private investigator by definition.", "So the answer is yes."], "conf_comments": ["(A)\nEmma Roberts'", "(A)\nEmma Roberts port", "(A)\nEmma Roberts port", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6061073188012222, 0.3338187069007687, 0.26583565577296797], "conf": [0.9650151011521652, 0.6541501812051725, 0.4323721642616954], "prob": [[1.342484425895361e-06, 14], [3.109631235080632e-13, 19], [2.4258171064090424e-13, 6]], "generated": ["Nancy Drew is a fictional character in many mystery books.", "Thus, Emma Roberts playing Nancy Drew would be considered a fictional private investigator.", "So the answer is yes."], "conf_comments": ["(A)\nEmma Roberts played", "(A)\nEmma Roberts is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5035854268035153, 0.2693932180452601, 0.1816365025603778, 0.12041407680324713, 0.09935758380468236], "conf": [0.9699471165054468, 0.7753499572816337, 0.640698211261159, 0.4513329411335563, 0.30910725609032824], "prob": [[2.6679473965571634e-08, 13], [3.1895045431393156e-17, 20], [2.2457346733892583e-21, 16], [1.8963981234180646e-24, 15], [1.8303816232348554e-24, 6]], "generated": ["An investigation is the process of finding out the facts about something.", "Thus, \"private investigator\" and \"detective\" are interchangeable terms.", "Emma Roberts's Nancy Drew is portrayed as a detective.", "Therefore, her Nancy Drew would be considered a private investigator.", "So the answer is yes."], "conf_comments": ["(A)\nA private investigator", "(A)\nEmma Roberts'", "(A)\nThus, Emma", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.60130040409613, 0.3684048604412736, 0.17965337732450712, 0.14752771569348586], "conf": [0.9659430600150674, 0.9065006352718695, 0.6027661093654461, 0.4157666181135009], "prob": [[2.080493518766223e-08, 18], [2.2332289122478597e-14, 15], [2.0456986154027713e-22, 18], [1.7860585391267754e-22, 6]], "generated": ["Nancy Drew is a fictional character who is an aspiring amateur detective.", "She is also the titular protagonist of the Nancy Drew series.", "Thus, a fictional character does not become a private investigator in real life.", "So the answer is no."], "conf_comments": ["(A)\nEmma Roberts'", "(A)\nEmma Roberts port", "(A)\nEmma Roberts'", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5035854268035153, 0.2693932180452601, 0.17531852642796347, 0.13910618541946984], "conf": [0.9699471165054468, 0.7753499572816337, 0.6244234887611245, 0.4509717375413536], "prob": [[2.6679473965571634e-08, 13], [3.1895045431393156e-17, 20], [1.587958835075244e-22, 19], [6.966942620699587e-23, 6]], "generated": ["An investigation is the process of finding out the facts about something.", "Thus, \"private investigator\" and \"detective\" are interchangeable terms.", "In Nancy Drew, Emma Roberts plays Nancy Drew, who is a private detective.", "So the answer is yes."], "conf_comments": ["(A)\nA private investigator", "(A)\nEmma Roberts'", "(A)\nThus, Emma", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 54.487937450408936}
{"index": 276, "question": "Can a minor replicate the double-slit experiment?", "answer": "yes", "explanation": "Yes. A minor is a human child. The double-slit experiment can theoretically be replicated by any human.", "generated": [{"finished": true, "length": 4, "score": [0.6736458270319261, 0.36147284931116985, 0.2842500547299318, 0.23340486753099118], "conf": [0.9686179246673592, 0.8076026141883105, 0.5875688457718785, 0.39735492363154507], "prob": [[0.00023868119701025072, 11], [3.37246579116031e-12, 17], [2.9429177440702486e-13, 15], [2.8904889630603786e-13, 6]], "generated": ["A minor is younger than 18 years old.", "The double-slit experiment would require the expertise of a physicist.", "Thus, a minor cannot replicate the double-slit experiment.", "So the answer is no."], "conf_comments": ["(A)\nThe double-sl", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5364609935336603, 0.3492746824817074, 0.27624680452217937], "conf": [0.9447348774098309, 0.6478977805969776, 0.41064704332258684], "prob": [[4.077485347199833e-13, 24], [7.07126440260425e-17, 18], [6.535603649364223e-17, 6]], "generated": ["The double-slit experiment is usually conducted by physicists in high-school classrooms and science fairs.", "Thus, it is possible for a minor to perform the double-slit experiment.", "So the answer is yes."], "conf_comments": ["(A)\nThus, a", "(A)\nHowever, the double", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.7123338406728925, 0.3100634061184391, 0.2241645801715042, 0.1377478984506755, 0.08013660262211414, 0.05495135976497428, 0.042279997522927784], "conf": [0.9606375893664404, 0.827980017387996, 0.7405877283825858, 0.5782129282313062, 0.474039132648786, 0.35885158350578167, 0.2322864250367673], "prob": [[1.939674296897487e-05, 17], [2.4695165660596587e-13, 12], [2.119316162095433e-19, 26], [3.5193403006714255e-29, 31], [1.0457455980809206e-40, 30], [1.448144105205686e-48, 38], [8.472978782255202e-49, 6]], "generated": ["A minor is an individual who is under the legal age of adulthood.", "This legal age depends on an individual’s jurisdiction.", "For example, in many places in the United States, the legal age of adulthood is 18 years old.", "However in some places in the United States and most other places in the world, the legal age of adulthood is 16 years old.", "If an individual in a country with a legal age of adulthood is 16 years old, they are a minor in that country.", "Therefore, if the legal age of adulthood for an individual’s jurisdiction is 16 years old, a minor can replicate the double-slit experiment.", "So the answer is yes."], "conf_comments": ["(A)\nThe double-sl", "(A)\nIn the United States", "(A)\nThus, a", "(A)\nThus, a", "(A)\nThe double-sl", "(A)\nHowever, if the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.7123338406728925, 0.3100634061184391, 0.2241645801715042, 0.1377478984506755, 0.08013660262211414, 0.05495135976497428, 0.028151061182967792], "conf": [0.9606375893664404, 0.827980017387996, 0.7405877283825858, 0.5782129282313062, 0.474039132648786, 0.35885158350578167, 0.2496893273175634], "prob": [[1.939674296897487e-05, 17], [2.4695165660596587e-13, 12], [2.119316162095433e-19, 26], [3.5193403006714255e-29, 31], [1.0457455980809206e-40, 30], [1.448144105205686e-48, 38], [2.37579272408172e-66, 42]], "generated": ["A minor is an individual who is under the legal age of adulthood.", "This legal age depends on an individual’s jurisdiction.", "For example, in many places in the United States, the legal age of adulthood is 18 years old.", "However in some places in the United States and most other places in the world, the legal age of adulthood is 16 years old.", "If an individual in a country with a legal age of adulthood is 16 years old, they are a minor in that country.", "Therefore, if the legal age of adulthood for an individual’s jurisdiction is 16 years old, a minor can replicate the double-slit experiment.", "Thus, the answer is yes, in a hypothetical situation where a 16 year old minor lives in a jurisdiction that has the legal age of adulthood of 16."], "conf_comments": ["(A)\nThe double-sl", "(A)\nIn the United States", "(A)\nThus, a", "(A)\nThus, a", "(A)\nThe double-sl", "(A)\nHowever, if the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 14, "score": [0.7123338406728925, 0.3100634061184391, 0.2241645801715042, 0.1377478984506755, 0.08013660262211414, 0.043496644971617836, 0.023395341486493276, 0.00986006598859518, 0.004160560321656585, 0.0023524463370540286, 0.0008434834681739383, 0.00034004294202095043, 0.00018383187794380074, 0.00013014426125001018], "conf": [0.9606375893664404, 0.827980017387996, 0.7405877283825858, 0.5782129282313062, 0.474039132648786, 0.3529182326546594, 0.26381545558292785, 0.21164268032624264, 0.148215669310778, 0.11207646201104669, 0.08285812851675135, 0.057965232842208565, 0.04325960592018115, 0.029878926811905886], "prob": [[1.939674296897487e-05, 17], [2.4695165660596587e-13, 12], [2.119316162095433e-19, 26], [3.5193403006714255e-29, 31], [1.0457455980809206e-40, 30], [8.222741963937907e-57, 40], [2.0823539803048164e-63, 16], [6.559400633308158e-91, 42], [5.083554422813193e-101, 17], [1.2528530846450482e-106, 15], [1.0655378999470884e-131, 33], [2.2346381095692215e-144, 20], [1.4814458681345343e-154, 25], [2.1629254869036605e-155, 6]], "generated": ["A minor is an individual who is under the legal age of adulthood.", "This legal age depends on an individual’s jurisdiction.", "For example, in many places in the United States, the legal age of adulthood is 18 years old.", "However in some places in the United States and most other places in the world, the legal age of adulthood is 16 years old.", "If an individual in a country with a legal age of adulthood is 16 years old, they are a minor in that country.", "However in a country with a legal age of adulthood of 18 years old, for an individual to be 16 years old would mean that they are not legal adults.", "When an individual is not a legal adult, they are still considered a minor.", "To do the double-slit experiment, an individual must:\n1) Have enough money to purchase a laser, slit apparatus, measuring device, beaker, and the necessary chemicals,", "2) Have access to a lab (e.g. a university), and,", "3) Be an expert in conducting the double-slit experiment.", "An individual under the age of 18 likely lacks the sufficient funds to purchase such resources, and is unlikely to be an expert at such laboratory experiments.", "Furthermore, most universities have 18 as their minimum age for a lab.", "Thus, in most places, at least 18 years old is necessary to conduct the double-slit experiment.", "So the answer is no."], "conf_comments": ["(A)\nThe double-sl", "(A)\nIn the United States", "(A)\nThus, a", "(A)\nThus, a", "(A)\nThe double-sl", "(A)\nThus, a", "(A)\nThus, a", "(A)\n2) Have enough", "(A)\n3) Be able", "(A)\nSince a minor is", "(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 14, "score": [0.7123338406728925, 0.3100634061184391, 0.2241645801715042, 0.1377478984506755, 0.08013660262211414, 0.043496644971617836, 0.023395341486493276, 0.00986006598859518, 0.004160560321656585, 0.0023524463370540286, 0.0008434834681739383, 0.00034004294202095043, 0.00018383187794380074, 0.000129070387621481], "conf": [0.9606375893664404, 0.827980017387996, 0.7405877283825858, 0.5782129282313062, 0.474039132648786, 0.3529182326546594, 0.26381545558292785, 0.21164268032624264, 0.148215669310778, 0.11207646201104669, 0.08285812851675135, 0.057965232842208565, 0.04325960592018115, 0.03283985105924546], "prob": [[1.939674296897487e-05, 17], [2.4695165660596587e-13, 12], [2.119316162095433e-19, 26], [3.5193403006714255e-29, 31], [1.0457455980809206e-40, 30], [8.222741963937907e-57, 40], [2.0823539803048164e-63, 16], [6.559400633308158e-91, 42], [5.083554422813193e-101, 17], [1.2528530846450482e-106, 15], [1.0655378999470884e-131, 33], [2.2346381095692215e-144, 20], [1.4814458681345343e-154, 25], [1.4810617492201425e-157, 16]], "generated": ["A minor is an individual who is under the legal age of adulthood.", "This legal age depends on an individual’s jurisdiction.", "For example, in many places in the United States, the legal age of adulthood is 18 years old.", "However in some places in the United States and most other places in the world, the legal age of adulthood is 16 years old.", "If an individual in a country with a legal age of adulthood is 16 years old, they are a minor in that country.", "However in a country with a legal age of adulthood of 18 years old, for an individual to be 16 years old would mean that they are not legal adults.", "When an individual is not a legal adult, they are still considered a minor.", "To do the double-slit experiment, an individual must:\n1) Have enough money to purchase a laser, slit apparatus, measuring device, beaker, and the necessary chemicals,", "2) Have access to a lab (e.g. a university), and,", "3) Be an expert in conducting the double-slit experiment.", "An individual under the age of 18 likely lacks the sufficient funds to purchase such resources, and is unlikely to be an expert at such laboratory experiments.", "Furthermore, most universities have 18 as their minimum age for a lab.", "Thus, in most places, at least 18 years old is necessary to conduct the double-slit experiment.", "Thus, a minor can not replicate the double-slit experiment.", "So the answer is no."], "conf_comments": ["(A)\nThe double-sl", "(A)\nIn the United States", "(A)\nThus, a", "(A)\nThus, a", "(A)\nThe double-sl", "(A)\nThus, a", "(A)\nThus, a", "(A)\n2) Have enough", "(A)\n3) Be able", "(A)\nSince a minor is", "(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[0.0009997407134997949, 16], [0.9423596107430634, 6]], 0.7591343092639051, "(A)\nSo the answer is", 14]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": [], "9": [], "10": [], "11": [], "12": [], "13": [], "14": []}, "run_time": 120.23254680633545}
{"index": 277, "question": "Has categories of Nobel prizes remained same since Alfred Nobel established them?", "answer": "no", "explanation": "No. Alfred Nobel established the Nobel prize in his will in 1895. Alfred Nobel established 5 Nobel prize categories: Chemistry, Literature, Peace, Physics, and Physiology or Medicine. The Nobel prize evolved to include a sixth category of Economic Sciences in 1968.", "generated": [{"finished": true, "length": 4, "score": [0.7352265456518194, 0.3727219184894927, 0.2711330278566284, 0.22742495303110793], "conf": [0.957695642986289, 0.6203542582614668, 0.45465300091340866, 0.32082465770232804], "prob": [[3.5884324873942383e-09, 34], [1.9546121679927505e-26, 43], [2.0927099894728624e-29, 21], [2.0561332564163272e-29, 6]], "generated": ["[Nobel prizes](https://en.wikipedia.org/wiki/Nobel_Prize) were established around 1901.", "Since then, there have been some changes to which [Nobel prizes](https://www.nobelprize.org/awards/lists/2020/) are awarded.", "Thus, the categories of the Nobel prizes have not remained the same since Alfred Nobel established them.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7455688010525505, 0.3464339617595147, 0.2433160281284137, 0.19903880001673663], "conf": [0.9674290453833818, 0.8714768534372944, 0.6968729849494165, 0.475385318007236], "prob": [[6.03517563006906e-08, 30], [1.2459258140927782e-16, 14], [1.2863060674538075e-20, 19], [1.1460295574404627e-20, 6]], "generated": ["Alfred Nobel established six categories of Nobel prizes: peace, chemistry, physics, physiology or medicine, literature, and economics.", "Only physiology has been renamed to physiology or medicine.", "Thus, categories of Nobel Prizes have remained the same since Alfred Nobel established them.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5256174784037319, 0.37476091436175946, 0.29347141575058655], "conf": [0.9512692405794402, 0.8341456101414112, 0.6284077119052824], "prob": [[3.176697467715056e-15, 27], [1.538876664093513e-18, 14], [4.47655779551432e-19, 6]], "generated": ["The five main categories of the awards are Physics, Chemistry, Physiology of Medicine or Medicine, Peace, and Literature.", "These five categories have remained the same since Alfred Nobel established them.", "So the answer is yes."], "conf_comments": ["(A)\nThe first Nobel Prize", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7352265456518194, 0.3727219184894927, 0.22860496050746631, 0.18525082630270637], "conf": [0.957695642986289, 0.6203542582614668, 0.4157445546262071, 0.2742407827519366], "prob": [[3.5884324873942383e-09, 34], [1.9546121679927505e-26, 43], [1.8851818182884392e-31, 20], [1.8349087326982193e-31, 6]], "generated": ["[Nobel prizes](https://en.wikipedia.org/wiki/Nobel_Prize) were established around 1901.", "Since then, there have been some changes to which [Nobel prizes](https://www.nobelprize.org/awards/lists/2020/) are awarded.", "Thus, the number of categories of Nobel Prizes has changed since Alfred Nobel established them.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7455688010525505, 0.3464339617595147, 0.20772638208320526, 0.16776282132558645], "conf": [0.9674290453833818, 0.8714768534372944, 0.6880777945594421, 0.45746406399869777], "prob": [[6.03517563006906e-08, 30], [1.2459258140927782e-16, 14], [1.8310758578000448e-23, 20], [1.6324446234294367e-23, 6]], "generated": ["Alfred Nobel established six categories of Nobel prizes: peace, chemistry, physics, physiology or medicine, literature, and economics.", "Only physiology has been renamed to physiology or medicine.", "Thus, the categories of Nobel prizes are still the same as when they were first created.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 50.885536670684814}
{"index": 278, "question": "Can members of the Green Party of England and Wales vote in the USA?", "answer": "no", "explanation": "No. Green Party of England Wales isn't registered in the USA. People who live in England can't vote in the USA.", "generated": [{"finished": true, "length": 3, "score": [0.6822240430970238, 0.5564064657247311, 0.4836845354845434], "conf": [0.9623228623161925, 0.8005759555468778, 0.6090954061459352], "prob": [[1.8535255240268785e-05, 15], [1.979174686933899e-06, 10], [1.9003248505925588e-06, 6]], "generated": ["The members of the Green Party of England and Wales are not US citizens.", "Thus, they cannot vote in the USA.", "So the answer is no."], "conf_comments": ["(A)\nThus, they", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6822240430970238, 0.40606522511196425, 0.30430983561281455, 0.2555828955431309], "conf": [0.9623228623161925, 0.8626663911386743, 0.6636514522389249, 0.4698961715972155], "prob": [[1.8535255240268785e-05, 15], [1.0631629177854885e-10, 13], [1.4348524762105702e-13, 21], [1.4028966073031025e-13, 6]], "generated": ["The members of the Green Party of England and Wales are not US citizens.", "US citizens are allowed to vote in the United States of America.", "Thus, members of the Green Party of England and Wales are not allowed to vote in the US.", "So the answer is no."], "conf_comments": ["(A)\nThus, they", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.730366689677818, 0.5313005318131278, 0.28270661185226137, 0.22737725320017327], "conf": [0.96975084147968, 0.8417801112479844, 0.5888643950999085, 0.39223891229654406], "prob": [[2.1261382922313363e-05, 18], [1.2426079488042632e-11, 29], [3.933518999056849e-17, 14], [3.2999060318988575e-17, 6]], "generated": ["The Green Party of England and Wales is recognized as a political party in England and Wales.", "Since England and Wales are part of the United Kingdom, the Green Party of England and Wales is recognized as a political party in the United Kingdom.", "Thus, citizens of England and Wales can vote in the USA.", "So the answer is yes."], "conf_comments": ["(A)\nThe Green Party of", "(A)\nThe United Kingdom is", "(A)\nSince the Green Party", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.730366689677818, 0.3469109773922449, 0.23102124603139135, 0.18877035057686936], "conf": [0.96975084147968, 0.844803853514644, 0.5970922353728476, 0.40577569844761424], "prob": [[2.1261382922313363e-05, 18], [5.837724295344426e-16, 18], [8.323496196975511e-20, 19], [7.485493694107649e-20, 6]], "generated": ["The Green Party of England and Wales is recognized as a political party in England and Wales.", "Similarly, USA recognizes political party like the Republican Party and the Democratic Party.", "As a result, members of the Green Party of England and Wales can vote in the USA.", "So the answer is yes."], "conf_comments": ["(A)\nThe Green Party of", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.655432342846792, 0.35278176350056367, 0.23142070961208563, 0.18259469660743954], "conf": [0.947122488047578, 0.6800593374153202, 0.44272593202188065, 0.27718822715375707], "prob": [[1.2674782719438934e-08, 23], [6.25164412453059e-15, 16], [5.4886159951005314e-18, 17], [5.30463113094377e-18, 6]], "generated": ["Members of the Green Party of England and Wales are members of the party of the same name in the UK.", "According to Wikipedia, the UK is not a member of the United Nations.", "Thus, members of the Green Party of England and Wales cannot vote in USA.", "So the answer is no."], "conf_comments": ["(A)\nThe Green Party of", "(A)\nThus, members", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 51.00567936897278}
{"index": 279, "question": "Did Alan Turing suffer the same fate as Abraham Lincoln?", "answer": "no", "explanation": "No. Alan Turing committed suicide via cyanide poisoning. Abraham Lincoln was killed by a gunshot wound to the head.", "generated": [{"finished": true, "length": 4, "score": [0.8106682547629821, 0.7084114131063609, 0.5729063962090518, 0.46959026108989166], "conf": [0.9789853535881226, 0.8359777297424896, 0.6160822122349316, 0.4158502235412908], "prob": [[0.008373782308809499, 12], [0.0015664460605411819, 15], [0.00023192733915577992, 16], [0.0002255211381418541, 6]], "generated": ["Alan Turing died in 1954.", "Abraham Lincoln was assassinated in 1865.", "Thus, Alan Turing did not suffer the same fate as Abraham Lincoln.", "So the answer is no."], "conf_comments": ["(A)\nAbraham Lincoln", "(A)\nThus, Alan", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8280292649653933, 0.5837857285452903, 0.45787317724786386, 0.3654146931853734], "conf": [0.9641967954326157, 0.7947078026940945, 0.557487851335563, 0.3564220532658782], "prob": [[0.00845200302134492, 14], [1.179828293710825e-05, 13], [1.4424715025047672e-06, 16], [1.4100091237228215e-06, 6]], "generated": ["Alan Turing committed suicide by cyanide poisoning.", "Abraham Lincoln died from an assassin’s bullet.", "Thus, Alan Turing did not suffer the same fate as Abraham Lincoln.", "So the answer is no."], "conf_comments": ["(A)\nAbraham Lincoln", "(A)\nThus, Alan", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8280292649653933, 0.5837857285452903, 0.4270081778477591, 0.3451615486047723], "conf": [0.9641967954326157, 0.7947078026940945, 0.5834454583082627, 0.3832635071935158], "prob": [[0.00845200302134492, 14], [1.179828293710825e-05, 13], [3.6317669000618033e-07, 11], [3.516975367943881e-07, 6]], "generated": ["Alan Turing committed suicide by cyanide poisoning.", "Abraham Lincoln died from an assassin’s bullet.", "Thus, they did not suffer the same fate.", "So the answer is no."], "conf_comments": ["(A)\nAbraham Lincoln", "(A)\nThus, Alan", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7500075077596329, 0.5366450843511769, 0.4370073796832946], "conf": [0.8791356779028744, 0.6659335434848574, 0.44453299658432505], "prob": [[0.0007892326804663151, 16], [2.3228330376843394e-05, 9], [2.232537210722185e-05, 6]], "generated": ["Alan Turing committed suicide, while Abraham Lincoln was assassinated.", "Thus, they suffered different fates.", "So the answer is no."], "conf_comments": ["(A)\nThus, Alan", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8701461967780977, 0.7885307892941845, 0.391775951721444, 0.3102307658731368], "conf": [0.9741030018888239, 0.8251630710630212, 0.6375499900425655, 0.42715611627860856], "prob": [[0.005036988849981306, 21], [0.002624509096692131, 21], [3.428516934092822e-16, 26], [2.3037591424994366e-16, 6]], "generated": ["Alan Turing was born in 1912 and died in 1954.", "Abraham Lincoln was born in 1809 and died in 1865.", "This shows that Alan Turing did not suffer the same fate as Abraham Lincoln since his death was 11 years beforehand.", "So the answer is no."], "conf_comments": ["(A)\nAbraham Lincoln", "(A)\nThus, Alan", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 50.44375824928284}
{"index": 280, "question": "Is Rand Paul guilty of catch-phrase used to attack John Kerry in 2004?", "answer": "yes", "explanation": "Yes. John Kerry was attacked by his opponents in the 2004 Presidential Campaign by being called a Flip-Flopper. A Flip-Flopper is someone that makes a complete change in policy from one thing to another. In May 2010 Rand Paul called for an electronic fence to keep out immigrants and deplored amnesty in any form. In 2013 Rand Paul said he was in favor of undocumented immigrants being granted  legal status.", "generated": [{"finished": true, "length": 4, "score": [0.7043286247411309, 0.5953539768550433, 0.36308762125639127, 0.285363315466118], "conf": [0.9744958723532975, 0.8643908826104469, 0.5228887863925156, 0.32506065636090714], "prob": [[5.270746546444318e-06, 18], [1.0743013712367549e-07, 18], [2.1281536539120162e-13, 27], [2.0479195684086754e-13, 6]], "generated": ["John Kerry was a Democratic nominee for president in 2004.", "Rand Paul is a Republican nominee for president in 2016.", "Thus, Rand Paul could not guilty of a catch-phrase used to attack John Kerry in 2004.", "So the answer is no."], "conf_comments": ["(A)\nRand Paul is", "(A)\nThus, Rand", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.678668211075152, 0.5176625720923569, 0.34038831844448436, 0.27309327935238903], "conf": [0.9643252056530669, 0.8329498786888436, 0.5920492034171703, 0.3834461068515602], "prob": [[4.939163266912283e-10, 29], [1.1481506559013524e-14, 27], [3.360874799766209e-19, 21], [3.2389893993525613e-19, 6]], "generated": ["Senator Rand Paul used the phrase “What does it matter?” to attack Secretary of State John Kerry in 2013.", "In 2004, then-Senator John Kerry used the same phrase to attack President George W. Bush.", "Thus, Rand Paul is guilty of using the same catch-phrase used by John Kerry.", "So the answer is yes."], "conf_comments": ["(A)\nIn 20", "(A)\nThus, Senator", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.7043286247411309, 0.39209012149719763, 0.32570600976487135, 0.21069043290134976, 0.16811839444006418, 0.139134211786133], "conf": [0.9744958723532975, 0.910254408466857, 0.7971442553470969, 0.6106045105291827, 0.4390696822887182, 0.30168148921755716], "prob": [[5.270746546444318e-06, 18], [4.565382753941956e-16, 21], [6.260056158880645e-18, 18], [3.9374787314050024e-22, 16], [1.665599104911968e-23, 26], [1.6341938785097124e-23, 6]], "generated": ["John Kerry was a Democratic nominee for president in 2004.", "The catch-phrase that was used to attack him was, “swift boat vets for truth.”", "Rand Paul is a Republican nominee for president in 2016.", "There are no “swift boat vets for truth” attacking Rand Paul.", "Thus, Rand Paul is not guilty of catch-phrase used to attack John Kerry in 2004.", "So the answer is no."], "conf_comments": ["(A)\nRand Paul is", "(A)\nRand Paul is", "(A)\nThe catch-phr", "(A)\nThus, Rand", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.678668211075152, 0.5176625720923569, 0.3236137473704926, 0.26034214450932797], "conf": [0.9643252056530669, 0.8329498786888436, 0.5934524853094687, 0.38670139574403745], "prob": [[4.939163266912283e-10, 29], [1.1481506559013524e-14, 27], [5.7202043926773945e-22, 28], [5.491399152297483e-22, 6]], "generated": ["Senator Rand Paul used the phrase “What does it matter?” to attack Secretary of State John Kerry in 2013.", "In 2004, then-Senator John Kerry used the same phrase to attack President George W. Bush.", "Thus, Senator Rand Paul is guilty of using a phrase that was also used in 2004 to attack John Kerry.", "So the answer is yes."], "conf_comments": ["(A)\nIn 20", "(A)\nThus, Senator", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.7043286247411309, 0.39209012149719763, 0.32570600976487135, 0.21069043290134976, 0.13936473676123856, 0.1146108156598938], "conf": [0.9744958723532975, 0.910254408466857, 0.7971442553470969, 0.6106045105291827, 0.4350022841597522, 0.29519082005535785], "prob": [[5.270746546444318e-06, 18], [4.565382753941956e-16, 21], [6.260056158880645e-18, 18], [3.9374787314050024e-22, 16], [1.846499660721041e-24, 11], [1.8094801593153824e-24, 6]], "generated": ["John Kerry was a Democratic nominee for president in 2004.", "The catch-phrase that was used to attack him was, “swift boat vets for truth.”", "Rand Paul is a Republican nominee for president in 2016.", "There are no “swift boat vets for truth” attacking Rand Paul.", "Thus, Rand Paul is not guilty of this.", "So the answer is no."], "conf_comments": ["(A)\nRand Paul is", "(A)\nRand Paul is", "(A)\nThe catch-phr", "(A)\nThus, Rand", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 63.695520877838135}
{"index": 281, "question": "Can you chew argon?", "answer": "no", "explanation": "No. Chewing is the act of breaking down solid objects with your teeth. Under normal conditions, argon exists as a gas.", "generated": [{"finished": true, "length": 2, "score": [0.6851858434740394, 0.5235747162472627], "conf": [0.9418048131995133, 0.7169899094997888], "prob": [[0.00011736163351015052, 13], [2.3892232465012058e-05, 6]], "generated": ["Argon is a gas and can’t be chewed.", "So the answer is no."], "conf_comments": ["(A)\nThus, you", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6519424914487653, 0.42737543497727853, 0.3699925842181413], "conf": [0.9535184505556282, 0.7698005305231358, 0.5959698442150164], "prob": [[0.00013803341122457875, 11], [2.520568650882338e-07, 10], [2.075026371482799e-07, 6]], "generated": ["Argon is a noble gas and unreactive.", "Therefore, argon cannot be chewed.", "So the answer is no."], "conf_comments": ["(A)\nThus, it", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6851858434740394, 0.3446397076782327, 0.2901504662981481, 0.24755251323125513], "conf": [0.9418048131995133, 0.7255963310137933, 0.5683433146687917, 0.41459725078163134], "prob": [[0.00011736163351015052, 13], [1.1079934362252723e-18, 29], [3.339950917526792e-19, 12], [3.2974249156270523e-19, 6]], "generated": ["Argon is a gas and can’t be chewed.", "Humans can chew anything solid or liquid, but gases are not solids or liquids and can’t be chewed.", "Thus, you can’t chew argon.", "So the answer is no."], "conf_comments": ["(A)\nThus, you", "(A)\nThus, you", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6851858434740394, 0.3446397076782327, 0.27807273405100574, 0.23865482373015934], "conf": [0.9418048131995133, 0.7255963310137933, 0.5623034663089433, 0.4150491319456855], "prob": [[0.00011736163351015052, 13], [1.1079934362252723e-18, 29], [1.3685581149379674e-19, 12], [1.3515510916440148e-19, 6]], "generated": ["Argon is a gas and can’t be chewed.", "Humans can chew anything solid or liquid, but gases are not solids or liquids and can’t be chewed.", "Thus, argon can’t be chewed.", "So the answer is no."], "conf_comments": ["(A)\nThus, you", "(A)\nThus, you", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6026174778379072, 0.33380261687649254, 0.2739776760251471], "conf": [0.9270753550571772, 0.7234270334010882, 0.5468244603794982], "prob": [[1.023639628308711e-11, 27], [1.398115487002923e-16, 12], [7.006951891886282e-17, 6]], "generated": ["Argon is colorless, odorless, tasteless, and a noble gas, meaning it is an inert element.", "Thus, argon is not an edible element.", "So the answer is no."], "conf_comments": ["(A)\nThus, arg", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 42.89036417007446}
{"index": 282, "question": "Do people remember Lucille Ball's winemaking as successful?", "answer": "no", "explanation": "No. Lucille Ball was the star of \"I Love Lucy\". On \"I Love Lucy\", Lucille's character fails miserably while stomping grapes for wine.", "generated": [{"finished": true, "length": 3, "score": [0.6100208309044232, 0.33830213559641314, 0.26718647347654556], "conf": [0.9509177239521645, 0.6904439770248668, 0.43783052589901406], "prob": [[1.9753744990746195e-06, 14], [6.828767593404682e-14, 20], [6.18576648707253e-14, 6]], "generated": ["Lucille Ball’s winemaking was a failed attempt.", "Thus, people would likely remember her winemaking as a failure rather than as a success.", "So the answer is no."], "conf_comments": ["(A)\nThus, people", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5503322492288817, 0.331119093120626, 0.26575235757634613], "conf": [0.9530283283685689, 0.7172958843420283, 0.4791559577183118], "prob": [[1.1147322143807216e-11, 22], [2.3540539016431593e-18, 21], [1.8926046665539493e-18, 6]], "generated": ["Lucille Ball became internationally known and successful as an actress, not as a winemaker.", "Thus, people probably don't remember or know that Lucille Ball was a winemaker.", "So the answer is no."], "conf_comments": ["(A)\nThus, people", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5651931907389295, 0.3148548364213065, 0.2534366669315853], "conf": [0.964622478511557, 0.6427046926986879, 0.4228889899068822], "prob": [[5.760683708770152e-07, 13], [1.3028465570318854e-11, 14], [1.1877164730490273e-11, 6]], "generated": ["Lucille Ball won multiple Emmy Awards during her career.", "Thus, people probably remember her winemaking as a success.", "So the answer is yes."], "conf_comments": ["(A)\nIn 19", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5651931907389295, 0.3238291080111815, 0.18687651843636732, 0.14334230143619467], "conf": [0.964622478511557, 0.8590168694802363, 0.561279883201665, 0.3337933652783109], "prob": [[5.760683708770152e-07, 13], [1.3371833757353762e-12, 13], [4.861711154478801e-19, 22], [4.558641887463212e-19, 6]], "generated": ["Lucille Ball won multiple Emmy Awards during her career.", "Emmy Awards are for excellence in the field of television.", "Thus, Lucille Ball’s winemaking could be successful and people could remember it as successful.", "So the answer is yes."], "conf_comments": ["(A)\nIn 19", "(A)\nThus, Luc", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.5503322492288817, 0.36342972413358926], "conf": [0.9530283283685689, 0.7297931488198927], "prob": [[1.1147322143807216e-11, 22], [1.233520805924617e-13, 8]], "generated": ["Lucille Ball became internationally known and successful as an actress, not as a winemaker.", "Thus, the answer is no."], "conf_comments": ["(A)\nThus, people", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 40.00211262702942}
{"index": 283, "question": "Can oysters be used in guitar manufacturing?", "answer": "yes", "explanation": "Yes. Oysters produce nacre. Nacre is also known as mother of pearl. Mother of pearl is commonly used as an inlay on guitar fretboards, headstocks, and soundboards.", "generated": [{"finished": true, "length": 3, "score": [0.5795077298230312, 0.3723431600392762, 0.3206425072611083], "conf": [0.9331993171578421, 0.7614209341564934, 0.5664219580501236], "prob": [[1.037264371311317e-12, 27], [2.919471110860575e-16, 12], [2.8651354679255807e-16, 6]], "generated": ["According to the New York Times, oyster shells are used in the guitar factories of Gibson Guitars.", "Thus, guitar manufacturing does use oysters.", "So the answer is yes."], "conf_comments": ["(A)\nThus, o", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6873669291793304, 0.36475792564427423, 0.2062244254447111, 0.16728384184360695, 0.13743629185572187], "conf": [0.9514429500000205, 0.7355957762443414, 0.5921692662736009, 0.41626318150911146, 0.28154368862517476], "prob": [[2.7537069148934585e-05, 15], [2.24677498577862e-15, 23], [1.3702278605556327e-20, 13], [5.433402289621153e-21, 14], [5.367527777608468e-21, 6]], "generated": ["Oysters are part of the phylum Mollusca.", "Guitars and drums are made of wood, which is part of the phylum Bryophyta.", "These two phyla are different and are not related.", "Thus, oysters cannot be used in guitar manufacturing.", "So the answer is no."], "conf_comments": ["(A)\nThe phylum", "(A)\nThus, o", "(A)\nThus, o", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5795077298230312, 0.35991245401726, 0.30154370929962065], "conf": [0.9331993171578421, 0.6885825706364707, 0.5012644537104274], "prob": [[1.037264371311317e-12, 27], [1.432741703420169e-22, 35], [1.1517106474843759e-22, 6]], "generated": ["According to the New York Times, oyster shells are used in the guitar factories of Gibson Guitars.", "Since Gibson Guitars uses oyster shells in guitar manufacturing, we can deduce that yes, oysters can be used in guitar manufacturing.", "So the answer is yes."], "conf_comments": ["(A)\nThus, o", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6197342642323221, 0.34236929423528134, 0.27828366143180405], "conf": [0.9564988741267021, 0.6021488353889556, 0.41003766927550817], "prob": [[1.2392640117842347e-10, 25], [2.379863244201563e-15, 15], [1.9849399989079253e-15, 6]], "generated": ["A guitar is an acoustic musical instrument that typically has six strings made of nylon, steel, or both.", "Therefore, oysters can not be used in guitar manufacturing.", "So the answer is no."], "conf_comments": ["(A)\nOysters are", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6873669291793304, 0.36475792564427423, 0.20870457340734644, 0.1622626382288719], "conf": [0.9514429500000205, 0.7355957762443414, 0.43368870193583947, 0.26342731331207336], "prob": [[2.7537069148934585e-05, 15], [2.24677498577862e-15, 23], [2.988256183130675e-21, 23], [2.90242138343788e-21, 6]], "generated": ["Oysters are part of the phylum Mollusca.", "Guitars and drums are made of wood, which is part of the phylum Bryophyta.", "Thus, oysters cannot be used in guitar manufacturing because they are not made of the same stuff.", "So the answer is no."], "conf_comments": ["(A)\nThe phylum", "(A)\nThus, o", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 48.15957307815552}
{"index": 284, "question": "Would James Cotton's instrument be too strident for a smooth jazz band?", "answer": "yes", "explanation": "Yes. Smooth jazz is an offshoot of jazz music that relies on a more melodic form. Smooth jazz employs the following instruments: saxophone. guitar. piano. trumpet. synthesizer. electric bass. and drums. James Cotton was a famous blues harmonica player.", "generated": [{"finished": true, "length": 4, "score": [0.7765997868974918, 0.4063720621214707, 0.2884844700714404, 0.2444849762019934], "conf": [0.9660790714208446, 0.8251357746430839, 0.6037030628226955, 0.4379815967724217], "prob": [[0.0035041470432918683, 12], [1.3237550599000508e-09, 13], [1.7226186581351676e-13, 24], [1.6216274940477033e-13, 6]], "generated": ["James Cotton is a famous blues harmonica player.", "Harmonicas and smooth jazz are different types of music.", "Thus, it makes sense that James Cotton's instrument would be too strident for a smooth jazz band.", "So the answer is yes."], "conf_comments": ["(A)\nSmooth jazz", "(A)\nThus, James", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8397857216599021, 0.6007725922676556, 0.2318885647650264, 0.18550688727052123, 0.15765877043294904], "conf": [0.9641081790629089, 0.8528035286751108, 0.6389884806503352, 0.47241105508727393, 0.3436064717810284], "prob": [[0.04386438596697333, 10], [3.571021228894856e-05, 13], [6.624831973243833e-20, 21], [3.697112182512854e-21, 20], [3.54578252405275e-21, 6]], "generated": ["James Cotton is a blues musician.", "Blues and smooth jazz are two different genres of music.", "So, a smooth jazz band would not be the right group for him to be a part of. ", "Thus, James Cotton's instrument would be too strident for a smooth jazz band.", "So the answer is yes."], "conf_comments": ["(A)\nBlues is a", "(A)\nThus, James", "(A)\n# Is the above", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8397857216599021, 0.6007725922676556, 0.2318885647650264, 0.16620588842777098, 0.14109099944122067], "conf": [0.9641081790629089, 0.8528035286751108, 0.6389884806503352, 0.47230620673017576, 0.34244442958969395], "prob": [[0.04386438596697333, 10], [3.571021228894856e-05, 13], [6.624831973243833e-20, 21], [1.964405944920553e-22, 16], [1.893508962053428e-22, 6]], "generated": ["James Cotton is a blues musician.", "Blues and smooth jazz are two different genres of music.", "So, a smooth jazz band would not be the right group for him to be a part of. ", "Thus, his instrument would be too strident for the smooth jazz band.", "So the answer is yes."], "conf_comments": ["(A)\nBlues is a", "(A)\nThus, James", "(A)\n# Is the above", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7765997868974918, 0.4063720621214707, 0.2884844700714404, 0.17224082690600623], "conf": [0.9660790714208446, 0.8251357746430839, 0.6037030628226955, 0.4064494068026367], "prob": [[0.0035041470432918683, 12], [1.3237550599000508e-09, 13], [1.7226186581351676e-13, 24], [3.795372561537213e-15, 6]], "generated": ["James Cotton is a famous blues harmonica player.", "Harmonicas and smooth jazz are different types of music.", "Thus, it makes sense that James Cotton's instrument would be too strident for a smooth jazz band.", "So the answer is no."], "conf_comments": ["(A)\nSmooth jazz", "(A)\nThus, James", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7765997868974918, 0.4063720621214707, 0.18782631733678024, 0.14545020245956822], "conf": [0.9660790714208446, 0.8251357746430839, 0.4929765736183115, 0.2978167237971942], "prob": [[0.0035041470432918683, 12], [1.3237550599000508e-09, 13], [1.1551668375321851e-21, 27], [1.105102479574071e-21, 6]], "generated": ["James Cotton is a famous blues harmonica player.", "Harmonicas and smooth jazz are different types of music.", "Thus the instruments that they play are different and James Cotton's instrument is not too strident for a smooth jazz band.", "So the answer is no."], "conf_comments": ["(A)\nSmooth jazz", "(A)\nThus, James", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 60.004321813583374}
{"index": 285, "question": "Are emus related to elks?", "answer": "no", "explanation": "No. Emus are a type of flightless bird. Elks are deer, which are mammals.", "generated": [{"finished": true, "length": 3, "score": [0.7663730007806557, 0.5809205261989822, 0.47449038423525125], "conf": [0.8631756875194576, 0.6285705566544948, 0.421094698200373], "prob": [[0.009848595860127752, 12], [0.0005735257496406029, 12], [0.0005594055978574391, 6]], "generated": ["Emus are birds and elks are mammals.", "Thus, emus are not related to elks.", "So the answer is no."], "conf_comments": ["(A)\nThus, em", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7471928277704301, 0.5556853686909098, 0.4668087481041346], "conf": [0.9344117394355889, 0.7498989324769455, 0.5316865646304756], "prob": [[9.4164640568003e-05, 18], [6.9531956712318785e-06, 7], [6.760627985409164e-06, 6]], "generated": ["Emus and elks are both members of the order Artiodactyla.", "Thus, they are related.", "So the answer is yes."], "conf_comments": ["(A)\nThus, em", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7663730007806557, 0.5556236969767149, 0.45345581025843507], "conf": [0.8631756875194576, 0.6375477919456536, 0.42696671077679954], "prob": [[0.009848595860127752, 12], [0.0006479104156786254, 8], [0.000627014922454566, 6]], "generated": ["Emus are birds and elks are mammals.", "Thus, they are not related.", "So the answer is no."], "conf_comments": ["(A)\nThus, em", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6755364391843441, 0.49638854876088795, 0.413151833841786], "conf": [0.886153296334823, 0.6786680790117451, 0.47166913838821584], "prob": [[3.3418377679741817e-06, 19], [7.147539561675304e-08, 11], [7.010217710892968e-08, 6]], "generated": ["Emus are flightless birds that are related to ostriches, not elks.", "Thus, they are not related to elks.", "So the answer is no."], "conf_comments": ["(A)\nElks are m", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7471928277704301, 0.4199481830418569, 0.3293334535108472, 0.2787299627395992], "conf": [0.9344117394355889, 0.8224839821142276, 0.6523532200854517, 0.4684181149459133], "prob": [[9.4164640568003e-05, 18], [9.296502821942452e-09, 9], [5.663615302833323e-10, 11], [5.581645083440109e-10, 6]], "generated": ["Emus and elks are both members of the order Artiodactyla.", "This means that they are cousins.", "Thus, emus and elks are related.", "So the answer is yes."], "conf_comments": ["(A)\nThus, em", "(A)\nThus, em", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 43.51627230644226}
{"index": 286, "question": "Is the Mona Lisa in the same museum as the Venus de Milo?", "answer": "yes", "explanation": "Yes. The Mona Lisa is in the Louvre. The Venus de Milo is in the Louvre.", "generated": [{"finished": true, "length": 4, "score": [0.8047527060219182, 0.6828709348693282, 0.5734559474487005, 0.4696138319719629], "conf": [0.9725860367513898, 0.7487095560832752, 0.5693322675543202, 0.38265546015234897], "prob": [[0.00506010111919252, 13], [0.0018563697345092458, 15], [0.0004112191726430488, 20], [0.00040579854416147073, 6]], "generated": ["The Mona Lisa is in the Louvre Museum in France.", "The Venus de Milo is in the Louvre Museum in France.", "Thus, the Mona Lisa is in the same museum as the Venus de Milo.", "So the answer is yes."], "conf_comments": ["(A)\nThe Venus de", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7632037039910405, 0.6477594807429303, 0.5587937350172275, 0.4675181502281428], "conf": [0.9686872047006913, 0.7785567061566481, 0.6073418092878012, 0.4260429113803589], "prob": [[0.00048580349450716437, 15], [0.00010486236900551626, 14], [4.0857434434680353e-05, 20], [4.0337917595496714e-05, 6]], "generated": ["The Venus de Milo is in the Louvre Museum in France.", "The Mona Lisa is also in the Louvre Museum in France.", "Thus, the Mona Lisa is in the same museum as the Venus de Milo.", "So the answer is yes."], "conf_comments": ["(A)\nThe Mona Lisa", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7632037039910405, 0.6243406075562862, 0.5378893076640824, 0.4468984792916005], "conf": [0.9686872047006913, 0.7695859511915922, 0.5982787879846383, 0.4138995250490797], "prob": [[0.00048580349450716437, 15], [6.199041423797046e-05, 12], [2.4561121978016433e-05, 20], [2.4237724848364787e-05, 6]], "generated": ["The Venus de Milo is in the Louvre Museum in France.", "The Mona Lisa is also in the Louvre Museum.", "Thus, the Mona Lisa is in the same museum as the Venus de Milo.", "So the answer is yes."], "conf_comments": ["(A)\nThe Mona Lisa", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7791939291740274, 0.6630544782982494, 0.5334199389589215, 0.43770266447844103], "conf": [0.971359510637014, 0.7430486058621529, 0.5720837298219479, 0.386137170209929], "prob": [[0.0035557685517060046, 12], [0.001649356647649513, 14], [6.090749444421759e-05, 19], [6.002027708243627e-05, 6]], "generated": ["The Mona Lisa is at the Louvre in Paris.", "The Venus de Milo is at the Louvre in Paris.", "Thus, the Mona Lisa and Venus de Milo are in the same museum.", "So the answer is yes."], "conf_comments": ["(A)\nThe Venus de", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8047527060219182, 0.6828709348693282, 0.523099925266283, 0.43521941360851774], "conf": [0.9725860367513898, 0.7487095560832752, 0.5894372704507391, 0.40897390098957637], "prob": [[0.00506010111919252, 13], [0.0018563697345092458, 15], [9.825234105508799e-05, 10], [9.688981333953766e-05, 6]], "generated": ["The Mona Lisa is in the Louvre Museum in France.", "The Venus de Milo is in the Louvre Museum in France.", "Thus, they are in the same museum.", "So the answer is yes."], "conf_comments": ["(A)\nThe Venus de", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 53.691543102264404}
{"index": 287, "question": "Was the first Vice President of the United States an Ottoman descendant?", "answer": "no", "explanation": "No. The first Vice President of the United States was John Adams. The Ottomans were a Turkic group that conquered Constantinople in 1453. John Adams was descended from English Puritans.", "generated": [{"finished": true, "length": 3, "score": [0.916245401192446, 0.6515113460606793, 0.45485600804976406], "conf": [0.969760233040938, 0.742634559562648, 0.496365091786693], "prob": [[0.17713767333561525, 12], [0.002788730827347682, 10], [1.040764163984311e-06, 25]], "generated": ["The first Vice President of the United States was John Adams.", "John Adams was not an Ottoman descendant.", "Thus, the first Vice President of the United States was not an Ottoman descendant, so the answer is no."], "conf_comments": ["(A)\nJohn Adams was born", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.916245401192446, 0.5970637939765785, 0.45701798586238246, 0.2817622574819125], "conf": [0.969760233040938, 0.8098332351791433, 0.6154749916932603, 0.3532412667311487], "prob": [[0.17713767333561525, 12], [9.151036979448061e-07, 18], [5.2309861387101953e-08, 11], [1.3783013169381091e-11, 20]], "generated": ["The first Vice President of the United States was John Adams.", "The Ottoman Empire existed from 1299-1923 CE.", "John Adams was born in 1735.", "Thus, John Adams could not have been an Ottoman descendant and the answer is no."], "conf_comments": ["(A)\nJohn Adams was born", "(A)\nThus, the", "(A)\nThus, John", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.916245401192446, 0.5970637939765785, 0.3119366618635811, 0.2193939134768466, 0.17668581321818352], "conf": [0.969760233040938, 0.8098332351791433, 0.42331606829568624, 0.2924915249439994, 0.19028918471358702], "prob": [[0.17713767333561525, 12], [9.151036979448061e-07, 18], [4.462757090031836e-16, 33], [4.1471726812615685e-18, 14], [4.0707315553334004e-18, 6]], "generated": ["The first Vice President of the United States was John Adams.", "The Ottoman Empire existed from 1299-1923 CE.", "John Adams was born in 1735 and died in 1826, so he did not live in the presence of the Ottoman Empire.", "Thus, he could not have been an Ottoman descendant.", "So the answer is no."], "conf_comments": ["(A)\nJohn Adams was born", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.916245401192446, 0.6515113460606793, 0.28209964327590753, 0.23429515653933844], "conf": [0.969760233040938, 0.742634559562648, 0.48917793214948785, 0.34051767185237786], "prob": [[0.17713767333561525, 12], [0.002788730827347682, 10], [1.1926837898704158e-13, 19], [1.1293220571635833e-13, 6]], "generated": ["The first Vice President of the United States was John Adams.", "John Adams was not an Ottoman descendant.", "Thus, it could not be true that his vice president was an Ottoman descendant.", "So the answer is no."], "conf_comments": ["(A)\nJohn Adams was born", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 7, "score": [0.916245401192446, 0.5970637939765785, 0.45701798586238246, 0.229666169889821, 0.15433575397717242, 0.10317239490962798, 0.07509549088872983], "conf": [0.969760233040938, 0.8098332351791433, 0.6154749916932603, 0.3841799936079419, 0.27691285737258325, 0.16225688769204472, 0.09715553775040402], "prob": [[0.17713767333561525, 12], [9.151036979448061e-07, 18], [5.2309861387101953e-08, 11], [3.4330556231880086e-20, 31], [3.087740572687835e-23, 15], [7.961350901363754e-26, 22], [3.819481755698014e-26, 6]], "generated": ["The first Vice President of the United States was John Adams.", "The Ottoman Empire existed from 1299-1923 CE.", "John Adams was born in 1735.", "According to the Wikipedia page for the Ottoman Empire, the empire existed from the 13th century to early 20th century.", "Thus, the Ottoman Empire existed during the lifetime of John Adams.", "Thus, it is possible that the first Vice President of the United States was an Ottoman descendant.", "So the answer is yes."], "conf_comments": ["(A)\nJohn Adams was born", "(A)\nThus, the", "(A)\nThus, John", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.916245401192446, 0.5970637939765785, 0.45701798586238246, 0.229666169889821, 0.15433575397717242, 0.10317239490962798, 0.07042260205763233], "conf": [0.969760233040938, 0.8098332351791433, 0.6154749916932603, 0.3841799936079419, 0.27691285737258325, 0.16225688769204472, 0.09255529601735005], "prob": [[0.17713767333561525, 12], [9.151036979448061e-07, 18], [5.2309861387101953e-08, 11], [3.4330556231880086e-20, 31], [3.087740572687835e-23, 15], [7.961350901363754e-26, 22], [2.36363625066474e-26, 6]], "generated": ["The first Vice President of the United States was John Adams.", "The Ottoman Empire existed from 1299-1923 CE.", "John Adams was born in 1735.", "According to the Wikipedia page for the Ottoman Empire, the empire existed from the 13th century to early 20th century.", "Thus, the Ottoman Empire existed during the lifetime of John Adams.", "Thus, it is possible that the first Vice President of the United States was an Ottoman descendant.", "So the answer is maybe."], "conf_comments": ["(A)\nJohn Adams was born", "(A)\nThus, the", "(A)\nThus, John", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 55.910221099853516}
{"index": 288, "question": "Will Gremlins sequels tie number of Matrix sequels?", "answer": "yes", "explanation": "Yes. The Matrix films had two sequels. Gremlins has one sequel, Gremlins 2: The New Batch. The script for a Gremlins 3 is being written by Carl Ellsworth.", "generated": [{"finished": true, "length": 4, "score": [0.7490659391438588, 0.6617720637800325, 0.4892556226700633, 0.4035909130312474], "conf": [0.9665146565611242, 0.8337427950774, 0.6032036132167836, 0.4124638994479102], "prob": [[0.007490053093254259, 9], [0.003364416703251408, 8], [2.862199782703011e-05, 17], [2.7799343418440992e-05, 6]], "generated": ["Gremlins has 2 sequels.", "The Matrix has 3 sequels.", "Thus, Gremlins sequels will not tie number of Matrix sequels.", "So the answer is no."], "conf_comments": ["(A)\nThe Matrix has ", "(A)\nThus, Gre", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7490659391438588, 0.5986908631684364, 0.40060039285638616, 0.32059119117686846], "conf": [0.9665146565611242, 0.8286433533834778, 0.5503478634933073, 0.3542116376064587], "prob": [[0.007490053093254259, 9], [0.0007114305817912765, 8], [3.965898845272982e-07, 19], [3.8500906812838207e-07, 6]], "generated": ["Gremlins has 2 sequels.", "The Matrix has 4 sequels.", "Thus, Gremlins sequels won’t tie with number of Matrix sequels.", "So the answer is no."], "conf_comments": ["(A)\nThe Matrix has ", "(A)\nThus, Gre", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7490659391438588, 0.5986908631684364, 0.3836660835329349, 0.31242258227665776], "conf": [0.9665146565611242, 0.8286433533834778, 0.5897315233704434, 0.39446478455112965], "prob": [[0.007490053093254259, 9], [0.0007114305817912765, 8], [3.5809652287307345e-08, 18], [3.398950501628396e-08, 6]], "generated": ["Gremlins has 2 sequels.", "The Matrix has 4 sequels.", "Thus, Gremlins sequels will never catch up to The Matrix sequels.", "So the answer is no."], "conf_comments": ["(A)\nThe Matrix has ", "(A)\nThus, Gre", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.7588328012090393, 0.484353077088769, 0.38852373885108377], "conf": [0.9039769755141768, 0.6401644104884083, 0.4382666115776242], "prob": [[0.0011535125548254041, 15], [1.993283205721676e-09, 24], [1.3738958226097294e-09, 6]], "generated": ["Gremlins has two sequels and the Matrix has three sequels.", "Thus, Gremlins sequels will tie number of Matrix sequels if Gremlins 3 is made.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.688395581589534, 0.4814522966316558, 0.36700330060121433, 0.2708107540094207, 0.16819055580186482, 0.12786795154807204], "conf": [0.942745773147972, 0.7875820429989403, 0.7020885353038301, 0.5418388401451233, 0.36018511831763217, 0.22086358041831544], "prob": [[8.602050156661052e-09, 27], [1.8325862703895315e-16, 33], [5.391056308448302e-20, 19], [2.1805641990556963e-24, 29], [2.572948357651207e-34, 42], [1.8045222285108294e-34, 6]], "generated": ["The sequels for the film Gremlins are Gremlins 2: The New Batch and Gremlins 3.", "Gremlins 2: The New Batch was released in 1990, but there are currently no plans to release Gremlins 3.", "In comparison, the Matrix sequels are The Matrix Reloaded and The Matrix Revolutions.", "The film The Matrix Reloaded was released in 2003 and The Matrix Revolutions was released in 2003.", "Thus, The Matrix Reloaded and The Matrix Revolutions were released in the same year, while Gremlins 2: The New Batch and a possible Gremlins 3 are not.", "So the answer is no."], "conf_comments": ["(A)\nThe sequels for", "(A)\nThe sequels for", "(A)\nThe Matrix Relo", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 57.764588832855225}
{"index": 289, "question": "Would Tony Stark be considered a polymath?", "answer": "yes", "explanation": "Yes. A polymath is a person who has knowledge in a wide variety of subjects. Tony Stark is considered a genius in mathematics, engineering, computer science, and physics, as well as demonstrating skills in metalworking, engine design, and genetics.", "generated": [{"finished": true, "length": 5, "score": [0.6746137897863962, 0.4103111339393933, 0.20095613120062794, 0.1588708194684939, 0.13412657521005628], "conf": [0.9800603491476466, 0.8338233027417223, 0.6270972266959627, 0.4724728323858787, 0.33728355912944075], "prob": [[4.66819431815652e-05, 13], [2.7230037806324042e-12, 20], [1.0483221782933706e-23, 23], [1.3420739353502616e-24, 11], [1.3295811655413816e-24, 6]], "generated": ["A polymath is someone who is a master of many subjects.", "Tony Stark graduated from MIT and founded a multi-billion dollar company.", "This can indicate that Tony Stark is master of engineering, entrepreneurship, science, and economics.", "Thus, Tony Stark is a polymath.", "So the answer is yes."], "conf_comments": ["(A)\nTony Stark", "(A)\nThus, Tony", "(A)\nThus, Tony", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6746137897863962, 0.3886050857837213, 0.18628621018475022, 0.15194143388185982, 0.12856230960174475], "conf": [0.9800603491476466, 0.8469064721940234, 0.6763592888486473, 0.5316842258696898, 0.3814525612504784], "prob": [[4.66819431815652e-05, 13], [7.270600611136309e-16, 26], [3.545264332802962e-33, 32], [4.04892601592057e-34, 13], [3.998237862981759e-34, 6]], "generated": ["A polymath is someone who is a master of many subjects.", "Tony Stark is a genius and billionaire who started the company Stark Industries at the age of 21.", "He is very accomplished in many subjects, including electrical engineering, robotics, chemistry, aeronautics, mechanical engineering, computer science, and physics.", "Thus, Tony Stark would be considered a polymath.", "So the answer is yes."], "conf_comments": ["(A)\nTony Stark", "(A)\nHe is also a", "(A)\nThus, Tony", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.720314919129441, 0.32164361457424046, 0.2265241307459942, 0.19168469981618683], "conf": [0.9797086083269843, 0.8619268534039825, 0.6686722648335011, 0.48127196078318857], "prob": [[0.0001365433463476064, 14], [7.701409136475535e-17, 19], [5.619118100825936e-19, 11], [5.4485429315203435e-19, 6]], "generated": ["A polymath is someone who knows a lot about many different things.", "Tony Stark is a scientist that invents many pieces of equipment for Iron Man.", "Thus, he could be considered a polymath.", "So the answer is yes."], "conf_comments": ["(A)\nTony Stark", "(A)\nThus, Tony", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6746137897863962, 0.3886050857837213, 0.18628621018475022, 0.14785871508106788, 0.12421779984311193], "conf": [0.9800603491476466, 0.8469064721940234, 0.6763592888486473, 0.5301109604068763, 0.3756566744542257], "prob": [[4.66819431815652e-05, 13], [7.270600611136309e-16, 26], [3.545264332802962e-33, 32], [2.072569612623382e-34, 13], [2.0230472688797206e-34, 6]], "generated": ["A polymath is someone who is a master of many subjects.", "Tony Stark is a genius and billionaire who started the company Stark Industries at the age of 21.", "He is very accomplished in many subjects, including electrical engineering, robotics, chemistry, aeronautics, mechanical engineering, computer science, and physics.", "Thus, Tony Stark could be considered a polymath.", "So the answer is yes."], "conf_comments": ["(A)\nTony Stark", "(A)\nHe is also a", "(A)\nThus, Tony", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 2, "score": [0.720314919129441, 0.3529395828739809], "conf": [0.9797086083269843, 0.6897574782813707], "prob": [[0.0001365433463476064, 14], [5.930504897847009e-28, 50]], "generated": ["A polymath is someone who knows a lot about many different things.", "Even though Tony Stark did not get a college degree, he did invent the arc reactor, multiple models of the Iron Man suit, J.A.R.V.I.S., Stark Tower, Stark Expo,"], "conf_comments": ["(A)\nTony Stark", "(A)\nand the A."], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 57.34856057167053}
{"index": 290, "question": "Can any person with a driver's license work in transport of aviation fuel?", "answer": "no", "explanation": "No. A vehicle operator must possess a Commercial Driver's License to work in the delivery of fuel. The process for getting a CDL is much lengthier than that of getting a Driver's License. ", "generated": [{"finished": true, "length": 3, "score": [0.6313450864462642, 0.4183138303316141, 0.32738029600707275], "conf": [0.9181246087068899, 0.6272463505588972, 0.414225587254781], "prob": [[3.911860155297086e-14, 37], [5.637282049944637e-18, 20], [3.588234785028881e-18, 6]], "generated": ["People who transport aviation fuel must have a Hazardous Materials (HAZMAT) endorsement, which a driver's license does not qualify for.", "Thus, only people without a driver's license could work in transport of aviation fuel.", "So the answer is no."], "conf_comments": ["(A)\nThus, any", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5609476827470249, 0.3079185157111533, 0.2347142184683539], "conf": [0.9190233439796011, 0.7126135830351119, 0.48092092327855085], "prob": [[1.7484541611597758e-17, 36], [2.178387297245654e-29, 29], [8.87280162625867e-30, 6]], "generated": ["According to the FAA, a \"driver's license with hazmat endorsement\" is required for any person working on the \"transport of aviation fuel.\"", "Hence, only people with a driver's license with hazmat endorsement is allowed to work in aviation fuel transporting.", "So the answer is no."], "conf_comments": ["(A)\nA \"driver'", "(A)\nThus, any", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5609476827470249, 0.29791228895908967, 0.16393508655053976, 0.13217294889164882], "conf": [0.9190233439796011, 0.7598026154350139, 0.5174179477471762, 0.3458196270840821], "prob": [[1.7484541611597758e-17, 36], [2.3079273447115035e-33, 34], [1.1069861278718289e-45, 35], [9.370036175513764e-46, 6]], "generated": ["According to the FAA, a \"driver's license with hazmat endorsement\" is required for any person working on the \"transport of aviation fuel.\"", "If a person has a \"hazmat endorsement,\" it means that they have taken a test to indicate they are knowledgeable about hazardous materials.", "Thus, a person with a driver's license could transport aviation fuel, if they have taken the test to obtain a hazardous materials endorsement.", "So the answer is yes."], "conf_comments": ["(A)\nA \"driver'", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5490841461635022, 0.26739859099167657, 0.20926970022580857], "conf": [0.8093464475845936, 0.49598234911314804, 0.30626306207603876], "prob": [[5.1219678428372674e-08, 17], [1.9417476852845907e-15, 18], [1.8492228600605715e-15, 6]], "generated": ["There is no connection between a driver’s license and an aviation fuel license.", "Thus, no driver’s license can help in the transport of aviation fuel.", "So the answer is no."], "conf_comments": ["(A)\nThus, any", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5490841461635022, 0.3183874660872351, 0.16909510972913025, 0.07400135465560806, 0.05812889203731601], "conf": [0.8093464475845936, 0.7043263725955148, 0.5220495209042353, 0.3790888438880053, 0.25195057520421593], "prob": [[5.1219678428372674e-08, 17], [3.268558869328491e-14, 15], [3.0151251499956747e-19, 12], [1.0209486214695792e-33, 25], [6.537117457667347e-34, 6]], "generated": ["There is no connection between a driver’s license and an aviation fuel license.", "A driver’s license is related to operating a vehicle on the road.", "Aviation fuel is related to the operation of a plane.", "Thus, the only information in the question is the driver’s license is not a transport of aviation fuel license.", "So the answer is no."], "conf_comments": ["(A)\nThus, any", "(A)\nAn aviation fuel", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 50.41809058189392}
{"index": 291, "question": "Does the country that received the most gold medals during the 1976 Olympics still exist?", "answer": "no", "explanation": "No. The Soviet Union received the most gold medals during the 1976 Summer Olympics. The Soviet Union existed from 1922 to 1991.", "generated": [{"finished": true, "length": 4, "score": [0.8489117304533154, 0.6054513602377982, 0.5125207618362517, 0.43821569755382117], "conf": [0.9706179979381161, 0.8498167191709196, 0.6724550446204818, 0.49349457901587807], "prob": [[0.0019240276996857842, 21], [2.1453502008845858e-08, 21], [1.9848712499675232e-09, 24], [1.9397026956629467e-09, 6]], "generated": ["The country that received the most gold medals during the 1976 Olympics was East Germany.", "East Germany merged with West Germany in 1990, so it does not exist anymore.", "Thus, the country that received the most gold medals during the 1976 Olympics does not exist.", "So the answer is no."], "conf_comments": ["(A)\nEast Germany no", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.849762657803447, 0.6509376324384685, 0.5010901927343523, 0.42847634539002716], "conf": [0.9762065571147376, 0.8540018228444948, 0.678015356646063, 0.4968303532245683], "prob": [[0.001315765262570003, 22], [8.037911160716146e-05, 7], [9.993736542929e-07, 15], [9.86387895507337e-07, 6]], "generated": ["The country that received the most gold medals during the 1976 Olympics was the United States.", "The United States still exists today.", "Thus, the country that received the most gold medals still exists.", "So the answer is yes."], "conf_comments": ["(A)\nThe United States still", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.849762657803447, 0.6509376324384685, 0.5070956518765741], "conf": [0.9762065571147376, 0.8540018228444948, 0.642327340089706], "prob": [[0.001315765262570003, 22], [8.037911160716146e-05, 7], [2.218010621794695e-05, 6]], "generated": ["The country that received the most gold medals during the 1976 Olympics was the United States.", "The United States still exists today.", "So the answer is yes."], "conf_comments": ["(A)\nThe United States still", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.8489117304533154, 0.6054513602377982, 0.496961390178481], "conf": [0.9706179979381161, 0.8498167191709196, 0.68619159510836], "prob": [[0.0019240276996857842, 21], [2.1453502008845858e-08, 21], [5.040010543504041e-09, 8]], "generated": ["The country that received the most gold medals during the 1976 Olympics was East Germany.", "East Germany merged with West Germany in 1990, so it does not exist anymore.", "Thus, the answer is no."], "conf_comments": ["(A)\nEast Germany no", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 1, "score": [0.7504213333792196], "conf": [0.8847367914827526], "prob": [[1.548149184630029e-10, 50]], "generated": ["According to this link: [https://en.wikipedia.org/wiki/List_of_Nations_at_the_Summer_Olympics](https://en.wikipedia.org/wiki/List_of_N"], "conf_comments": ["(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8489117304533154, 0.6054513602377982, 0.5125207618362517, 0.002148871871989894], "conf": [0.9706179979381161, 0.8498167191709196, 0.6724550446204818, 7.364464378538511e-05], "prob": [[0.0019240276996857842, 21], [2.1453502008845858e-08, 21], [1.9848712499675232e-09, 24], [3.3951078829998024e-14, 6]], "generated": ["The country that received the most gold medals during the 1976 Olympics was East Germany.", "East Germany merged with West Germany in 1990, so it does not exist anymore.", "Thus, the country that received the most gold medals during the 1976 Olympics does not exist.", "So the answer is no ."], "conf_comments": ["(A)\nEast Germany no", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 45.62460947036743}
{"index": 292, "question": "Does open heart surgery finish before entirety of American Ballet Theatre's Swan Lake?", "answer": "no", "explanation": "No. The American Ballet theatre's Swan Lake has a run time of 145 minutes. The National Heart, Lung, and Blood Institute states that a coronary artery bypass takes 3 to 6 hours.", "generated": [{"finished": true, "length": 4, "score": [0.671706956397581, 0.43956947322568485, 0.3286197817762848, 0.26027975904063333], "conf": [0.9758493535431055, 0.8717842974277981, 0.6942784789765365, 0.4795215190480685], "prob": [[2.040218989740369e-05, 14], [1.7473888359042935e-11, 19], [3.559379223163537e-15, 24], [1.9984265456041273e-15, 6]], "generated": ["Open heart surgery lasts around 2 to 5 hours.", "American Ballet Theatre’s Swan Lake’s run time is about 3 hours.", "Thus, open heart surgery is shorter than the entirety of American Ballet Theatre’s Swan Lake.", "So the answer is yes."], "conf_comments": ["(A)\nAmerican Ballet Theatre", "(A)\nThus, open", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.671706956397581, 0.43956947322568485, 0.29560297985300077, 0.2504316089962425], "conf": [0.9758493535431055, 0.8717842974277981, 0.7258437702643613, 0.5456046300561455], "prob": [[2.040218989740369e-05, 14], [1.7473888359042935e-11, 19], [8.728882753764264e-17, 20], [6.614748450165841e-17, 6]], "generated": ["Open heart surgery lasts around 2 to 5 hours.", "American Ballet Theatre’s Swan Lake’s run time is about 3 hours.", "Thus, the entirety of the ballet would finish before open heart surgery finishes.", "So the answer is yes."], "conf_comments": ["(A)\nAmerican Ballet Theatre", "(A)\nThus, open", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.671706956397581, 0.43956947322568485, 0.3286197817762848, 0.24619604861212807], "conf": [0.9758493535431055, 0.8717842974277981, 0.6942784789765365, 0.4507316233815475], "prob": [[2.040218989740369e-05, 14], [1.7473888359042935e-11, 19], [3.559379223163537e-15, 24], [1.4863353618063184e-15, 6]], "generated": ["Open heart surgery lasts around 2 to 5 hours.", "American Ballet Theatre’s Swan Lake’s run time is about 3 hours.", "Thus, open heart surgery is shorter than the entirety of American Ballet Theatre’s Swan Lake.", "So the answer is no."], "conf_comments": ["(A)\nAmerican Ballet Theatre", "(A)\nThus, open", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6794361135589353, 0.4238496915271791, 0.3033003065453266, 0.24349574807175806], "conf": [0.9634166371242375, 0.8231124665406928, 0.5917195096313845, 0.39105060459323343], "prob": [[1.9504972870011853e-07, 21], [6.696510818254181e-13, 16], [3.8413510270804575e-16, 22], [3.3052472284992433e-16, 6]], "generated": ["American Ballet Theatre's Swan Lake runs at approximately 2 hours and 20 minutes.", "An open heart surgery takes approximately 2 hours and 15 minutes.", "Thus, it is possible for open heart surgery to finish before the entirety of Swan Lake.", "So the answer is yes."], "conf_comments": ["(A)\nOpen heart surgery", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.596482870237835, 0.3651012002124763, 0.26726600680557244, 0.22622719809608854], "conf": [0.9761850435425489, 0.8508233574345206, 0.6470491279762737, 0.4653197174988843], "prob": [[5.494973907724748e-06, 12], [4.0410296429311025e-11, 14], [1.1101568103689655e-15, 30], [1.0857059822572201e-15, 6]], "generated": ["Open heart surgery can take from two to six hours.", "Swan Lake is about 3 hours and 45 minutes.", "Thus, there is not enough time for open heart surgery to finish before the entirety of American Ballet Theatre's Swan Lake.", "So the answer is no."], "conf_comments": ["(A)\nAmerican Ballet Theatre", "(A)\nThus, open", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.671706956397581, 0.43956947322568485, 0.29560297985300077, 0.2191353130454195], "conf": [0.9758493535431055, 0.8717842974277981, 0.7258437702643613, 0.5118833164593309], "prob": [[2.040218989740369e-05, 14], [1.7473888359042935e-11, 19], [8.728882753764264e-17, 20], [1.95449454428735e-17, 6]], "generated": ["Open heart surgery lasts around 2 to 5 hours.", "American Ballet Theatre’s Swan Lake’s run time is about 3 hours.", "Thus, the entirety of the ballet would finish before open heart surgery finishes.", "So the answer is no."], "conf_comments": ["(A)\nAmerican Ballet Theatre", "(A)\nThus, open", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 1, "score": [0.665788113473631], "conf": [0.8404538847926482], "prob": [[1.2820627494605867e-14, 50]], "generated": ["According to the Mayo Clinic website ( https://www.mayoclinic.org/tests-procedures/open-heart-surgery/about/pac-20395522 ),"], "conf_comments": ["(A)\nopen heart surgery"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 46.08425521850586}
{"index": 293, "question": "Is the Yellow Pages the fastest way to find a phone number?", "answer": "no", "explanation": "No. The Yellow Pages is a book that contains alphabetized phone listings. Yellow pages involves going through many listings and remembering your alphabet. Google allows a person to type in a name quickly and look for a phone number. Household AI assistants like Echo allow people to merely speak a name and ask for number.", "generated": [{"finished": true, "length": 5, "score": [0.599513567849326, 0.3170759432361019, 0.16537736151824897, 0.11772721716483074, 0.09613606357098506], "conf": [0.9528447086897492, 0.8525157923958597, 0.5895035989757811, 0.4150487199015863, 0.2781085277145813], "prob": [[4.452855360999621e-07, 15], [1.1600021511746286e-15, 17], [5.569463635386171e-25, 23], [2.8895998805048175e-27, 16], [2.8071125903530378e-27, 6]], "generated": ["The Yellow Pages is an American telephone directory for businesses.", "This means that the Yellow Pages does not contain phone numbers from private individuals.", "However, there is a website called whitepages.com which contains both phone numbers from businesses and private individuals.", "Thus, it is not the fastest way to find a phone number.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5000215736656021, 0.26879765493357616, 0.1910796748368075, 0.15054556306436864], "conf": [0.9681549103872157, 0.7713482098228113, 0.48892025125938676, 0.3049844921029364], "prob": [[1.515268133214843e-09, 15], [6.4905050972459184e-18, 19], [8.759871301479537e-20, 19], [8.505521439621464e-20, 6]], "generated": ["Yellow Pages is an obsolete phone book for landlines.", "Thus, there are very few phone numbers that can be found in Yellow Pages.", "Thus, Yellow Pages is not the fastest way to find a phone number.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5000215736656021, 0.2328325155032348, 0.17064477818866552, 0.13803450463499756], "conf": [0.9681549103872157, 0.7738380350502283, 0.5819943832315171, 0.3815318537760554], "prob": [[1.515268133214843e-09, 15], [2.811952255920488e-24, 26], [4.696544142884022e-27, 19], [4.643465749899229e-27, 6]], "generated": ["Yellow Pages is an obsolete phone book for landlines.", "Nowadays, the fastest way to find phone numbers is online because you can search any phone number on a search engine.", "Thus, the Yellow Pages is not the fastest way to find phone number.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5000215736656021, 0.26879765493357616, 0.16391957578816455, 0.1336690915980276], "conf": [0.9681549103872157, 0.7713482098228113, 0.5778414323415453, 0.39000344422650823], "prob": [[1.515268133214843e-09, 15], [6.4905050972459184e-18, 19], [3.5827239637935985e-22, 14], [3.276852298541081e-22, 6]], "generated": ["Yellow Pages is an obsolete phone book for landlines.", "Thus, there are very few phone numbers that can be found in Yellow Pages.", "It would not be the fastest way to find a phone number.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.599513567849326, 0.26454877296384455, 0.10569189667194405, 0.09090406242682086, 0.07793120570482687], "conf": [0.9528447086897492, 0.8694054869026744, 0.6827051716843693, 0.5373290101088792, 0.395848519593435], "prob": [[4.452855360999621e-07, 15], [8.011056156016305e-20, 19], [2.841729650996251e-41, 31], [8.224412028958697e-42, 20], [8.107924248873989e-42, 6]], "generated": ["The Yellow Pages is an American telephone directory for businesses.", "It first published in 1886 in the US and was printed on yellow paper.", "In the modern era, it has been surpassed by other phone directories like 411 on the internet, Google, and smart phone apps.", "Thus, the Yellow Pages is not the fastest way to find a phone number.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 59.05114459991455}
{"index": 294, "question": "Has a neanderthal ever served on the Supreme Court of the United States?", "answer": "no", "explanation": "No. The Supreme Court was established in 1789. Neanderthals are primitive humans that lived 40,000 years ago.", "generated": [{"finished": true, "length": 2, "score": [0.7221252462935963, 0.5889090326661317], "conf": [0.9347944520705456, 0.7391659931421506], "prob": [[0.00015762821835305584, 15], [3.94819089300854e-05, 8]], "generated": ["No neanderthal served on the Supreme Court of the United States.", "Thus, the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7767752594468431, 0.5465398523568711, 0.3963805500126096, 0.33143507727315263], "conf": [0.976827814427216, 0.8784281053228962, 0.6095337153977952, 0.42767309405993426], "prob": [[3.63154603551969e-06, 26], [7.1930794526563956e-12, 22], [3.724047328427808e-14, 19], [3.6455547472787707e-14, 6]], "generated": ["Neanderthals lived from around 100,000 to 40,000 years ago.", "In comparison, Antonin Scalia served from 1986 to 2016.", "Thus, it is impossible for a neanderthal to serve on the Supreme Court.", "So the answer is no."], "conf_comments": ["(A)\nThe Supreme Court of", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7267188281157693, 0.5503117709870243, 0.3623265483600729, 0.3038770661778973], "conf": [0.9737530474034581, 0.8791961287972505, 0.698236139922845, 0.5224670517472292], "prob": [[0.0006477507658438318, 12], [1.2015896101157604e-08, 24], [1.2134253418856954e-13, 19], [8.372338367035458e-14, 6]], "generated": ["The Supreme Court is the highest court in the United States.", "Neanderthals died out 30,000 to 40,000 years ago.", "This means Neanderthals could not have served on the Supreme Court of the United States.", "So the answer is no."], "conf_comments": ["(A)\nNeanderthals", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7767752594468431, 0.5465398523568711, 0.34301671335067074, 0.2783222917602148], "conf": [0.976827814427216, 0.8784281053228962, 0.7011886676823522, 0.48748051454332264], "prob": [[3.63154603551969e-06, 26], [7.1930794526563956e-12, 22], [8.892962912698174e-17, 16], [6.413642859112292e-17, 6]], "generated": ["Neanderthals lived from around 100,000 to 40,000 years ago.", "In comparison, Antonin Scalia served from 1986 to 2016.", "Thus, Scalia lived after Neanderthals went extinct.", "So the answer is no."], "conf_comments": ["(A)\nThe Supreme Court of", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7767752594468431, 0.44145080659963387, 0.26854780510396714, 0.21539422126965646], "conf": [0.976827814427216, 0.8257845312484347, 0.6099330187952795, 0.45130563473265434], "prob": [[3.63154603551969e-06, 26], [2.250847691293835e-20, 34], [1.778362251419879e-25, 17], [5.806445951082041e-26, 8]], "generated": ["Neanderthals lived from around 100,000 to 40,000 years ago.", "The first U.S. Supreme Court was created in 1789, so Neanderthals weren't around for the first supreme court.", "This can indicate that Neanderthals have never served on the supreme court.", "Thus, the answer is no."], "conf_comments": ["(A)\nThe Supreme Court of", "(A)\nThus, a", "(A)\nHowever, the Supreme", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7767752594468431, 0.44145080659963387, 0.26854780510396714, 0.20587458590240396], "conf": [0.976827814427216, 0.8257845312484347, 0.6099330187952795, 0.41480078909387236], "prob": [[3.63154603551969e-06, 26], [2.250847691293835e-20, 34], [1.778362251419879e-25, 17], [7.407117485599901e-26, 6]], "generated": ["Neanderthals lived from around 100,000 to 40,000 years ago.", "The first U.S. Supreme Court was created in 1789, so Neanderthals weren't around for the first supreme court.", "This can indicate that Neanderthals have never served on the supreme court.", "So the answer is no."], "conf_comments": ["(A)\nThe Supreme Court of", "(A)\nThus, a", "(A)\nHowever, the Supreme", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 45.68966221809387}
{"index": 295, "question": "Did Maroon 5 go on tour with Nirvana?", "answer": "no", "explanation": "No. Maroon 5 formed in 2001. Nirvana's final performances were in 1994.", "generated": [{"finished": true, "length": 3, "score": [0.7666850125565783, 0.5900601162152652, 0.5061039598934924], "conf": [0.9438929218049827, 0.7691941572276005, 0.5680652761594931], "prob": [[2.61793566917947e-07, 32], [8.394113672796951e-10, 18], [8.202060072514713e-10, 6]], "generated": ["Maroon 5 was formed in 2001, around 6 years after Nirvana disbanded in 1994.", "Thus, Maroon 5 and Nirvana never went on tour together.", "So the answer is no."], "conf_comments": ["(A)\nThus, Mar", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7666850125565783, 0.5368762758855847, 0.4604541810787269], "conf": [0.9438929218049827, 0.7394295559294474, 0.5468566713837275], "prob": [[2.61793566917947e-07, 32], [5.475051843548849e-12, 23], [5.2999829714432715e-12, 6]], "generated": ["Maroon 5 was formed in 2001, around 6 years after Nirvana disbanded in 1994.", "Thus, it was physically impossible for Maroon 5 to go on tour with Nirvana.", "So the answer is no."], "conf_comments": ["(A)\nThus, Mar", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8273142622476855, 0.42204709934209117, 0.33169047962842435, 0.2686645120794678], "conf": [0.970177460004114, 0.7520954539458974, 0.5261960972158501, 0.34628865163847805], "prob": [[0.010724275122181015, 13], [1.919463819401452e-16, 29], [1.797631615576986e-17, 19], [1.7647551253503058e-17, 6]], "generated": ["Maroon 5 was founded in 1994.", "Nirvana didn’t make any concerts after its lead singer Kurt Cobain’s suicide on 1994.", "Thus, Maroon 5 didn’t go on tour with Nirvana.", "So the answer is no."], "conf_comments": ["(A)\nNirvana", "(A)\nThus, Mar", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8273142622476855, 0.42204709934209117, 0.32161711998672304, 0.2598817490927167], "conf": [0.970177460004114, 0.7520954539458974, 0.5318989254134936, 0.3484096899128473], "prob": [[0.010724275122181015, 13], [1.919463819401452e-16, 29], [6.72954205390941e-18, 17], [6.6017383574147524e-18, 6]], "generated": ["Maroon 5 was founded in 1994.", "Nirvana didn’t make any concerts after its lead singer Kurt Cobain’s suicide on 1994.", "Thus, Maroon 5 never went on tour with Nirvana.", "So the answer is no."], "conf_comments": ["(A)\nNirvana", "(A)\nThus, Mar", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6850972056042446, 0.4699874339785276, 0.3092842276852831, 0.2566511093550277], "conf": [0.9807751819871668, 0.8109848724522712, 0.5849123916918371, 0.40501951412991494], "prob": [[3.30437207302745e-05, 14], [2.5102057406529232e-11, 25], [1.227591329769017e-18, 33], [1.1873192989154146e-18, 6]], "generated": ["Maroon 5 was officially formed in 2001.", "Nirvana was a grunge band that lasted from 1987 to 1994.", "Thus, Maroon 5 did not go on tour with Nirvana because they weren’t even a band when Nirvana existed.", "So the answer is no."], "conf_comments": ["(A)\nNirvana", "(A)\nThus, Mar", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 48.241363763809204}
{"index": 296, "question": "Is Antarctica a good location for Groundhog Day?", "answer": "no", "explanation": "No. Groundhog Day relies on a groundhog seeing their shadow. Antarctica has an irregular sun pattern and some days have no sun rise or 24 hour sunlight. Antarctica has temperatures can range from -10C to -60C. Groundhogs live in forests or woodlands with plenty of sunlight.", "generated": [{"finished": true, "length": 4, "score": [0.7181633788704244, 0.3644172165879793, 0.27753756419157005, 0.22635203003321566], "conf": [0.9565227854818923, 0.8095728868999506, 0.5785921523578439, 0.387384569628713], "prob": [[4.315427468428232e-06, 20], [3.521241221924167e-24, 35], [2.347449605466492e-26, 24], [2.2569870177227017e-26, 6]], "generated": ["The movie Groundhog Day takes place in the town of Punxsutawney.", "According to US news, Punxsutawney is ranked number 1274 and Antarctica is ranked number 219276.", "Thus, Antarctica is a worse location for Groundhog Day than Punxsutawney.", "So the answer is no."], "conf_comments": ["(A)\nPunxsut", "(A)\nThus, Ant", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5728151698451883, 0.32591661349283546, 0.24193093228597554, 0.20017731347820714], "conf": [0.966693911262555, 0.7561640468546147, 0.5893884773249545, 0.4045707668620794], "prob": [[2.33819357238262e-06, 12], [6.170265419038299e-16, 25], [1.6975984050477309e-18, 17], [1.6709340212465907e-18, 6]], "generated": ["Groundhog day takes place on Feb 2.", "Antarctica is in the Southern hemisphere, so it doesn’t experience summer or winter on Feb 2.", "Thus, it isn’t a good location for Groundhog Day.", "So the answer is no."], "conf_comments": ["(A)\nAntarctica is", "(A)\nThus, Ant", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7181633788704244, 0.3644172165879793, 0.2467038767428197, 0.1590758817853852, 0.11876624923474595], "conf": [0.9565227854818923, 0.8095728868999506, 0.6110042728529577, 0.39197488323151, 0.2340604557906931], "prob": [[4.315427468428232e-06, 20], [3.521241221924167e-24, 35], [7.309973116245504e-28, 17], [1.4288420958991667e-32, 25], [9.454309550706201e-33, 6]], "generated": ["The movie Groundhog Day takes place in the town of Punxsutawney.", "According to US news, Punxsutawney is ranked number 1274 and Antarctica is ranked number 219276.", "Therefore, Punxsutawney is more developed than Antarctica.", "Thus, Punxsutawney is a better place to host Groundhog Day than Antarctica.", "So the answer is no."], "conf_comments": ["(A)\nPunxsut", "(A)\nThus, Ant", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7181633788704244, 0.3644172165879793, 0.2467038767428197, 0.1590758817853852, 0.1173938825989581], "conf": [0.9565227854818923, 0.8095728868999506, 0.6110042728529577, 0.39197488323151, 0.25847490586060634], "prob": [[4.315427468428232e-06, 20], [3.521241221924167e-24, 35], [7.309973116245504e-28, 17], [1.4288420958991667e-32, 25], [4.534369251620333e-33, 6]], "generated": ["The movie Groundhog Day takes place in the town of Punxsutawney.", "According to US news, Punxsutawney is ranked number 1274 and Antarctica is ranked number 219276.", "Therefore, Punxsutawney is more developed than Antarctica.", "Thus, Punxsutawney is a better place to host Groundhog Day than Antarctica.", "So the answer is yes."], "conf_comments": ["(A)\nPunxsut", "(A)\nThus, Ant", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7181633788704244, 0.3644172165879793, 0.2467038767428197, 0.1462670109564072, 0.11412301225137998], "conf": [0.9565227854818923, 0.8095728868999506, 0.6110042728529577, 0.40050385252993276, 0.2508747903839587], "prob": [[4.315427468428232e-06, 20], [3.521241221924167e-24, 35], [7.309973116245504e-28, 17], [4.361324305548281e-34, 23], [3.674840648771994e-34, 6]], "generated": ["The movie Groundhog Day takes place in the town of Punxsutawney.", "According to US news, Punxsutawney is ranked number 1274 and Antarctica is ranked number 219276.", "Therefore, Punxsutawney is more developed than Antarctica.", "Thus, Groundhog Day is more fitting at Punxsutawney than Antarctica.", "So the answer is no."], "conf_comments": ["(A)\nPunxsut", "(A)\nThus, Ant", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7181633788704244, 0.35563765015604015, 0.20305142208241145, 0.16825169985129726], "conf": [0.9565227854818923, 0.8244979687816604, 0.6029156526731018, 0.41710336000569115], "prob": [[4.315427468428232e-06, 20], [5.2061596227718375e-17, 20], [6.372831125910928e-22, 14], [6.090468987484374e-22, 6]], "generated": ["The movie Groundhog Day takes place in the town of Punxsutawney.", "Punxsutawney is in Pennsylvania, which is also famous with its groundhogs.", "Thus, the location is suitable for Groundhog Day.", "So the answer is yes."], "conf_comments": ["(A)\nPunxsut", "(A)\nPennsylvania", "(A)\nAntarctica is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7181633788704244, 0.3644172165879793, 0.2467038767428197, 0.1462670109564072, 0.10222264235288142], "conf": [0.9565227854818923, 0.8095728868999506, 0.6110042728529577, 0.40050385252993276, 0.2723409679266881], "prob": [[4.315427468428232e-06, 20], [3.521241221924167e-24, 35], [7.309973116245504e-28, 17], [4.361324305548281e-34, 23], [5.989482586920827e-35, 6]], "generated": ["The movie Groundhog Day takes place in the town of Punxsutawney.", "According to US news, Punxsutawney is ranked number 1274 and Antarctica is ranked number 219276.", "Therefore, Punxsutawney is more developed than Antarctica.", "Thus, Groundhog Day is more fitting at Punxsutawney than Antarctica.", "So the answer is yes."], "conf_comments": ["(A)\nPunxsut", "(A)\nThus, Ant", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.813170194625854}
{"index": 297, "question": "Does the density of helium cause voices to sound deeper?", "answer": "no", "explanation": "No. Helium is less dense than air. Sound travels more quickly through helium than it does through air.  When sound travels more quickly, the tone of it raises and sounds higher.", "generated": [{"finished": true, "length": 3, "score": [0.7578928269863585, 0.5715882678382317, 0.47034707399065584], "conf": [0.9407736439630465, 0.6836298615391678, 0.4659769257692486], "prob": [[6.11199643686291e-07, 29], [1.2134594451472785e-08, 16], [1.1662365040599996e-08, 6]], "generated": ["The density of helium is 0.17 g/cm^3, which is less than the density of Earth’s atmosphere.", "Thus, the density of helium does not cause voices to sound deeper.", "So the answer is no."], "conf_comments": ["(A)\nThe density of Earth", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7892302456112593, 0.5591632302923017, 0.47023217023867786], "conf": [0.9354960458027474, 0.692352809140572, 0.49413082599665537], "prob": [[5.764632535480478e-05, 24], [3.7048199034297225e-07, 13], [3.507235698890726e-07, 6]], "generated": ["The density of helium is around 0.18 g/cm^3, which is less than water.", "Thus, helium does not cause voices to sound deeper.", "So the answer is no."], "conf_comments": ["(A)\nThe density of water", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7511078790367872, 0.5767576572054043, 0.44872435870252303, 0.23752064132034786, 0.1928790107947506], "conf": [0.9554627255202713, 0.8153441981354506, 0.695896120310527, 0.46354956826165467, 0.31234843246451516], "prob": [[4.493935563762337e-05, 19], [5.7918117701809516e-08, 18], [6.649868190826091e-10, 13], [8.578366293698993e-15, 13], [7.536141451256997e-15, 6]], "generated": ["The density of helium is 0.1788 g/cm^3.", "The density of air is 1.2041 g/cm^3.", "The density of helium is less than the density of air.", "Thus, the density of helium makes voices sound lower.", "So the answer is yes."], "conf_comments": ["(A)\nThe density of air", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.7511078790367872, 0.47598432603769003, 0.2976158077042037, 0.1931512241157878, 0.12028787152199115, 0.09100421387840797], "conf": [0.9554627255202713, 0.84807273622277, 0.7254530675947177, 0.5918965761308714, 0.383219287525781, 0.2388819829685676], "prob": [[4.493935563762337e-05, 19], [2.8361586451416414e-11, 18], [4.6977342660583335e-17, 17], [4.590008433858163e-19, 7], [3.5162079620224445e-22, 14], [2.1073384054890622e-22, 6]], "generated": ["The density of helium is 0.1788 g/cm^3.", "This is a lot lower than water, which is 1 g/cm^3.", "In comparison, the density of lead is 11 g/cm^3.", "This is much higher than water.", "Thus, the density of lead can cause voices to sound deeper.", "So the answer is yes."], "conf_comments": ["(A)\nThe density of air", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.7511078790367872, 0.5767576572054043, 0.44872435870252303, 0.23798018088163783, 0.1150281070701004, 0.06861122540448163, 0.053892039807757676], "conf": [0.9554627255202713, 0.8153441981354506, 0.695896120310527, 0.5732782170418899, 0.4234756738404779, 0.2614923664997384, 0.16555691647548923], "prob": [[4.493935563762337e-05, 19], [5.7918117701809516e-08, 18], [6.649868190826091e-10, 13], [1.6688919207988327e-15, 12], [1.6747435715843156e-27, 24], [3.306742287081382e-34, 28], [2.831547291074503e-34, 6]], "generated": ["The density of helium is 0.1788 g/cm^3.", "The density of air is 1.2041 g/cm^3.", "The density of helium is less than the density of air.", "The deeper a sound is, the lower its frequency is.", "In comparison, the higher the density of a substance, the higher the frequency of sound waves of the substance.", "Since the density of helium is less than the density of air, thus the density of helium does not cause voices to sound deeper.", "So the answer is no."], "conf_comments": ["(A)\nThe density of air", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 7, "score": [0.7511078790367872, 0.5767576572054043, 0.44872435870252303, 0.23798018088163783, 0.1150281070701004, 0.06861122540448163, 0.038490717809178704], "conf": [0.9554627255202713, 0.8153441981354506, 0.695896120310527, 0.5732782170418899, 0.4234756738404779, 0.2614923664997384, 0.16685322739095862], "prob": [[4.493935563762337e-05, 19], [5.7918117701809516e-08, 18], [6.649868190826091e-10, 13], [1.6688919207988327e-15, 12], [1.6747435715843156e-27, 24], [3.306742287081382e-34, 28], [3.380642859945251e-38, 13]], "generated": ["The density of helium is 0.1788 g/cm^3.", "The density of air is 1.2041 g/cm^3.", "The density of helium is less than the density of air.", "The deeper a sound is, the lower its frequency is.", "In comparison, the higher the density of a substance, the higher the frequency of sound waves of the substance.", "Since the density of helium is less than the density of air, thus the density of helium does not cause voices to sound deeper.", "In contrast, the density of air causes voices to sound deeper.", "So yes, because the density of air would cause voices to sound deeper."], "conf_comments": ["(A)\nThe density of air", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[0.0001022348452479221, 13], [6.468401067655463e-10, 15]], 0.6380806813766992, "(A)\nSo the answer is", 7]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 65.50657272338867}
{"index": 298, "question": "Can petroleum jelly be used as fuel in a car?", "answer": "no", "explanation": "No. Petroleum is a highly reactive liquid used to power cars. Petroleum jelly is a solid substance used as an ointment on cuts and scrapes to promote healing. Petroleum jelly does not oxidize on exposure to the air and is not readily acted on by chemical reagents.", "generated": [{"finished": true, "length": 4, "score": [0.6937065490492481, 0.45727978637986477, 0.3496038285137689, 0.2998608881322066], "conf": [0.9424823882584179, 0.7844396426352533, 0.5838383848391857, 0.43105424546382903], "prob": [[0.0006149811462749621, 11], [4.828911301183187e-07, 11], [6.235101805319503e-09, 18], [6.102834201997573e-09, 6]], "generated": ["Petroleum jelly is a wax.", "Wax is insoluble in gasoline.", "Thus, petroleum jelly cannot be used as a fuel in a car.", "So the answer is no."], "conf_comments": ["(A)\nIt is not fl", "(A)\nThus, pet", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6993265024279177, 0.41219854055215877, 0.2609820224919633, 0.21283064273263896], "conf": [0.9409248989933415, 0.8091213054758449, 0.5639097840238725, 0.3763803748691225], "prob": [[0.0007479199932907542, 11], [3.7684532807800276e-10, 16], [3.111516490767676e-14, 17], [3.044744350091603e-14, 6]], "generated": ["Petroleum jelly is hydrocarbon.", "Hydrocarbons are a type of fuel which can be burned.", "Thus, petroleum jelly is a type of fuel in a car.", "So the answer is yes."], "conf_comments": ["(A)\nHydrocarb", "(A)\nThus, pet", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6993265024279177, 0.41219854055215877, 0.2603860639480371, 0.20952770219228556], "conf": [0.9409248989933415, 0.8091213054758449, 0.5199904040840929, 0.3376907523050932], "prob": [[0.0007479199932907542, 11], [3.7684532807800276e-10, 16], [1.8399975458769408e-13, 16], [1.807834940071316e-13, 6]], "generated": ["Petroleum jelly is hydrocarbon.", "Hydrocarbons are a type of fuel which can be burned.", "Thus, petroleum jelly is used as fuel in a car.", "So the answer is yes."], "conf_comments": ["(A)\nHydrocarb", "(A)\nThus, pet", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.6993265024279177, 0.3775334290122622, 0.24746440701837574, 0.1325593595519705, 0.10504670935875968, 0.08841295381302484], "conf": [0.9409248989933415, 0.7748922443485005, 0.6070398150038384, 0.4861965619960279, 0.35551963606752857, 0.25263265919538896], "prob": [[0.0007479199932907542, 11], [3.939740300101608e-15, 25], [4.356169336334698e-20, 19], [8.708015619054622e-31, 24], [6.547718857891618e-32, 17], [6.42593003781507e-32, 6]], "generated": ["Petroleum jelly is hydrocarbon.", "A hydrocarbon is any chemical compound that consists of one atom of carbon and one or more atoms of hydrogen.", "Some examples of hydrocarbons are methane, propane, and octane.", "Fuel for a car is often a hydrocarbon called gasoline which is a mixture of hydrocarbons.", "Thus, petroleum jelly can be used as fuel in a car.", "So the answer is yes."], "conf_comments": ["(A)\nHydrocarb", "(A)\nPetroleum", "(A)\nMethane,", "(A)\nThus, pet", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.6993265024279177, 0.3775334290122622, 0.24746440701837574, 0.10669387804615234, 0.07131536444778613, 0.05820859878901121], "conf": [0.9409248989933415, 0.7748922443485005, 0.6070398150038384, 0.4817788230442464, 0.3244453650420521, 0.2171241391541954], "prob": [[0.0007479199932907542, 11], [3.939740300101608e-15, 25], [4.356169336334698e-20, 19], [1.5256311659156622e-29, 15], [5.349129422566671e-34, 25], [5.2063380343815765e-34, 6]], "generated": ["Petroleum jelly is hydrocarbon.", "A hydrocarbon is any chemical compound that consists of one atom of carbon and one or more atoms of hydrogen.", "Some examples of hydrocarbons are methane, propane, and octane.", "Methane is a part of the fuel you put into your car.", "Thus, petroleum jelly is a form of hydrocarbon that can be used as fuel in a car.", "So the answer is yes."], "conf_comments": ["(A)\nHydrocarb", "(A)\nPetroleum", "(A)\nMethane,", "(A)\nThus, pet", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 64.67566847801208}
{"index": 299, "question": "Is greed the most prevalent of the Seven Deadly Sins?", "answer": "no", "explanation": "No. Greed is a longing for wealth and power. White collar crime involves businesses stealing money or assets from people.  5-10% of arrests per 100,000 arrests were for  white collar crime according to Department of Justice statistics. Gluttony is the overindulgence in food or similar things. 39.8% of US adults were classified as overweight according to the National Center for Health Statistics.", "generated": [{"finished": true, "length": 6, "score": [0.781335112599527, 0.4193340693842354, 0.23375918548615432, 0.16785688983500877, 0.14614608946650254, 0.12718539734409182], "conf": [0.9514092823288002, 0.8200382283189536, 0.7353866955761584, 0.6496381935492556, 0.5364731903711676, 0.41509336495767835], "prob": [[2.581735044381024e-06, 29], [1.3529403918226393e-20, 30], [8.760864784569284e-36, 33], [2.7247386516965783e-39, 15], [4.917184122638812e-40, 20], [4.324446081364517e-40, 6]], "generated": ["The 7 Deadly Sins are pride, envy, gluttony, lust, anger, greed, and sloth.", "To decide the most prevalent of the 7 Deadly Sins, I investigated how many times each sin is mentioned in the Bible.", "Gluttony is not mentioned at all, and envy, sloth, anger, pride, and lust are each only mentioned 10 times.", "Greed, on the other hand, is mentioned 39 times.", "Thus, greed is the most prevalent of the 7 Deadly Sins.", "So the answer is yes."], "conf_comments": ["(A)\nGreed is the", "(A)\nThe Bible mentions pride", "(A)\nGreed is mentioned", "(A)\nThus, gre", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6588084589468652, 0.32898679388746027, 0.2745315968854954, 0.23064072107431702], "conf": [0.9380680646642978, 0.8171339994222876, 0.6033372754299253, 0.42706003922452757], "prob": [[2.060514868523346e-05, 14], [4.436359667969404e-20, 27], [1.457750506651837e-20, 19], [1.4329609141983293e-20, 6]], "generated": ["Greed is considered the worst of the Seven Deadly Sins.", "Greed also has the greatest degree of seriousness, according to the Catholic Church’s Catechism of the Catholic Church.", "Thus, greed is the most prevalent of the Seven Deadly Sins.", "So the answer is yes."], "conf_comments": ["(A)\nThe Seven Deadly", "(A)\nThus, gre", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.781335112599527, 0.4193340693842354, 0.23375918548615432, 0.16785688983500877, 0.1165751063724858, 0.10196851234161647], "conf": [0.9514092823288002, 0.8200382283189536, 0.7353866955761584, 0.6496381935492556, 0.5417513886716255, 0.42593505237683693], "prob": [[2.581735044381024e-06, 29], [1.3529403918226393e-20, 30], [8.760864784569284e-36, 33], [2.7247386516965783e-39, 15], [9.247739091733305e-45, 23], [7.854139862826802e-45, 6]], "generated": ["The 7 Deadly Sins are pride, envy, gluttony, lust, anger, greed, and sloth.", "To decide the most prevalent of the 7 Deadly Sins, I investigated how many times each sin is mentioned in the Bible.", "Gluttony is not mentioned at all, and envy, sloth, anger, pride, and lust are each only mentioned 10 times.", "Greed, on the other hand, is mentioned 39 times.", "Thus, greed is by far the most prevalently mentioned of the 7 Deadly Sins.", "So the answer is yes."], "conf_comments": ["(A)\nGreed is the", "(A)\nThe Bible mentions pride", "(A)\nGreed is mentioned", "(A)\nThus, gre", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 11, "score": [0.781335112599527, 0.4193340693842354, 0.23231604738659262, 0.1489259065230059, 0.10787048224661025, 0.07379570779965515, 0.0541494136725203, 0.03855570562758956, 0.02824528397287356, 0.018734333443959635, 0.013875543325576093], "conf": [0.9514092823288002, 0.8200382283189536, 0.7155517137147314, 0.5829190142206679, 0.4822176029508566, 0.3972133933238945, 0.3172647952139993, 0.25573408840473144, 0.20655108651684095, 0.1327275348019992, 0.07648091885234938], "prob": [[2.581735044381024e-06, 29], [1.3529403918226393e-20, 30], [3.1705034046105763e-24, 8], [2.6353144742492696e-26, 7], [4.3742291189829e-28, 9], [4.750186413201985e-30, 8], [2.025485531797014e-31, 8], [4.960364609646942e-33, 8], [1.885034649834097e-34, 8], [3.095235051650885e-38, 23], [2.3039987894380685e-38, 6]], "generated": ["The 7 Deadly Sins are pride, envy, gluttony, lust, anger, greed, and sloth.", "To decide the most prevalent of the 7 Deadly Sins, I investigated how many times each sin is mentioned in the Bible.", "Greed is only mentioned 5 times", "Sloth is mentioned 6 times", "Gluttony is mentioned 8 times", "Anger is mentioned 13 times", "Envy is mentioned 15 times", "Pride is mentioned 19 times", "Lust is mentioned 24 times", "Thus, it seems like lust is the most prevalent of the 7 Deadly Sins.", "So the answer is no."], "conf_comments": ["(A)\nGreed is the", "(A)\nThe Bible mentions pride", "(A)\nPride is mentioned", "(A)\nThus, gre", "(A)\nLust is mentioned", "(A)\nLust is mentioned", "(A)\nLust is mentioned", "(A)\nLust is mentioned", "(A)\nThus, gre", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 11, "score": [0.781335112599527, 0.4193340693842354, 0.23231604738659262, 0.1489259065230059, 0.10787048224661025, 0.07379570779965515, 0.0541494136725203, 0.03740990295757236, 0.02707651268775713, 0.017342806974839406, 0.013305301109844216], "conf": [0.9514092823288002, 0.8200382283189536, 0.7155517137147314, 0.5829190142206679, 0.4822176029508566, 0.3972133933238945, 0.3172647952139993, 0.25391109097783077, 0.20333861571134493, 0.152712120449422, 0.10066870709701345], "prob": [[2.581735044381024e-06, 29], [1.3529403918226393e-20, 30], [3.1705034046105763e-24, 8], [2.6353144742492696e-26, 7], [4.3742291189829e-28, 9], [4.750186413201985e-30, 8], [2.025485531797014e-31, 8], [3.2414252854101065e-33, 8], [1.0867479782436692e-34, 8], [2.9588353764796403e-41, 25], [1.4991970051167976e-41, 6]], "generated": ["The 7 Deadly Sins are pride, envy, gluttony, lust, anger, greed, and sloth.", "To decide the most prevalent of the 7 Deadly Sins, I investigated how many times each sin is mentioned in the Bible.", "Greed is only mentioned 5 times", "Sloth is mentioned 6 times", "Gluttony is mentioned 8 times", "Anger is mentioned 13 times", "Envy is mentioned 15 times", "Lust is mentioned 26 times", "Pride is mentioned 34 times", "Because Greed is the least mentioned of the 7 Deadly Sins, it is the least prevalent.", "So the answer is no."], "conf_comments": ["(A)\nGreed is the", "(A)\nThe Bible mentions pride", "(A)\nPride is mentioned", "(A)\nThus, gre", "(A)\nLust is mentioned", "(A)\nLust is mentioned", "(A)\nLust is mentioned", "(A)\nPride is mentioned", "(A)\nThus, gre", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": [], "9": [], "10": [], "11": []}, "run_time": 97.12357664108276}
{"index": 300, "question": "Is CEO of Nissan an internationally wanted fugitive?", "answer": "yes", "explanation": "Yes. Carlos Ghosn was CEO of Nissan.  With help from an American private-security contractor, Carlos Ghosn fled from Japan to Lebanon on 30 December, breaking his bail conditions. On 2 January 2020, Interpol issued a red notice to Lebanon seeking Carlos Ghosn's arrest.", "generated": [{"finished": true, "length": 4, "score": [0.8796762200521336, 0.709794606690238, 0.6348890149414491, 0.5586760307990971], "conf": [0.9674502603928843, 0.8746381492106949, 0.7441497569913503, 0.5773653514246877], "prob": [[0.05485569937474029, 13], [5.559554469720916e-05, 21], [1.7286530197645553e-05, 19], [1.7080961057394056e-05, 6]], "generated": ["The CEO of Nissan is Carlos Ghosn.", "According to the New York Times, Carlos Ghosn is an internationally wanted fugitive.", "Thus, the CEO of Nissan is an internationally wanted fugitive.", "So the answer is yes."], "conf_comments": ["(A)\nCarlos Ghos", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.8796762200521336, 0.709794606690238, 0.5751844981641138], "conf": [0.9674502603928843, 0.8746381492106949, 0.7137709615753615], "prob": [[0.05485569937474029, 13], [5.559554469720916e-05, 21], [1.509214489625757e-05, 6]], "generated": ["The CEO of Nissan is Carlos Ghosn.", "According to the New York Times, Carlos Ghosn is an internationally wanted fugitive.", "So the answer is yes."], "conf_comments": ["(A)\nCarlos Ghos", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.720460498395714, 0.602024104069063, 0.5048009881824611], "conf": [0.8781650089032627, 0.6817728902580235, 0.4806813103323656], "prob": [[2.3860656067707055e-07, 29], [3.181374589219266e-08, 19], [3.128855845083071e-08, 6]], "generated": ["According to Nissan’s CEO Carlos Ghosn’s Wikipedia page, he is an internationally wanted fugitive.", "Thus, the CEO of Nissan is an internationally wanted fugitive.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.720460498395714, 0.5603422638388271, 0.4683476442302785], "conf": [0.8781650089032627, 0.6641992387950738, 0.46553497736857746], "prob": [[2.3860656067707055e-07, 29], [6.685117668857023e-09, 16], [6.554882535417262e-09, 6]], "generated": ["According to Nissan’s CEO Carlos Ghosn’s Wikipedia page, he is an internationally wanted fugitive.", "Thus, Carlos Ghosn is an internationally wanted fugitive.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8796762200521336, 0.5750196194310221, 0.3721740922130787, 0.27016816291861645, 0.222555058536524], "conf": [0.9674502603928843, 0.8407873049432956, 0.5837900568649156, 0.39071164031152394, 0.26664992495950385], "prob": [[0.05485569937474029, 13], [5.380583080774886e-06, 13], [2.7490039563186573e-09, 15], [3.718353715398834e-11, 18], [3.5931852652636755e-11, 6]], "generated": ["The CEO of Nissan is Carlos Ghosn.", "Carlos Ghosn is internationally wanted by Japan.", "So, Carlos Ghosn is an internationally wanted fugitive.", "Thus, CEO of Nissan is an internationally wanted fugitive.", "So the answer is yes."], "conf_comments": ["(A)\nCarlos Ghos", "(A)\nThus, the", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 47.513256788253784}
{"index": 301, "question": "Do drag kings take testosterone to look masculine?", "answer": "no", "explanation": "No. Drag Kings will use contouring and makeup to make their facial features appear more masculine and chiseled.  Testosterone is prescribed for transgender men to help with transitioning and dysphoria. Drag kings often identify as women, but dress as men for show.", "generated": [{"finished": true, "length": 4, "score": [0.5933001586712834, 0.4236096051024272, 0.34213691026132304, 0.2838367420428502], "conf": [0.9658106875520097, 0.78387612170448, 0.5812725097491022, 0.40173974915331084], "prob": [[2.002349973265777e-06, 13], [4.7412637345876985e-09, 13], [8.959029376636427e-10, 13], [8.7356465050778e-10, 6]], "generated": ["Trans women take testosterone to masculinize their bodies.", "Drag kings take estrogen to feminize their bodies.", "Thus, drag kings do not take testosterone.", "So the answer is no."], "conf_comments": ["(A)\nDrag kings are", "(A)\nThus, drag", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5933001586712834, 0.4049972183928499, 0.2853518684494249, 0.23284934704813492], "conf": [0.9658106875520097, 0.8389905771169721, 0.5982871122580413, 0.39994214927600635], "prob": [[2.002349973265777e-06, 13], [2.5584121878027178e-08, 7], [5.4194104504246645e-11, 17], [5.293690180377002e-11, 6]], "generated": ["Trans women take testosterone to masculinize their bodies.", "Drag kings are trans women.", "Thus, drag kings do take testosterone to look more masculine.", "So the answer is yes."], "conf_comments": ["(A)\nDrag kings are", "(A)\nThus, drag", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5933001586712834, 0.4236096051024272, 0.26763942860698275, 0.21331926384082447], "conf": [0.9658106875520097, 0.78387612170448, 0.5810653949852803, 0.38880600815676125], "prob": [[2.002349973265777e-06, 13], [4.7412637345876985e-09, 13], [1.99395054027327e-14, 20], [1.4602473828473415e-14, 6]], "generated": ["Trans women take testosterone to masculinize their bodies.", "Drag kings take estrogen to feminize their bodies.", "Thus, drag kings would not take testosterone, but would take estrogen.", "So the answer is no."], "conf_comments": ["(A)\nDrag kings are", "(A)\nThus, drag", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.521104056530568, 0.3030088691350662, 0.2328481951349375, 0.1945555542651129], "conf": [0.9588808770781965, 0.8541564941922231, 0.6536578852801512, 0.45970788624271086], "prob": [[6.043155288816651e-09, 15], [3.354450559232213e-18, 22], [1.1537389883976104e-19, 13], [1.1039971840181057e-19, 6]], "generated": ["Drag kings are males who dress up and look like they are females.", "Testosterone is a male hormone that helps males look like males, such as facial hair.", "Thus, drag kings do not take testosterone.", "So the answer is no."], "conf_comments": ["(A)\nThus, drag", "(A)\nThus, drag", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5933001586712834, 0.4236096051024272, 0.26763942860698275, 0.19255981426237634], "conf": [0.9658106875520097, 0.78387612170448, 0.5810653949852803, 0.38247796318273514], "prob": [[2.002349973265777e-06, 13], [4.7412637345876985e-09, 13], [1.99395054027327e-14, 20], [4.716422670918461e-15, 6]], "generated": ["Trans women take testosterone to masculinize their bodies.", "Drag kings take estrogen to feminize their bodies.", "Thus, drag kings would not take testosterone, but would take estrogen.", "So the answer is yes."], "conf_comments": ["(A)\nDrag kings are", "(A)\nThus, drag", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.521104056530568, 0.3030088691350662, 0.2280056167069896, 0.18959047102494309], "conf": [0.9588808770781965, 0.8541564941922231, 0.6415181960765681, 0.44505137543779255], "prob": [[6.043155288816651e-09, 15], [3.354450559232213e-18, 22], [4.8445625120063436e-20, 15], [4.747872786015389e-20, 6]], "generated": ["Drag kings are males who dress up and look like they are females.", "Testosterone is a male hormone that helps males look like males, such as facial hair.", "Thus, drag kings take testosterone to look masculine.", "So the answer is yes."], "conf_comments": ["(A)\nThus, drag", "(A)\nThus, drag", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 53.90743684768677}
{"index": 302, "question": "Do all crustaceans live in the ocean?", "answer": "no", "explanation": "No. The woodlice family of crustaceans is terrestrial. There are also many crustacean species living in fresh water rivers and lakes.", "generated": [{"finished": true, "length": 5, "score": [0.6477837373716794, 0.3753764245744967, 0.259110253363962, 0.22525237502468062, 0.1959763580434736], "conf": [0.9341841064961635, 0.7580917119055048, 0.5574008313779043, 0.44918070505968916, 0.34079425773536154], "prob": [[7.545420320655295e-12, 32], [2.4055941553761455e-23, 30], [1.1162908268589755e-27, 23], [4.543624848635083e-28, 14], [4.481124974876226e-28, 6]], "generated": ["There are many kinds of crustaceans (e.g. shrimps, lobsters, crabs, barnacles, etc.)", "A few of them are marine crustaceans (e.g. crabs, shrimps) and they live in the ocean.", "However, most crustaceans are freshwater crustaceans and they do not live in the ocean.", "Thus, not all crustaceans live in the ocean.", "So the answer is no."], "conf_comments": ["(A)\nMany of them live", "(A)\nHowever, there are", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6986118871520715, 0.5184885852091006, 0.32321057224229155], "conf": [0.9195241244479501, 0.82337813675706, 0.6116433258375906], "prob": [[3.967720114431301e-05, 16], [1.4686434148065845e-09, 21], [8.235060937525896e-12, 8]], "generated": ["It is true that the majority of crustaceans live in the ocean.", "For example, crabs, lobsters, and crayfish all live in the ocean.", "Thus, the answer is yes."], "conf_comments": ["(A)\nHowever, there are", "(A)\nHowever, there are", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6477837373716794, 0.3753764245744967, 0.259110253363962, 0.17074320079660302, 0.14714934676853095], "conf": [0.9341841064961635, 0.7580917119055048, 0.5574008313779043, 0.4361973943473963, 0.32493033364561186], "prob": [[7.545420320655295e-12, 32], [2.4055941553761455e-23, 30], [1.1162908268589755e-27, 23], [2.9282485478278853e-31, 14], [2.8770263446202843e-31, 6]], "generated": ["There are many kinds of crustaceans (e.g. shrimps, lobsters, crabs, barnacles, etc.)", "A few of them are marine crustaceans (e.g. crabs, shrimps) and they live in the ocean.", "However, most crustaceans are freshwater crustaceans and they do not live in the ocean.", "Thus, not all crustaceans live in the sea.", "So the answer is no."], "conf_comments": ["(A)\nMany of them live", "(A)\nHowever, there are", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.682978583384999, 0.3463213736934496, 0.27264983333119397], "conf": [0.9398886922416866, 0.7624636005206649, 0.4991055527841114], "prob": [[1.6558354568077609e-06, 19], [1.7180857804095623e-14, 16], [1.2379879583249376e-14, 6]], "generated": ["Shrimp and lobster are crustaceans and they live in the ocean.", "Thus, one part of crustaceans does live in the ocean.", "So the answer is yes."], "conf_comments": ["(A)\nThus, all", "(A)\nThus, all", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6618652486314682, 0.3534622683942301, 0.2705625812290557], "conf": [0.9119536107778056, 0.5379638501240083, 0.31869370949398157], "prob": [[2.055391826234937e-07, 21], [3.787859624971227e-12, 15], [3.546235444001894e-12, 6]], "generated": ["Crustaceans are a group of arthropods which live in the ocean and on land.", "Thus, some crustaceans can live outside of the ocean.", "So the answer is no."], "conf_comments": ["(A)\nThus, all", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 50.960031032562256}
{"index": 303, "question": "Would Paul Bunyan hypothetically be a poor choice for an urban planner?", "answer": "yes", "explanation": "Yes. Paul Bunyan was a legendary giant from tall tale stories. Urban planners need to design buildings and ceilings in cities. Paul Bunyan lived in the country side with a giant blue ox. New York city apartment ceilings average around 8 feet in height. Paul Bunyan was over seven feet tall.", "generated": [{"finished": true, "length": 3, "score": [0.509520328242577, 0.41499647522148164, 0.35095433096664946], "conf": [0.9318869592981104, 0.737836689568457, 0.5316520341452413], "prob": [[7.928589627971714e-12, 20], [5.581199154712034e-13, 15], [5.335787204224416e-13, 6]], "generated": ["Paul Bunyan was a lumberjack by profession, and would not have experience in urban planning.", "Thus, he would be a poor choice for an urban planner.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Paul", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.49413783102344666, 0.2883947216126866, 0.22444011634488736, 0.19200826466867624], "conf": [0.9624537567406496, 0.8592466391277386, 0.687112524654134, 0.5082017177890514], "prob": [[1.3473800578730514e-18, 30], [1.509240286387048e-26, 19], [1.3400964174476623e-28, 17], [1.2581131811565155e-28, 6]], "generated": ["The role of an urban planner is to plan cities, which means they would be concerned with where to put buildings, parks, and roads.", "Paul Bunyan is a fictional figure that has been portrayed as a lumberjack.", "Thus, Paul Bunyan would be a poor choice for an urban planner.", "So the answer is yes."], "conf_comments": ["(A)\nPaul Bunyan is", "(A)\nThus, Paul", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.5075101280777884, 0.39892568238389964], "conf": [0.8247612609418137, 0.6288541705905195], "prob": [[2.1448472732369743e-16, 31], [3.988222394999945e-17, 8]], "generated": ["Yes, Paul Bunyan, the lumberjack, would be a poor choice for an urban planner because he does not have the knowledge or background.", "Thus, the answer is yes."], "conf_comments": ["(A)\nUrban pl", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.49413783102344666, 0.2883947216126866, 0.18233489021374283, 0.11417849044743095, 0.09481354335623489], "conf": [0.9624537567406496, 0.8592466391277386, 0.6294042437958208, 0.4763688010557313, 0.3356759459931625], "prob": [[1.3473800578730514e-18, 30], [1.509240286387048e-26, 19], [5.743134045943901e-30, 13], [5.803468352187778e-40, 35], [5.096348495092069e-40, 6]], "generated": ["The role of an urban planner is to plan cities, which means they would be concerned with where to put buildings, parks, and roads.", "Paul Bunyan is a fictional figure that has been portrayed as a lumberjack.", "The role of a lumberjack is to cut down trees.", "Thus, Paul Bunyan would not be concerned with where to put buildings, parks, and roads, and he would be an unsuitable candidate for urban planning.", "So the answer is yes."], "conf_comments": ["(A)\nPaul Bunyan is", "(A)\nThus, Paul", "(A)\nThus, Paul", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.49413783102344666, 0.2883947216126866, 0.18233489021374283, 0.11417849044743095, 0.07054669041755564], "conf": [0.9624537567406496, 0.8592466391277386, 0.6294042437958208, 0.4763688010557313, 0.3455153691806532], "prob": [[1.3473800578730514e-18, 30], [1.509240286387048e-26, 19], [5.743134045943901e-30, 13], [5.803468352187778e-40, 35], [1.2338189318614762e-41, 6]], "generated": ["The role of an urban planner is to plan cities, which means they would be concerned with where to put buildings, parks, and roads.", "Paul Bunyan is a fictional figure that has been portrayed as a lumberjack.", "The role of a lumberjack is to cut down trees.", "Thus, Paul Bunyan would not be concerned with where to put buildings, parks, and roads, and he would be an unsuitable candidate for urban planning.", "So the answer is no."], "conf_comments": ["(A)\nPaul Bunyan is", "(A)\nThus, Paul", "(A)\nThus, Paul", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6247313254768525, 0.27440284798080933, 0.13866616386192213, 0.09394034557143204], "conf": [0.9384059671236814, 0.6655817950742199, 0.40654848692943246, 0.2539142291392577], "prob": [[1.1141546302965092e-05, 13], [3.678284398825796e-14, 15], [7.152552573774278e-23, 23], [1.1261409380282054e-23, 6]], "generated": ["According to Wikipedia, Paul Bunyan was an American legend.", "Paul Bunyan and Babe the Blue Ox are fictional creatures.", "Thus, Paul Bunyan did not exist and you cannot be an urban planner if you are fictional.", "So the answer is no."], "conf_comments": ["(A)\nThus, Paul", "(A)\nThus, Paul", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 46.933979511260986}
{"index": 304, "question": "Can sunlight travel to the deepest part of the Black Sea?", "answer": "no", "explanation": "No. The Black Sea has a maximum depth of 2,212 meters. Sunlight does not penetrate water below 1000 meters.", "generated": [{"finished": true, "length": 5, "score": [0.7795758365987848, 0.4988879805149743, 0.32139422553664215, 0.27028771682856795, 0.22798932278154727], "conf": [0.9723245955973435, 0.7092883991135202, 0.5729224533929766, 0.44569817933663325, 0.317786629788099], "prob": [[0.0003391508758854057, 17], [5.8434635337149705e-09, 19], [5.98355166675742e-20, 38], [1.1849333468794286e-20, 17], [1.1699942853198598e-20, 6]], "generated": ["The deepest part of the Black Sea is about 2200 meters.", "The distance from the surface to the deepest part is about 2200 meters.", "Since sunlight travels at about 300,000 km/s, it would take less than 0.78 seconds to travel to the deepest part.", "Thus, sunlight can travel to the deepest part of the Black Sea.", "So the answer is yes."], "conf_comments": ["(A)\nThe sun is about", "(A)\nThe speed of light", "(A)\nThus, sun", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6566666389001424, 0.4054212542234878, 0.3251060474003588, 0.2795494810820154], "conf": [0.9715062505128947, 0.7593247635139384, 0.601792122098317, 0.4458106318712883], "prob": [[3.909583090737877e-08, 21], [7.245740751553548e-17, 28], [1.6827098360626114e-18, 18], [1.6633720609394184e-18, 6]], "generated": ["The Black Sea is about 2,212 m deep in the deepest part of it.", "Sunlight only travels around 100 meters in water, which is much less than 2,212 m.", "Thus, sunlight can not travel to the deepest part of the Black Sea.", "So the answer is no."], "conf_comments": ["(A)\nThe speed of light", "(A)\nThus, sun", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6566666389001424, 0.4054212542234878, 0.30979799328323565, 0.2631306204829791], "conf": [0.9715062505128947, 0.7593247635139384, 0.594767258681077, 0.42993482824402773], "prob": [[3.909583090737877e-08, 21], [7.245740751553548e-17, 28], [3.6620741011424446e-19, 18], [3.6183215016033775e-19, 6]], "generated": ["The Black Sea is about 2,212 m deep in the deepest part of it.", "Sunlight only travels around 100 meters in water, which is much less than 2,212 m.", "Thus, sunlight can’t reach the deepest part of the Black Sea.", "So the answer is no."], "conf_comments": ["(A)\nThe speed of light", "(A)\nThus, sun", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7795758365987848, 0.4988879805149743, 0.32139422553664215, 0.17662558421546265, 0.1376444968179823], "conf": [0.9723245955973435, 0.7092883991135202, 0.5729224533929766, 0.37240117139672585, 0.23622899575620387], "prob": [[0.0003391508758854057, 17], [5.8434635337149705e-09, 19], [5.98355166675742e-20, 38], [1.30792090131991e-24, 14], [1.0071899460399552e-24, 6]], "generated": ["The deepest part of the Black Sea is about 2200 meters.", "The distance from the surface to the deepest part is about 2200 meters.", "Since sunlight travels at about 300,000 km/s, it would take less than 0.78 seconds to travel to the deepest part.", "Thus, sunlight can possibly travel to the deepest part.", "So the answer is yes."], "conf_comments": ["(A)\nThe sun is about", "(A)\nThe speed of light", "(A)\nThus, sun", "(A)\nHowever, the Black", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7208455982597388, 0.38648599291815106, 0.23599749035898607, 0.18963177777632903], "conf": [0.9714403245003511, 0.8141098717786871, 0.5731308096742382, 0.37683238610924086], "prob": [[0.0005485554008095733, 12], [2.370886102133848e-12, 18], [2.549693650160141e-17, 18], [2.286472569516179e-17, 6]], "generated": ["The Black Sea is 2212 meters deep.", "Sunlight typically travels around 96,000 km/s.", "Thus, sunlight can probably travel to the deepest part of the Black Sea.", "So the answer is yes."], "conf_comments": ["(A)\nSunlight can", "(A)\nThus, sun", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 58.86786675453186}
{"index": 305, "question": "Would a slingshot be improperly classified as artillery?", "answer": "yes", "explanation": "Yes. Artillery refers to ranged weaponry that is predominantly used in breaching fortifications. Examples of artillery include: howitzers, mortars, and rockets. Mortars can have a range up to 4,680m. A slingshot is a string weapon that propels a rock or other small projectile. Some slingshots can fire projectiles up to 9m.", "generated": [{"finished": true, "length": 4, "score": [0.7181041033265043, 0.5851645504030661, 0.40546220346589135, 0.3065792173594876], "conf": [0.947725025006836, 0.8190256984188459, 0.5782812987289789, 0.3401507660788133], "prob": [[0.0006734566078791583, 12], [3.711383543786047e-05, 11], [3.585848490216829e-08, 18], [3.023473187999627e-08, 6]], "generated": ["A slingshot is a type of catapult.", "A catapult is a type of artillery.", "Thus, a slingshot is properly classified as a type of artillery.", "So the answer is no."], "conf_comments": ["(A)\nArtillery is a", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7181041033265043, 0.5851645504030661, 0.3120576892481147, 0.23020344009883972], "conf": [0.947725025006836, 0.8190256984188459, 0.601180813190537, 0.3365513112365926], "prob": [[0.0006734566078791583, 12], [3.711383543786047e-05, 11], [8.353443735873046e-14, 21], [7.04874098207323e-14, 6]], "generated": ["A slingshot is a type of catapult.", "A catapult is a type of artillery.", "Thus, a slingshot and a catapult are indeed the same type of artillery.", "So the answer is no."], "conf_comments": ["(A)\nArtillery is a", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5879209523918231, 0.3970177665017452, 0.3245061646224575], "conf": [0.9633342495117324, 0.74211859769405, 0.5490152865565648], "prob": [[7.546993841974538e-08, 16], [6.010581380026478e-12, 18], [3.259911930808154e-12, 6]], "generated": ["Artillery is defined as military weapons, such as guns and cannons.", "Thus, it is improper to classify a slingshot as artillery.", "So the answer is no."], "conf_comments": ["(A)\nA slingshot", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5879209523918231, 0.2965977390400117, 0.23093274579105338, 0.18518575015815048], "conf": [0.9633342495117324, 0.7389907844931796, 0.5177329092092785, 0.33427818210862115], "prob": [[7.546993841974538e-08, 16], [2.891849525249101e-21, 28], [1.8508849688462007e-22, 19], [1.8064685960450457e-22, 6]], "generated": ["Artillery is defined as military weapons, such as guns and cannons.", "A slingshot would not be classified as artillery, because it is not a military weapon, and is a more primitive device.", "Thus, a slingshot would not be improperly classified as artillery.", "So the answer is no."], "conf_comments": ["(A)\nA slingshot", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.4831064451584188, 0.3132905626431112, 0.23607549818462853, 0.1814941960712218], "conf": [0.9688237472842839, 0.8133149121438409, 0.5962878362446622, 0.3699376432188538], "prob": [[9.203044829019151e-09, 13], [2.2986432282551712e-12, 12], [4.973033778108081e-14, 15], [3.7181583110623225e-14, 6]], "generated": ["Artillery refers to devices that shoot projectiles or shells.", "Slingshots shoot stones, which are projectiles.", "Thus, slingshots can be classified as artillery.", "So the answer is no."], "conf_comments": ["(A)\nSlingshots", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.4831064451584188, 0.3132905626431112, 0.23607549818462853, 0.16881894925678528], "conf": [0.9688237472842839, 0.8133149121438409, 0.5962878362446622, 0.3957008875284312], "prob": [[9.203044829019151e-09, 13], [2.2986432282551712e-12, 12], [4.973033778108081e-14, 15], [1.0413537861054157e-14, 6]], "generated": ["Artillery refers to devices that shoot projectiles or shells.", "Slingshots shoot stones, which are projectiles.", "Thus, slingshots can be classified as artillery.", "So the answer is yes."], "conf_comments": ["(A)\nSlingshots", "(A)\nThus, a", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 50.625957012176514}
{"index": 306, "question": "During the time immediately after 9/11, was don't ask don't tell still in place?", "answer": "yes", "explanation": "Yes. Don't ask don't tell was the official military policy for LGBT service members until 2011. 9/11 Occured on September 11th, 2001.", "generated": [{"finished": true, "length": 4, "score": [0.8825606573933523, 0.6252364592730602, 0.49126005147163887, 0.4084804807195749], "conf": [0.9786468500444566, 0.8456935383913319, 0.6967746452480325, 0.4843998292637946], "prob": [[0.005246284867614835, 23], [5.809160565369927e-08, 21], [3.199947988474188e-11, 26], [3.0959317219716046e-11, 6]], "generated": ["Don't ask don't tell was repealed on September 20, 2011.", "Don't ask don't tell was implemented on October 1, 1993.", "Thus, don't ask don't tell was still in effect during the time immediately after 9/11.", "So the answer is yes."], "conf_comments": ["(A)\nThus, don", "(A)\nThus, don", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.8825606573933523, 0.6322570270144436], "conf": [0.9786468500444566, 0.8082048706297016], "prob": [[0.005246284867614835, 23], [2.454646461046588e-13, 50]], "generated": ["Don't ask don't tell was repealed on September 20, 2011.", "Since 9/11 was on September 11, 2001, the time immediately after 9/11 is on September 11, 2001 - September 20, 201"], "conf_comments": ["(A)\nThus, don", "(A)\nThus, don"], "info": null}, {"finished": true, "length": 4, "score": [0.8825606573933523, 0.6252364592730602, 0.45182055637860374, 0.3755897148808695], "conf": [0.9786468500444566, 0.8456935383913319, 0.6927288988283884, 0.4813785608954831], "prob": [[0.005246284867614835, 23], [5.809160565369927e-08, 21], [7.523858156439683e-13, 25], [7.275660543913761e-13, 6]], "generated": ["Don't ask don't tell was repealed on September 20, 2011.", "Don't ask don't tell was implemented on October 1, 1993.", "Thus, don't ask don't tell was still in place at time immediately after 9/11.", "So the answer is yes."], "conf_comments": ["(A)\nThus, don", "(A)\nThus, don", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.715944087162902, 0.5570117900260516, 0.4786947167573403], "conf": [0.976913875728334, 0.8655028066793146, 0.6420905549639289], "prob": [[1.3120205533085923e-06, 21], [4.3282868623324e-09, 15], [4.213860832527846e-09, 6]], "generated": ["Don't ask don't tell has been repealed in December of 2010.", "Thus, it was not in place after 9/11.", "So the answer is no."], "conf_comments": ["(A)\nThus, don", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7510664025211579, 0.5272485993706693, 0.4428570883045321], "conf": [0.9645661029153341, 0.8168121955933015, 0.5786684182302766], "prob": [[2.0502851089873945e-08, 33], [5.352611706990291e-15, 28], [5.220334399580658e-15, 6]], "generated": ["The \"don't ask don't tell\" policy was repealed in 2010, 10 years after 9/11.", "Thus, the \"don't ask don't tell\" rule was no longer in place immediately after 9/11.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 41.44999885559082}
{"index": 307, "question": "Did Heracles famous labors exceed a baker's dozen?", "answer": "no", "explanation": "No. Heracles had twelve labors he was tasked with such as cleaning the Augean stables and killing the nine-headed Lernaean Hydra. A baker's dozen is a term used in cooking that refers to 13 of anything.", "generated": [{"finished": true, "length": 4, "score": [0.8158437080707696, 0.6271449162520273, 0.48436519948435697, 0.40928726861008663], "conf": [0.9685291129304531, 0.8196315662634, 0.6323929065170417, 0.4529051165810574], "prob": [[0.01614781745604962, 11], [0.0006371720870840961, 9], [8.023545183576692e-06, 17], [7.879678824528662e-06, 6]], "generated": ["A baker’s dozen is 13.", "Heracles had 12 labors.", "Thus, Heracles labors did not exceed a baker’s dozen.", "So the answer is no."], "conf_comments": ["(A)\nHeracles had ", "(A)\nThus, Her", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7350838370499029, 0.5724224826929429, 0.4413560114510393, 0.3770668997076511], "conf": [0.9379450478616005, 0.7920098657153035, 0.6146509716549308, 0.45070607066965573], "prob": [[0.0004435564867704589, 14], [1.162035983226079e-05, 11], [1.6336819273495828e-07, 16], [1.589025476552156e-07, 6]], "generated": ["According to Wikipedia, Heracles has 12 labors.", "A baker’s dozen is 13.", "Thus, Heracles famous labors exceed a baker’s dozen.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Her", "(A)\nThus, Her", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8158437080707696, 0.5683936038530377, 0.42581578845329443, 0.35416342870235523], "conf": [0.9685291129304531, 0.7738777987609389, 0.584284471313052, 0.4053524834672635], "prob": [[0.01614781745604962, 11], [6.71252232473334e-05, 11], [3.223764717736328e-07, 18], [3.168807171634303e-07, 6]], "generated": ["A baker’s dozen is 13.", "Heracles’ famous labors were 12.", "Thus, the number of labors did not exceed a baker’s dozen.", "So the answer is no."], "conf_comments": ["(A)\nHeracles had ", "(A)\nThus, Her", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7350838370499029, 0.5724224826929429, 0.3980299122956644, 0.3268708017219388], "conf": [0.9379450478616005, 0.7920098657153035, 0.5840349930723063, 0.39879459226783626], "prob": [[0.0004435564867704589, 14], [1.162035983226079e-05, 11], [3.822305219372227e-09, 19], [3.548036039962033e-09, 6]], "generated": ["According to Wikipedia, Heracles has 12 labors.", "A baker’s dozen is 13.", "Therefore, Heracles’s labors did not exceed a baker’s dozen.", "So the answer is no."], "conf_comments": ["(A)\nThus, Her", "(A)\nThus, Her", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7711362434332572, 0.4954338804392434, 0.423548454073339], "conf": [0.9644065048637195, 0.7262619575223316, 0.5350892742012466], "prob": [[0.00489804870248211, 11], [5.93148234261967e-07, 15], [5.651637815311031e-07, 6]], "generated": ["There are 12 labors of Heracles.", "Thus, the labors exceeded a baker's dozen.", "So the answer is yes."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 51.09526014328003}
{"index": 308, "question": "Is Thanksgiving sometimes considered a day of mourning?", "answer": "yes", "explanation": "Yes. The Native American People in the United States were brutalized during the colonization period. Native Americans in the US often choose to mourn the genocide of their people on Thanksgiving.", "generated": [{"finished": true, "length": 3, "score": [0.5414707835065697, 0.4057848395826493, 0.33705385836979196], "conf": [0.9334864525558924, 0.7135909671403541, 0.4952201804873692], "prob": [[2.5954882218157166e-15, 29], [1.3737995988402695e-17, 17], [1.326390098828306e-17, 6]], "generated": ["Some Native Americans consider the arrival of Columbus in America and Thanksgiving as a day of mourning for the loss of their culture.", "Thus, Thanksgiving could sometimes be considered a day of mourning.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Thanks", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 1, "score": [0.6802892871154145], "conf": [0.8818949259529478], "prob": [[9.96600271379138e-15, 50]], "generated": ["According to [the wikipedia article](https://en.wikipedia.org/wiki/Day_of_Mourning_(United_States)), the Day of Mourning was first started in 1970 and"], "conf_comments": ["(A)\nThanksgiving was"], "info": null}, {"finished": true, "length": 4, "score": [0.5670958172024867, 0.3343925290404288, 0.23566640775516556, 0.20966871844201548], "conf": [0.9574487279665128, 0.8581224237952668, 0.715140281314379, 0.5721251001782328], "prob": [[1.1223920770906233e-10, 21], [2.2807311807846053e-21, 26], [2.053044925829991e-25, 18], [1.925889629787016e-25, 6]], "generated": ["Thanksgiving is the day where many Americans celebrate the colonization of North America by Europeans.", "For many Native Americans, Thanksgiving symbolizes the mistreatment and theft of their land by European colonizers.", "Thus, many Native Americans treat Thanksgiving as a day of mourning.", "So the answer is yes."], "conf_comments": ["(A)\nMany Native Americans consider", "(A)\nThus, Thanks", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5670958172024867, 0.3343925290404288, 0.18522616528857927, 0.16244982792695156], "conf": [0.9574487279665128, 0.8581224237952668, 0.6972563230052822, 0.5413517917056664], "prob": [[1.1223920770906233e-10, 21], [2.2807311807846053e-21, 26], [6.085304639623624e-32, 25], [5.75390465582783e-32, 6]], "generated": ["Thanksgiving is the day where many Americans celebrate the colonization of North America by Europeans.", "For many Native Americans, Thanksgiving symbolizes the mistreatment and theft of their land by European colonizers.", "Thus, Thanksgiving is a day of mourning for some and people observe National Day of Mourning.", "So the answer is yes."], "conf_comments": ["(A)\nMany Native Americans consider", "(A)\nThus, Thanks", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5206657462390247, 0.2338714256994751, 0.12076848610200455, 0.09798231140548766], "conf": [0.9509474511806726, 0.8437039617568446, 0.7051866827617731, 0.5421203881588345], "prob": [[1.5463468632768772e-10, 18], [2.487539549998153e-25, 23], [2.9165556863443763e-34, 18], [1.1493416336690146e-34, 6]], "generated": ["Originally, Thanksgiving was in commemoration of the Pequot War.", "The War ended in May 1637, when the colonists burned their fort to the ground.", "Thus, some people commemorated the war by mourning on that day.", "So the answer is yes."], "conf_comments": ["(A)\nThe Pequot", "(A)\nThus, the", "(A)\nThus, Thanks", "(A)\n# Is the above"], "info": null}, {"finished": false, "length": 4, "score": [0.5206657462390247, 0.2338714256994751, 0.12076848610200455, 0.07512270138811243], "conf": [0.9509474511806726, 0.8437039617568446, 0.7051866827617731, 0.5477124242017608], "prob": [[1.5463468632768772e-10, 18], [2.487539549998153e-25, 23], [2.9165556863443763e-34, 18], [2.585850681129744e-40, 20]], "generated": ["Originally, Thanksgiving was in commemoration of the Pequot War.", "The War ended in May 1637, when the colonists burned their fort to the ground.", "Thus, some people commemorated the war by mourning on that day.", "Therefore, the day of Thanksgiving can sometimes be considered a day of mourning.", "So the answer is yes."], "conf_comments": ["(A)\nThe Pequot", "(A)\nThus, the", "(A)\nThus, Thanks", "(A)\nSo the answer is"], "info": [[[8.866111122914512e-07, 20], [0.9533587592840251, 6]], 0.7766913890896456, "(A)\nSo the answer is", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 41.234763622283936}
{"index": 309, "question": "Would Hapshetsut be considered a monarch?", "answer": "yes", "explanation": "Yes. A monarch is the head of a monarchy and was appointed for life. Hapshetsut was the second known female pharaoh. Pharaohs ruled for life and their wealth was even buried with them to take into the afterlife.", "generated": [{"finished": true, "length": 3, "score": [0.7535663043739724, 0.517567498244069, 0.45138204943754845], "conf": [0.9600430365486609, 0.7855171982999598, 0.6101574300925944], "prob": [[1.6255376511893748e-05, 21], [2.1923489785516826e-08, 12], [1.932501792120319e-08, 6]], "generated": ["Hapshetsut was a female pharaoh who ruled Egypt for about 20 years.", "Therefore, Hapshetsut was a monarch.", "So the answer is yes."], "conf_comments": ["(A)\nThus, H", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7535663043739724, 0.509244912739909, 0.4465183455680051], "conf": [0.9600430365486609, 0.8182619242935854, 0.6328866039322839], "prob": [[1.6255376511893748e-05, 21], [5.916656758524715e-08, 9], [5.7072422411644253e-08, 6]], "generated": ["Hapshetsut was a female pharaoh who ruled Egypt for about 20 years.", "Thus, she was considered a monarch.", "So the answer is yes."], "conf_comments": ["(A)\nThus, H", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6904321162151492, 0.4553706621722629, 0.3667473590828271, 0.31717034313917397], "conf": [0.9617315055800056, 0.8531736544903474, 0.6809237822556387, 0.5105627318827346], "prob": [[7.172057934680883e-10, 30], [1.633848439177354e-14, 15], [1.3568308840803087e-15, 12], [1.3363770090236945e-15, 6]], "generated": ["Queen Hapshetsut was an Egyptian pharaoh who lived from 1508-1458 BCE.", "A pharaoh is the title of the king or queen of Egypt.", "Thus, Hapshetsut was a monarch.", "So the answer is yes."], "conf_comments": ["(A)\nThus, H", "(A)\nThus, H", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6904321162151492, 0.4553706621722629, 0.2858119088744752, 0.24163176813894582], "conf": [0.9617315055800056, 0.8531736544903474, 0.6675462317748416, 0.5090966078121459], "prob": [[7.172057934680883e-10, 30], [1.633848439177354e-14, 15], [8.611221148819577e-18, 11], [5.834936733272391e-18, 6]], "generated": ["Queen Hapshetsut was an Egyptian pharaoh who lived from 1508-1458 BCE.", "A pharaoh is the title of the king or queen of Egypt.", "So Queen Hapshetsut is a monarch.", "So the answer is yes."], "conf_comments": ["(A)\nThus, H", "(A)\nThus, H", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6644413351150137, 0.3524633608093934, 0.26767250514700386, 0.22687970266632645], "conf": [0.9438448852532517, 0.8372590596839051, 0.6766218213586226, 0.4881671209929158], "prob": [[5.373292682826627e-07, 19], [1.7695435346930755e-13, 13], [1.573146708932295e-15, 14], [1.5336866682662968e-15, 6]], "generated": ["Hapshetsut, also known as Hatshepsut, was a woman.", "Queen is the title used to refer to a female monarch.", "Thus, Hapshetsut can be considered a monarch.", "So the answer is yes."], "conf_comments": ["(A)\nThus, H", "(A)\nThus, H", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 47.91443753242493}
{"index": 310, "question": "Is Noah's Ark an upgrade for Golden Age of Piracy pirates?", "answer": "yes", "explanation": "Yes. The Golden Age of Piracy took place between 1650 and 1720 and had many famous pirates including Blackbeard. Noah's Ark had dimensions in feet of 450 x 75 x 45. Blackbeard's ship, Queen Anne's Revenge, had a length of 103 feet. William Kidd's ship, Adventure Galley, was 124 feet long.", "generated": [{"finished": true, "length": 4, "score": [0.6892568002118418, 0.4278943735233577, 0.297737619028477, 0.24219142889092116], "conf": [0.9642526641768185, 0.7897223562647265, 0.5358545070770548, 0.35709279281196654], "prob": [[7.102732072164521e-07, 20], [1.932377803472742e-12, 17], [1.6141732874760062e-15, 21], [1.5468410010469102e-15, 6]], "generated": ["The Golden Age of Piracy occurred between the 15th and 19th centuries.", "Noah's Arc was built around 4,000 years ago.", "Thus, Noah's Ark cannot be an upgrade for Golden Age of Piracy pirates.", "So the answer is no."], "conf_comments": ["(A)\nNoah's", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6892568002118418, 0.4278943735233577, 0.269746917210232, 0.21573154436381042], "conf": [0.9642526641768185, 0.7897223562647265, 0.5193449967508078, 0.3345662512535325], "prob": [[7.102732072164521e-07, 20], [1.932377803472742e-12, 17], [8.761591459800138e-19, 29], [8.39289672442497e-19, 6]], "generated": ["The Golden Age of Piracy occurred between the 15th and 19th centuries.", "Noah's Arc was built around 4,000 years ago.", "Thus, Noah's Arc is not an upgrade for the pirates of the Golden Age of Piracy because it is too old.", "So the answer is no."], "conf_comments": ["(A)\nNoah's", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5871903454420266, 0.3696806861838327, 0.3004755439119801], "conf": [0.9455724321628812, 0.6719374253811131, 0.4625423802700356], "prob": [[9.76805665456473e-08, 16], [2.6675860635594056e-12, 18], [2.0843500655722632e-12, 6]], "generated": ["A pirate ship is a lot smaller than the Noah's Ark.", "Therefore, the Noah's Ark is not an upgrade for pirate ships.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nThus, the", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7453319550032125, 0.37066502066112766, 0.21946222783091998, 0.1817339348928837], "conf": [0.9727401944239512, 0.7479157816180532, 0.570014190045867, 0.3931916095897501], "prob": [[1.448314590182758e-06, 24], [2.0371391603191952e-16, 20], [7.5353846805966735e-25, 25], [7.272922140198311e-25, 6]], "generated": ["The Golden Age of Piracy occurred from the early 1600s to 1730s.", "However, Noah’s Ark was believed to have occurred around 600 BCE.", "Thus, the Golden Age of Piracy pirates did not have access to or upgrade to Noah's Ark.", "So the answer is no."], "conf_comments": ["(A)\nNoah's", "(A)\nThus, No", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7453319550032125, 0.33139054952011354, 0.15955526830786448, 0.12589553558175065, 0.10745071263025768], "conf": [0.9727401944239512, 0.9038950982323924, 0.7700099224829929, 0.5935870920042804, 0.43466049739955387], "prob": [[1.448314590182758e-06, 24], [5.852189123677875e-14, 11], [1.1780842027132324e-28, 26], [1.6420298337330202e-30, 20], [1.591381636888139e-30, 6]], "generated": ["The Golden Age of Piracy occurred from the early 1600s to 1730s.", "It was a time when piracy was profitable.", "The Noah's Ark was the most famous ship during the Golden Age of Piracy and it was built by Henry Morgan.", "Thus, Noah's Ark was an upgrade for Golden Age of Piracy pirates.", "So the answer is yes."], "conf_comments": ["(A)\nNoah's", "(A)\nNoah's", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 52.8247811794281}
{"index": 311, "question": "Are two cans of Campbell's Soup a day good for hypertension?", "answer": "no", "explanation": "No. Hypertension is a medical condition in which the blood pressure is high. Salt increases blood pressure. Campbell's Soup has an average of 1400 to 1800 mg of sodium (salt). The FDA states that sodium intake per day should not exceed 2300 mg.", "generated": [{"finished": true, "length": 5, "score": [0.6123947418300092, 0.3597539425370123, 0.18122148350187228, 0.13813637354323763, 0.11443059077987097], "conf": [0.9598639347924753, 0.8452188121323807, 0.6139136201255109, 0.4341184871284061, 0.2987748005573955], "prob": [[7.547419344612242e-07, 15], [9.90976441254796e-12, 12], [8.871684062473883e-22, 22], [1.4342217825608774e-23, 21], [1.409325418168698e-23, 6]], "generated": ["Campbell's tomato soup is high in potassium.", "Potassium has beneficial effects on blood pressure.", "Since hypertension is just high blood pressure, having a lot of potassium is good for it.", "Thus, two cans of Campbell's Soup a day are good for hypertension.", "So the answer is yes."], "conf_comments": ["(A)\nPotassium", "(A)\nThus, Campbell", "(A)\nThus, two", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6123947418300092, 0.3597539425370123, 0.21410680551819133, 0.17074844208466775], "conf": [0.9598639347924753, 0.8452188121323807, 0.5658807277732869, 0.3633294488906876], "prob": [[7.547419344612242e-07, 15], [9.90976441254796e-12, 12], [4.3293924747890386e-18, 23], [4.089668570814667e-18, 6]], "generated": ["Campbell's tomato soup is high in potassium.", "Potassium has beneficial effects on blood pressure.", "Thus, 2 cans of Campbell's tomato soup can actually be good for hypertension.", "So the answer is yes."], "conf_comments": ["(A)\nPotassium", "(A)\nThus, Campbell", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6123947418300092, 0.4340912157934225, 0.18475017933980648, 0.13093662100319264, 0.10345625924927392], "conf": [0.9598639347924753, 0.8488562938523969, 0.6773333978326113, 0.45299866817036555, 0.3046360042692357], "prob": [[7.547419344612242e-07, 15], [1.5664528672092084e-10, 15], [3.4316951867594266e-20, 15], [4.7385414793335106e-23, 23], [3.033098820347666e-23, 6]], "generated": ["Campbell's tomato soup is high in potassium.", "Potassium is a nutrient that lowers blood pressure.", "In moderation, it may be beneficial to hypertensive patients.", "Thus, two cans of Campbell's tomato soup a day might be good for hypertension.", "So the answer is yes."], "conf_comments": ["(A)\nPotassium", "(A)\nThus, Campbell", "(A)\nThus, two", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6244232228809605, 0.3362801923598484, 0.18945442725006742, 0.1136596166593666, 0.09612664627292519], "conf": [0.9612375562027891, 0.855125417920915, 0.7366848997842859, 0.5560083921265078, 0.40146126756958367], "prob": [[8.836749066127112e-08, 18], [3.677063156371397e-12, 9], [6.2431818230431e-17, 11], [4.842484839825793e-23, 19], [4.5766287538263436e-23, 6]], "generated": ["Campbell's chicken noodle soup is relatively high in sodium.", "Too much salt can increase blood pressure.", "Hypertension is also associated with high blood pressure.", "Thus, two cans of soup a day would not be ideal for hypertension.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, Campbell", "(A)\nThus, Campbell", "(A)\nThus, Campbell", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6244232228809605, 0.36373843954360313, 0.18451776356605212, 0.15141164113650393], "conf": [0.9612375562027891, 0.8664480621633318, 0.6237407547404143, 0.4260791047709755], "prob": [[8.836749066127112e-08, 18], [2.694654210088144e-13, 13], [2.9809729238399375e-26, 29], [2.734603440444736e-26, 6]], "generated": ["Campbell's chicken noodle soup is relatively high in sodium.", "An excessive intake of sodium can increase blood pressure.", "Thus, eating two cans of Campbell’s soup could slightly raise blood pressure and it is not good for hypertension patients.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, Campbell", "(A)\nThus, Campbell", "(A)\nHowever, the amount", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6123947418300092, 0.4340912157934225, 0.18475017933980648, 0.13093662100319264, 0.09182043188191003], "conf": [0.9598639347924753, 0.8488562938523969, 0.6773333978326113, 0.45299866817036555, 0.28472121276638845], "prob": [[7.547419344612242e-07, 15], [1.5664528672092084e-10, 15], [3.4316951867594266e-20, 15], [4.7385414793335106e-23, 23], [1.0870404683100444e-23, 6]], "generated": ["Campbell's tomato soup is high in potassium.", "Potassium is a nutrient that lowers blood pressure.", "In moderation, it may be beneficial to hypertensive patients.", "Thus, two cans of Campbell's tomato soup a day might be good for hypertension.", "So the answer is maybe."], "conf_comments": ["(A)\nPotassium", "(A)\nThus, Campbell", "(A)\nThus, two", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 61.579383850097656}
{"index": 312, "question": "Did Electronic Arts profit from Metroid sales?", "answer": "no", "explanation": "No. Metroid was created and published by Nintendo. Electronic Arts is a video game company that is a competitor to Nintendo. Companies cannot profit of the work owned by another company typically. Companies do not typically share profits with their competitors.", "generated": [{"finished": true, "length": 4, "score": [0.6250441810937384, 0.510674309529254, 0.3802179674882868, 0.3117736222441822], "conf": [0.9551093024556929, 0.8148023980847543, 0.5835991520188774, 0.3948765957151612], "prob": [[7.032917379841761e-09, 21], [2.2681857426755854e-10, 14], [3.759386482715468e-12, 16], [3.6200729185279915e-12, 6]], "generated": ["Metroid is an action-adventure video game originally released in August 1986.", "Electronic Arts was founded in 1982.", "Thus, Electronic Arts could not have profited from Metroid sales.", "So the answer is no."], "conf_comments": ["(A)\nElectronic", "(A)\nThus, Electron", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5905746891975481, 0.33017271783366103, 0.2751237118705392, 0.22849318095185203], "conf": [0.9621204056495706, 0.7427371601477978, 0.5551487316657202, 0.3839034569052325], "prob": [[4.237253568206069e-09, 19], [2.4040919718379966e-17, 21], [8.569268315048501e-18, 14], [8.437433721158707e-18, 6]], "generated": ["Metroid is a video game franchise for Nintendo released in 1986.", "Electronic Arts is a video game developer that is not involved in developing the Metroid series.", "Thus, Electronic Arts did not profit from Metroid sales.", "So the answer is no."], "conf_comments": ["(A)\nElectronic", "(A)\nThus, Electron", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5905746891975481, 0.35780684867446877, 0.2695503617444275, 0.22785118801479898], "conf": [0.9621204056495706, 0.834033317987087, 0.6647245892998995, 0.47633762555777076], "prob": [[4.237253568206069e-09, 19], [2.697083277287393e-20, 30], [3.263690006928107e-22, 13], [3.2077948300112556e-22, 6]], "generated": ["Metroid is a video game franchise for Nintendo released in 1986.", "In 2005, Nintendo and Electronic Arts formed a partnership to develop games for each other's video game consoles.", "Thus, Electronic Arts did profit from Metroid sales.", "So the answer is yes."], "conf_comments": ["(A)\nElectronic", "(A)\nThus, Electron", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5905746891975481, 0.33017271783366103, 0.2404830028421429, 0.1978928145900049], "conf": [0.9621204056495706, 0.7427371601477978, 0.5432557671350824, 0.3688112145869799], "prob": [[4.237253568206069e-09, 19], [2.4040919718379966e-17, 21], [1.9440580746596852e-19, 15], [1.9145127717602586e-19, 6]], "generated": ["Metroid is a video game franchise for Nintendo released in 1986.", "Electronic Arts is a video game developer that is not involved in developing the Metroid series.", "Thus, Electronic Arts didn’t profit from Metroid sales.", "So the answer is no."], "conf_comments": ["(A)\nElectronic", "(A)\nThus, Electron", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6263388111382753, 0.3027288601280634, 0.22677340443389485, 0.19080891883750797], "conf": [0.9592919424659788, 0.8510375593332971, 0.6464936439668204, 0.45894417043166447], "prob": [[2.502381152916563e-07, 17], [3.521289279841198e-17, 17], [6.86586786519778e-19, 13], [6.754612831590028e-19, 6]], "generated": ["Electronic Arts is a company that is well known for making video games.", "Electronic Arts produced a Metroid based video game called Metroid Prime.", "Thus, Electronic Arts profited from Metroid sales.", "So the answer is yes."], "conf_comments": ["(A)\nMetroid is a", "(A)\nMetroid Prime was", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 52.887765884399414}
{"index": 313, "question": "Snowboarding is a rarity in Hilo?", "answer": "yes", "explanation": "Yes. Snowboarding is a sport that involves descending snow-covered slopes. Hilo, a town in Hawaii, has not had snow in almost 200 years.", "generated": [{"finished": true, "length": 4, "score": [0.7191698335253159, 0.4910789704123356, 0.3505905459883532, 0.2893346858653789], "conf": [0.9676770340970877, 0.8173802994033772, 0.556914135226749, 0.3847458082325512], "prob": [[0.0010168511667752, 11], [8.766485682948524e-06, 8], [1.1267571622370583e-07, 15], [1.0344749779300412e-07, 6]], "generated": ["Hilo is located in the state of Hawaii.", "There is no snow in Hawaii.", "Therefore, snowboarding is a rarity in Hilo.", "So the answer is yes."], "conf_comments": ["(A)\nHawaii", "(A)\nThus, snow", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6378872369866501, 0.3542964084845317, 0.2936406104287374, 0.24491854911299613], "conf": [0.9636851283120742, 0.7912144529680604, 0.598439300944033, 0.4186550164560657], "prob": [[7.604652162826326e-05, 11], [1.6944880220128346e-12, 18], [3.9960519573576636e-13, 15], [3.8643876826424724e-13, 6]], "generated": ["Snowboarding is a sport which requires snow.", "As Hilo is a place in Hawaii, it is unlikely that it has snow.", "Thus, snowboarding is a rarity in Hilo.", "So the answer is yes."], "conf_comments": ["(A)\nHilo is a", "(A)\nThus, snow", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6378872369866501, 0.44044235340118854, 0.24506353286382157, 0.1977841769967656, 0.16716379055253405], "conf": [0.9636851283120742, 0.8734831994213529, 0.7419082874174389, 0.5788654169133615, 0.41650630555549534], "prob": [[7.604652162826326e-05, 11], [4.4549901550222747e-07, 8], [3.6946111898858055e-17, 23], [2.4634493919204273e-18, 15], [2.3587818585331136e-18, 6]], "generated": ["Snowboarding is a sport which requires snow.", "Hilo is located in Hawaii.", "Hawaii is located in the Pacific Ocean and doesn't have any mountains, which usually have snow.", "Thus, snowboarding is a rarity in Hilo.", "So the answer is yes."], "conf_comments": ["(A)\nHilo is a", "(A)\nHawaii", "(A)\nThus, snow", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7191698335253159, 0.4910789704123356, 0.27043101442926815, 0.2193283879989069], "conf": [0.9676770340970877, 0.8173802994033772, 0.5264591141794447, 0.35601246320899343], "prob": [[0.0010168511667752, 11], [8.766485682948524e-06, 8], [1.0861082376267794e-10, 15], [9.198774684799977e-11, 6]], "generated": ["Hilo is located in the state of Hawaii.", "There is no snow in Hawaii.", "Thus, it is impossible for anyone to snowboard in Hawaii.", "So the answer is no."], "conf_comments": ["(A)\nHawaii", "(A)\nThus, snow", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.662582250784598, 0.3389625988992418, 0.2613318763678165], "conf": [0.9636651317568394, 0.7116228854393231, 0.4867775544914371], "prob": [[0.0008452419714438085, 9], [4.618342908466729e-15, 25], [1.9883352740431456e-15, 6]], "generated": ["Hilo is a region in Hawaii.", "Hawaii does not have snow, thus it is implied that snowboarding is rarely seen in Hilo.", "So the answer is yes."], "conf_comments": ["(A)\nHilo is known", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 51.90747117996216}
{"index": 314, "question": "Can a sniper shoot a fish past Bathypelagic Zone in ocean?", "answer": "no", "explanation": "No. The Bathypelagic Zone extends 4000 meters down in the ocean. The longest recorded sniper kill is 3,540 meters.", "generated": [{"finished": true, "length": 5, "score": [0.6714912310990867, 0.42692844697466453, 0.2719953266640607, 0.22525839101943246, 0.19881123331899012], "conf": [0.9631558747651999, 0.8169450541403167, 0.7143513007224356, 0.5882558606109229, 0.4594875191009503], "prob": [[5.74913933037537e-09, 25], [1.94029459591741e-14, 17], [1.9428330601203863e-21, 21], [1.6735409350024426e-23, 26], [1.6463072875143374e-23, 6]], "generated": ["The Bathypelagic Zone is about 5000-10000 feet below the sea.", "The maximum range for a sniper rifle is about 2.5 miles.", "In comparison, 2.5 miles is nowhere close to the Bathypelagic Zone.", "Thus, it is impossible for a sniper to shoot a fish past the Bathypelagic Zone in the ocean.", "So the answer is no."], "conf_comments": ["(A)\nA sniper rif", "(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6714912310990867, 0.42692844697466453, 0.2719953266640607, 0.22062331514691455, 0.19302997078623393], "conf": [0.9631558747651999, 0.8169450541403167, 0.7143513007224356, 0.5882366903474484, 0.45132739796207016], "prob": [[5.74913933037537e-09, 25], [1.94029459591741e-14, 17], [1.9428330601203863e-21, 21], [1.7448910245764602e-23, 21], [1.7211187723770626e-23, 6]], "generated": ["The Bathypelagic Zone is about 5000-10000 feet below the sea.", "The maximum range for a sniper rifle is about 2.5 miles.", "In comparison, 2.5 miles is nowhere close to the Bathypelagic Zone.", "Thus, a sniper rifle cannot shoot a fish past the Bathypelagic Zone.", "So the answer is no."], "conf_comments": ["(A)\nA sniper rif", "(A)\nThus, a", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6714912310990867, 0.4228988711923212, 0.3640256486421069], "conf": [0.9631558747651999, 0.7209138919344223, 0.539102244328881], "prob": [[5.74913933037537e-09, 25], [6.241696210493886e-14, 18], [5.906383693532334e-14, 6]], "generated": ["The Bathypelagic Zone is about 5000-10000 feet below the sea.", "Thus, it is impossible for a sniper to shoot a fish past that zone.", "So the answer is no."], "conf_comments": ["(A)\nA sniper rif", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6250733697046406, 0.35805358967648937, 0.24396106230117137, 0.2016440372123285], "conf": [0.9695514895015268, 0.8621569641426549, 0.6286934288694889, 0.43336928191607005], "prob": [[7.856698443018913e-08, 18], [4.342340629097618e-19, 26], [2.1055055858675216e-23, 22], [1.9953523538206645e-23, 6]], "generated": ["A sniper’s bullet travels at about 880 m/s.", "The maximum depth of the Bathypelagic Zone is about 7 km (4.4 mi) below sea level.", "Thus, the sniper should be able to shoot a fish past the Bathypelagic Zone.", "So the answer is yes."], "conf_comments": ["(A)\nThe Bathypel", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6249303941530051, 0.34615342448716735, 0.18158647858286112, 0.1518608532928001], "conf": [0.9681669356558804, 0.8126599233093827, 0.588679682100203, 0.4262738894349172], "prob": [[1.300951083301591e-08, 20], [4.828612110564188e-16, 17], [2.4674413379606473e-30, 34], [2.003257035290561e-30, 6]], "generated": ["The Bathypelagic Zone lies around -6,000 meters below sea level.", "The fastest bullet travels at 2,000 m/s.", "Thus, it takes more than 6,000 meter/second for a fish to pass through the Bathypelagic Zone, which is impossible.", "So the answer is no."], "conf_comments": ["(A)\nThe average depth of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 56.610506534576416}
{"index": 315, "question": "Were Depeche Mode heavily influenced by blues music?", "answer": "no", "explanation": "No. Blues incorporated spirituals, work songs, field hollers, shouts, chants, and rhymed simple narrative ballads and was derived from African-Americans. Blues music uses instruments like slide guitar, harmonica, piano, and bass drums. Depeche Mode are a British pop synth group. Depeche Mode uses computer synthesizers to create their unique sound as well as heavy rock guitars. Depeche Mode was influenced by The Cure, and Ultravox, new wave rock bands.", "generated": [{"finished": true, "length": 4, "score": [0.7509998628182787, 0.2844453276266288, 0.21597785370720243, 0.17120968066611322], "conf": [0.9415558509407633, 0.5938166119320972, 0.38670440127091493, 0.24357362503541766], "prob": [[0.009928934823599866, 9], [7.062503474053828e-17, 22], [1.0056888907270727e-17, 16], [9.917128473976359e-18, 6]], "generated": ["Depeche Mode is an electronic band.", "Therefore, they are unlikely to be heavily influenced by blues music with its roots in the electric guitar.", "Thus, Depeche Mode were not heavily influenced by blues music.", "So the answer is no."], "conf_comments": ["(A)\nBlues music is", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7509998628182787, 0.3627222633549812, 0.183059457070637, 0.13337025802569874, 0.10371945855636336], "conf": [0.9415558509407633, 0.7397916745216961, 0.4885810321446543, 0.3013265014726791, 0.18287045724065257], "prob": [[0.009928934823599866, 9], [1.2189419292633152e-10, 15], [6.458892300790967e-19, 20], [5.854431137882358e-20, 16], [5.734061782409286e-20, 6]], "generated": ["Depeche Mode is an electronic band.", "They use synthesizers, samplers, etc in their music.", "Blues music mostly uses instruments such as guitars, harps, pianos, etc.", "Thus, Depeche Mode was not heavily influenced by blues music.", "So the answer is no."], "conf_comments": ["(A)\nBlues music is", "(A)\nBlues music is", "(A)\nThus, De", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5812104242682193, 0.3603007946569082, 0.16316568605252324, 0.1255180139527801], "conf": [0.9398167924662932, 0.7919544691488286, 0.4397626741443793, 0.26290481469279053], "prob": [[3.6045262799370376e-09, 19], [1.2616950051974035e-14, 16], [2.062137770426342e-22, 18], [1.939828602965844e-22, 6]], "generated": ["Depeche Mode is a britpop band that was formed in the 70s.", "Blues music is a musical genre from the early 20th century.", "Thus, Depeche Mode cannot have possibly been greatly influenced by blues music.", "So the answer is no."], "conf_comments": ["(A)\nBlues music is", "(A)\nThus, De", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7496375138659472, 0.32029495923209034, 0.1508099362080006, 0.11583454366063418], "conf": [0.9557519855573874, 0.8086485669451549, 0.4751001310936474, 0.29592161141437123], "prob": [[0.001707217968923316, 12], [1.7696211537181132e-19, 24], [4.6370277638977646e-30, 25], [3.348020652408409e-30, 6]], "generated": ["Depeche Mode is an English synth-pop band.", "Blues is a genre of music that comes from the United States in which the lyrics usually lament life experiences.", "Depeche Mode does not compose any blues music so Depeche Mode was not heavily influenced by blues music.", "So the answer is no."], "conf_comments": ["(A)\nBlues is a", "(A)\nThus, De", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.7509998628182787, 0.2844453276266288, 0.1963670186686534], "conf": [0.9415558509407633, 0.5938166119320972, 0.38490694851548557], "prob": [[0.009928934823599866, 9], [7.062503474053828e-17, 22], [1.1157711550389866e-17, 6]], "generated": ["Depeche Mode is an electronic band.", "Therefore, they are unlikely to be heavily influenced by blues music with its roots in the electric guitar.", "So the answer is no."], "conf_comments": ["(A)\nBlues music is", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 53.37031054496765}
{"index": 316, "question": "Do you need lactobacillus to make pickles?", "answer": "no", "explanation": "No. Lactobacillus is used in the production of fermented-style pickles.  \"Quick\" pickles are made with a mixture of brine and vinegar.", "generated": [{"finished": true, "length": 4, "score": [0.754568211529411, 0.38570503863021777, 0.31910810958851615, 0.2748767852037509], "conf": [0.96318280425506, 0.7918368380548761, 0.6437899299770676, 0.4789499246047215], "prob": [[4.592799758971782e-05, 19], [1.6471885592540255e-17, 25], [1.0491766345549816e-18, 16], [1.0327036007152832e-18, 6]], "generated": ["Lactobacillus is a type of bacteria which produces lactic acid.", "Pickles are usually made from fermented vegetables infused with the lactic acid produced by bacteria.", "Thus, you need lactobacillus to make pickles.", "So the answer is yes."], "conf_comments": ["(A)\nLactic acid is", "(A)\nThus, la", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6286548568598587, 0.38510742159211525, 0.31404397774890025, 0.26835659929227884], "conf": [0.901088723106343, 0.7230231302575948, 0.6071832974060211, 0.4448034695635776], "prob": [[5.852088715719201e-09, 23], [7.685570819491495e-21, 36], [1.836829004319827e-22, 16], [1.8015252389060721e-22, 6]], "generated": ["According to Wikipedia, pickling is the process of preserving food in an anaerobic liquid.", "Lactobacillus is a type of bacteria that is known to produce lactic acid by metabolizing glucose in anaerobic conditions.", "Thus, lactobacillus is needed to make pickles.", "So the answer is yes."], "conf_comments": ["(A)\nLactobac", "(A)\nThus, la", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6338733409681241, 0.4043558005128036, 0.3475871081301901], "conf": [0.9328747020983315, 0.7676388626618563, 0.569863285685562], "prob": [[6.040712728604355e-07, 17], [2.6131501514435657e-10, 11], [2.541451414183356e-10, 6]], "generated": ["Lactobacillus is the bacteria responsible for causing pickling.", "Thus, they are needed to make pickles.", "So the answer is yes."], "conf_comments": ["(A)\nThus, you", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6286548568598587, 0.38510742159211525, 0.270927911143978, 0.23128048638182752], "conf": [0.901088723106343, 0.7230231302575948, 0.6020464115652854, 0.44057268178060305], "prob": [[5.852088715719201e-09, 23], [7.685570819491495e-21, 36], [1.8652247234603833e-24, 16], [1.8189825330400636e-24, 6]], "generated": ["According to Wikipedia, pickling is the process of preserving food in an anaerobic liquid.", "Lactobacillus is a type of bacteria that is known to produce lactic acid by metabolizing glucose in anaerobic conditions.", "Thus, lactobacillus are required to make pickles.", "So the answer is yes."], "conf_comments": ["(A)\nLactobac", "(A)\nThus, la", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.754568211529411, 0.38570503863021777, 0.2774469080316542, 0.22909401946041405], "conf": [0.96318280425506, 0.7918368380548761, 0.6221135331320471, 0.45855776425517814], "prob": [[4.592799758971782e-05, 19], [1.6471885592540255e-17, 25], [4.757252683048883e-20, 14], [2.9799992746342283e-20, 6]], "generated": ["Lactobacillus is a type of bacteria which produces lactic acid.", "Pickles are usually made from fermented vegetables infused with the lactic acid produced by bacteria.", "So lactobacillus is needed to make pickles.", "So the answer is yes."], "conf_comments": ["(A)\nLactic acid is", "(A)\nThus, la", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.754568211529411, 0.38570503863021777, 0.31910810958851615, 0.13111599872594756], "conf": [0.96318280425506, 0.7918368380548761, 0.6437899299770676, 0.4920878583316946], "prob": [[4.592799758971782e-05, 19], [1.6471885592540255e-17, 25], [1.0491766345549816e-18, 16], [5.514862908461899e-22, 5]], "generated": ["Lactobacillus is a type of bacteria which produces lactic acid.", "Pickles are usually made from fermented vegetables infused with the lactic acid produced by bacteria.", "Thus, you need lactobacillus to make pickles.", "The answer is yes."], "conf_comments": ["(A)\nLactic acid is", "(A)\nThus, la", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": false, "length": 4, "score": [0.6286548568598587, 0.38510742159211525, 0.270927911143978, 0.10669292287436065], "conf": [0.901088723106343, 0.7230231302575948, 0.6020464115652854, 0.4814488301880773], "prob": [[5.852088715719201e-09, 23], [7.685570819491495e-21, 36], [1.8652247234603833e-24, 16], [3.081982692539132e-52, 39]], "generated": ["According to Wikipedia, pickling is the process of preserving food in an anaerobic liquid.", "Lactobacillus is a type of bacteria that is known to produce lactic acid by metabolizing glucose in anaerobic conditions.", "Thus, lactobacillus are required to make pickles.", "This example is also known as a [sneaky example](http://www.gunnarlih.se/jokes/jokes/sneaky.html).", "You can use a dictionary to check the ana"], "conf_comments": ["(A)\nLactobac", "(A)\nThus, la", "(A)\n# Is the above", "(A)\n# Is the above"], "info": [[[1.652338537965231e-28, 39], [6.710989056028091e-12, 10]], 0.7996872349697071, "(A)\n# Is the above", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 50.87857103347778}
{"index": 317, "question": "Did King James I despise fairy beings?", "answer": "yes", "explanation": "Yes. King James I wrote Daemonologie in which he stated that a fairy was a being that could act as a familiar. A familiar was an animal or spirit that conspired with The Devil. King James I presided over the execution of Agnes Sampson. Agnes Sampson was accused of conspiring with familiars and was burned at the stake.", "generated": [{"finished": true, "length": 4, "score": [0.6473077588317103, 0.47606643236664026, 0.29183198146826667, 0.2190038201756286], "conf": [0.9602277275655261, 0.8452314773685565, 0.6150574153562629, 0.4178724041604846], "prob": [[9.07534949025581e-06, 14], [9.931901771346213e-09, 14], [6.774295355362257e-14, 18], [2.1974631928475145e-14, 6]], "generated": ["King James I was a follower of the Church of England.", "The Church of England does not accept the existence of fairies.", "Therefore, we can assume that King James I did not believe in fairies.", "So the answer is yes."], "conf_comments": ["(A)\nThe Church of England", "(A)\nThus, King", "(A)\nThus, King", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.630902438715167, 0.4122587132112684, 0.2173212678871388, 0.17075060505683148], "conf": [0.9597372528981286, 0.8146613077449167, 0.5307482667325903, 0.3751392123266629], "prob": [[0.0001505638103832213, 10], [9.999726336107546e-09, 14], [1.2001639525324873e-14, 16], [4.064151567592689e-15, 8]], "generated": ["King James I wrote The Daemonologie.", "In The Daemonologie, he wrote about fairy beings.", "So it is clear that King James did not despise fairy beings.", "Thus, the answer is no."], "conf_comments": ["(A)\nThe Daemonologie", "(A)\nThus, King", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6473077588317103, 0.35181619762107386, 0.20273564025887642, 0.17015182822996328], "conf": [0.9602277275655261, 0.8422153939768712, 0.6556510777291163, 0.46547905055947963], "prob": [[9.07534949025581e-06, 14], [4.669302801544634e-18, 26], [1.315031476421224e-23, 15], [1.2544408505767537e-23, 6]], "generated": ["King James I was a follower of the Church of England.", "In English folklore, fairies were considered to be pagan and were viewed negatively by the Church of England.", "Thus, King James I would not like them and despise them.", "So the answer is yes."], "conf_comments": ["(A)\nThe Church of England", "(A)\nThus, King", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.630902438715167, 0.4122587132112684, 0.2173212678871388, 0.16499938518185092], "conf": [0.9597372528981286, 0.8146613077449167, 0.5307482667325903, 0.35602983330238025], "prob": [[0.0001505638103832213, 10], [9.999726336107546e-09, 14], [1.2001639525324873e-14, 16], [4.832978801727317e-15, 6]], "generated": ["King James I wrote The Daemonologie.", "In The Daemonologie, he wrote about fairy beings.", "So it is clear that King James did not despise fairy beings.", "So the answer is no."], "conf_comments": ["(A)\nThe Daemonologie", "(A)\nThus, King", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.6473077588317103, 0.47606643236664026, 0.29183198146826667, 0.12011349890492742, 0.09316256116800985], "conf": [0.9602277275655261, 0.8452314773685565, 0.6150574153562629, 0.40883625793712003, 0.2645950761534343], "prob": [[9.07534949025581e-06, 14], [9.931901771346213e-09, 14], [6.774295355362257e-14, 18], [2.144915031308613e-23, 16], [1.3836020155312156e-23, 6]], "generated": ["King James I was a follower of the Church of England.", "The Church of England does not accept the existence of fairies.", "Therefore, we can assume that King James I did not believe in fairies.", "Therefore is possible for King James I to despise fairy beings.", "So the answer is yes."], "conf_comments": ["(A)\nThe Church of England", "(A)\nThus, King", "(A)\nThus, King", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6473077588317103, 0.47606643236664026, 0.29183198146826667, 0.12011349890492742, 0.07780217037025326], "conf": [0.9602277275655261, 0.8452314773685565, 0.6150574153562629, 0.40883625793712003, 0.2360675395962557], "prob": [[9.07534949025581e-06, 14], [9.931901771346213e-09, 14], [6.774295355362257e-14, 18], [2.144915031308613e-23, 16], [3.157094686796532e-24, 6]], "generated": ["King James I was a follower of the Church of England.", "The Church of England does not accept the existence of fairies.", "Therefore, we can assume that King James I did not believe in fairies.", "Therefore is possible for King James I to despise fairy beings.", "So the answer is maybe."], "conf_comments": ["(A)\nThe Church of England", "(A)\nThus, King", "(A)\nThus, King", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 57.0229766368866}
{"index": 318, "question": "Is dyslexia the most common intellectual disability in US?", "answer": "no", "explanation": "No. An intellectual disability is reflected in below-average IQ and a lack of skills needed for daily living. Learning disabilities are weaknesses in certain academic skills. usually, Reading, writing and math. Dyslexia is characterized by difficulties with accurate and/or fluent word recognition and by poor spelling and decoding abilities. Thomas Jefferson, George Washington, and John F. Kennedy were successful presidents while being dyslexic.", "generated": [{"finished": true, "length": 4, "score": [0.6405685174278943, 0.5212527237680553, 0.3995934556122318, 0.32895708005997804], "conf": [0.9296902369810809, 0.8227011883527616, 0.5648253335199876, 0.3856637935196031], "prob": [[8.913734323963772e-18, 48], [2.7466249793618934e-19, 12], [1.431171484932362e-20, 19], [1.3682766639682565e-20, 6]], "generated": ["According to the Centers for Disease Control and Prevention, intellectual developmental disorders are one of the three most common developmental disabilities in the US, along with autism spectrum disorders, and learning disorders.", "Dyslexia is a type of learning disorder.", "Thus, dyslexia is not the most common intellectual disability in the US.", "So the answer is no."], "conf_comments": ["(A)\nDyslexia", "(A)\nThus, d", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5520908553236964, 0.4407980894232643, 0.28802195482221066, 0.23762153402718597, 0.2059543982681587], "conf": [0.8387522685089202, 0.7619351861920739, 0.6355517574794282, 0.5023465649789036, 0.37883393027089574], "prob": [[0.002303161527269775, 6], [7.963574345208218e-06, 16], [2.5749848157687558e-09, 12], [2.0273503557625793e-10, 17], [1.9809773953795935e-10, 6]], "generated": ["According to this website,", "dyslexia is the most common learning disability in the United States.", "A learning disability is a type of intellectual disability.", "Thus, dyslexia is the most common intellectual disability in US.", "So the answer is yes."], "conf_comments": ["(A)\ndyslexia", "(A)\nThus, d", "(A)\nThus, d", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6405685174278943, 0.5212527237680553, 0.3367821918694131, 0.2738636385004628], "conf": [0.9296902369810809, 0.8227011883527616, 0.536879932000428, 0.35751220582929844], "prob": [[8.913734323963772e-18, 48], [2.7466249793618934e-19, 12], [2.3125771464234816e-23, 21], [2.2173643906970546e-23, 6]], "generated": ["According to the Centers for Disease Control and Prevention, intellectual developmental disorders are one of the three most common developmental disabilities in the US, along with autism spectrum disorders, and learning disorders.", "Dyslexia is a type of learning disorder.", "Thus, dyslexia is not one of the most common intellectual disabilities in the US.", "So the answer is no."], "conf_comments": ["(A)\nDyslexia", "(A)\nThus, d", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.5520908553236964, 0.4407980894232643, 0.28802195482221066, 0.20768633741655643, 0.178180077065925], "conf": [0.8387522685089202, 0.7619351861920739, 0.6355517574794282, 0.4878054454003757, 0.36095962801487375], "prob": [[0.002303161527269775, 6], [7.963574345208218e-06, 16], [2.5749848157687558e-09, 12], [1.0669734855867038e-12, 20], [1.0334662251553028e-12, 6]], "generated": ["According to this website,", "dyslexia is the most common learning disability in the United States.", "A learning disability is a type of intellectual disability.", "Thus, dyslexia is the most common form of intellectual disability in the US.", "So the answer is yes."], "conf_comments": ["(A)\ndyslexia", "(A)\nThus, d", "(A)\nThus, d", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6405685174278943, 0.4152875654907094, 0.3390495249212166], "conf": [0.9296902369810809, 0.7530256177133136, 0.5477313035946206], "prob": [[8.913734323963772e-18, 48], [6.721087380769363e-25, 25], [3.979818116959227e-25, 6]], "generated": ["According to the Centers for Disease Control and Prevention, intellectual developmental disorders are one of the three most common developmental disabilities in the US, along with autism spectrum disorders, and learning disorders.", "Thus, dyslexia is one of the three most common intellectual (developmental) disabilities in the US.", "So the answer is yes."], "conf_comments": ["(A)\nDyslexia", "(A)\nHowever, according to", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 49.48716759681702}
{"index": 319, "question": "Do Bing (search engine) searches earn the searcher more than competitors do?", "answer": "yes", "explanation": "Yes. Bing (search engine) has a search rewards program that gives the user points, from conducting searches, to redeem for prizes. Bing (search engine) has several competitors such as Google, and DuckDuckGo. Google and DuckDuckGo do not have search rewards programs.", "generated": [{"finished": true, "length": 7, "score": [0.4264951080274479, 0.25454926302042996, 0.2169070172571839, 0.16267242905422216, 0.09355005996185903, 0.03888477385041783, 0.02643738633531977], "conf": [0.9487403505134769, 0.8587723447739076, 0.7388164662062477, 0.6113160902986069, 0.47607178419132934, 0.30490309620322725, 0.18274054460011754], "prob": [[4.504169529227587e-15, 20], [3.787810530571823e-21, 15], [1.7889502439750469e-22, 18], [8.044858198907954e-25, 14], [1.4720798188693148e-41, 45], [2.2729097262716862e-52, 19], [4.784218272900771e-53, 6]], "generated": ["Bing is a search engine that can search web, images, videos, and other similar tasks.", "DuckDuckGo is a search engine that searches web and images.", "Google is a search engine that searches web, images, videos, and other similar tasks.", "Yahoo! Search is a search engine that searches web and images.", "According to statistics, Bing has only 2% of search market share while DuckDuckGo has 1.5%, Google has 80%, and Yahoo! Search has 1.3%.", "The data seems to indicate that Bing seems to earn less than its competitors do.", "So the answer is no."], "conf_comments": ["(A)\nGoogle is a search", "(A)\nGoogle is a search", "(A)\nThus, B", "(A)\nThus, B", "(A)\nThus, B", "(A)\nThus, the", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.4264951080274479, 0.25454926302042996, 0.15276184715018604, 0.08578959087239568, 0.06839796342820338], "conf": [0.9487403505134769, 0.8587723447739076, 0.6524544265926805, 0.3880997658062149, 0.25006421808185003], "prob": [[4.504169529227587e-15, 20], [3.787810530571823e-21, 15], [3.377704919285216e-31, 31], [2.9271025715160597e-37, 22], [2.6983097166014575e-37, 6]], "generated": ["Bing is a search engine that can search web, images, videos, and other similar tasks.", "DuckDuckGo is a search engine that searches web and images.", "Bing is owned by Microsoft, a multinational tech company, and DuckDuckGo is a privacy-oriented search engine.", "Thus, it can be inferred that Bing earns the searcher more than competitors do.", "So the answer is yes."], "conf_comments": ["(A)\nGoogle is a search", "(A)\nGoogle is a search", "(A)\nThus, B", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 8, "score": [0.4264951080274479, 0.25454926302042996, 0.2169070172571839, 0.16267242905422216, 0.09355005996185903, 0.03888477385041783, 0.017613900076300853, 0.012600268469002827], "conf": [0.9487403505134769, 0.8587723447739076, 0.7388164662062477, 0.6113160902986069, 0.47607178419132934, 0.30490309620322725, 0.18739427052038168, 0.11083485609399361], "prob": [[4.504169529227587e-15, 20], [3.787810530571823e-21, 15], [1.7889502439750469e-22, 18], [8.044858198907954e-25, 14], [1.4720798188693148e-41, 45], [2.2729097262716862e-52, 19], [9.312693411320064e-65, 26], [3.907025352058044e-65, 6]], "generated": ["Bing is a search engine that can search web, images, videos, and other similar tasks.", "DuckDuckGo is a search engine that searches web and images.", "Google is a search engine that searches web, images, videos, and other similar tasks.", "Yahoo! Search is a search engine that searches web and images.", "According to statistics, Bing has only 2% of search market share while DuckDuckGo has 1.5%, Google has 80%, and Yahoo! Search has 1.3%.", "The data seems to indicate that Bing seems to earn less than its competitors do.", "Thus, Bing earns less than DuckDuckGo, Yahoo! Search and its competitor Google do.", "So the answer is yes."], "conf_comments": ["(A)\nGoogle is a search", "(A)\nGoogle is a search", "(A)\nThus, B", "(A)\nThus, B", "(A)\nThus, B", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 8, "score": [0.4264951080274479, 0.25454926302042996, 0.2169070172571839, 0.16267242905422216, 0.09355005996185903, 0.03888477385041783, 0.017613900076300853, 0.012413604351390634], "conf": [0.9487403505134769, 0.8587723447739076, 0.7388164662062477, 0.6113160902986069, 0.47607178419132934, 0.30490309620322725, 0.18739427052038168, 0.10336203362285265], "prob": [[4.504169529227587e-15, 20], [3.787810530571823e-21, 15], [1.7889502439750469e-22, 18], [8.044858198907954e-25, 14], [1.4720798188693148e-41, 45], [2.2729097262716862e-52, 19], [9.312693411320064e-65, 26], [4.965412544182286e-65, 6]], "generated": ["Bing is a search engine that can search web, images, videos, and other similar tasks.", "DuckDuckGo is a search engine that searches web and images.", "Google is a search engine that searches web, images, videos, and other similar tasks.", "Yahoo! Search is a search engine that searches web and images.", "According to statistics, Bing has only 2% of search market share while DuckDuckGo has 1.5%, Google has 80%, and Yahoo! Search has 1.3%.", "The data seems to indicate that Bing seems to earn less than its competitors do.", "Thus, Bing earns less than DuckDuckGo, Yahoo! Search and its competitor Google do.", "So the answer is no."], "conf_comments": ["(A)\nGoogle is a search", "(A)\nGoogle is a search", "(A)\nThus, B", "(A)\nThus, B", "(A)\nThus, B", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.4264951080274479, 0.24722046039196852, 0.12390610637226238, 0.06420955007755512, 0.037035107542418476, 0.029862890656757346], "conf": [0.9487403505134769, 0.8560170879447727, 0.6703436099523861, 0.4742299810060063, 0.3397960592503292, 0.2351711703105518], "prob": [[4.504169529227587e-15, 20], [1.6659422291787944e-18, 8], [2.785582611130224e-36, 36], [1.2207259138024658e-47, 27], [2.643595279763539e-53, 17], [1.8172694285925557e-53, 6]], "generated": ["Bing is a search engine that can search web, images, videos, and other similar tasks.", "Its main competitor is Google.", "According to Statista, Bing’s monthly search volume is about 327 million, which is lower than Google’s 7.02 billion.", "Also, Bing’s market share is about 0.7%, while Google’s is around 92.1%.", "This can show that Bing searches do not earn more than competitors do.", "So the answer is no."], "conf_comments": ["(A)\nGoogle is a search", "(A)\nBing is owned", "(A)\nThus, B", "(A)\nThus, B", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 8, "score": [0.4264951080274479, 0.25454926302042996, 0.2169070172571839, 0.16267242905422216, 0.09355005996185903, 0.04481396925875149, 0.03493110429793619, 0.0036231750678180834], "conf": [0.9487403505134769, 0.8587723447739076, 0.7388164662062477, 0.6113160902986069, 0.47607178419132934, 0.33441191754291105, 0.2170704193723668, 0.14185398890005657], "prob": [[4.504169529227587e-15, 20], [3.787810530571823e-21, 15], [1.7889502439750469e-22, 18], [8.044858198907954e-25, 14], [1.4720798188693148e-41, 45], [1.885795906782086e-62, 43], [1.268142730455548e-62, 6], [1.5336760554591623e-71, 5]], "generated": ["Bing is a search engine that can search web, images, videos, and other similar tasks.", "DuckDuckGo is a search engine that searches web and images.", "Google is a search engine that searches web, images, videos, and other similar tasks.", "Yahoo! Search is a search engine that searches web and images.", "According to statistics, Bing has only 2% of search market share while DuckDuckGo has 1.5%, Google has 80%, and Yahoo! Search has 1.3%.", "Since Bing is less than the sum of DuckDuckGo and Google (~2% < 2% + 82%), it does not earn the searcher more than competitors.", "So the answer is no.", "# 4. Review", "# 5. Extensions/Modifications", "There are some possible extensions/modifications:\n- Add more questions", "- Make questions of varying difficulty, not all having simple yes or no answers"], "conf_comments": ["(A)\nGoogle is a search", "(A)\nGoogle is a search", "(A)\nThus, B", "(A)\nThus, B", "(A)\nThus, B", "(A)\nSo the answer is", "(A)\n\n\n\n\n", "(A)\n\n\n\n\n"], "info": [[[1.2093875702053098e-09, 5], [3.848392726883353e-09, 9], [2.2208398079750868e-10, 13], [3.820709276757534e-16, 15]], 0.6534929508599581, "(A)\n\n\n\n\n", 8]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": []}, "run_time": 339.843159198761}
{"index": 320, "question": "If you're reducing salt intake, are olives a healthy snack?", "answer": "no", "explanation": "No. The average amount of sodium a healthy person should have is 2,300mg daily. A half cup serving of olives has an average of 735mg sodium in it.", "generated": [{"finished": true, "length": 3, "score": [0.6700596166043469, 0.37647999691343287, 0.30621266882446635], "conf": [0.9342603923675182, 0.6785763397355347, 0.46917501622764674], "prob": [[0.0013671953222631848, 9], [3.439376155135009e-11, 21], [2.638989529726657e-11, 6]], "generated": ["Olives are a source of salt.", "Thus, they are not a healthy snack if you have a lot of salt intake.", "So the answer is no."], "conf_comments": ["(A)\nThus, ol", "(A)\nHowever, olives", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5512649985015892, 0.30244762115562945, 0.23034686769543647, 0.19353663081079736], "conf": [0.8731073848054804, 0.726651112155629, 0.5382443008215154, 0.3827325562076935], "prob": [[3.8266670858313735e-18, 38], [1.1865377227435244e-25, 17], [4.941077566627723e-27, 13], [4.730382506314172e-27, 6]], "generated": ["Olives contain sodium (about 100 mg per medium-sized olive), cholesterol, saturated fat, and unsaturated fat.", "Saturated fat and cholesterol increase one’s salt intake.", "Thus, olives are not a healthy snack.", "So the answer is no."], "conf_comments": ["(A)\nSalt contains sod", "(A)\nUnsaturated fat", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5512649985015892, 0.2950686334506844, 0.22551802419309597, 0.19197223711820843], "conf": [0.8731073848054804, 0.6811032805103943, 0.5327051133078357, 0.39445276098155296], "prob": [[3.8266670858313735e-18, 38], [1.5206119304980862e-35, 40], [4.580667565680768e-37, 12], [4.0231477433869395e-37, 6]], "generated": ["Olives contain sodium (about 100 mg per medium-sized olive), cholesterol, saturated fat, and unsaturated fat.", "Since eating too many olives would lead to increased sodium and fat intake, olives would not be a healthy snack if you are trying to cut down on salt intake.", "Thus, olives are not a healthy snack", "So the answer is no."], "conf_comments": ["(A)\nSalt contains sod", "(A)\n# Is the above", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5369714730084625, 0.3470043468145558, 0.28597048668609426], "conf": [0.9024068475645314, 0.6868905070289764, 0.47462209814799566], "prob": [[1.3365994883785156e-14, 28], [2.455903083919574e-20, 22], [2.2145432896597233e-20, 6]], "generated": ["Olives are not a nutrient-dense food, so you would only get a little bit of sodium from them.", "Therefore, olives would still be a healthy snack if you are reducing your salt intake.", "So the answer is yes."], "conf_comments": ["(A)\nThus, ol", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5512649985015892, 0.2950686334506844, 0.24797748175402273], "conf": [0.8731073848054804, 0.6811032805103943, 0.5068490810781268], "prob": [[3.8266670858313735e-18, 38], [1.5206119304980862e-35, 40], [1.1114620675071802e-35, 6]], "generated": ["Olives contain sodium (about 100 mg per medium-sized olive), cholesterol, saturated fat, and unsaturated fat.", "Since eating too many olives would lead to increased sodium and fat intake, olives would not be a healthy snack if you are trying to cut down on salt intake.", "So the answer is no."], "conf_comments": ["(A)\nSalt contains sod", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 45.899672985076904}
{"index": 321, "question": "Did the Democratic Party's nominee for President of the U.S. in 1908 watch TV?", "answer": "no", "explanation": "No. William Jennings Bryan was the Democratic Party's nominee for President of the U.S. in 1908. William Jennings Bryan died Jul 26, 1925. Television was invented in 1927.", "generated": [{"finished": true, "length": 3, "score": [0.7395956710514981, 0.5697235974658597, 0.4867575834596002], "conf": [0.9808031530506092, 0.8111767973358475, 0.595233469918083], "prob": [[1.4701375046207746e-06, 23], [3.651814158306044e-10, 25], [3.538832003161261e-10, 6]], "generated": ["TV was invented in 1927, well after the 1908 presidential election.", "Thus, the Democratic Party’s nominee for President of the U.S. could not have watched TV.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7594446670598305, 0.6059012220269053, 0.42137039739216997, 0.31269660631258833, 0.2805088889705612], "conf": [0.945045547784046, 0.8804530153942073, 0.7641393057000334, 0.6719578308599141, 0.5452381495392423], "prob": [[1.3717186997435508e-07, 32], [1.4189278790742081e-09, 12], [3.8107012819592985e-14, 18], [2.1329756657039076e-17, 16], [2.029561091744692e-17, 6]], "generated": ["According to Wikipedia, William Jennings Bryan was the nominee of the Democratic Party for U.S. President in 1908.", "Bryan was born in 1860.", "According to this timeline, TV was invented in 1927.", "Thus, Bryan did not watch TV in 1908.", "So the answer is no."], "conf_comments": ["(A)\nAccording to Wikipedia", "(A)\nIn 19", "(A)\nThus, Bry", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.833465045818988, 0.5341269367773352, 0.3696801341341687, 0.3149388995789236], "conf": [0.9599742960622628, 0.8586618623247553, 0.7054334498671833, 0.51478032566224], "prob": [[1.6730773683350032e-05, 34], [2.997045900787335e-11, 17], [5.350121377341839e-15, 16], [5.178101965956446e-15, 6]], "generated": ["The Democratic Party's nominee for President in 1908 was William Jennings Bryan, and he was born in 1860.", "Television sets were invented in the 1920's.", "Thus, William Jennings Bryan never had the opportunity to watch TV.", "So the answer is no."], "conf_comments": ["(A)\nIn 19", "(A)\nThus, William", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.7395956710514981, 0.4770814492866513, 0.40688621159393873], "conf": [0.9808031530506092, 0.8059526351452693, 0.5979811540262362], "prob": [[1.4701375046207746e-06, 23], [8.653016096869483e-18, 38], [7.681834801196013e-18, 6]], "generated": ["TV was invented in 1927, well after the 1908 presidential election.", "It is thus impossible for the Democratic Party’s 1908 nominee for President of the U.S. to watch TV because it wasn’t invented yet.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7594446670598305, 0.6059012220269053, 0.42137039739216997, 0.23594224899102056, 0.2059482448764496], "conf": [0.945045547784046, 0.8804530153942073, 0.7641393057000334, 0.654722283183399, 0.5045205423305943], "prob": [[1.3717186997435508e-07, 32], [1.4189278790742081e-09, 12], [3.8107012819592985e-14, 18], [6.003323431379328e-19, 11], [5.60905781371777e-19, 6]], "generated": ["According to Wikipedia, William Jennings Bryan was the nominee of the Democratic Party for U.S. President in 1908.", "Bryan was born in 1860.", "According to this timeline, TV was invented in 1927.", "Thus, Bryan likely did not see TV.", "So the answer is no."], "conf_comments": ["(A)\nAccording to Wikipedia", "(A)\nIn 19", "(A)\nThus, Bry", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 54.10712027549744}
{"index": 322, "question": "Was Great Recession the period of severest unemployment?", "answer": "no", "explanation": "No. The Great Recession had an unemployment peak of 10%. The Great Depression saw global GDP decline by almost 30% and unemployment approach 25%. US unemployment numbers approached 15% in May 2020 due to the Coronavirus.", "generated": [{"finished": true, "length": 4, "score": [0.791692952053867, 0.5128470494361987, 0.45045614037050324, 0.3984105177056167], "conf": [0.9717032964777661, 0.8490641919159332, 0.7116238531293975, 0.5580912802237146], "prob": [[0.00010027149919045285, 21], [3.084628901102366e-15, 33], [7.542781270623655e-16, 17], [7.429207284794507e-16, 6]], "generated": ["The Great Recession lasted from December 2007 - June 2009.", "During the Great Recession, the unemployment rate peaked at 10 percent, which is the highest recorded unemployment rate in recent history.", "Thus, the Great Recession was the period of severest unemployment.", "So the answer is yes."], "conf_comments": ["(A)\nThe period of sever", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8016959138807846, 0.5381424795276527, 0.42893198244108, 0.3692718594918551, 0.31467623267168343], "conf": [0.9763417757516355, 0.8742844649528018, 0.7777528298667027, 0.630845457203916, 0.4593638975161311], "prob": [[0.0008186475247255929, 17], [3.789842581879823e-15, 38], [4.276070578593037e-19, 27], [8.434517310103521e-20, 18], [8.296018263584046e-20, 6]], "generated": ["The Great Recession occurred from 2007-2009.", "During this period, the unemployment rate went from about 6% in 2007 to about 10% at its peak in 2009.", "In comparison, the peak unemployment rate during the Great Depression was around 25% in 1933.", "Thus, the Great Recession was not the period of severest unemployment.", "So the answer is no."], "conf_comments": ["(A)\nThe period of sever", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.8016959138807846, 0.5381424795276527, 0.42893198244108, 0.3051571060546063, 0.2527186710998848], "conf": [0.9763417757516355, 0.8742844649528018, 0.7777528298667027, 0.6428752928930238, 0.44463645388625456], "prob": [[0.0008186475247255929, 17], [3.789842581879823e-15, 38], [4.276070578593037e-19, 27], [2.3482507854670066e-23, 20], [2.232752536590263e-23, 6]], "generated": ["The Great Recession occurred from 2007-2009.", "During this period, the unemployment rate went from about 6% in 2007 to about 10% at its peak in 2009.", "In comparison, the peak unemployment rate during the Great Depression was around 25% in 1933.", "Thus, unemployment was higher during the Great Depression than during the Great Recession.", "So the answer is no."], "conf_comments": ["(A)\nThe period of sever", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6785845672484246, 0.45717758217707233, 0.37857174647739483, 0.31891603073066843], "conf": [0.973288482812769, 0.8574470891155044, 0.6578309504382572, 0.4678426506052468], "prob": [[2.981455520397533e-06, 17], [2.572293592663168e-14, 28], [3.044334705792826e-15, 19], [3.0055029978072506e-15, 6]], "generated": ["Unemployment peaked at 9.6% during the Great Recession.", "This is lower than the peak unemployment during the period of the Great Depression (25% in 1933).", "Thus, the Great Recession was not the period of the severest unemployment.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, un", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6785845672484246, 0.45717758217707233, 0.35901843465386285, 0.30105173926526474], "conf": [0.973288482812769, 0.8574470891155044, 0.6412290025497006, 0.45190095996428326], "prob": [[2.981455520397533e-06, 17], [2.572293592663168e-14, 28], [6.595065048191035e-16, 19], [6.506255911852015e-16, 6]], "generated": ["Unemployment peaked at 9.6% during the Great Recession.", "This is lower than the peak unemployment during the period of the Great Depression (25% in 1933).", "Thus, the Great Recession is not the period of the severest unemployment.", "So the answer is no."], "conf_comments": ["(A)\nIn comparison, un", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.8016959138807846, 0.5381424795276527, 0.42893198244108, 0.3051571060546063, 0.15802636761339298], "conf": [0.9763417757516355, 0.8742844649528018, 0.7777528298667027, 0.6428752928930238, 0.44894157500058807], "prob": [[0.0008186475247255929, 17], [3.789842581879823e-15, 38], [4.276070578593037e-19, 27], [2.3482507854670066e-23, 20], [2.8919302932656303e-26, 7]], "generated": ["The Great Recession occurred from 2007-2009.", "During this period, the unemployment rate went from about 6% in 2007 to about 10% at its peak in 2009.", "In comparison, the peak unemployment rate during the Great Depression was around 25% in 1933.", "Thus, unemployment was higher during the Great Depression than during the Great Recession.", "So, the answer is no."], "conf_comments": ["(A)\nThe period of sever", "(A)\nThus, the", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 53.90693998336792}
{"index": 323, "question": "Did occupants of Vellore Fort need to defend themselves from Grizzly Bears?", "answer": "no", "explanation": "No. The Vellore Fort was a 16th century stronghold in India. Grizzly Bears are native to the North American continent.", "generated": [{"finished": true, "length": 4, "score": [0.8354571953047005, 0.6360612253285757, 0.4856484960643801, 0.3942253755975135], "conf": [0.9691124244741268, 0.7945969038148967, 0.5798207927586922, 0.3841587823288879], "prob": [[0.014033474088963329, 13], [0.0003092496468690438, 11], [1.1292565580275816e-06, 25], [1.0928512825953427e-06, 6]], "generated": ["Vellore Fort is located in Vellore, India.", "Grizzly Bears live in North America.", "Thus, the occupants of Vellore Fort would not need to defend themselves from Grizzly Bears.", "So the answer is no."], "conf_comments": ["(A)\nGrizzly", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8021705558363471, 0.5742238743448287, 0.4698616680286072, 0.3867218176683893], "conf": [0.9672936956563579, 0.8346471622492462, 0.6366773378793298, 0.43313571494315983], "prob": [[0.01697269336594195, 10], [3.265612690137086e-05, 12], [1.2531165422681609e-06, 25], [1.2215401266821012e-06, 6]], "generated": ["The Vellore Fort is located in India.", "Grizzly Bears do not live in India.", "Thus, occupants of the Vellore Fort did not need to defend themselves from Grizzly Bears.", "So the answer is no."], "conf_comments": ["(A)\nGrizzly", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8354571953047005, 0.6360612253285757, 0.4626014502958108, 0.37348985921724664], "conf": [0.9691124244741268, 0.7945969038148967, 0.5780180994676597, 0.37901614993663957], "prob": [[0.014033474088963329, 13], [0.0003092496468690438, 11], [7.80674461721321e-08, 26], [7.534148909312153e-08, 6]], "generated": ["Vellore Fort is located in Vellore, India.", "Grizzly Bears live in North America.", "Thus, the occupants of the Vellore Fort would not need to defend themselves from Grizzly Bears.", "So the answer is no."], "conf_comments": ["(A)\nGrizzly", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8021705558363471, 0.5742238743448287, 0.43195528954043644, 0.3609405913712423], "conf": [0.9672936956563579, 0.8346471622492462, 0.6525429850388308, 0.4579974021256511], "prob": [[0.01697269336594195, 10], [3.265612690137086e-05, 12], [7.023179247940805e-08, 19], [6.807279221605262e-08, 6]], "generated": ["The Vellore Fort is located in India.", "Grizzly Bears do not live in India.", "Thus, occupants did not need to defend themselves from Grizzly Bears.", "So the answer is no."], "conf_comments": ["(A)\nGrizzly", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8024003760888352, 0.4983932970957876, 0.3386313357952817, 0.27920400816118524], "conf": [0.9824388790047736, 0.8726219304771754, 0.667156582019524, 0.45697449398378526], "prob": [[0.004113364363202581, 13], [5.409868981628975e-10, 19], [4.943977672369905e-15, 23], [4.725257118680984e-15, 6]], "generated": ["Vellore Fort was built in 1566.", "Grizzly Bears were hunted into extinction in 1920.", "Thus, the occupants of the fort would not have to defend themselves from Grizzly Bears.", "So the answer is no."], "conf_comments": ["(A)\nGrizzly", "(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 54.45490741729736}
{"index": 324, "question": "Would a week be enough time to watch every episode of Ugly Betty?", "answer": "yes", "explanation": "Yes. There are 168 hours in a week. The entire Ugly Betty series is 85 hours long.", "generated": [{"finished": true, "length": 5, "score": [0.7134718423091281, 0.44132716872070543, 0.3554271369513572, 0.25836309433868004, 0.2198168759485036], "conf": [0.9743359755570509, 0.8678614300623008, 0.7560247611273234, 0.6122342812592109, 0.44479514612840193], "prob": [[0.00021606488469841066, 13], [1.146336884841918e-14, 28], [4.46836392431687e-16, 11], [1.3400890395885327e-19, 19], [1.3111305290414409e-19, 6]], "generated": ["The first season of Ugly Betty has 21 episodes.", "It would take 21 times 60, which is 1260 minutes, to watch all 21 episodes.", "There are 168 hours in a week.", "Thus, there is enough time to watch every episode of Ugly Betty in a week.", "So the answer is yes."], "conf_comments": ["(A)\nThe second season of", "(A)\nThe average length of", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7639757436769936, 0.42820258375325754, 0.27839370762087556, 0.23747940104233758], "conf": [0.9748821392331556, 0.8584435120459541, 0.6418502736052024, 0.4687829390299622], "prob": [[0.0035422343163744032, 11], [9.296422055830415e-07, 8], [3.1658577788087014e-10, 14], [3.0964306636471776e-10, 6]], "generated": ["There were 85 episodes of Ugly Betty.", "A week would have 7 days.", "Thus, a week would be enough time to watch every episode.", "So the answer is yes."], "conf_comments": ["(A)\nThe average length of", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7134718423091281, 0.4489029462437486, 0.24781416126592679, 0.21607722095040272], "conf": [0.9743359755570509, 0.8447073488216678, 0.6638245479907822, 0.5097861971646619], "prob": [[0.00021606488469841066, 13], [8.942523696893991e-07, 7], [6.032043971870461e-13, 15], [5.678717929607944e-13, 6]], "generated": ["The first season of Ugly Betty has 21 episodes.", "A week is 7 days.", "Thus, it is not possible to watch every episode in seven days.", "So the answer is no."], "conf_comments": ["(A)\nThe second season of", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7639757436769936, 0.4760228680352553, 0.2502396293212194, 0.21343981025962014], "conf": [0.9748821392331556, 0.8739680432532255, 0.6644241189786898, 0.48637657425384556], "prob": [[0.0035422343163744032, 11], [2.3366604740576063e-05, 6], [3.178898701714818e-17, 27], [3.062987217984858e-17, 6]], "generated": ["There were 85 episodes of Ugly Betty.", "A week has seven days.", "Thus, it would not have been enough time to watch every episode of Ugly Betty, since it would take 7 weeks.", "So the answer is no."], "conf_comments": ["(A)\nThe average length of", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 9, "score": [0.74074095850306, 0.532617820124901, 0.260237506214422, 0.10829137819959721, 0.059491460133175376, 0.02912028949505031, 0.014340501856619493, 0.005696239355333394, 0.00435915218742949], "conf": [0.9649086271045453, 0.8679556135527351, 0.7262936083994354, 0.5294632558333048, 0.4248591504448608, 0.31425524152375955, 0.21206931087815598, 0.13494124561923548, 0.08131497725181128], "prob": [[7.108485935373414e-06, 21], [1.4729516479515505e-07, 7], [1.501132515506523e-13, 11], [1.165818744049607e-26, 21], [1.321307352383081e-32, 14], [2.353279105566069e-44, 24], [1.4101387340647362e-50, 14], [4.683753455923023e-59, 14], [3.9464323546105205e-59, 6]], "generated": ["Ugly Betty is a TV show that has 4 seasons and a total of 85 episodes.", "A week has 7 days.", "In a day there are about 24 hours.", "To estimate how long each episode is, I assume each episode would be about 40 minutes long.", "This means there are 3520 minutes in a week.", "This means each episode would take about 9 minutes and 20 seconds if it was 40 minutes long.", "This means about 352 episodes would fit in a week.", "But there are 85 episodes, so that is enough time.", "So the answer is yes."], "conf_comments": ["(A)\nThe average length of", "(A)\nThus, it", "(A)\nThus, a", "(A)\nThus, each", "(A)\nThus, it", "(A)\nThus, it", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": [], "9": []}, "run_time": 70.51611876487732}
{"index": 325, "question": "Can you find a snow leopard in the Yucatan?", "answer": "no", "explanation": "No. Snow leopards are native to mountain ranges in Central and South Asia. The Yucatan is a peninsula in Mexico. Mexico is located in North America.", "generated": [{"finished": true, "length": 4, "score": [0.7924601741518187, 0.5197921693863427, 0.4144910093015606, 0.35336603673281913], "conf": [0.9642515932693145, 0.8196622903223232, 0.6537792958867082, 0.4766530299962075], "prob": [[0.032368025509876795, 8], [3.037637434127925e-07, 17], [4.0968730616842375e-09, 19], [4.02106994901859e-09, 6]], "generated": ["The Yucatan is in Mexico.", "Snow leopards live in the Himalayas and parts of Asia.", "Thus, you can’t find a snow leopard in the Yucatan.", "So the answer is no."], "conf_comments": ["(A)\nThe Yucatan", "(A)\nThus, you", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.79806978030636, 0.46762365966458874, 0.4039442882661903, 0.3500730924641332], "conf": [0.9668537212759656, 0.7947929515831079, 0.6416925229677763, 0.48303618628967887], "prob": [[0.023360849551898776, 9], [2.0025179530340963e-08, 16], [5.245486022379678e-09, 17], [5.175075849783328e-09, 6]], "generated": ["The Yucatan is located in Mexico.", "Snow leopards live in Central and South Asia and not in Mexico.", "Thus, you cannot find a snow leopard in the Yucatan.", "So the answer is no."], "conf_comments": ["(A)\nThe Yucatan", "(A)\nThus, you", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.79806978030636, 0.46762365966458874, 0.38152217351848167, 0.3261967789896004], "conf": [0.9668537212759656, 0.7947929515831079, 0.636886409688288, 0.4666072027571585], "prob": [[0.023360849551898776, 9], [2.0025179530340963e-08, 16], [5.900795616887833e-10, 19], [5.822271436000633e-10, 6]], "generated": ["The Yucatan is located in Mexico.", "Snow leopards live in Central and South Asia and not in Mexico.", "Thus, you can’t find a snow leopard in the Yucatan.", "So the answer is no."], "conf_comments": ["(A)\nThe Yucatan", "(A)\nThus, you", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7924601741518187, 0.5197921693863427, 0.37266446203886544, 0.3228707092388019], "conf": [0.9642515932693145, 0.8196622903223232, 0.6459046576626747, 0.4867644642439811], "prob": [[0.032368025509876795, 8], [3.037637434127925e-07, 17], [2.1284698194974446e-10, 17], [2.0782262130508026e-10, 6]], "generated": ["The Yucatan is in Mexico.", "Snow leopards live in the Himalayas and parts of Asia.", "Thus, there would be no snow leopards in the Yucatan.", "So the answer is no."], "conf_comments": ["(A)\nThe Yucatan", "(A)\nThus, you", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7061726273755754, 0.4240917833130859, 0.30746592256850513, 0.26394662060779717], "conf": [0.9806895044980528, 0.7737397549433086, 0.5867508212153327, 0.43429601278446467], "prob": [[2.6273286281116964e-06, 19], [9.55141182314007e-12, 16], [6.257154728799551e-15, 20], [6.095558269867194e-15, 6]], "generated": ["Snow leopards are native to the Himalayas and other parts of Asia.", "The Yucatan is in Mexico and is known for its jungles.", "Thus, it is impossible for a snow leopard to be in the Yucatan.", "So the answer is no."], "conf_comments": ["(A)\nThe Yucatan", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 54.65208029747009}
{"index": 326, "question": "Was Pi an acceptable number of children in 1980s China?", "answer": "no", "explanation": "No. Pi, the ratio of a circle's circumference to diameter, is equal to 3.14. In the 1980's China instituted a one-child policy. People that violated China's one child policy were fined heavily and some were sterilized.", "generated": [{"finished": true, "length": 4, "score": [0.7303145084212385, 0.3582187713556699, 0.3024132391868188, 0.2567983213488114], "conf": [0.9781585509565561, 0.837356924854537, 0.6492590577766564, 0.46920035798106996], "prob": [[2.943219288418751e-06, 21], [2.781838256483203e-17, 20], [5.1556361166464114e-18, 20], [5.087880677850915e-18, 6]], "generated": ["China enforced the one-child policy from 1979-2016.", "One child per family was the rule, so Pi (3.14) was not accepted.", "Thus, Pi was not an acceptable number of children in 1980s China.", "So the answer is no."], "conf_comments": ["(A)\nThus, Pi", "(A)\nThus, Pi", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6884205121308056, 0.4265959555165891, 0.30280734213061083, 0.25631633484565974], "conf": [0.9712291709312958, 0.864048381639256, 0.6982825586700747, 0.5034315269849109], "prob": [[2.85827235333608e-07, 21], [7.728139810314619e-14, 18], [3.793195311681943e-18, 21], [3.654841211827197e-18, 6]], "generated": ["In China, a family was allowed to have 1 child in the 1980s.", "This means that the number of children in a family was either 1 or 2.", "Thus, Pi (3.14159) was not an acceptable number of children.", "So the answer is no."], "conf_comments": ["(A)\nThus, Pi", "(A)\nThus, Pi", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6884205121308056, 0.4265959555165891, 0.2897704885223616, 0.24231860045585782], "conf": [0.9712291709312958, 0.864048381639256, 0.654057379580076, 0.46122013585639254], "prob": [[2.85827235333608e-07, 21], [7.728139810314619e-14, 18], [2.0325421751168173e-16, 12], [1.9331978283670572e-16, 6]], "generated": ["In China, a family was allowed to have 1 child in the 1980s.", "This means that the number of children in a family was either 1 or 2.", "Thus, Pi is not an acceptable number of children.", "So the answer is no."], "conf_comments": ["(A)\nThus, Pi", "(A)\nThus, Pi", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7263761421278666, 0.38596026959269836, 0.22955672520114792, 0.15282769100123528, 0.12121229263079128], "conf": [0.9740317181551286, 0.825112471621366, 0.573234672208924, 0.3435193021345733, 0.21729726604728264], "prob": [[5.495417495688623e-05, 16], [9.294387300066855e-10, 10], [4.9201741990947565e-15, 18], [1.3183930264349453e-16, 12], [1.2751508387021885e-16, 6]], "generated": ["China had a One Child Policy in the 1980s.", "Thus, only a single child was allowed.", "In comparison, Pi is an irrational number with infinite digits after the decimal point.", "Thus, Pi is not an acceptable number of children.", "So the answer is no."], "conf_comments": ["(A)\nThe number Pi is", "(A)\nThus, Pi", "(A)\nThus, Pi", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6453709537777261, 0.41078134113710474, 0.21613362294307167, 0.11817112865772833, 0.09565315110641957], "conf": [0.9655470395607824, 0.8751612343279933, 0.7503217327382512, 0.5253511112625935, 0.3467797531153573], "prob": [[1.4372300895925836e-06, 16], [3.2583472781186255e-13, 19], [4.550070325214717e-21, 16], [1.4343840157340516e-29, 23], [1.3718156301966916e-29, 6]], "generated": ["The 1980s were a time of economic crisis in China.", "To control population growth, China implemented a one-child policy in 1979.", "This one-child policy is often referred to as the one-child policy.", "Thus, Pi would have been accepted as an ideal number of children in 1980s China.", "So the answer is yes."], "conf_comments": ["(A)\nThe 19", "(A)\nThus, Pi", "(A)\nThus, Pi", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 59.68070363998413}
{"index": 327, "question": "Can surgery prevent an existential crisis?", "answer": "no", "explanation": "No. Surgery is used to correct medical problems or make physical alterations to the body. An existential crisis is a metaphysical affliction.", "generated": [{"finished": true, "length": 4, "score": [0.6047589707839243, 0.2702496045895167, 0.21722330777313473, 0.18234375410486386], "conf": [0.9493866901480695, 0.7566348483876734, 0.5604081943246374, 0.39625264478151473], "prob": [[1.3449572898226749e-08, 19], [1.2958147472218768e-17, 15], [2.1937259682011145e-18, 13], [2.148769469276266e-18, 6]], "generated": ["A surgery is a medical procedure in which incisions are made into a living body.", "Unlike an existential crisis, which occurs in one’s mind.", "Thus, a surgery cannot prevent an existential crisis.", "So the answer is no."], "conf_comments": ["(A)\nAn existential crisis", "(A)\nThus, surg", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6047589707839243, 0.2702496045895167, 0.2133557551516046, 0.17865126778412618], "conf": [0.9493866901480695, 0.7566348483876734, 0.5493625236833615, 0.3865405344983699], "prob": [[1.3449572898226749e-08, 19], [1.2958147472218768e-17, 15], [2.074979076288285e-18, 12], [2.0315079435327023e-18, 6]], "generated": ["A surgery is a medical procedure in which incisions are made into a living body.", "Unlike an existential crisis, which occurs in one’s mind.", "Thus, surgery cannot prevent an existential crisis.", "So the answer is no."], "conf_comments": ["(A)\nAn existential crisis", "(A)\nThus, surg", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6095080985847523, 0.26844440567764605, 0.14229788290299272, 0.12232907495859745, 0.10563472068595503], "conf": [0.9579332423992308, 0.8781790213412771, 0.6915912127541417, 0.5519739603594646, 0.41271273699159533], "prob": [[3.937557392891177e-08, 18], [5.404747649637791e-25, 25], [3.7284083866721277e-32, 16], [1.4813035481126922e-32, 12], [1.457445525404353e-32, 6]], "generated": ["An existential crisis is usually defined as a period of questioning one's existence.", "These questions involve questions like “what is the right way to live?” and “what is my purpose in life?”.", "These are abstract questions and not questions that can be answered by surgery.", "Thus, surgery cannot prevent an existential crisis.", "So the answer is no."], "conf_comments": ["(A)\nSurgery is", "(A)\nSurgery is", "(A)\nThus, surg", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6095080985847523, 0.23938363280927644, 0.13639387176764395, 0.10162366270463746, 0.08745923647741125], "conf": [0.9579332423992308, 0.8522926787482715, 0.6556644219115648, 0.5087823164970092, 0.37787012456221203], "prob": [[3.937557392891177e-08, 18], [4.548060666129977e-21, 17], [2.583078015468816e-26, 14], [3.3211010245191743e-28, 13], [3.266997873579995e-28, 6]], "generated": ["An existential crisis is usually defined as a period of questioning one's existence.", "This is often related to a person's self-consciousness/identity.", "However, surgery doesn't change a person's identity.", "Thus, surgery can not prevent an existential crisis.", "So the answer is no."], "conf_comments": ["(A)\nSurgery is", "(A)\nSurgery is", "(A)\nThus, surg", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5433147670797902, 0.25691647915158927, 0.16223174910637905, 0.1342664038033766], "conf": [0.9627509620875965, 0.8554647176629216, 0.594352407381195, 0.4105248796011112], "prob": [[1.7587012935927274e-10, 19], [1.7910995414867802e-16, 10], [3.9849613833382025e-19, 11], [3.789914877229827e-19, 6]], "generated": ["An existential crisis is a period of doubt where someone questions whether their life is meaningful.", "Surgery has been used to save lives.", "Thus, it could prevent an existential crisis.", "So the answer is yes."], "conf_comments": ["(A)\nSurgery is", "(A)\nThus, surg", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6095080985847523, 0.23938363280927644, 0.13639387176764395, 0.10162366270463746, 0.025437675191223856], "conf": [0.9579332423992308, 0.8522926787482715, 0.6556644219115648, 0.5087823164970092, 0.3800998480532187], "prob": [[3.937557392891177e-08, 18], [4.548060666129977e-21, 17], [2.583078015468816e-26, 14], [3.3211010245191743e-28, 13], [9.693448528033068e-36, 7]], "generated": ["An existential crisis is usually defined as a period of questioning one's existence.", "This is often related to a person's self-consciousness/identity.", "However, surgery doesn't change a person's identity.", "Thus, surgery can not prevent an existential crisis.", "The answer is a strong no."], "conf_comments": ["(A)\nSurgery is", "(A)\nSurgery is", "(A)\nThus, surg", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 59.55931115150452}
{"index": 328, "question": "Would a greyhound be able to outrun a greyhound bus?", "answer": "no", "explanation": "No. A greyhound bus can travel speeds upward of 60 mph. A greyhound dog can run at speeds up to 45 mph.", "generated": [{"finished": true, "length": 4, "score": [0.7073886259791516, 0.5672853295220318, 0.46795355102858427, 0.39660166650925177], "conf": [0.9775354737293387, 0.7812853950885164, 0.5907931459529258, 0.4256282634292081], "prob": [[2.2229919958206324e-05, 16], [5.52464231024528e-07, 17], [6.696334025282953e-08, 20], [6.577918583662482e-08, 6]], "generated": ["The top speed of a greyhound is about 44 mph.", "The top speed of a greyhound bus is about 70 mph.", "Thus, a greyhound would not be able to outrun a greyhound bus.", "So the answer is no."], "conf_comments": ["(A)\nThe top speed of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5938240179410801, 0.40010240561907134, 0.31907251509807055, 0.26630197793971755], "conf": [0.9749000785049757, 0.6894030388032519, 0.49504282182465564, 0.34587217870871273], "prob": [[5.314515442220169e-10, 21], [1.8226807677080064e-13, 18], [1.6071205387338554e-14, 20], [1.5784514799756474e-14, 6]], "generated": ["The fastest greyhound can run at a maximum speed of nearly 70 km/h.", "The maximum speed of a greyhound bus is 110 km/h.", "Thus, a greyhound would not be able to outrun a greyhound bus.", "So the answer is no."], "conf_comments": ["(A)\nThe average speed of", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5971951723728331, 0.4523875130458816, 0.370075394283082], "conf": [0.8566878710377339, 0.5857452844860667, 0.3936825048026987], "prob": [[4.696122503625097e-06, 14], [1.6822329140031432e-07, 19], [1.6391453286805635e-07, 6]], "generated": ["Greyhound buses are not faster than greyhounds.", "Thus, a greyhound would be able to outrun a greyhound bus.", "So the answer is yes."], "conf_comments": ["(A)\nGreyhounds", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.7381181925612964, 0.5343262464199953, 0.313293363169454, 0.19119263673196318, 0.14419834617462188, 0.12077036996263585], "conf": [0.9644836634311613, 0.7610012508583572, 0.6369474290788396, 0.5324336140625807, 0.3955848869689988, 0.2783887162154617], "prob": [[0.0033080299527387252, 10], [4.741335095958866e-06, 16], [2.5212064048183672e-15, 24], [1.2061837005258823e-21, 18], [7.542053578979483e-24, 19], [7.396383121203859e-24, 6]], "generated": ["A greyhound is a dog breed.", "A greyhound bus is a bus operated by Greyhound Lines.", "Greyhound buses have a speed of 55 mph and can reach 75 mph.", "In comparison, the racing speed of greyhounds is about 45 mph.", "Thus, a greyhound is not able to outrun a greyhound bus.", "So the answer is no."], "conf_comments": ["(A)\nA greyhound", "(A)\nThus, a", "(A)\nGreyhounds", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.7381181925612964, 0.5343262464199953, 0.313293363169454, 0.19119263673196318, 0.12921511205775266, 0.10540673432026058], "conf": [0.9644836634311613, 0.7610012508583572, 0.6369474290788396, 0.5324336140625807, 0.3899814847130635, 0.2617903477816169], "prob": [[0.0033080299527387252, 10], [4.741335095958866e-06, 16], [2.5212064048183672e-15, 24], [1.2061837005258823e-21, 18], [5.948871866177463e-26, 21], [5.644654469405954e-26, 6]], "generated": ["A greyhound is a dog breed.", "A greyhound bus is a bus operated by Greyhound Lines.", "Greyhound buses have a speed of 55 mph and can reach 75 mph.", "In comparison, the racing speed of greyhounds is about 45 mph.", "Thus, it is very likely that a greyhound can outrun a greyhound bus.", "So the answer is yes."], "conf_comments": ["(A)\nA greyhound", "(A)\nThus, a", "(A)\nGreyhounds", "(A)\nThus, a", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": []}, "run_time": 62.15406346321106}
{"index": 329, "question": "Is Sirius part of a constellation of an animal?", "answer": "yes", "explanation": "Yes. Sirius is the brightest star in the constellation Canis Major. Canis Major represents a large dog.", "generated": [{"finished": true, "length": 4, "score": [0.8330376449718153, 0.5527111463594929, 0.4966074776503263, 0.4370545450082402], "conf": [0.9571638589181379, 0.8415504548625706, 0.7098046684524809, 0.550686150404883], "prob": [[0.02109189300523176, 12], [3.258531861591073e-10, 26], [1.5471601798450238e-10, 17], [1.5318353849060022e-10, 6]], "generated": ["Sirius is the brightest star in the sky.", "According to NASA, it is part of the Canis Major constellation, which is a large dog constellation.", "Thus, Sirius is part of a constellation of an animal.", "So the answer is yes."], "conf_comments": ["(A)\nIt is part of", "(A)\nThus, Si", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8330376449718153, 0.5527111463594929, 0.4535799185207851, 0.39846906724973385], "conf": [0.9571638589181379, 0.8415504548625706, 0.7033830230659203, 0.5437973289745852], "prob": [[0.02109189300523176, 12], [3.258531861591073e-10, 26], [8.28729764553861e-12, 17], [8.200330244378285e-12, 6]], "generated": ["Sirius is the brightest star in the sky.", "According to NASA, it is part of the Canis Major constellation, which is a large dog constellation.", "Thus, Sirius is part of the constellation of an animal.", "So the answer is yes."], "conf_comments": ["(A)\nIt is part of", "(A)\nThus, Si", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8796561304251324, 0.5043658508398663, 0.4472066153102927, 0.39650502437079455], "conf": [0.9584542253218412, 0.8511831327826327, 0.7149012068815145, 0.563114886040578], "prob": [[0.06190252782280148, 13], [1.886730971014014e-14, 29], [6.13543207853881e-15, 17], [6.062126482108322e-15, 6]], "generated": ["Sirius is the brightest star in the night sky.", "It is often called the \"dog star\", because it is the closest star to the constellation of Canis Major (\"Greater Dog\").", "Thus, Sirius is part of a constellation of an animal.", "So the answer is yes."], "conf_comments": ["(A)\nIt is part of", "(A)\nThus, Si", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8796561304251324, 0.44279612208321395, 0.373884080636675, 0.3283132022481468], "conf": [0.9584542253218412, 0.8624825069923927, 0.7302949426114903, 0.564211687220026], "prob": [[0.06190252782280148, 13], [2.1585503022969688e-12, 19], [9.769946017826498e-14, 18], [9.657082279787713e-14, 6]], "generated": ["Sirius is the brightest star in the night sky.", "It is the central star in a constellation of Canis Major, the greatest dog.", "Thus, Sirius is a part of a constellation of an animal.", "So the answer is yes."], "conf_comments": ["(A)\nIt is part of", "(A)\nThus, Si", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.7294561150031497, 0.434956764466848, 0.3657972079217514], "conf": [0.9452399757601572, 0.7927895205021459, 0.621053564412402], "prob": [[5.821789817237154e-09, 33], [1.4935742536151725e-14, 15], [6.594263620584386e-15, 8]], "generated": ["[Sirius](https://en.wikipedia.org/wiki/Sirius_(astronomy)) is the brightest star in the Milky Way.", "It doesn’t form part of a constellation of an animal.", "Thus, the answer is no."], "conf_comments": ["(A)\nIt is also known", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 50.14069318771362}
{"index": 330, "question": "Is Snow White an example of good consent?", "answer": "no", "explanation": "No. Snow White is unknowingly poisoned by a witch. Snow White is unconscious when a man kisses her without her knowledge. Consent involves knowingly allowing something to happen to oneself. ", "generated": [{"finished": true, "length": 3, "score": [0.5232097134218082, 0.3855265210035196, 0.31359224669380026], "conf": [0.9317578774920479, 0.7727393251206662, 0.5601703453424847], "prob": [[3.08160073147924e-09, 16], [8.187552634959585e-12, 14], [4.73330177189252e-12, 6]], "generated": ["Within Snow White, Prince Charming kissed Snow White without consent.", "This can indicate that Snow White is not an example of good consent.", "So the answer is no."], "conf_comments": ["(A)\nThus, Snow", "(A)\nHowever, Snow White", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5232097134218082, 0.33632387702388905, 0.16957738693676777, 0.14082226869024994, 0.12084318139268423], "conf": [0.9317578774920479, 0.7777089321777568, 0.6060848645373427, 0.4710586370119879, 0.3478717454536084], "prob": [[3.08160073147924e-09, 16], [1.636339636196631e-13, 14], [3.047585769537447e-23, 20], [6.438791460321719e-24, 13], [6.329202802647019e-24, 6]], "generated": ["Within Snow White, Prince Charming kissed Snow White without consent.", "Snow White was unconscious at the time of the kiss.", "This shows that Prince Charming did not ask Snow White’s consent when kissing her.", "Thus, Snow White is not an example of good consent.", "So the answer is no."], "conf_comments": ["(A)\nThus, Snow", "(A)\nThus, the", "(A)\nThus, Prince", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 6, "score": [0.5232097134218082, 0.33632387702388905, 0.16957738693676777, 0.08082029086840925, 0.05027294252683863, 0.04124781833828094], "conf": [0.9317578774920479, 0.7777089321777568, 0.6060848645373427, 0.44165586862526957, 0.30697060464757536, 0.21029758446249328], "prob": [[3.08160073147924e-09, 16], [1.636339636196631e-13, 14], [3.047585769537447e-23, 20], [2.5645337640724335e-29, 12], [3.919526071984461e-33, 15], [3.528651087061458e-33, 6]], "generated": ["Within Snow White, Prince Charming kissed Snow White without consent.", "Snow White was unconscious at the time of the kiss.", "This shows that Prince Charming did not ask Snow White’s consent when kissing her.", "Also, Snow White did not know Prince Charming.", "Thus, Prince Charming kissed Snow White without her consent.", "So the answer is no."], "conf_comments": ["(A)\nThus, Snow", "(A)\nThus, the", "(A)\nThus, Prince", "(A)\nThus, Snow", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 7, "score": [0.5232097134218082, 0.33632387702388905, 0.16957738693676777, 0.08082029086840925, 0.03884607269787858, 0.028118822315653894, 0.0238262449988804], "conf": [0.9317578774920479, 0.7777089321777568, 0.6060848645373427, 0.44165586862526957, 0.3084563359568552, 0.22131026186487837, 0.16034695828363074], "prob": [[3.08160073147924e-09, 16], [1.636339636196631e-13, 14], [3.047585769537447e-23, 20], [2.5645337640724335e-29, 12], [2.2830435228624405e-40, 23], [7.968436022560584e-43, 18], [7.54605134781498e-43, 6]], "generated": ["Within Snow White, Prince Charming kissed Snow White without consent.", "Snow White was unconscious at the time of the kiss.", "This shows that Prince Charming did not ask Snow White’s consent when kissing her.", "Also, Snow White did not know Prince Charming.", "As a stranger, Prince Charming could not reasonably expect to obtain consent from Snow White to kiss her.", "Thus, Prince Charming did not obtain good consent when kissing Snow White.", "So the answer is no."], "conf_comments": ["(A)\nThus, Snow", "(A)\nThus, the", "(A)\nThus, Prince", "(A)\nThus, Snow", "(A)\nThus, Prince", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.47650807178123217, 0.23509032774667593, 0.17708113740280138], "conf": [0.9306582034630703, 0.6321498574274121, 0.4251675220730095], "prob": [[8.110678994604731e-15, 23], [2.1485378908178966e-22, 17], [7.743785017645641e-23, 6]], "generated": ["When Snow White was offered the poisonous apple by the queen, she answered “No” and declined it.", "Thus, we can say Yes, Snow White is an example of good consent.", "So the answer is yes."], "conf_comments": ["(A)\nThus, Snow", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": []}, "run_time": 67.59750413894653}
{"index": 331, "question": " Is The Invisible Man more prevalent in films than Picnic at Hanging Rock?", "answer": "yes", "explanation": "Yes. H.G. Wells's book The Invisible Man has been adapted into more than 7 films. Joan Lindsay's book Picnic at Hanging Rock was adapted into one film and one TV series.", "generated": [{"finished": true, "length": 3, "score": [0.6936888032463576, 0.5499352494796319, 0.45960336606512514], "conf": [0.9035450480653981, 0.6881357623305969, 0.5190260810956175], "prob": [[1.754469795624194e-09, 32], [2.114670619182134e-11, 23], [1.1435853793531623e-11, 8]], "generated": ["According to IMDB, The Invisible Man has had 14 film adaptations and Picnic at Hanging Rock has had 1 film adaptation.", "This can indicate that The Invisible Man is more prevalent in films than Picnic at Hanging Rock.", "Thus, the answer is yes."], "conf_comments": ["(A)\nThus, The", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.6936888032463576, 0.534877585390168, 0.4512455810172924], "conf": [0.9035450480653981, 0.7032596063074877, 0.5040729685524818], "prob": [[1.754469795624194e-09, 32], [5.288374962817044e-11, 13], [5.0694236577162246e-11, 6]], "generated": ["According to IMDB, The Invisible Man has had 14 film adaptations and Picnic at Hanging Rock has had 1 film adaptation.", "Thus, The Invisible Man is more prevalent.", "So the answer is yes."], "conf_comments": ["(A)\nThus, The", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6346723874170158, 0.511519863144423, 0.41582456561521475], "conf": [0.9406720862289037, 0.6907831646032603, 0.46068799348174416], "prob": [[2.352096790099883e-14, 37], [1.582508328307843e-15, 22], [1.4980516487003535e-15, 6]], "generated": ["The movie, The Invisible Man, debuted in 1933. In comparison, Picnic at Hanging Rock did not premiere until 1975.", "Thus, The Invisible Man is more prevalent in films than Picnic at Hanging Rock.", "So the answer is yes."], "conf_comments": ["(A)\nThus, The", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6936888032463576, 0.5499352494796319, 0.40464272716368926], "conf": [0.9035450480653981, 0.6881357623305969, 0.4775632767720622], "prob": [[1.754469795624194e-09, 32], [2.114670619182134e-11, 23], [4.7667693452127386e-12, 6]], "generated": ["According to IMDB, The Invisible Man has had 14 film adaptations and Picnic at Hanging Rock has had 1 film adaptation.", "This can indicate that The Invisible Man is more prevalent in films than Picnic at Hanging Rock.", "So the answer is yes."], "conf_comments": ["(A)\nThus, The", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6346723874170158, 0.42971855720201396, 0.3417982555125976], "conf": [0.9406720862289037, 0.6715976610805472, 0.43972325067115164], "prob": [[2.352096790099883e-14, 37], [1.3759834420110814e-18, 22], [1.1200063208050152e-18, 6]], "generated": ["The movie, The Invisible Man, debuted in 1933. In comparison, Picnic at Hanging Rock did not premiere until 1975.", "Consequently, The Invisible Man is more prevalent in films than Picnic at Hanging Rock.", "So the answer is yes."], "conf_comments": ["(A)\nThus, The", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 3, "score": [0.5924268593712139, 0.39744504663387886, 0.31360993449071084], "conf": [0.9525363263136953, 0.6661183480378735, 0.4493798136806719], "prob": [[2.4545145160535153e-16, 36], [1.446785188945778e-24, 43], [2.4774007789914775e-25, 22]], "generated": ["According to a website on the top 100 sci-fi movies, The Invisible Man is ranked at number 44 out of 100.", "However, according to a website on the top 100 sci-fi movies, Picnic at Hanging Rock is not listed as one of the top 100 sci-fi movies.", "Thus, The Invisible Man is more prevalent in films than Picnic at Hanging Rock.", "So the answer is yes."], "conf_comments": ["(A)\nAccording to a", "(A)\nThus, The", "(A)\nSo the answer is"], "info": [[[0.17123487286987457, 22], [0.9681639145940026, 6]], 0.6746245843615788, "(A)\nSo the answer is", 3]}, {"finished": false, "length": 3, "score": [0.5924268593712139, 0.39744504663387886, 0.2944506476039199], "conf": [0.9525363263136953, 0.6661183480378735, 0.4411187223469721], "prob": [[2.4545145160535153e-16, 36], [1.446785188945778e-24, 43], [8.657756935878628e-26, 15]], "generated": ["According to a website on the top 100 sci-fi movies, The Invisible Man is ranked at number 44 out of 100.", "However, according to a website on the top 100 sci-fi movies, Picnic at Hanging Rock is not listed as one of the top 100 sci-fi movies.", "Thus, The Invisible Man is more prevalent in films.", "So the answer is yes."], "conf_comments": ["(A)\nAccording to a", "(A)\nThus, The", "(A)\nSo the answer is"], "info": [[[0.05984134342837193, 15], [0.9641921986930414, 6]], 0.6622227471234456, "(A)\nSo the answer is", 3]}], "all_generated": {"1": [], "2": [], "3": []}, "run_time": 40.38094663619995}
{"index": 332, "question": "Could Ryan Crouser throw a bengal fox with ease?", "answer": "yes", "explanation": "Yes. Ryan Crouser is a professional shot putter who won the gold medal at the 2016 Olympics. The men's shot weighs 16.01 pounds. The typical weight of a Bengal fox is between 5 to 9 pounds.", "generated": [{"finished": true, "length": 4, "score": [0.6236436475066116, 0.3931611018625923, 0.30856825067320265, 0.2653864420867501], "conf": [0.9622606878560755, 0.8532765331986423, 0.6682574456209924, 0.4954454054012902], "prob": [[8.940175294212512e-10, 23], [8.61349806723726e-18, 23], [1.452797590482158e-19, 17], [1.4329236189402357e-19, 6]], "generated": ["Bengal foxes are medium-sized canines that weigh around 15 pounds on average.", "In 2019, Ryan Crouser threw a 22 pound shot put with ease.", "Thus, Ryan Crouser could easily throw a bengal fox.", "So the answer is yes."], "conf_comments": ["(A)\nRyan Crou", "(A)\nThus, Ryan", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.6920574801963789, 0.48183629209281204, 0.3012284768110312, 0.24992105862907985], "conf": [0.9666898534912004, 0.8605559514161791, 0.6530668667180662, 0.45352712603882267], "prob": [[2.6604653749317222e-05, 15], [2.566784901651455e-10, 19], [6.286681122416814e-15, 16], [5.962517137178116e-15, 6]], "generated": ["The bengal fox weighs about 12 kg.", "In comparison, Ryan Crouser has thrown a 7.26 kg shot put.", "Thus, Ryan Crouser could lift a bengal fox.", "So the answer is yes."], "conf_comments": ["(A)\nRyan Crou", "(A)\nThus, Ryan", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6920574801963789, 0.48183629209281204, 0.2335149244005401, 0.1966958833770006, 0.16363138016120976], "conf": [0.9666898534912004, 0.8605559514161791, 0.671535270703273, 0.5125095060511711, 0.3552692395273157], "prob": [[2.6604653749317222e-05, 15], [2.566784901651455e-10, 19], [1.9367141171554294e-25, 29], [5.211615731178381e-26, 18], [5.1605283961936653e-26, 6]], "generated": ["The bengal fox weighs about 12 kg.", "In comparison, Ryan Crouser has thrown a 7.26 kg shot put.", "It is easier to throw an object that is less than half your own body weight, which is true in this case for Ryan Crouser.", "Thus, Ryan Crouser could throw a bengal fox with ease.", "So the answer is yes."], "conf_comments": ["(A)\nRyan Crou", "(A)\nThus, Ryan", "(A)\nThus, Ryan", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.63112011233338, 0.40258828676563563, 0.21556287363266569, 0.169945917016686, 0.14047321801009516], "conf": [0.9599148359980635, 0.8031730218521292, 0.5300547120041662, 0.37636062473893644, 0.25776489164162697], "prob": [[5.518682357050433e-08, 19], [7.146863036391726e-15, 22], [5.196482455506686e-25, 28], [4.1430541017090585e-26, 19], [4.083162440892538e-26, 6]], "generated": ["Bengal foxes are small weighing around 10-15 pounds.", "According to Wikipedia, Ryan Crouser’s personal best is 76 feet 8 inches.", "To throw a bengal fox with ease, he would have to throw it further than that 76 feet 8 inches.", "Thus, Ryan Crouser could not throw a bengal fox with ease.", "So the answer is no."], "conf_comments": ["(A)\nRyan Crou", "(A)\nThus, Ryan", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.63112011233338, 0.40258828676563563, 0.2143395896328784, 0.17701220600038667], "conf": [0.9599148359980635, 0.8031730218521292, 0.5789497746907821, 0.3996638386491143], "prob": [[5.518682357050433e-08, 19], [7.146863036391726e-15, 22], [8.638369992844026e-24, 22], [8.033741252848428e-24, 6]], "generated": ["Bengal foxes are small weighing around 10-15 pounds.", "According to Wikipedia, Ryan Crouser’s personal best is 76 feet 8 inches.", "Thus, Ryan Crouser could not throw with enough force to reach a bengal fox.", "So the answer is no."], "conf_comments": ["(A)\nRyan Crou", "(A)\nThus, Ryan", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 59.181804895401}
{"index": 333, "question": "Did Amy Winehouse always perform live perfectly?", "answer": "no", "explanation": "No. Amy Winehouse was known for getting intoxicated before an during sets. Amy Winehouse forgot the lyrics to her songs during her last performance.", "generated": [{"finished": true, "length": 3, "score": [0.5744194736382767, 0.4247544622788644, 0.36258265747200913], "conf": [0.9630178195531522, 0.8177097846410238, 0.5993962938810807], "prob": [[3.6073125831705415e-08, 16], [1.8340721305840531e-10, 12], [1.7699304071353982e-10, 6]], "generated": ["In an interview, Amy Winehouse stated that she suffered from stage fright.", "Thus, she didn’t always perform live perfectly.", "So the answer is no."], "conf_comments": ["(A)\nThus, Amy", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.45297884422578727, 0.3824132371658815, 0.3239389331337518], "conf": [0.9477726837397376, 0.7617690301775328, 0.5482100809777902], "prob": [[1.1078594703893605e-14, 21], [2.0585283750171165e-15, 14], [2.022913883657502e-15, 6]], "generated": ["Amy Winehouse was known for live singing mistakes and her shows were often poorly reviewed.", "Thus, Amy Winehouse did not always perform live perfectly.", "So the answer is no."], "conf_comments": ["(A)\nThus, Amy", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5577255616944788, 0.31941656913371536, 0.2684690940425868], "conf": [0.9320507765780244, 0.712106982400956, 0.5165155326295308], "prob": [[4.5210857121677656e-14, 28], [3.265850412868626e-19, 14], [2.787466886222195e-19, 6]], "generated": ["Amy Winehouse died about ten years ago (in 2011), so it is impossible for her to continue performing.", "Hence, it is also impossible for her to perform live perfectly.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6571591159742906, 0.45441538725150943, 0.16438508967794607, 0.13493336391008798], "conf": [0.9721881656525829, 0.8424534153215156, 0.6356472894875881, 0.4567584949663648], "prob": [[2.0147637227413456e-07, 19], [1.6049339087924306e-10, 12], [2.0624690670675145e-20, 13], [1.401676964194342e-20, 6]], "generated": ["Amy Winehouse had died in July 23, 2011.", "Thus, she did not perform live after her death.", "And live performances prior to her death may have been imperfect.", "So the answer is no."], "conf_comments": ["(A)\nThus, Amy", "(A)\nSo the answer is", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.6571591159742906, 0.45441538725150943, 0.17545950412333766, 0.0581764666436042, 0.03603458358490034], "conf": [0.9721881656525829, 0.8424534153215156, 0.7316945064698941, 0.5322858606889829, 0.3435670094633725], "prob": [[2.0147637227413456e-07, 19], [1.6049339087924306e-10, 12], [4.006133672221232e-28, 23], [8.589347921931485e-39, 13], [2.0854757431512285e-42, 16]], "generated": ["Amy Winehouse had died in July 23, 2011.", "Thus, she did not perform live after her death.", "The number of days that Amy Winehouse has not performed since her death is 7350 days.", "This is less than or equal to 271 years.", "Thus, she did not always perform live perfectly and the answer is no."], "conf_comments": ["(A)\nThus, Amy", "(A)\nSo the answer is", "(A)\nThus, Amy", "(A)\nThus, Amy", "(A)\n\n\n\n\n"], "info": null}, {"finished": false, "length": 5, "score": [0.6571591159742906, 0.45441538725150943, 0.17545950412333766, 0.0581764666436042, 0.03479730594747915], "conf": [0.9721881656525829, 0.8424534153215156, 0.7316945064698941, 0.5322858606889829, 0.2743929343310927], "prob": [[2.0147637227413456e-07, 19], [1.6049339087924306e-10, 12], [4.006133672221232e-28, 23], [8.589347921931485e-39, 13], [5.772069447716859e-42, 20]], "generated": ["Amy Winehouse had died in July 23, 2011.", "Thus, she did not perform live after her death.", "The number of days that Amy Winehouse has not performed since her death is 7350 days.", "This is less than or equal to 271 years.", "Thus, Amy Winehouse did not perform live for more than 271 years.", "So the answer is no."], "conf_comments": ["(A)\nThus, Amy", "(A)\nSo the answer is", "(A)\nThus, Amy", "(A)\nThus, Amy", "(A)\nSo the answer is"], "info": [[[0.0006720032184257935, 20], [0.8027773067255782, 6]], 0.5154991980736113, "(A)\nSo the answer is", 5]}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 46.96673893928528}
{"index": 334, "question": "Could Hurricane Harvey catch a Peregrine falcon?", "answer": "no", "explanation": "No. Hurricane Harvey had maximum winds of 130 MPH. The Peregrine falcon is the fastest animal on Earth. A Peregrine falcon can reach a maximum speed of 240 MPH.", "generated": [{"finished": true, "length": 4, "score": [0.7437248084168253, 0.5146840633675781, 0.4209287364267611, 0.34883963594243633], "conf": [0.9730705628905144, 0.8675085420349902, 0.6440023977031454, 0.4434390204876764], "prob": [[1.2405207920972197e-05, 20], [7.700804232764767e-12, 23], [1.1790488796928184e-12, 18], [1.1610749763696613e-12, 6]], "generated": ["Hurricane Harvey’s maximum wind speed was around 130 miles per hour.", "In comparison, the world record speed for a Peregrine falcon is 242 miles per hour.", "Thus, Hurricane Harvey could not catch a Peregrine falcon.", "So the answer is no."], "conf_comments": ["(A)\nThe Peregrine", "(A)\nThus, H", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7437248084168253, 0.51016531500689, 0.41679034823488853, 0.3445645904260523], "conf": [0.9730705628905144, 0.8551419654142144, 0.6323262788659271, 0.43322161041103396], "prob": [[1.2405207920972197e-05, 20], [3.823723596622995e-12, 24], [6.048871733268728e-13, 18], [5.960696139510936e-13, 6]], "generated": ["Hurricane Harvey’s maximum wind speed was around 130 miles per hour.", "In comparison, a Peregrine falcon’s maximum flight speed is about 170 miles per hour.", "Thus, Hurricane Harvey could not catch a Peregrine falcon.", "So the answer is no."], "conf_comments": ["(A)\nThe Peregrine", "(A)\nThus, H", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6015936214107876, 0.42189295028074414, 0.3370312524184053, 0.2822249006309512], "conf": [0.970069504711824, 0.8153169801982727, 0.5961924254022555, 0.41936320193208293], "prob": [[2.7459778144913464e-12, 27], [3.0364265521742585e-16, 17], [2.6189546900681787e-17, 18], [2.570435121201064e-17, 6]], "generated": ["Hurricane Harvey was a category 4 hurricane when maximum sustained wind reached 209 km/h.", "Peregrine falcon can fly up to 200 km/h.", "Thus, Hurricane Harvey could not catch a Peregrine falcon.", "So the answer is no."], "conf_comments": ["(A)\nThe Peregrine", "(A)\nThus, H", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7391020186128685, 0.38269497570622313, 0.2866931261358009, 0.24381939581997186], "conf": [0.9687604857809273, 0.7732575896716558, 0.5458191639508744, 0.3962937156804614], "prob": [[3.322480871943206e-05, 18], [2.2882817804130634e-20, 32], [1.8535443116640626e-22, 21], [1.811357548891707e-22, 6]], "generated": ["Peregrine falcons can fly up to about 200 miles per hour.", "Hurricane Harvey reached winds as fast as 130 miles per hour and sustained around 100 mph for several hours.", "Thus, it is impossible for Hurricane Harvey to catch a Peregrine falcon.", "So the answer is no."], "conf_comments": ["(A)\nHurricane Har", "(A)\nThus, H", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6015936214107876, 0.3581948272869949, 0.2738509063246619, 0.2376041249627447], "conf": [0.970069504711824, 0.8483396950637008, 0.659375854051846, 0.4982449679463316], "prob": [[2.7459778144913464e-12, 27], [1.5990682939539548e-20, 21], [3.0258674176082616e-23, 22], [2.9584765120212805e-23, 6]], "generated": ["Hurricane Harvey was a category 4 hurricane when maximum sustained wind reached 209 km/h.", "The top running speed of a Peregrine falcon is 389 km/h. ", "Thus, it was not possible for Hurricane Harvey to catch a Peregrine falcon.", "So the answer is no."], "conf_comments": ["(A)\nThe Peregrine", "(A)\nThus, H", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7391020186128685, 0.38269497570622313, 0.2866931261358009, 0.17216689001155777], "conf": [0.9687604857809273, 0.7732575896716558, 0.5458191639508744, 0.39568985793014094], "prob": [[3.322480871943206e-05, 18], [2.2882817804130634e-20, 32], [1.8535443116640626e-22, 21], [6.951387992668161e-25, 8]], "generated": ["Peregrine falcons can fly up to about 200 miles per hour.", "Hurricane Harvey reached winds as fast as 130 miles per hour and sustained around 100 mph for several hours.", "Thus, it is impossible for Hurricane Harvey to catch a Peregrine falcon.", "Thus, the answer is no."], "conf_comments": ["(A)\nHurricane Har", "(A)\nThus, H", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 53.73267388343811}
{"index": 335, "question": "Do Chinese Americans face discrimination at a Federal level in the US?", "answer": "yes", "explanation": "Yes. The President of the United States frequently referred to the COVID-19 pandemic as a 'Chinese Virus' and 'Kung Flu', encouraging the use of derogatory language towards Chinese Americans. The President of the United States has not called for the violence and hate towards Chinese Americans in response to COVID-19 to end.", "generated": [{"finished": true, "length": 5, "score": [0.564702273965548, 0.25739220030799675, 0.14151033318642156, 0.09244880106480037, 0.07705424067491427], "conf": [0.9458752316763986, 0.8143070849252185, 0.7070863237824883, 0.4861725982906573, 0.33948089538314746], "prob": [[6.3952484012482034e-06, 11], [1.8094337236563382e-32, 43], [4.726676310183941e-37, 10], [5.960338645321185e-40, 14], [5.779162516893538e-40, 6]], "generated": ["Chinese Americans are a subgroup of the Asian Americans.", "According to US Census Bureau, there are 38,443,897 individuals who are Asian Americans and 17,671,248 individuals who are Chinese Americans.", "This indicates that Chinese Americans are a minority.", "Thus, Chinese Americans face discrimination at a Federal level.", "So the answer is yes."], "conf_comments": ["(A)\nAsian Americans are", "(A)\nThus, Chinese", "(A)\nThus, Chinese", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.49756068156242694, 0.2727760444799796, 0.1756749420974313, 0.07789921311086084, 0.05914289533547058], "conf": [0.9064032510491274, 0.7923934001242365, 0.6848682951356113, 0.3638177099360954, 0.21179015608807158], "prob": [[3.368916596445318e-18, 31], [1.2028676825948087e-31, 29], [6.164290671507255e-40, 26], [3.693992999544961e-51, 26], [3.481758068282595e-51, 6]], "generated": ["According to a Pew Research article from 2017, Chinese Americans are one of the most successful ethnic in groups in the US.", "For example, the median earnings of a Chinese American household in 2015 was over 74,000 dollars.", "The 2015 median earnings for all households in the US is around 56,000 dollars.", "Thus, Chinese Americans do not experience discrimination at a federal level because otherwise they would not be so economically successful.", "So the answer is no."], "conf_comments": ["(A)\nThus, Chinese", "(A)\nIn comparison, the", "(A)\nThus, Chinese", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 8, "score": [0.564702273965548, 0.25739220030799675, 0.14151033318642156, 0.06713267439117894, 0.0412147005941476, 0.015844055732662687, 0.012033500376870222, 0.010098505930627662], "conf": [0.9458752316763986, 0.8143070849252185, 0.7070863237824883, 0.6028357743559644, 0.4900605053389663, 0.37356054849815856, 0.2753508303535766, 0.19470364030171153], "prob": [[6.3952484012482034e-06, 11], [1.8094337236563382e-32, 43], [4.726676310183941e-37, 10], [7.942206496204119e-54, 29], [1.679184130634289e-59, 17], [1.7796641389858136e-69, 14], [5.7504237658958544e-71, 14], [5.612489636960336e-71, 6]], "generated": ["Chinese Americans are a subgroup of the Asian Americans.", "According to US Census Bureau, there are 38,443,897 individuals who are Asian Americans and 17,671,248 individuals who are Chinese Americans.", "This indicates that Chinese Americans are a minority.", "According to US Census Bureau, 46.32% of Americans are white and 1.46% are Asian Americans.", "This indicates that non-white Americans, including Chinese Americans, are a minority.", "With a lack of majority representation, there could be discrimination.", "Thus, Chinese Americans face discrimination at a Federal level.", "So the answer is yes."], "conf_comments": ["(A)\nAsian Americans are", "(A)\nThus, Chinese", "(A)\nThus, Chinese", "(A)\nThis indicates that Chinese", "(A)\nThus, Chinese", "(A)\nThus, Chinese", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 6, "score": [0.49756068156242694, 0.2727760444799796, 0.1756749420974313, 0.07013352654397352, 0.03960554961598389, 0.0315734741066034], "conf": [0.9064032510491274, 0.7923934001242365, 0.6848682951356113, 0.45090341121192745, 0.2673184077364194, 0.1714591870821511], "prob": [[3.368916596445318e-18, 31], [1.2028676825948087e-31, 29], [6.164290671507255e-40, 26], [4.260255839145957e-46, 10], [2.0935431547510685e-50, 16], [1.981009086513827e-50, 6]], "generated": ["According to a Pew Research article from 2017, Chinese Americans are one of the most successful ethnic in groups in the US.", "For example, the median earnings of a Chinese American household in 2015 was over 74,000 dollars.", "The 2015 median earnings for all households in the US is around 56,000 dollars.", "Therefore, Chinese Americans are doing well overall.", "Thus, there is no evidence of discrimination at the Federal level.", "So the answer is no."], "conf_comments": ["(A)\nThus, Chinese", "(A)\nIn comparison, the", "(A)\nThus, Chinese", "(A)\nHowever, the article", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 2, "score": [0.564702273965548, 0.2971206984470264], "conf": [0.9458752316763986, 0.8350476170736897], "prob": [[6.3952484012482034e-06, 11], [4.200733262752665e-31, 50]], "generated": ["Chinese Americans are a subgroup of the Asian Americans.", "In 2013, it was reported that 48% of Asian Americans faced discrimination in hiring, promotions in the workplace, and wages, which were highest among all ethnic or racial groups."], "conf_comments": ["(A)\nAsian Americans are", "(A)\nThus, it"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": [], "6": [], "7": [], "8": []}, "run_time": 71.06654858589172}
{"index": 336, "question": "Can you write a whole Haiku in a single tweet?", "answer": "yes", "explanation": "Yes. A Haiku is a Japanese poetry in three phrases. The average Haiku is composed of 60 to 70 characters. A tweet is a short message sent on Twitter. The character limit of a single tweet on Twitter is 140.", "generated": [{"finished": true, "length": 3, "score": [0.5706684008738964, 0.37399419885778507, 0.31265319756360455], "conf": [0.8229063705162732, 0.5739042401637718, 0.4151201238792539], "prob": [[3.2138096457443776e-15, 36], [2.5219660133615767e-21, 29], [2.051678054489867e-21, 6]], "generated": ["According to Wikipedia, a tweet can be as many as 140 characters and a Haiku is a three lined poem of 17 syllables.", "Since you cannot fit 17 syllables in a single tweet, you cannot write a whole Haiku in a single tweet.", "So the answer is no."], "conf_comments": ["(A)\nThus, it", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6605025135166394, 0.42009027848300445, 0.26292510826026944, 0.20728950870087454], "conf": [0.9694430964461676, 0.769482282674779, 0.614506340750333, 0.42702772378124954], "prob": [[1.2730710979153366e-06, 17], [1.1306630446515846e-20, 48], [1.498664445246381e-26, 19], [7.67483974061802e-27, 6]], "generated": ["A tweet is a message that has a limit of 140 characters.", "In comparison, a Haiku is a poem that has 17 syllables arranged in 3 lines consisting of 5 (first line), 7 (second line), and 5 (third line) syllables respectively.", "Thus, 17 syllables is much longer than 140 characters.", "So the answer is no."], "conf_comments": ["(A)\nA Haiku is", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.8415676623687299, 0.489074922403728, 0.2636570123965287, 0.1858587986765009], "conf": [0.9535585414707549, 0.72748415271953, 0.47365697927339573, 0.2988738452651758], "prob": [[1.5395092186297184e-06, 45], [1.2862132648220736e-13, 20], [3.980492288546751e-24, 30], [9.495515668726708e-25, 6]], "generated": ["A Haiku is a Japanese poem consisting of 3 lines, where the first line has 5 syllables, the second line has 7 syllables, and the third line has 5 syllables.", "Due to the nature of Twitter, a tweet is limited to 280 characters.", "Thus, in order to write a Haiku in a single tweet, a tweet has to be limited to 140 characters.", "So the answer is yes."], "conf_comments": ["(A)\nA tweet is", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.6605025135166394, 0.4541262721673368, 0.2657808667712397], "conf": [0.9694430964461676, 0.8073139183753202, 0.5909755575106381], "prob": [[1.2730710979153366e-06, 17], [1.7860081178478087e-15, 36], [5.757554636528099e-32, 50]], "generated": ["A tweet is a message that has a limit of 140 characters.", "In English, a Haiku poem contains five syllables in the first line, seven syllables in the second line, and five syllables in the third line.", "Thus, since a tweet is a maximum of 140 characters, and since there are limits on the number of syllables in a Haiku, there is no way to write a whole Haiku in a single tweet."], "conf_comments": ["(A)\nA Haiku is", "(A)\nThus, it", "(A)\nSo the answer is"], "info": null}, {"finished": true, "length": 3, "score": [0.8415676623687299, 0.489074922403728, 0.24473597693045276], "conf": [0.9535585414707549, 0.72748415271953, 0.5778379367728842], "prob": [[1.5395092186297184e-06, 45], [1.2862132648220736e-13, 20], [3.49757594267832e-38, 49]], "generated": ["A Haiku is a Japanese poem consisting of 3 lines, where the first line has 5 syllables, the second line has 7 syllables, and the third line has 5 syllables.", "Due to the nature of Twitter, a tweet is limited to 280 characters.", "According to this [link](http://www.japanesemama.com/2016/04/how-to-write-a-haiku/), the following is the basic structure of a Haiku."], "conf_comments": ["(A)\nA tweet is", "(A)\nThus, it", "(A)\nThus, it"], "info": null}, {"finished": false, "length": 4, "score": [0.8415676623687299, 0.489074922403728, 0.2636570123965287, 0.07493644080908125], "conf": [0.9535585414707549, 0.72748415271953, 0.47365697927339573, 0.29754303589823766], "prob": [[1.5395092186297184e-06, 45], [1.2862132648220736e-13, 20], [3.980492288546751e-24, 30], [2.7523201641879917e-47, 26]], "generated": ["A Haiku is a Japanese poem consisting of 3 lines, where the first line has 5 syllables, the second line has 7 syllables, and the third line has 5 syllables.", "Due to the nature of Twitter, a tweet is limited to 280 characters.", "Thus, in order to write a Haiku in a single tweet, a tweet has to be limited to 140 characters.", "A Haiku in a single tweet, however, can be difficult to understand in text and therefore it might not be effective.", "Thus, Haikus should not be written in a single tweet.", "So the answer is no."], "conf_comments": ["(A)\nA tweet is", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": [[[6.914522035647114e-24, 26], [6.142415623188552e-05, 15], [0.9284343050077588, 6]], 0.6281825222013571, "(A)\nSo the answer is", 4]}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 46.32676577568054}
{"index": 337, "question": "Can an adult human skull hypothetically pass through the birth canal?", "answer": "yes", "explanation": "Yes. The largest baby ever born was 22 pounds.  The average human skull weighs between 10 and 11 pounds.", "generated": [{"finished": true, "length": 4, "score": [0.5758210687811196, 0.33654667218017636, 0.25374512911817143, 0.21707436993879045], "conf": [0.9269961139163386, 0.76489423060281, 0.5648886000329552, 0.4151693382613537], "prob": [[3.2843285201520246e-09, 19], [1.2310372189024439e-17, 22], [1.439159810659948e-19, 17], [1.4030213248889244e-19, 6]], "generated": ["The skull is one of the largest bones in the human body and it is solid.", "In comparison, the birth canal is a muscular tube that allows babies to pass through it.", "Thus, it is impossible for a skull to pass through the birth canal.", "So the answer is no."], "conf_comments": ["(A)\nThe birth canal is", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5758210687811196, 0.33654667218017636, 0.25258153945147, 0.2165622067306088], "conf": [0.9269961139163386, 0.76489423060281, 0.5733295160320251, 0.4231077102342439], "prob": [[3.2843285201520246e-09, 19], [1.2310372189024439e-17, 22], [7.188622828199137e-20, 18], [7.023257825313294e-20, 6]], "generated": ["The skull is one of the largest bones in the human body and it is solid.", "In comparison, the birth canal is a muscular tube that allows babies to pass through it.", "Thus, it is impossible for an adult skull to pass through the birth canal.", "So the answer is no."], "conf_comments": ["(A)\nThe birth canal is", "(A)\nThus, it", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 3, "score": [0.5323221156264101, 0.3678453562119042, 0.3172268885265415], "conf": [0.8890845615062417, 0.6927870730092983, 0.5185969875511408], "prob": [[3.9080548720755026e-14, 27], [4.1163118822583103e-17, 14], [3.958982228368303e-17, 6]], "generated": ["The maximum width of the birth canal is around 2 cm and the diameter of the human skull is approximately 20 cm.", "Thus, the skull would not pass through the birth canal.", "So the answer is no."], "conf_comments": ["(A)\nThus, the", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5067027447345412, 0.23450597498392486, 0.1945273404083479, 0.1684266724710391], "conf": [0.8467894591946468, 0.6923090876649953, 0.5304784214818432, 0.39859720921144615], "prob": [[1.1047824276582917e-13, 25], [1.4255453534544845e-23, 17], [2.839669408271256e-24, 15], [2.800471968170166e-24, 6]], "generated": ["While the skull is tough, it is the only part of the body that cannot pass through the birth canal.", "This is why a C-Section is required for babies with large heads.", "Thus, an adult human skull cannot pass through the birth canal.", "So the answer is no."], "conf_comments": ["(A)\nThe skull is", "(A)\nThus, an", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5758210687811196, 0.21441549910956728, 0.11250386824682977, 0.09466113100919292], "conf": [0.9269961139163386, 0.6990719086740088, 0.5313586781208207, 0.3947218017517835], "prob": [[3.2843285201520246e-09, 19], [6.401957771324245e-24, 20], [2.0350522391194085e-31, 17], [1.524777059015865e-31, 6]], "generated": ["The skull is one of the largest bones in the human body and it is solid.", "The pelvis consists of the pelvic cavity which an adult human skull cannot fit into.", "This indicates that it is impossible for the skull to pass through a birth canal.", "So the answer is no."], "conf_comments": ["(A)\nThe birth canal is", "(A)\nThus, an", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5758210687811196, 0.21441549910956728, 0.11250386824682977, 0.08417389682723031], "conf": [0.9269961139163386, 0.6990719086740088, 0.5313586781208207, 0.4199103838976132], "prob": [[3.2843285201520246e-09, 19], [6.401957771324245e-24, 20], [2.0350522391194085e-31, 17], [1.2899845827405802e-32, 8]], "generated": ["The skull is one of the largest bones in the human body and it is solid.", "The pelvis consists of the pelvic cavity which an adult human skull cannot fit into.", "This indicates that it is impossible for the skull to pass through a birth canal.", "Thus, the answer is no."], "conf_comments": ["(A)\nThe birth canal is", "(A)\nThus, an", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 48.41693830490112}
{"index": 338, "question": "Was King Kong (2005 film) the lead actress's biggest box office role?", "answer": "yes", "explanation": "Yes. Naomi Watts starred in King Kong (2005 film). Naomi Watts has starred in several movies such as Divergent and the RIng. Divergent: Insurgent made 295 million at the box office worldwide . The Ring made 248 million at the box office worldwide . King Kong (2005 film) made 550 million at the box office worldwide.", "generated": [{"finished": true, "length": 5, "score": [0.7942951828162553, 0.4783944060291092, 0.3151761606558788, 0.26888090947460447, 0.2406398172294602], "conf": [0.9742334393490272, 0.8321514101084728, 0.6401169682350135, 0.54170417686778, 0.4355794647538651], "prob": [[7.0569681202489e-05, 22], [1.5080381568185705e-14, 26], [3.0622227728932926e-24, 39], [1.744752475396471e-25, 19], [1.7044834738377346e-25, 6]], "generated": ["King Kong (2005 film) has a worldwide gross of $550 million.", "According to the box office of King Kong (2005 film), the lead actress was Naomi Watts.", "Naomi Watts also starred in King Kong (2005 film) and the highest grossing film she has starred in is King Kong (2005 film).", "Thus, King Kong (2005 film) was her biggest box office role.", "So the answer is yes."], "conf_comments": ["(A)\nThe lead actress'", "(A)\nThus, King", "(A)\nThus, King", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.5979285762634442, 0.48666047215719954, 0.41344600046855307], "conf": [0.9462917648590681, 0.7456022377041692, 0.5406762868592616], "prob": [[1.7233637045398407e-07, 16], [3.1902064500407643e-09, 23], [3.1013496584064754e-09, 6]], "generated": ["The highest grossing film starring Naomi Watts is King Kong.", "Thus, King Kong (2005 film) was the lead actress's biggest box office role.", "So the answer is yes."], "conf_comments": ["(A)\nThe highest grossing", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7942951828162553, 0.48914412694186343, 0.3347779065431639, 0.286538206474547], "conf": [0.9742334393490272, 0.8424908039748601, 0.6759604891412782, 0.4978538359502409], "prob": [[7.0569681202489e-05, 22], [1.523441742864048e-14, 27], [2.775657274206649e-18, 16], [2.6877665848178568e-18, 6]], "generated": ["King Kong (2005 film) has a worldwide gross of $550 million.", "In comparison, the lead actress' biggest box office role was Troy, with a worldwide gross of $360 million.", "Thus, King Kong (2005 film) was her biggest.", "So the answer is yes."], "conf_comments": ["(A)\nThe lead actress'", "(A)\nThus, King", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 5, "score": [0.7282390356571746, 0.4013601023102848, 0.31884471958302923, 0.2478839741913555, 0.19516326485222685], "conf": [0.8748472028130428, 0.7373035615960012, 0.6277926140512317, 0.50452331204426, 0.3210605679182232], "prob": [[6.703755332665969e-08, 33], [2.611537982683272e-20, 28], [4.844180809476952e-23, 21], [1.5867233896066413e-24, 12], [1.3553839151610686e-24, 6]], "generated": ["The lead actress of King Kong (2005 film) is Naomi Watts, who was also in King Kong (2005 film).", "In 2002, she starred in The Ring, which made $249M globally in the box office.", "In comparison, King Kong (2005 film) made $550M worldwide.", "Thus, The Ring was her biggest box office role.", "So the answer is no."], "conf_comments": ["(A)\nThe lead actress of", "(A)\nIn 20", "(A)\nThus, King", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.7942951828162553, 0.48914412694186343, 0.3253194671680673, 0.26870146147500445], "conf": [0.9742334393490272, 0.8424908039748601, 0.6535007010373344, 0.448124750935592], "prob": [[7.0569681202489e-05, 22], [1.523441742864048e-14, 27], [3.735364794397922e-20, 23], [3.6218919785876146e-20, 6]], "generated": ["King Kong (2005 film) has a worldwide gross of $550 million.", "In comparison, the lead actress' biggest box office role was Troy, with a worldwide gross of $360 million.", "Thus, King Kong (2005) was indeed the lead actress’s biggest box office role.", "So the answer is yes."], "conf_comments": ["(A)\nThe lead actress'", "(A)\nThus, King", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.7282390356571746, 0.4013601023102848, 0.31884471958302923, 0.2478839741913555, 0.12176195014862015], "conf": [0.8748472028130428, 0.7373035615960012, 0.6277926140512317, 0.50452331204426, 0.3301413648851769], "prob": [[6.703755332665969e-08, 33], [2.611537982683272e-20, 28], [4.844180809476952e-23, 21], [1.5867233896066413e-24, 12], [1.4704637435676073e-27, 7]], "generated": ["The lead actress of King Kong (2005 film) is Naomi Watts, who was also in King Kong (2005 film).", "In 2002, she starred in The Ring, which made $249M globally in the box office.", "In comparison, King Kong (2005 film) made $550M worldwide.", "Thus, The Ring was her biggest box office role.", "So, the answer is no."], "conf_comments": ["(A)\nThe lead actress of", "(A)\nIn 20", "(A)\nThus, King", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 54.12854027748108}
{"index": 339, "question": "Were mollusks an ingredient in the color purple?", "answer": "yes", "explanation": "Yes. Mollusks are a group of invertebrate animals including snails and slugs. Purple dye was used in the early 15th century BC to color clothing. The purple dye used in ancient Tyre was made from a liquid extracted from a gland in snails.", "generated": [{"finished": true, "length": 5, "score": [0.5061215685491336, 0.31755145190159006, 0.19368069321379142, 0.17055294875833926, 0.1486507580688164], "conf": [0.9676348090405064, 0.891960140455112, 0.7585979735966358, 0.6290060890500121, 0.4787682800206794], "prob": [[7.564188806207207e-13, 21], [1.1886705928226052e-17, 13], [4.0819677722546615e-24, 18], [1.222128710085226e-24, 18], [1.207782071289837e-24, 6]], "generated": ["The compound indigo was a primary ingredient in the first synthetic purple dye.", "Indigo comes from the bark of the indigo plant.", "In turn, the bark of the indigo plant comes from a mollusk.", "Thus, mollusks were an ingredient in the color purple.", "So the answer is yes."], "conf_comments": ["(A)\nIndigo is a", "(A)\nThe indigo plant", "(A)\nThus, m", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.6532893067027911, 0.29481142406255006, 0.24155594106688252, 0.19306171412970696], "conf": [0.9496593621472503, 0.7149654406945132, 0.5129748856363024, 0.328406982918908], "prob": [[6.78764218865479e-05, 12], [4.079479386459482e-15, 18], [1.1539433516627511e-15, 19], [1.138749041456008e-15, 6]], "generated": ["The color purple is made by combining red and blue.", "Mollusks, such as sea shells, are not a source of colors.", "Thus, mollusks were not an ingredient in the color purple.", "So the answer is no."], "conf_comments": ["(A)\nMollusks", "(A)\nThus, m", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 5, "score": [0.5061215685491336, 0.31755145190159006, 0.19368069321379142, 0.14523423233786487, 0.127027766030847], "conf": [0.9676348090405064, 0.891960140455112, 0.7585979735966358, 0.6277340375806464, 0.4812330994850771], "prob": [[7.564188806207207e-13, 21], [1.1886705928226052e-17, 13], [4.0819677722546615e-24, 18], [5.731814236477915e-27, 17], [5.659384926606645e-27, 6]], "generated": ["The compound indigo was a primary ingredient in the first synthetic purple dye.", "Indigo comes from the bark of the indigo plant.", "In turn, the bark of the indigo plant comes from a mollusk.", "Thus, mollusks were an ingredient in color purple.", "So the answer is yes."], "conf_comments": ["(A)\nIndigo is a", "(A)\nThe indigo plant", "(A)\nThus, m", "(A)\n# Is the above", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5061215685491336, 0.2982787596946377, 0.23052455355773643, 0.18982978416675497], "conf": [0.9676348090405064, 0.7774089534350357, 0.5766342639392029, 0.39180262874275157], "prob": [[7.564188806207207e-13, 21], [1.3736884044196388e-21, 24], [2.7847544725092087e-23, 18], [2.751385714941912e-23, 6]], "generated": ["The compound indigo was a primary ingredient in the first synthetic purple dye.", "Indigo is derived from plants of the genus indigofera, most of which are not mollusks.", "Thus, mollusks were not ingredients in the color purple.", "So the answer is no."], "conf_comments": ["(A)\nIndigo is a", "(A)\nThus, m", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.5061215685491336, 0.2982787596946377, 0.19654035391494076, 0.16409481246980057], "conf": [0.9676348090405064, 0.7774089534350357, 0.5942306508931506, 0.4153003635460206], "prob": [[7.564188806207207e-13, 21], [1.3736884044196388e-21, 24], [9.15997565829985e-26, 17], [9.019203075218234e-26, 6]], "generated": ["The compound indigo was a primary ingredient in the first synthetic purple dye.", "Indigo is derived from plants of the genus indigofera, most of which are not mollusks.", "Thus, mollusks were not a source of the color purple.", "So the answer is no."], "conf_comments": ["(A)\nIndigo is a", "(A)\nThus, m", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": [], "5": []}, "run_time": 59.48944592475891}
{"index": 340, "question": "Can an anchovy born in 2020 survive 25th US census?", "answer": "no", "explanation": "No. The US Census takes place every ten years. The 24th US Census took place in 2020. The 25th US Census will take place in 2030. The average lifespan of an anchovy is five years.", "generated": [{"finished": true, "length": 4, "score": [0.8012307425169654, 0.5311060869153997, 0.41634984655539375, 0.3523511753489032], "conf": [0.9766174500456801, 0.8763435441968171, 0.6490753712273107, 0.46669273233410474], "prob": [[0.0012152109193243415, 16], [2.3089839148314854e-07, 12], [1.2403935790250799e-09, 28], [1.2115793205259385e-09, 6]], "generated": ["The 25th US census was held in 1930.", "Anchovies can live up to 9 years.", "Thus, it was impossible for an anchovy born in 2020 to survive the 25th US census.", "So the answer is no."], "conf_comments": ["(A)\nAnchovies", "(A)\nThus, an", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8498012474384113, 0.5407036102415234, 0.37963887745927777, 0.3278777025544704], "conf": [0.9759025118476241, 0.848557951956317, 0.6502144460097282, 0.4867078007485627], "prob": [[0.008084491025428187, 16], [3.979410224523484e-09, 19], [6.471599184209451e-14, 25], [6.336302275220642e-14, 6]], "generated": ["The 25th US census will happen in 2030.", "According to the US census, anchovies can live for up to 5 years.", "Thus, it is impossible for an anchovy born in 2020 to survive the next US census.", "So the answer is no."], "conf_comments": ["(A)\nAnchovies", "(A)\nThus, an", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 3, "score": [0.7679054373285896, 0.48325574703463553, 0.41067855531624275], "conf": [0.9674687763935169, 0.7606586921786426, 0.5560065688457566], "prob": [[3.0517509067739307e-05, 21], [4.5360414406949044e-15, 33], [4.219261569393916e-15, 6]], "generated": ["The 25th US census occurred from 1950 to 1952.", "Thus, when an anchovy was born in 2020, it was impossible for the fish to survive the 25th US census.", "So the answer is no."], "conf_comments": ["(A)\nAnchovies", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.8383427001674248, 0.397645082827256, 0.2916004491461344, 0.24759996523604913], "conf": [0.9751712411458358, 0.7047568566385154, 0.5437700043556347, 0.39444474428217235], "prob": [[0.007352488228284779, 15], [4.051729324663635e-22, 38], [3.3968768661692834e-26, 26], [3.274936884429232e-26, 6]], "generated": ["The 25th US census happened in 1930.", "This is 138 after 2020, and the average lifespan for an anchovy is 18 months, which is shorter than 138.", "Thus, it is impossible for an anchovy born in 2020 to live until 2030.", "So the answer is no."], "conf_comments": ["(A)\nAnchovies", "(A)\nThus, an", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}, {"finished": true, "length": 4, "score": [0.7679054373285896, 0.4323734203154432, 0.29861316481678635, 0.23964889911125542], "conf": [0.9674687763935169, 0.7932766999406513, 0.5458369217903813, 0.35340682894551834], "prob": [[3.0517509067739307e-05, 21], [1.3171669920991282e-10, 13], [6.651308617989971e-15, 27], [6.445146391302358e-15, 6]], "generated": ["The 25th US census occurred from 1950 to 1952.", "The life expectancy of an anchovy is approximately one year.", "Thus, an anchovy born in 2020 should be able to survive the 25th US census.", "So the answer is yes."], "conf_comments": ["(A)\nAnchovies", "(A)\nThus, an", "(A)\nSo the answer is", "(A)\n# Is the above"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 51.513277530670166}
{"index": 341, "question": "When en route from China to France, must pilots know their altitude in the imperial foot?", "answer": "yes", "explanation": "Yes. Most international airports and aviators use the foot to measure altitude . China and North Korea require pilots to use meters for altitude. Pilots must communicate their altitude with local air traffic control .", "generated": [{"finished": true, "length": 4, "score": [0.5009971816580128, 0.3151734859909039, 0.233829950254169, 0.1818765341697593], "conf": [0.9574413752800456, 0.8347862537039084, 0.624757919765009, 0.38060136789102866], "prob": [[1.3046450213051e-10, 17], [4.529458626801053e-15, 13], [3.31953599102062e-17, 16], [3.184522511681159e-17, 6]], "generated": ["The standard unit of length for international purposes is the meter, not the imperial foot.", "In comparison, the standard unit for altitude is the meter.", "Thus, pilots must not know their altitude in the imperial foot.", "So the answer is no."], "conf_comments": ["(A)\nThus, pil", "(A)\nThus, pil", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.576534574650501, 0.2812680548127517, 0.22086351947306418, 0.17603605871348513], "conf": [0.9523901365941597, 0.666617886630166, 0.4892222361569649, 0.3125557454241231], "prob": [[5.902620318645629e-09, 18], [1.6309374211506859e-15, 14], [7.099701062077337e-17, 18], [6.861894738383939e-17, 6]], "generated": ["The Imperial Foot is an obsolete unit of length in the pre-metric system.", "The altitude above sea level is measured in meters, not feet.", "Thus, pilots do not need to know their altitude in the Imperial Foot.", "So the answer is no."], "conf_comments": ["(A)\nThe Imperial Foot is", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.576534574650501, 0.2812680548127517, 0.20519267924960316, 0.16251677051300012], "conf": [0.9523901365941597, 0.666617886630166, 0.4895868709471713, 0.30914436604596873], "prob": [[5.902620318645629e-09, 18], [1.6309374211506859e-15, 14], [7.170671107080911e-19, 24], [6.892967339859893e-19, 6]], "generated": ["The Imperial Foot is an obsolete unit of length in the pre-metric system.", "The altitude above sea level is measured in meters, not feet.", "Thus, when en route from China to France, pilots must not know their altitude in the Imperial Foot.", "So the answer is no."], "conf_comments": ["(A)\nThe Imperial Foot is", "(A)\nThus, the", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5347300750749029, 0.29572307221604294, 0.18368728447716187, 0.15065038519325474], "conf": [0.9682748509475338, 0.8142954108662944, 0.626718049191143, 0.4243329379754811], "prob": [[5.0431942910737495e-06, 10], [1.5432937300867558e-09, 8], [2.45476235011228e-14, 16], [2.3599132201000017e-14, 6]], "generated": ["The international aviation standard uses the metric system.", "A foot is not a metric unit.", "Thus, a plane must not know its altitude in the imperial foot.", "So the answer is no."], "conf_comments": ["(A)\nThe imperial foot is", "(A)\nThus, pil", "(A)\nSo the answer is", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5009971816580128, 0.3151734859909039, 0.18338444067081555, 0.14446246632931198], "conf": [0.9574413752800456, 0.8347862537039084, 0.6468488699449433, 0.41244058517891263], "prob": [[1.3046450213051e-10, 17], [4.529458626801053e-15, 13], [1.5247962710631557e-21, 18], [1.2958981751820443e-21, 6]], "generated": ["The standard unit of length for international purposes is the meter, not the imperial foot.", "In comparison, the standard unit for altitude is the meter.", "Therefore, a pilot need not know his/her altitude in the imperial foot.", "So the answer is no."], "conf_comments": ["(A)\nThus, pil", "(A)\nThus, pil", "(A)\n# Is the above", "(A)\n\n\n\n\n"], "info": null}, {"finished": true, "length": 4, "score": [0.5347300750749029, 0.29572307221604294, 0.18368728447716187, 0.06511320244284055], "conf": [0.9682748509475338, 0.8142954108662944, 0.626718049191143, 0.4110527922603096], "prob": [[5.0431942910737495e-06, 10], [1.5432937300867558e-09, 8], [2.45476235011228e-14, 16], [1.6351390343616557e-21, 10]], "generated": ["The international aviation standard uses the metric system.", "A foot is not a metric unit.", "Thus, a plane must not know its altitude in the imperial foot.", "If it did, the answer would be no."], "conf_comments": ["(A)\nThe imperial foot is", "(A)\nThus, pil", "(A)\nSo the answer is", "(A)\nSo the answer is"], "info": null}], "all_generated": {"1": [], "2": [], "3": [], "4": []}, "run_time": 54.71657705307007}
